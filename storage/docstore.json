{"docstore/metadata": {"306c842f-5201-4c69-bde4-51e163f6a8d3": {"doc_hash": "93fc36583d7c3243aadb2488d10baeccfcae79f1bd91f1421e8ca9b81aff0cc9"}, "1c4638a3-09fe-4150-82da-08358b86148d": {"doc_hash": "ff0a6ac8bc4fbab66a851955bfd48a81791bcea9d2eb287d8579ae5a81747470"}, "4d845450-949d-4be6-a703-0fb85bddcc88": {"doc_hash": "a9da393206056fa1527403a47b7d01a8c105ae91a55768bbf99bdbca32d5d754"}, "4858a74b-9d67-419d-875c-ebc25cb4769d": {"doc_hash": "db9b32fe5548bbd1c25fa4b2ea275549f38bda73d1c2d818aa8e4bbaac185ad3"}, "99baeb5b-9dde-474b-9e5b-5491b3377a67": {"doc_hash": "3058b7a501be5d2f0c866a69d01275bf3b02089df7d133a6180cb2a5d96013d3"}, "4626d870-ea0c-443c-a936-f1eaa3f26a07": {"doc_hash": "8e25e4cd9da96eb5c940387e0bc87a6738f3da21c3a1075a36b0efb1c0ed44eb"}, "91e7a275-998f-4f10-9179-50adf7b60c7f": {"doc_hash": "0d328aef75b6c83db339a414ac1076c8f630559c31d43d20e94be027340d43bd"}, "8a6ce8f6-7443-4759-892a-29effe63279e": {"doc_hash": "9460b32339598bdc118b1ef51ec20b280614b846536f96adb08aa3da2fc46737"}, "4542d1f1-69f1-4c7b-9b9a-3ff526452567": {"doc_hash": "ecddab5e803b68b0a75460dd6478254f7457fc41dfeca66eb2446de688ab0e7c"}, "27f4fe09-6d1a-43b7-80d6-1141e649d833": {"doc_hash": "d8339f84dc77eaebc4b12d6ee160b5cf259bb9d09cb262a4f90da3f3c767a7ac"}, "f8770bec-e174-49fe-b9f3-2e84fdcd9c29": {"doc_hash": "55ced52a93176b7b321d8933de39244427dde9319e049eebed76100a71797bed"}, "38c8a6e0-b8eb-4c17-b531-cf82f11fea38": {"doc_hash": "54ea74847895bcc37dbafc88b8f73fe45270b7ca8c1ff4b9aad132430c157be9"}, "f4452e4d-eadb-49ff-9224-5d250d10655b": {"doc_hash": "530196e30b5a8f21be09a2aa85a46be0baf692df4d7bf6cee7b9e4f909fc4229"}, "231bed0b-b454-4b9b-9e4d-dc51752a53b7": {"doc_hash": "4e850983d59ec4e4617960ad5b4a7d564d31e05d1688259e073abc54139d6235"}, "4141474b-5fb3-40f8-aa98-3e6384c57222": {"doc_hash": "d39296b145cab687e2d52289d3415260d0070541feba3ac626010192c2a0fe8c"}, "9b7f80e0-10cd-437f-a08e-c3b127dd1538": {"doc_hash": "f1299110e32567d6b3333954f7a7cab26b1a3b659c1c424483e8b92c80bb5624"}, "06d088b8-5eda-4b9e-ac62-da838d762942": {"doc_hash": "b49e2b0fea5cd826b611f70a168ae1b259694c128bc9c8e28f78b176b02a9090"}, "138366aa-9edc-4d4c-b373-19046c971eba": {"doc_hash": "54f50a9ae46fc7d9c217a8843aee3947aff973865ed66612c363011f2250c39c"}, "8b4cc148-ac40-4b06-9f69-f4df8a60dd9d": {"doc_hash": "c6c5809e67a4d30130c434676ef99f7e8b0303c8294201ffa77799883962d7b5"}, "39624053-dde6-4ea9-852c-443830e4ecf8": {"doc_hash": "917b3fdabcb5e088fefd82b0c258f5a28fd7079f3c1707b6d8b5965e4312490d"}, "2294b0a3-849d-45fb-a53b-2f76c832eee7": {"doc_hash": "a6a5469cc8802a2e22573ce5537684ff6e1830be625574f577e9ea01f21190f7"}, "5f6bde8d-81bb-4b6c-91b2-e8d863479e7d": {"doc_hash": "a856bf0eb6f4a37ea8ad18aa4722d93bf40ada1f629fac01c5ee92fee1d44512"}, "e04f264f-2e84-4943-a290-1fa8f433aeab": {"doc_hash": "7d8419fc837791ec023b3e7439830e7f89cf683e876cda46bcd5a8a46ab9222a"}, "c439521c-3007-4df2-8c24-b6059ff2cd3c": {"doc_hash": "2155f7c72605feaf249ae7edbb06a393298aee6b71b7d26d16c74078f124801b"}, "bed7d6f8-9979-401e-99db-b997782f4e41": {"doc_hash": "29eb890857abb87299f85fcddcb556da8dbd96499ed6e2fce08d74e386b88a81"}, "e6a668b1-4ef3-41cc-8a07-4ba5588f5fe4": {"doc_hash": "7230b41acb70d0523813ab346946c94f509144dca8f13749967ccebb346143d7"}, "b77532fb-90c3-4db7-9740-9e258c503842": {"doc_hash": "e59c31318e583a2f0bbf4afa8b32879cc3d7d71173f82a17b9b2b83219f6e3f5"}, "03c5d1b1-39ca-48d9-b7a4-0f43551df237": {"doc_hash": "6ac1de41b4cf0227be5cc5b59e3c2c27c66add979366afc49a7c69f8df485f61"}, "d706acd9-cfd8-47a6-99a9-b25a633422dc": {"doc_hash": "6b98c05aa1ba7a63478373f812afbe59beb9ea75fb339e324440caa3011f93e0"}, "0402fbb8-a671-40ea-b1b9-a9c000f70e84": {"doc_hash": "04d06b69c97794c7291ec23be75d21c9fc9066d99b59ca1902879705ee89450f"}, "5a51cfb0-1f89-439b-84a4-a4cdeb44c35e": {"doc_hash": "06b67283d0c2dfaac55f89bfa7a735aff2a0733fe8a5df42ffa1b9d2387b4d9d"}, "7b6b7623-3e92-486a-9246-e55bf4f57213": {"doc_hash": "5fabd387ce6986400b9cdc68ff7b1cfea5abfdfdb4ec9895734fc3b2a92dda7e"}, "5cfcc56d-1f80-4d9b-bfbc-7abf798b6e90": {"doc_hash": "0cb7f45a3b411bf319bcaeff582d17f742d5d13a323329f240fdd6edbc1d0f81"}, "2cc29683-f6a1-4dd5-9462-306396fa44fb": {"doc_hash": "f66dc4cb59bccc330e7ff31aecd3919fe8e3c53ac2dc8aa202b03d80b841daf7"}, "667d0d09-2139-45ed-80e1-35b4bd002b26": {"doc_hash": "74407abcb14d450f1378789fb3ee7fc92e3f616796137da0f70956515ebeee59"}, "1431b9b7-8bd3-48b5-9831-67496411bc5b": {"doc_hash": "fbba88a92beeb77bf833ba2ea0acc2ec708d1b35caf22a99137039ae8d68fff1"}, "914b30a5-3038-470e-9154-5085e03e47da": {"doc_hash": "8b58f84cc98f41b204ec2a52fff5fa20d9adabdef719f45c058f199293aca36d"}, "ac9d831f-0f21-4599-998d-b88c04e19f60": {"doc_hash": "eb33d0c59d08fde2243e5991746c1530525d960929c90310bd1fa8786a1d49df"}, "bbb97f66-b566-4573-afc2-3bce0fa780e8": {"doc_hash": "ec778be36f9741463e2ec110091a12339b0c5b2b313badfcd0e70f4d35691edd"}, "b78b1b71-1564-4d20-aeb1-de79e1028204": {"doc_hash": "71a7fe8bb09f9294fe5d72e9e943e5c723ab4eb48259e82bdde8a800b14a2223"}, "2481ee02-d949-4574-9175-9f9a58fecf88": {"doc_hash": "5d6bf31779080515db79d8177a62cfa00b166185248109f5f69e522b18ec344a"}, "7735730c-373c-4966-b578-a1aba8ffb625": {"doc_hash": "8815575c475f7697d933b4462ad6b3351029c2511abca4afc3556217cfa957f4"}, "0bd10107-94d6-49b0-8764-6d1666bd7262": {"doc_hash": "48134be1933aa1e88f310e20c3752d34ddd5f82e17e9ce500fb95a911b6d1afb"}, "984147b1-2e2e-4b79-9307-d44f5882b081": {"doc_hash": "17eaa537555f7db2ad4b7c68a55ffad7383c00fd5c854e4caaa03ba227ccb403"}, "4df5ac04-29ee-4501-83b6-fb56e12e3315": {"doc_hash": "a4b71426dea1072024b0f80527e342463351a5f0b6ddbd01b588d5daf616234e"}, "cd407ef0-0a81-49f4-adb6-70ff92f93364": {"doc_hash": "cbffbc589d85b93aab14f0a9cf94bbd66157239b23dd408023871ba3beae4bb8"}, "1e237025-f9ab-488f-9524-e65599a2abf3": {"doc_hash": "3c21c37aa81254d9601be9ed4d98eaac17c79ace52472a7a377c29f43fc2b461"}, "f87f2543-fba7-4ac7-8708-74fe89a048bd": {"doc_hash": "9513cdb1df2c0ddb64f73fc2be8e4a4e3572d2fc4a1c2a7d358ffe7a251a7a60"}, "7ee731ee-b2d7-417c-b7a2-6e3b3a714f03": {"doc_hash": "bc1e91dd9f330fc860a57a485b51901bf77ea2cd04795ed1d60a77b6e8c2874a"}, "8cad4573-a53f-4798-b58c-f0b612fd25b8": {"doc_hash": "eb286a3a0a8ba2cb9462e35986c27f3d8cfd831fd56a2fec6c52b11fc1391c4f"}, "a8380bcd-c579-4529-ab37-a2c9f5646886": {"doc_hash": "b7bdc70d7fb259027ed3e52ff637fe3b5e903d23e2a5f2915b028ce18520f816"}, "6da4689f-7c37-45fe-8110-a8d6af0f0c09": {"doc_hash": "cf5a4db887f1a878ebe5b895318df58321455b58538aed76595ff5f8e4207451"}, "f5d845b4-39c7-4b19-b4f7-77c73d3eaa08": {"doc_hash": "72f122cc835e1eacc331076eb15368d8aa953d8e29a7886923284847158b92ee"}, "8c2363f4-e830-4e7e-8505-f935c9e48962": {"doc_hash": "a9098a194f59c6503dd2ffe43ea741a9958fa841f1f3a09bd44ead3760b7cf57"}, "2221a159-a76a-4119-90b6-b457ec229b33": {"doc_hash": "94961ec14d017ab8574c41df370e9a469182916de3e54440c06507436f4a4316"}, "d6d03a67-9728-4ea9-b55b-ddda143197e8": {"doc_hash": "30c8c1fa9824aadcf52d5f342d624f6674dd55df0097fab2516a016aa95c0906"}, "274f6e9a-bb3b-42f6-9eb3-eb72e83f3e96": {"doc_hash": "b10d4171910b094107324c8572c869c20705c8fd25b80bc08d610e98759882f4"}, "ea9614f7-ab93-4cf1-af04-549f91502c03": {"doc_hash": "38e4f267b68dbe23b20641d44efb52e24f31d11bc8f882bad9bab6951b0f7b00"}, "8daebd54-b463-4982-a1d9-1c7d1cf947f0": {"doc_hash": "9873ad7bac86b511291ce3b6f0aef19fe60812790202ef0c2884d145141e4fef"}, "323ca365-962f-4ade-9206-f8d36b1f015e": {"doc_hash": "7a6a3759f3bc0f9e8bfe324172c064e6ee0e508dcfdd8e53bde19c8761f37ffd"}, "cd6b915b-9782-4c2d-bcf7-8cde5e59aec6": {"doc_hash": "9a10f142bfdc7c7ba06bb2928f675931536308ff8d3e41dc63625124bae79e6a"}, "cf712e6d-2c81-4649-9a80-adeaa87cc907": {"doc_hash": "136f8d6c41a6650674ba6f54939c5e5aeff8657b7940e421414f993d0322c532"}, "2d9e2197-d5f3-4c00-bde0-b01b92c9ed46": {"doc_hash": "f638e71eb9e3b71ff28c34afb60e91b3fa48a2003db6083df20bf012170091f2"}, "4b7ee8e3-6316-414f-a92f-6e308c079892": {"doc_hash": "331962fdfa0123ae343d232e1b1ce63974df4dcaa8fe73e09ec2b1896d93c82e"}, "11673d4b-a2ae-45f7-8a68-196580f007e8": {"doc_hash": "2f0413bd7ef95507d24b94c089f3e9a9bc4824353a011ae132f4644e7603a3fe"}, "3ea5e683-6a8d-4e5b-9ba6-824b60f08dfc": {"doc_hash": "a1681206d317d8304375103c13d482d5401e1ba28520c036865f6f1fceeda5b7"}, "41d884ed-0515-4ce6-971f-748c36f2619f": {"doc_hash": "44d64f2e07ce959a7b44e20e6c1755f0102013595dab45fc2a3c169307324442"}, "a4dd0e45-3619-465c-9e70-09f5a399d0dc": {"doc_hash": "568b604ceeac675fc3438de858855d90f1a801bd23b5089b0f8b141beba58b70"}, "52d2dbb2-3c59-41e4-95f7-270c6048e0e4": {"doc_hash": "9d2e9083462125f5720d8da5e2a42f26e4a18155fab494228e54e958eb07b440"}, "2ae393ab-75a5-47b0-8ae2-798daeed29a2": {"doc_hash": "ce552842d62aee46d50a48b81f5bfbdfdf791d098094a0f0adfab6c1e6211e7d"}, "b84781e5-6e46-4cd3-b5e4-c8ab50759d3a": {"doc_hash": "93bcdbedee426efe628cde152e7c2a021ad75749a15ff9e5f680e84eea9da7c9"}, "2e153496-6d8e-47d5-a72f-3175d9336797": {"doc_hash": "506d0c10122b8a627e4a2594282e364bb00dc630dcbdae2955348bcaedffdb15"}, "f37351e4-d2ae-43df-bfb4-c94794026b36": {"doc_hash": "7a6f39a68c189fbead96c23fa2510bc694c950b43b9f316e1a260a9b1a995508"}, "d048a513-38aa-4ef4-944f-de2813d6ba70": {"doc_hash": "642d07e47110ada05a6f9e6d3088d17fdf6d78f408cfd2cf873ece3d8c335681"}, "4201f27a-ef95-4b58-a338-4d40632ce134": {"doc_hash": "ee99f329485d36d6ed0edf6374555384bb219e7c6957bc86c79f171fc5ce4e49"}, "4e369e70-bb2b-411a-8110-fca7beb518d2": {"doc_hash": "c9027093711136c4646c0fd98f6611a16798a2af371c13cbb6206ae55a7418fd"}, "e1d11c65-d73c-4faf-a3c9-9e37af943cc2": {"doc_hash": "45e713729029616c182cbc221f11bb4ff119207777c05afb4e66ef8abb563a71"}, "846b0ffc-7719-407a-9f67-a9f28b480b08": {"doc_hash": "1718b379abf76ce5af83c7bcae9dff5f2ec0f913f227752a53f4d936f892a404"}, "1e7bfadd-c783-4818-ae59-548fb6e549e3": {"doc_hash": "389bdbb9e7480588f196ac101111ef8dcfcfc12a7ee6f3ef2924192337904109"}, "30bc7272-d963-4c1c-892c-34eb0f595d63": {"doc_hash": "1d323d60d9d6f9142fee38feaf2d29b32a5e8cb36ff2644b7343f1c81be63107"}, "9a75c85f-3a99-4a17-ad8d-da2a0e62d1eb": {"doc_hash": "6c1e6226f5332de74f7f8a985d17c66f717142f8479ffa6a45bd133f4b6aa270"}, "0ad90e06-39aa-4342-8832-6ee4286c8db9": {"doc_hash": "6a500357226fe3a73aded52623a32d9941f2957b398bf5cf0612a1fe62229359"}, "21df284e-7d94-4293-9f96-ac69368fe69a": {"doc_hash": "57a9adf58968ac5a737dfc493c534cb82d3940e983fa3e4bc528c2071861da5e"}, "79ec8438-36b0-40cf-b80e-be587f4a64a2": {"doc_hash": "1b90cabab72a6e3cee0dd99a60280183aa5c9a4f98bf03e00f84c24350ba37de"}, "881cb765-d982-49fa-b600-0d81db28fc02": {"doc_hash": "efe8e2297bf7a5e36b3238268a5421fd35c4aeff6d69a23bd431b74954118ab7"}, "d4fe791f-3650-46d6-8c7b-18ba71594281": {"doc_hash": "53473f5b0a958ff1ea0284daaf9f71d60e072407c8405eec1cf21f3a20c1ca1a"}, "1c0c5472-3d1e-44ba-80db-6ffabc7e8951": {"doc_hash": "773ee8bff01ff0c1736eed58fe04fbcff72aa7880a570ceace5be735c7d79c79"}, "6e7afc4c-a116-40e4-ae81-c34ed7dfaf16": {"doc_hash": "e4c02ab68552c544952b201e1fad5519d652024fed94ffc59edf6c8ac41f4772"}, "accffe54-4dc0-4d46-9257-63ea5d7f77c2": {"doc_hash": "fd9058ffc664c81adc05b932c59dd4ef49ba3957708e7bd12d8c803771ddc60b"}, "2884ad43-1757-4563-a2e4-ff52c6b9a0f8": {"doc_hash": "29123d4f6477c023ba173b5da30354687916e047d5927708270d4778957efa1f"}, "6846c9f3-ff0f-4970-9650-043ebedc77a3": {"doc_hash": "73a5667f43e5cf0ebd3a87a4c8fba04ee5e2a54f1331dc75b60132025dc49a4e"}, "61cfebe8-7b9e-422a-9794-8b0275e58f81": {"doc_hash": "0b99c0bc6c9eabfc1ff97ced9e6d0834e255e774e5c3ecec674eb40af55327d1"}, "a935f314-e0ce-4f7f-8f21-c0a4d92863b2": {"doc_hash": "3c58db95f9c0bc0907ce531e817a81f4e400a8e9c210a3fdce14d7e66cb267ff"}, "bc0f2687-7615-4a54-b6d1-e7092477b375": {"doc_hash": "848361bf2cad5b109c0bd36b3a0e9d7908529f5a5d056c2cde257363c6911213"}, "5b8cbb35-81d7-455b-a84d-4336f9016ef4": {"doc_hash": "9d55ecd6543139f4580333728f1e87ea007b5bc8e860cbdeae335c5c54ae5bb2"}, "330a0a2c-53c9-47a5-a2bd-7d1e76673699": {"doc_hash": "fc9bc57b39e00ac731e1be964ee6fb3a2240a1d7cbdb569de54fd57ac3ed1b1d"}, "ed5e741b-fc69-400d-97ec-bb53e292a299": {"doc_hash": "548f3498fc5932083cf5b99fca09cea3ca7457fbf2b01894f0df3be76c3ded39"}, "737902c8-e286-48ad-a69d-033f5ed61b44": {"doc_hash": "138c9606fc7793de830b2d074e867af41db101983ebb334fec61621581aabcad"}, "d4c26588-708d-4aca-837e-52b92ad098f4": {"doc_hash": "2aaa6b605391c8df089b1ead0a66b5f8960673702876e57bcad7d3919e673513"}, "f88d11df-ec28-4033-8018-80718239ae2b": {"doc_hash": "752af43898bed1d295cbbae89ab13b112c317edb81bbd17201b21d2a35b3b78a"}, "12a5d0ac-dac5-4548-aa38-6be3fdaa41ef": {"doc_hash": "266771e442e0d5f11b17341b07e7ae34756bac3128be3f328724c1e9eec76f2a"}, "3de3963b-5b0e-4560-8920-a6af9e3ba92a": {"doc_hash": "6f7272625765e167d496d4584a733b96d13b4905556b69dfaab535aec267b863"}, "60bd711d-98a0-4cb9-90db-ec803541e88a": {"doc_hash": "3b7cc155b707255ee6ea935e38701d03d1958dd16731649e436cf1f8c8b3c698"}, "d8e0f6f7-bb8b-4c93-b8d1-cf74fc7a924f": {"doc_hash": "878e3fd5c17790457ecc9ad98158061b36733fe526e03420b54de26da08ce434"}, "91b9a779-53c5-427d-b575-9f520f1a4b15": {"doc_hash": "76e9eb82b10aca15cc51d4a6917777bcf86f4fe0fb421648575549fa57b4495b"}, "2a684c09-42b9-48cf-bb90-6ea90cbcb765": {"doc_hash": "80f8ca3028bb9f1a0d34a0c5dcfd3b4b05783fb7f5afda0c80299289b542e57d"}, "1f97e834-e51a-4773-890b-e1839fb3889e": {"doc_hash": "7cec42c07a81e81eb4a3f8e7b2e7babf02a49fdf662328038c8bceeda4803850"}, "a8717b08-1164-4898-8d05-18b7f9226ec1": {"doc_hash": "7fb3abd7c0dd29f9e6507205e07a06b784a0a18f7c4049df4bf3ba65b16f2370"}, "71e7147a-33d9-4090-9e41-417f6f1d7e72": {"doc_hash": "f05efa927271d2dfc05973c1b8ca9849cfd4e7086cf4872409ba0bcbf01863ed"}, "ec9fccdc-a752-4178-a26e-b00c8f66be44": {"doc_hash": "d3829c8156d895712edb0fcfc9fc38c92947d74f7bf7832b932558f6f0ede46f"}, "7727deff-3681-4e2a-90cb-6429bc578b9b": {"doc_hash": "335fc44ae5b7ac70e2e7cc5822294c3add1b846e25de17b051b16e1ba56a1b57"}, "40740a4f-274d-41dc-86c0-0c33d00605dc": {"doc_hash": "09bee7cb4fc5dde0dfe0cfff93927e408f29ef374d29130c5ffb4549ceae066e"}, "201fca88-45d6-4f71-92e6-2ec0cc8b9c0b": {"doc_hash": "a830ad610b008a4691381f1d4bc0626957ed66577941cadd739b0e12b569f939"}, "4215217f-2d80-4f77-a3b8-8a185a5bca01": {"doc_hash": "8885545bd01a64a3a2270640a63f9464184bb35cb5815ca1518589327237bc55"}, "7a5b009d-73ef-44de-9256-6404be516951": {"doc_hash": "cb94338a969f76be378a56fb2c328da63d3a273159fdbca2aadfa3f457a3df6c"}, "3ffe9e80-3fef-45ac-85a3-c278a1531c18": {"doc_hash": "27a4d33f15ef2c2b984683cbc4d074573a1c0f4a6dcbb1585f28abf053dad391"}, "cc50cd5e-571b-4bd1-9669-58dd68716c74": {"doc_hash": "8808921242c800e400d4e64a799e97dbf040095faf51c7aa51b130ed65de890a"}, "a994c212-2d13-48c7-ae31-c5ca9f70eb42": {"doc_hash": "fb0ecdf46ff9492e9cb782d1544a3e27fff0b6031da5aac9979dee9b6e3fa08e"}, "66a2d553-7e1b-4216-a0be-1048ec53a9ee": {"doc_hash": "05e5fccb5b27e3242f2d8e14c76a68145108eca11b4059cbd17eb5ff2b98a4fc"}, "9be0f482-850b-4d49-b4ce-efd96bcf646e": {"doc_hash": "c811238f781f45c22413d9f455722dbdfcb7aa35e306f10831250c9f4e71bae2"}, "4fb0113f-4dd4-48d9-882a-3ed50046463e": {"doc_hash": "bd200a476e709741e27949073edd7001837ee8a2dd7968044db13fb35e8ba356"}, "ca4eed97-c081-421d-ad92-f813d13c6692": {"doc_hash": "853ccbd4ed389d96821426c53f623e8a1941c734955b6ef65eb850e61c077411"}, "06c1c216-46a7-435d-91b6-b77e69b2e663": {"doc_hash": "415eb0c3a19d1eb5bea5a07e0ddfe71b97f85ce9c0661081d91c98273c7686df"}, "4261a0f0-9abd-4ccb-8a42-2bf93f779a81": {"doc_hash": "b6a49824bb4a6e6b61db79353a0707b31c6099b741c412adf95a3dfce23e0a03"}, "9c5a6ef9-f171-4a62-b1a1-1110ea63f69a": {"doc_hash": "48ebe68b12ce9111ef418b0f839c1b3d9a542dbffba0c97e6c0b94c891752c3f"}, "5365b0d7-9626-40cf-9a2b-90ef407e45e7": {"doc_hash": "6fd2853e0393c5a9a03ee2a193e14f1d92d63ea1d4e8c537afc7da52e5624d2c"}, "a44ea320-629c-4d26-a9c9-d62bf41962ed": {"doc_hash": "ce49370a6ef89f91c34fbc93f4b084ea1820b7e63f2450b4ea8f2099c9632c3a"}, "935dc53e-b916-4faa-a741-7a3fa240a804": {"doc_hash": "cc65df719bced6322f726227dbd6e18e6d0623f387487dfdbc8cb673675a6265"}, "abc1ee0c-61cf-4f23-b569-9d2dc0a8e7db": {"doc_hash": "d93093e2bfb5d502be189bdbb6a63cdf1f6b044ee890cf836ef891b7947add39"}, "46e2d04d-a4d8-4764-849d-f4c6c04a7fc9": {"doc_hash": "2aa6ac65ade9e3b37e21e102dd4886d725d84b0d1009a1ffb956c51fbd681b23"}, "30d464d1-866e-418c-b789-9e7801de44b9": {"doc_hash": "b20fd51fa52c8c4aa77ab49bd856778b20fef154d159c8d4913f7840ab28c691"}, "157debf0-ea7e-4062-a6eb-1b79508a6a8d": {"doc_hash": "c15620c4ab1908b9bc69582d3f2f7d45f5208a8736eeccf1b7eb19645576a1a0"}, "338628df-1354-44fe-bece-70aad2d91871": {"doc_hash": "5e8e31912ca1578c1d05e63a84ccc487ee805834c8e16ba2f1654f2aaa0c3e9b"}, "34989f6d-d8f1-4dae-8666-a07c3565abae": {"doc_hash": "8a7ab2bb2cd8e4f96e10cd9ed41873ff7d3240d598e69a55f5ed42607ef74fe0"}, "ecda3c23-1fe5-4285-a605-24743c348ae0": {"doc_hash": "29edc7a1597d0a95c0b438ad94d35ba5e1577c23f4be42fddae5f4a00f3c25e9"}, "0dad4af6-36f9-4ec4-9181-2623b16be04a": {"doc_hash": "aba108a498d9515a5f4e1939ca304f85911fb42b5f69afbc70dcf4ba933ccf10"}, "bcbd7fe7-5488-40ae-8c34-56cce0cdef21": {"doc_hash": "3bcf282fa131fb572a25ca7210e24424d0648b901dd1d1e20d697dd1883149d6"}, "08f71681-5cea-4347-a5fd-e5e3fb8c0ac6": {"doc_hash": "bde030d405c2691f12b7647627d6361d811d16986e0e1babec829e5214e39de9"}, "c200a591-8def-4aa6-9f1c-1dbeeedf89a3": {"doc_hash": "12547414760f72dcf9ccf1584d3f822a6dcac025c436859a15bcb1f3ac034130"}, "3ec61808-230a-4b6f-9917-87e45308fc81": {"doc_hash": "b603ae0879c2c147800a4e909c8a81caea7ceab680c1a6b3d54123ad802a36db"}, "40e43c23-3dfe-4f9f-b36b-673633c9ad2f": {"doc_hash": "95cf72f8acee05d93ba452b568f550d02d0a78316ecebe329e19fa564d41da68"}, "62bdfbf7-2d85-4125-af36-9ac687fa789a": {"doc_hash": "45ec507fcc4a8bad72cdd73b2dbacc3fde15128d0ac4a6855bcb9180c9a14994"}, "531e2299-5003-43d4-97ab-3305b6a7e6bf": {"doc_hash": "ed77b3ef07b14d0491f185f7e57d44a662c6a1e21f163c17debf9a97566c14a5"}, "b08f4748-544d-4530-86b0-acb6bd9f2ae4": {"doc_hash": "e2d3f15c5bdfb77d6de319c7193af3856643a183fd694c77dedfaa41c77bc080"}, "c81fcd7d-f5a6-49e5-9ef7-266b2205171b": {"doc_hash": "0c06d382cdd8a2707e1f1086aec24a38e2d4f783315d2b85f2f279bb46bd4150"}, "56119e97-408d-43be-8433-5fcde63abbc9": {"doc_hash": "7c2d555a54938fc32755fd9e378d5b75c2ca42205a4801c5cfdcd5def4445aac"}, "bb11912f-41bb-4f7b-9328-ea892513e3bc": {"doc_hash": "2ab38b581d4080c28e83c220391bbe4b2fc32e376fe7705fc0fef7c72ec7bd2b"}, "d06b8f31-9819-4fa3-a618-95a782d11fa1": {"doc_hash": "a55bb73cf2adf8b9c919e1fde1ab9b5a69d3e2268bcf0863b29a2ed0686db022"}, "7149e6a7-531d-41d9-9437-233b59a8b686": {"doc_hash": "1f7c9688a25f24247b2dfd7615c2317e738d7232011bf184fb05cbb115931ffc"}, "10abe83b-9c38-48f9-9a3a-e3cb73e52724": {"doc_hash": "e354fc9f2d99ab6fba6d9264c436b74c83199c8b2a84b9d40f8553e669f7877d"}, "fa2e441f-0455-454f-b46c-5c7bedec8131": {"doc_hash": "afd6a8c5d6a275334455bfa939cd9eb3001626eec0f97d2a136ede18ea80535e"}, "8713fa42-15a5-4ac5-8c66-28b1ff130f52": {"doc_hash": "0737d82e4f46aaa2e9db0a36ac570c8f29cbc9dbba84dad77f657e20d7a811c2"}, "8829a23e-e3d8-48a3-a27a-83d910e08765": {"doc_hash": "01d0f21aef1ec1fc5459c9902075806a32fccae42b0dab9d45884b6bcfaa77ca"}, "74cd4ef2-e295-40df-81b6-e75b10d8727b": {"doc_hash": "64973a6b98d67b779607feff4097cc570fff958239201e59b8edc720d467a1de"}, "89af7da1-d7fa-44d3-be3f-79a9a7533a1b": {"doc_hash": "a64da77f4201bd4a8c2a19cb892ef2249038d1006fc3d9f4044afcc655117949"}, "7f12ccd9-3bcc-4199-9ee0-bcd3146cb88a": {"doc_hash": "0c81b2acd8c82b5141481e8f538d77ea2e42420a3155b874dd84132edc6348c9"}, "5e364d65-4c36-48ae-9925-4e244c0066b3": {"doc_hash": "9dc3e599d61bb559db22b53cade5074eb132bc78745c5f00d9e77879acfa9cb4"}, "4983977d-78bf-49bb-a0c5-75c3f7bdfce8": {"doc_hash": "4ffcade3975ad210f145bdbbe82513479e556affec2a2c71fa4065b2c8fbc813"}, "df932b3f-cc3b-4ace-8670-dd32e92af638": {"doc_hash": "b8447fb3c1d7dff5279d5e88aea2e3d7489c2d52dcdcfd1554fa07aaf67e217c"}, "79b9ad7d-6921-41e3-9f85-5ce9fce13116": {"doc_hash": "b0b3abe8175c67d3614c81ebd155addc252aa8e6f5a9eb824ab4c42999dea505"}, "0f4c4639-d618-4a73-86ea-c46f2d9aa921": {"doc_hash": "6241fbc51d7af3422020cc5ad7e490f1196dd880a92733db6acb4853bd29d488"}, "b80e9edb-4c3a-4660-82d6-6d3cc258b82e": {"doc_hash": "6d440bc1825942c8daaa6d39ff7186a056ca8bc83859e480416face2b811523d"}, "a33e732d-be5a-4593-921f-33d103453654": {"doc_hash": "d547bc7046a1331d604275249a1f863de239a529a46a3cc9772aa252d478a466"}, "a18f9b68-e101-4e5a-89fa-cde23ec2bcf8": {"doc_hash": "09d587141363be2758038f0db76f808c986d0857d54e32946b03b8754cccae13"}, "8ce3dc78-dd2b-4865-bc38-f1e4807b8ca0": {"doc_hash": "f1f948178efc5ff46b93085192c2063c4a9a5ae2c12b8eed743fc7d4e4357fb9"}, "d6e7e157-53e0-4e1a-af8e-d43314ec5c61": {"doc_hash": "87ae7db80994bcc257d186e0c5b088bb05cda19eaec43a1f80403e088ba34691"}, "ea5f0cbd-f5fc-48f6-9699-6f5bd18497ae": {"doc_hash": "1948fd7a51992b36c3e390abf34363150f3a88464d8cb6d31754cb004130c397"}, "9fb68e17-fe10-489d-9d2b-c69ddcfdcbc9": {"doc_hash": "48284782336773f7679a09880358ecba02070ae080cbf32c457af69732f482af"}, "d1e6a54f-08a6-44cb-9a0e-7958bcbbb3f3": {"doc_hash": "d117c8c669da475e243158c321eb6693f8536270612f0e42d265bc8e24693c27"}, "246fb635-a3d6-42c5-b4a8-a0258b96bc82": {"doc_hash": "aea6166abcc32324c1bf82435bb8816f671fca74da06dc9cbf809b39ef955fd4"}, "84859801-425a-4208-96c8-ff12c482a9fd": {"doc_hash": "b601b00873b8278f4d45f416052d4d1f22974381c265ffcbc785b041de075065"}, "02457e77-2805-41c7-9b76-6f54ceb3d8b8": {"doc_hash": "9ff928c82245f94a022bc40e1345d671c95ef49979d7263fb8a7e90edd7a663f"}, "cb069b1b-c21a-4a17-8942-d93fb785bce2": {"doc_hash": "7f51f23ecfb7c44641eeecb1c146ca81bce54c263afbb5747514dc0b3a542914"}, "db0357ce-f2c1-4ffd-a2e4-11fde0aa692a": {"doc_hash": "325158a3eec723435a06b3e1393fc87c674e06ace24a9eb50f28b6bcc09f966b"}, "63514ab1-bc97-44ef-8ecb-3644ca44d06f": {"doc_hash": "b9c553aa888873be452e674a607858b1d1b9d0fcd5806a4fa8bb7b48116bd955"}, "fa19d8b5-b9d5-4e68-9c91-1e3793da50c2": {"doc_hash": "b32e032a010c5ffa146160ef19aa197c87cb3da2c71b814c9f2bdda2aeedb615"}, "e5e4d27a-e300-4b12-914f-e187c7c3625d": {"doc_hash": "bb5129ace813ca275a22f6f0a86df4ef3464f652a9af7e44b4924009da5d37e5"}, "25253ec0-9c70-46d6-94cb-048378c8c666": {"doc_hash": "e4fa6c31b6ccd38cd51615d755cd04c4e9566b660839a43d07c442e4becd16b6"}, "df869a45-fd74-47b9-ad50-4d0a86e34c52": {"doc_hash": "d8a22611a6fcdc03948b5323c15d7fc79d5e074cd80383b0ba11a2813f35abb2"}, "46c365f3-dc6e-484b-840f-5b3e6ddcbf3e": {"doc_hash": "9bea1cdb717b607091d772522cf5ceecdb425028a40027ebc6904a7302bd232d"}, "1293e67f-3115-43f3-a907-bcda77482792": {"doc_hash": "b2a256573eee0952a4c4a237e60151b2c3d0a5b43e7e44d6bd470a08907656bf"}, "b61d2b8f-918b-48f6-a64b-79f29dd658e1": {"doc_hash": "6d0339b711fc9958dbbf88f6753a3d2ec6e6167f0a8a0d28beccb58e4c4c54b2"}, "d87e31e2-84dc-44de-b814-206f0884f4d4": {"doc_hash": "2343d952bcc4414d385078fbf6934dfc8b0115c5c7b920c5c7b46ded83335c9a"}, "e776003c-e220-4638-b7a5-da9c6f13d5aa": {"doc_hash": "4619c7fe7cba7fcdd474dbd0fee06aec7873ec3f6e6d5ec9d5fcc10c756dce00"}, "bb668445-6997-46c7-90f0-ef3f83e1b62a": {"doc_hash": "9e1a184ba9a0625ffcedf8f70d6258d84515ed3148c09ec2cce13a17c6a6ad56"}, "a40950c0-eae1-417d-9e54-51b368975165": {"doc_hash": "67608a1ccad563fe238d7f87a0eeeeac7de46afc40987b9a79dee64cfc106401"}, "e92e8db5-6032-4ca4-8d54-adb8fc6b145c": {"doc_hash": "09b3587f514325afee39b45d9375a51a87e5b0090ece858d5b65a716ba2ce06a"}, "901e01c0-2e10-40bf-88f6-dc8f32aae36e": {"doc_hash": "611b876faa738055b714bfba49538a94df8810113401682aa58753877a0b5d50"}, "69c41b84-56b2-4880-b79a-94cb7687707a": {"doc_hash": "408a67c97c92beda819ccca08d831743037325e7bd0af137079cf0c3f955e81f"}, "419a0728-958b-42f4-b58b-72ad9d01987a": {"doc_hash": "7769c32f0bf1d8193422ae013b6a7c9d86199453ed483300936dadbc66b2760c"}, "d4a4302b-7d5d-427d-aad4-2b6e947028bf": {"doc_hash": "b13622bc6539d48d5f35a73f5535cb4b845e148f7dc56621bd9f8228502d709a"}, "51aebedf-63d5-4a7d-bb06-b251620c30fd": {"doc_hash": "ef231db6921b35d3fb9f179c41d39027571d57a809b06928b39f35b019d8c910"}, "eeadb1ad-4504-4af1-bfbe-e6421ee02e94": {"doc_hash": "a75470acad5a677b38d572def033ca7e268e0f2bc9b8b9262a34f2f268b46fba"}, "d5578a06-afc6-4310-9811-dfa11f1d3f80": {"doc_hash": "ddc7ced41cf10f055a7f3be50fa7354956894f2cc5bc7d88a04b138fb8cd2096"}, "46b4e596-a3af-4322-9644-bdb7a8456ffc": {"doc_hash": "871418f3c88170946e73fb8515aba977c2b25d76e6819c1e41194653a4eff95c"}, "67b4751c-ad18-4e27-8e5a-1eff6afbd4bf": {"doc_hash": "1c2b3f9a0f4691edc9cf1209f2e305ef30a502352d79cd565b186bcb4d217aea"}, "a2d2562e-a2f4-459b-8289-f4c6a244106f": {"doc_hash": "44507d18225e85f3dc292386a3ed5eceec62730a1b0d00a942e391442d1ec6f2"}, "0f6e9222-24be-4e11-bc8e-d26ec5b80b46": {"doc_hash": "069b64123d200a02525fe24e5e4b71194e3c971a39c5d3401eb44cef7c62ed20"}, "0fc8f12d-a3d0-4915-a3d3-f813f7bc0285": {"doc_hash": "488bb124b5cfc2e624361dd7bd2faa1ac2ed18e478828797cac634c5fc77d20b"}, "84c8b8d3-b090-4b82-b5d0-e71ea38f13b1": {"doc_hash": "07dba9584d40565837e2aa992b7e1c9ab6ed6a3205b1ef0152ee3381610d0821"}, "eafd62c9-fa7e-4668-aa3a-c6480e5bbee9": {"doc_hash": "fb35af2ba1349c58daa3bd991734c22543b70574de86ae6ce6f4492c9840676b"}, "65efad3b-d97f-46bb-8549-1ad86fa424ce": {"doc_hash": "e598bbb0fbb833869f3ce549a4b6c6515d82cee1f7de169f4730e7ed728ba2f1"}, "1375e909-94e2-44c1-9dd6-1554fe0ed9d0": {"doc_hash": "e01e827f91346298364f504dcf63726e0f58ce7d2bf60c8ad1199deef00f6387"}, "2c39ec53-9359-47a4-a215-8a631188dfdc": {"doc_hash": "5dfe888ffeb683c33ca5c4bfa4d64407db3ed4d16e3c3d39f1feea329e3d95fc"}, "63e7e542-6cde-4e91-a0f4-abd22140a1d3": {"doc_hash": "df880e3768af66df7557d2ad178d7364b8de8cdd6f333bd800131cf439bbd1cb"}, "9846989c-e9f6-4203-9f43-ead78b93b056": {"doc_hash": "128a334a9edff4181baa8ae9ce9d4b549774bde8213fe444568890ee6bc6c693"}, "0625e51f-9931-491b-a8bb-38e09c44314d": {"doc_hash": "d0c2e9dc40aa7cf5ca7f50210ebc2b385c005232b490ae2710c00f815fd24097"}, "9df3aaf0-a362-413c-90fb-5dffc961dacf": {"doc_hash": "b6299f15065d47fa608a4ddf139e2f02fdcd5236dec3adbd3849834c997eda99"}, "d7528921-b73d-466c-99e0-ea1a798b6e4e": {"doc_hash": "c9c4acb697a687b8b2b47526d26307656b5575d906d6c360592b5dc9bda19fa5"}, "1a29522c-4f4a-4bf2-81e6-1d74ca488d43": {"doc_hash": "a63b64898a3458e10fa42f85ce0f66075b4d5acfe8926288a3e3e0322f596bab"}, "da37ecfb-be3a-4872-8984-87093d4332c5": {"doc_hash": "6c513b46abc855d3cabe7255799e65d8335d3d9de1d3106e666c15e97db4211e"}, "7ab61080-3d7b-45a2-9eab-9deb73de4042": {"doc_hash": "40d2205d16079b8c3a58fd7f8d4549fd75172a30f27e9602387658d739d24dd2"}, "aecdd0ca-fe22-4f21-9364-de299f1bb421": {"doc_hash": "d6fc6f19554c3255da809682934343f8c7c7372ba8b761ec982f35382bb975f2"}, "2a786d27-e161-4888-8a09-dbe8ac79cd75": {"doc_hash": "99f08b7ca198425111f5cdc9f2cc98b5b9f701d756fc480d4b62905fd16d0fa1"}, "4a078511-b5f0-4ff3-90db-2eb2cd1df1e8": {"doc_hash": "aa3c29b283bb3a976dea2a502118189dde29ec54daa8a88a5a0fbae141f4b88f"}, "db27aafa-5ffe-437c-80fe-782cb013c41b": {"doc_hash": "3758845215e03be793afe141060bbff9733ddcacf3fc87c4689543ccf8e72862"}, "47f69e3e-d902-436e-8701-98897f601fe8": {"doc_hash": "f0326b9797360ca76d05dceee486a1baaa683893ea944dc3a1382c3f09c8fec2"}, "3d28dc3a-4b54-4ba9-af63-f6b372141d05": {"doc_hash": "3d8458330ec5154456cfe88f8021245608a1111ecd721f0c06cf2e0a4896af77"}, "f1a34d79-00b3-46a1-9166-473f2b292e8c": {"doc_hash": "83008f90605a83e35cd9e70b7c8f077f590f432bb176dee70a815a8aa05c2795"}, "d0efa5e7-736c-4f30-b3f8-a2e1e61a1817": {"doc_hash": "7cfd2c4e5ce139f1a39ce4b3af6f3cb07e12d2f0eeafbe87530cf4378e4b60af"}, "6ffc809a-9fa2-45bc-a00a-f46dd7a7f53a": {"doc_hash": "5f051a2f5f0a1250e5f9ebddd6ab7ddc90f776e98dd3321532829cb38d753afa"}, "00c53246-49c5-480b-9396-3ef42bb4f090": {"doc_hash": "33c078a2bb845305f882820c6d3fc03cba9d146a45cc025723ae8097f09ac756"}, "bf5098dc-6d7a-4671-b3c8-f7f8e6972b63": {"doc_hash": "e22becfa7fc7b5348be80701a214f0cbea0c230c7a5417d2ed4f746ab8144e83"}, "2d773a3f-33a0-4638-ac85-00b61f5e71f8": {"doc_hash": "f1d204dfc43dc953be90dad053e7db984a29eb653aada820ece9518654e9b4a3"}, "8538221a-6eaf-465d-8c19-1bcddb3035ef": {"doc_hash": "37e441e8fef293787b97b1095dd8d8f4aa46e5f80f3295e3066474efef1e4977"}, "e5c9c990-88d5-48d9-9f41-eb9c2c7143b8": {"doc_hash": "41a3a88c3ba57c52c21916dd16b4fe4ff57c13b19ecd72a9d696deb1948e3d90"}, "e78bb85f-ebb4-405b-87cd-18a2f5dd63df": {"doc_hash": "3cd847179a9bee08323d340da1a773387d4152eeaad765541ee121bedd72db4e"}, "6629e62c-cc7b-4d80-8ea7-101f50d01dad": {"doc_hash": "42a17c6b670336878f0233397730fc83aeea35d7fcf2a8063115143a24e973f0"}, "15f33460-1672-40f0-9c38-0bc561f4bc5c": {"doc_hash": "3bcf27b51c97afafe34f22b06998081c5af8628a809fd39ff69f31e3a4a34942"}, "dee80d4d-37ba-428b-b5ad-70ead45bf44d": {"doc_hash": "5e4b020b43303c179cc9ede3a8efe48823dcf7daeb1ef015992e55f5010569cc"}, "5765a62e-fcdf-4246-b636-4f91b6cc1b68": {"doc_hash": "19bf9d62b72d28aad5d9ff1c1073e464efa16e295f8a9598f7c48297899042b3"}, "fa6ab322-8b62-4268-a9b0-5841763f99e1": {"doc_hash": "890018c8d9f56bf55a3ca1d9639cc1c2e3b3ee969121ab7bb8f8989bd45b77fe"}, "25c5489c-61fd-436a-be49-d9374ed3b412": {"doc_hash": "50182c7f53007dd05ef5b64f7abde0628640d4fbac684d03e13389a23e31e1d9"}, "bba78ebc-a9bf-4312-a3d9-269b18e25020": {"doc_hash": "ad841b315d40bcf9cc400839e385907a7a4fe12289285289d5ff1efa81ebda88"}, "3c927528-dca5-4559-bf82-53e132977f9d": {"doc_hash": "4a98a3721dcd64aa870c707721bb0dea6bbb49a2509492021f4c77b2d423e0e4"}, "26a88d0d-72df-4257-a412-51b92921486a": {"doc_hash": "1643d8d1194edbfbe4b1880eaca978b0cc24d25fcd74f46730692d3e8d2addb0"}, "b2985ad3-ffc4-4d25-9afd-db2268e34fd7": {"doc_hash": "bf2446a2d9e19e8f73618d71f5dc9e74572ea1048d1edfa7218968b3d0d8c625"}, "76794624-b8cd-4fdd-a907-0620c2ad634f": {"doc_hash": "511f7449fac00049e03ea0f936a7ba608e7d9ad0055c4d0a8bce330a23b9613b"}, "e1b31a7a-ebb2-477d-ad0a-d0a09799ea61": {"doc_hash": "3f3aa15431f507b15b947bea24972a4d4c78090f85c278195b617a7a58b9325e"}, "fab0a071-c1ac-4cc9-b04d-072e0e5abe4f": {"doc_hash": "91cf1e29ed97a857926b107af955b2abb320e5cd309f387c8c3524c697e4bfd3"}, "35c748ce-0f7f-487a-94cc-c304e78cb68e": {"doc_hash": "e2d4bd9ae97b78ea4074ffb3784e97553447ab378a5a3e23cb2a29dd11d7241c"}, "b56ffd43-7fb5-4423-aac7-b1ed99005058": {"doc_hash": "ddad45a84a4d993353f7b557977d2fef2dcb46db11be4c288bd0bf13d3fcc8df"}, "b470c5e4-e2fa-408b-acc0-7610283b9f46": {"doc_hash": "4273e071325678cf1b675ac1c31f6eade62285d020246916dc44bef4eda5c05e"}, "765a50f0-f35a-4c48-84b1-4b78e745372a": {"doc_hash": "25a0c0c9c156047c935b936c9ab1a09a904f0f719e0b6383a09559b6a66e2ce0"}, "169a72c6-f20e-45eb-805b-7529751a4caa": {"doc_hash": "834966d59855ea0b14db62e851512487754de2813527d24f9f239659384912ce"}, "72fa8667-c192-4edf-bd23-0de79807fa4e": {"doc_hash": "356b0493cbfebc8a3bed99f97da8a9eca61bc2c03f732f842ec02c6f48269589"}, "2efd9480-141a-4edb-a4f8-e407d6af6ab9": {"doc_hash": "1f6aaf3d2c2ef7be9ae674f154c6b32e8ff412004048f452985a52da624c48c0"}, "19d95850-8684-4093-ac2d-c7d4db626f66": {"doc_hash": "84462b03007dfa544a9936d1b2a3f93ce9e27194272289141a64519b893e83c0"}, "d69cf9d1-e3c3-4a64-8666-a9929c425f97": {"doc_hash": "b6a781fdd93c2ff132fbcffe7fb718b6d7a286a386125e0a42f9f8378cb035ca"}, "55251ba9-1e8e-447d-81fc-9ddf6f66fd9c": {"doc_hash": "919f524429140351f4f0b4c8e7423cb8a87725db881da46837ee280714a0eb32"}, "127ce84c-719d-489e-a1cc-ec0023b7fb41": {"doc_hash": "99b233fc7fcb90a4e0b834168c1ca05e56664724d52a8699a362163a8297a07b"}, "2b5c7e98-453c-4cd5-a436-8804597000b7": {"doc_hash": "d43551c7269ca45db43a699f3c06835f0d52ef10642e6bb4d8b9c673624fd4d6"}, "d9fcba23-6d54-4844-aada-75d373dfc994": {"doc_hash": "58bb135515deabfb3fc77a1f76d23a4a1feceb3d6ad48cc414ce7f7677dbfc31"}, "73f6ac98-d20a-43b7-8ac7-7b35280b7bb3": {"doc_hash": "05dfaa32ab129308b56bca2672d07aa5b5c6b274fc550e403155ec2dbf45a921"}, "46f34564-ba2e-4b64-bdd4-5e80b316fb0b": {"doc_hash": "3e60bbe954057ed150c2ff5a06ab1f5d485c770bf043be96b393066023066ec5"}, "fe92000c-6b6c-4e23-a8e2-48fc992ed62a": {"doc_hash": "d97f16c5414e43f9e223c1d5c51871b710dbcb51c08df20fcbf0d5a6684bcfd8"}, "290d33d1-2795-4809-ae6b-ed770523a435": {"doc_hash": "3a7b647be017e981fc8f159dfd2ae22fd75d4d0cb9f9cdcee4ffdc2eb2ec5d56"}, "c85e02ff-d6f9-48d0-beb1-6fa5e01c785b": {"doc_hash": "65826f5060ba2907d5818a69cf2f5841812b50c16653579acaf76638b7ac96ec"}, "51b18c4a-6a26-4173-a5ce-5e4555842d51": {"doc_hash": "46dc2081e7d38497786e20283d6babc72a2a3fe88de9be98b33de6da4e72b494"}, "44f75753-6a5f-4f89-bf50-200e23cf3644": {"doc_hash": "ee122525db48fd729673c7402c2ab325591bbad337d83abafcaa978064abf427"}, "b65e1569-a524-4fd5-9d6d-6f16c45e7a2b": {"doc_hash": "bac37de03a6962f81edee879bfcf70ffee8b7e4e11fb6a7393e858bbdb2f5b0e"}, "0df19d52-f2b4-433d-8dba-194eaac0bcd3": {"doc_hash": "186ef598d862092025717c4ed175ac38c7b3aaa7d874421225558a96c84ab7a8"}, "7d744064-fca6-4808-a94f-5d0f4b60f93c": {"doc_hash": "1bef806dc7b12d368c3f9a2595e2b4f27ed6087ae58d726a7a8f327f7826716d"}, "41487428-7b98-4c4e-b2aa-db8956dee02e": {"doc_hash": "0385c83df8936c5278594a56ef522e0f8abb7f0fb5df8ba80550e432a435faef"}, "74d5a7d6-5102-4cd9-a7bc-b2f946a04eca": {"doc_hash": "a2fda93715758d8c7a57a7e6c6b5c2a433c5b6b15e4721ceec02e41ca5858313"}, "081dc218-d569-4b78-afcc-03d5ae64639b": {"doc_hash": "0a097741ebbb66e83e8cccce21877cd784eccac804faa908d296b5409aecb250"}, "4b1fe07d-f576-430e-a5a5-5609bac620d8": {"doc_hash": "229f0c30979f226cc88a2bc78f501c190cb61c733662595b12b2d8ee8a6627c5"}, "6f0be2ca-a6e1-48d1-8a09-803b0ac6aadc": {"doc_hash": "06be0ec0eaf6870398e53b70ba5af46c71e5ab8adf6280926618d17ec01fd6fe"}, "5d86632e-4098-49ef-9bce-5941abfaa41b": {"doc_hash": "c2b87c4cdbe72b5224f92fb8cd0a8e85cc84198aeca16a9c9ec698d63042264b"}, "31bab90b-6577-4f76-becd-d41b6ef67ca6": {"doc_hash": "0126f78e5ddb3cde527d3aa3b2e7392bd517964b05da36948e3abeab8d326503"}, "37402106-d5de-44b2-ac3a-42a2c3335742": {"doc_hash": "fbb8f6db6b2111fe654a90d6a21cc684d7ab7a9a28044c6d54114f470ec485ee"}, "35adcf12-65ca-4b3b-8256-37b0e2acd4c0": {"doc_hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc"}, "1b5b985f-d9a6-4348-b481-d8b5751fbaec": {"doc_hash": "8ac9e77dec5f433baeff312dbe70b11481831058415cffd2b6481baa5422ec0e"}, "964ccec2-45cb-479b-994f-2efb4ddd721f": {"doc_hash": "ee1b46fb0a0d5c35b4dff79c7891c87d4c5018c40f8f119a48848ffd7053e69a"}, "9fbda692-108f-459c-908b-7a1e6e7c315b": {"doc_hash": "0c03e96afee360788fc1a8834fa5b2d167f12854fa3c1d2ad8e92fe6e30a79fe"}, "4a87b7d5-cc94-47d2-bbc1-fb14e2720577": {"doc_hash": "1acebc44597a61098e657b86236d27b0ecb7907b0c76fa814c2fc1056333194b"}, "fe550835-8566-4978-9bf7-dda40e2b3bb3": {"doc_hash": "934b068181bed4714dce1ead36486fff47fa486bb9ed212412f66bebbc1baf37"}, "cbc66741-005e-4e68-9e5f-d4861015a063": {"doc_hash": "4067bb9ede87e0f0d5edf053386a337156b5f860fe2d66e83801ce8ade3cb26b"}, "b6b75af7-ba77-46cb-8dcc-4a14b1b4a579": {"doc_hash": "8d66db3a5ec29316eab752df7046cfed2cf459dbce684a383205c3fe4912c437"}, "d7368dbb-4b16-4821-a93b-f086b2ac0bd3": {"doc_hash": "fb175442101f7a83e4dc4c8d599f53ad49c09a51c6fa9f52a94d1cf511e75d50"}, "85991672-7277-4561-809c-642434fefec2": {"doc_hash": "2f5c66a2f23ef022e6da4f9221c2c9acf654935f1bbd0d1846a9692338f0374e"}, "f951dd04-b13e-4a67-8333-0828e8420835": {"doc_hash": "c6a886aa0dc984f195fedd10f2de369a28a9f1efb86adb09dccb1a0a17a20f25"}, "097d0a30-4c1d-4e9a-8e0a-78fa3a40bdaa": {"doc_hash": "64f5ce0c56bb79c6614c168a1e06ebac23a35430ccd2673b4c4e26065f2ed9b7"}, "1246465d-6e2d-4544-b407-84a9a1652ebb": {"doc_hash": "9b48f7c279b351dcb9a8eb6be99e45178b7d3e60a34e25bb410213e020a05d3b"}, "75729f0b-27fe-4751-ad50-dd5a1a24779e": {"doc_hash": "58290c879ae871030af969685b9d34d00f1be07c1ebad24a648d370cb4759cd2"}, "d8a68a8d-8160-4632-85a0-5771f9c59475": {"doc_hash": "061945f3f8cd24bd4ef85e344076873df3bb4e2fc5112fa7308c095e752cc998"}, "cc393f62-a902-44e6-808e-d7f8c16b6c4f": {"doc_hash": "92993a863c3ed02a8b8d2e22c6897e01cc5fba8617e8fd5d12bae737d7a1dfd3"}, "92a62af3-d928-4365-af5b-a53c78692b78": {"doc_hash": "4da5ba8f12ae606be5265956450f5ec6a95709885f4e2e88293f5aad76ee6da6"}, "c0869ce3-4467-4487-a5ec-5e285d44a0ed": {"doc_hash": "c4b8bf66cd78d9f980455246e0cd4ffa088cb32a1112f7c6be9a30c8e3c074d9"}, "b2c519fe-18bb-473f-9883-092ac3946401": {"doc_hash": "c1454e0476eb8b328062e06704bfb1c7e116abcc6c920428d16636036c5b15f6"}, "ecfcc254-dc58-4cdb-a5a9-25a68d1cc523": {"doc_hash": "7aa83c543aa3e905e322744ae40630cd29872390c3af47f4bd1075c08abaed84"}, "8e26dbcb-1217-41a6-9162-602b56510796": {"doc_hash": "5a7a2118b85d3009a33dadfb58b465dafeda67c2d5961933e66f5638cda4c694"}, "f1d5aeb3-c4c4-41df-a150-c313644ee1ae": {"doc_hash": "624c4d9137a8bd276d91293209223d573a2c3bc036c30a3c9d08110dd2b1fa33"}, "d3ad39fa-e5c4-420a-bfe6-1d890e2db942": {"doc_hash": "c4fd35fa6d84d9abc54df8e85321b723b6ccbe208e87b9758267fa408980fb74"}, "e34da759-c53d-4e2e-8e45-ad9067ea6b07": {"doc_hash": "75a5b25b7f059b74d2f803b2bcc5104777b5710f0ae06ab3a13beb7e8559b58e"}, "22c61995-6340-4dc5-a3f8-9e39b1234199": {"doc_hash": "211f2450bf62d38b5bb67ae9d2857cba72e3b7364aa4245774a62ce6fa3c2023"}, "f4ffa069-fe6c-44d8-9c11-c97461e72a94": {"doc_hash": "0b57db45b619e13a55d5f0fd0941a49c3685c74304a3296a4fce7e4f358f5d7c"}, "424a0381-3eba-4d92-8a5c-e471aedd99cc": {"doc_hash": "cf6156d5da4d2934462d63fc4c18672283366d4dd882822583971d710d48c105"}, "f133e66e-f4aa-4101-a91d-77e6716d0b94": {"doc_hash": "7348f6811a29b9ce664f6ad7313ca79677b918dfac4ac37b8e74ea9d91ad77bc"}, "e33c8ee3-0f3c-40c2-9ec1-709a23f9b896": {"doc_hash": "27c647cad3a1572ad040f82bb60ae8e2939b2a8ec376f91ba1ae5539a481090d"}, "6ab9d980-17e1-4252-afb4-078c8c8b7a47": {"doc_hash": "d72b1f102122353b23ea3dba495baf6e92f1b85e6fc5f64a1994ad7500609e34"}, "d865c805-9711-4514-9cf6-da476d623707": {"doc_hash": "023eb861f9f02efd13fdebe3b1f3f322cb8ec40c6ede5fa98a19ab413a802b63"}, "c90da06b-2d2d-4da1-a71b-54219707de7e": {"doc_hash": "f3f8521024535ec21816e941baf23efc8451135e7b64ce7d8ee77a7d02e42571"}, "c11b7070-63c7-4487-801f-b89d4cab6f3b": {"doc_hash": "5e85830fcb4460a2bd6b07f1407562bc064ed67e0fc948094880136f5cb6c355"}, "3c2aa291-1b69-43f9-b095-0f6ec4d4ee0d": {"doc_hash": "c33eca0fcc6add684a69e513e91297bcd81a12047f462c6034062fcd5208833b"}, "26543aee-e965-4abc-a05d-c989100253ff": {"doc_hash": "0ea93346b818e89cec3693cdf3345f8a7a46c1320a4141a2c8b8eed957c3f7dc"}, "1191fc05-c2d6-46fb-a40e-a81d11a95925": {"doc_hash": "ab6d64f2e49487058bebcdefecac414484d7aa05073932e8b9b6eba68213453c"}, "1f1b73a8-4a72-476a-83b3-25cbf32febde": {"doc_hash": "4a6ef79bdc89be8ad5c7e46477b814a7e6ff50b72d40e31c0cea66f8ec3cb6a4"}, "4ac14750-0d08-4951-8f28-d995dbcba3f4": {"doc_hash": "f800961ecf960806719f02b9ea69c1d8aa0b32b650197f9d97c43ba2d88cfae2"}, "34e1d986-db11-4c4f-8889-68d952cc3892": {"doc_hash": "424ad304fc798d9af4422bc29828e88aea466b6936ae394731cf4185f151e13d"}, "ab60f9b5-6375-4c31-a59b-b11006df4710": {"doc_hash": "8367efae6dc4d7cef8c19a4921bc3476e82d70f97b99e8afa48fae05414a6d58"}, "9c3b73f3-b95f-4239-a4e3-a33368aa8338": {"doc_hash": "43271b9c46b8b2ef72996a31999946880f1dc3417c3efc9074c69c4f170f5eb1"}, "f8ca75fb-fdee-4bfa-8228-3c57d2058e06": {"doc_hash": "c9d9c10e2f6f5ac722cbd10410965d0e8db66a8a0576124baf6f804760f9cc3b"}, "6e46d55e-fc88-4337-873e-5873ebbd5d54": {"doc_hash": "096d53b0f5965490da5f80e547952532341aae372b05c1d5d54ab83342dd202a"}, "1f581f06-c46f-4585-bec3-f839e5343a0d": {"doc_hash": "8781289d1c74f19bee149ddfb6f086fdb99dac985be4c2d2e004f05287e908b1"}, "be32d9b0-fb6e-40a4-be5d-7d81e285062e": {"doc_hash": "f1f507e7890b54e1aaa9bc9e280e633987e668143234cf23dbcf192b9df6f479"}, "f57dd00b-449d-41bb-af66-7b2bbff74d00": {"doc_hash": "f97c871f4a0de1c68558df838f387eaa2af8780224323ed0e15225d0c5add916"}, "93175af7-25b4-4800-abab-f0827d0f9f29": {"doc_hash": "a5c4824fd7c9ce47c49fb325bf18518ee3bdb09de9700f2177ef3e65a130ccd1"}, "6438614d-0a54-4fd4-b0a9-0065fa40a2e1": {"doc_hash": "aeb0614d0329b2aea224fc6eb7b4dfb4c191a7e5d8f760b311bd3f09912c2771"}, "dfbd86a8-4fb6-49dc-b7b5-5f7bf0044228": {"doc_hash": "93fc36583d7c3243aadb2488d10baeccfcae79f1bd91f1421e8ca9b81aff0cc9", "ref_doc_id": "306c842f-5201-4c69-bde4-51e163f6a8d3"}, "1b1a973e-8068-4232-80f2-27d58c512046": {"doc_hash": "b8582cbe303b26472826dc6871ba6a26e79714cd00cf22f6f26093234f618e53", "ref_doc_id": "1c4638a3-09fe-4150-82da-08358b86148d"}, "aa9b9ad1-a17f-403e-ae50-4edaa65e1699": {"doc_hash": "32b188524503dd88c574e87010acb1e4538ee655a03b8f35051f014bcf2ba66d", "ref_doc_id": "1c4638a3-09fe-4150-82da-08358b86148d"}, "97d85b92-91d7-4b08-b9b3-17b801c27740": {"doc_hash": "7d83811c88e2022563aabc8e502bce850720094d6f7ee06077dc7cc15b20e695", "ref_doc_id": "4d845450-949d-4be6-a703-0fb85bddcc88"}, "f1f0289d-46b1-42b0-b194-03028f8d6844": {"doc_hash": "26d565ecc59143fd0dbea33b487b232ee10425a85442357b26943fc7bd210f13", "ref_doc_id": "4d845450-949d-4be6-a703-0fb85bddcc88"}, "e70d5e86-7c5c-4c13-ac53-f84b4be6dc72": {"doc_hash": "ec66ab2d148102a1c3aae1bf622326c5fe8842fdbb67c77547feafdb9d98022e", "ref_doc_id": "4858a74b-9d67-419d-875c-ebc25cb4769d"}, "d1d408ab-8634-4fd0-af31-95bcb98d370a": {"doc_hash": "88516eab6f9094e03a4f7f72ec045561a9b70078200cfa2b52d5f7536acc2f18", "ref_doc_id": "4858a74b-9d67-419d-875c-ebc25cb4769d"}, "50eeb5d3-27b1-40e8-929f-c86167b8f268": {"doc_hash": "3458ee54750f9bf72bca5830074974f2ecc72993cc4cb5c84b33c1de4f609b66", "ref_doc_id": "99baeb5b-9dde-474b-9e5b-5491b3377a67"}, "19d1a6e7-c1d7-4d61-ad31-1ac9dc7ebb93": {"doc_hash": "12b224d9b75b41226f947c72aa5cc658d5a43906079c2bff104c180de13d89ba", "ref_doc_id": "99baeb5b-9dde-474b-9e5b-5491b3377a67"}, "d876b1b9-7372-4a6a-9d6b-21aa0b6862c5": {"doc_hash": "5a66684b834e86f69446da132a273d4c4cfe2ae49330f1980d7acb628f88374b", "ref_doc_id": "4626d870-ea0c-443c-a936-f1eaa3f26a07"}, "c9534790-348b-4e19-8ccc-e82228affa98": {"doc_hash": "60fadd49e412395d50ec0f9bfe6b44a1ee9cde2ac877002fd96139e77e822c9d", "ref_doc_id": "4626d870-ea0c-443c-a936-f1eaa3f26a07"}, "9fbbaab6-57d1-44d2-a124-522b61c76309": {"doc_hash": "ec10328886f74f504c062354c6778be5c60a57ccff403c23b28ae52f7c27bd75", "ref_doc_id": "91e7a275-998f-4f10-9179-50adf7b60c7f"}, "dbc7c63e-5ef5-490e-acc5-fd2f6c0ebf5b": {"doc_hash": "f79f40646a2ae0ea9a9ab7a506309ae40c4e391f5471d4499e99070ed2e442ec", "ref_doc_id": "91e7a275-998f-4f10-9179-50adf7b60c7f"}, "706cd728-283b-4a79-bd2c-b3e9aed8768e": {"doc_hash": "9460b32339598bdc118b1ef51ec20b280614b846536f96adb08aa3da2fc46737", "ref_doc_id": "8a6ce8f6-7443-4759-892a-29effe63279e"}, "afae5beb-35b5-481b-a8a1-c4fed277fa88": {"doc_hash": "148d94b5c750eaff83364835e9a481a91e6b322dce0381cc0803319c15598dd1", "ref_doc_id": "4542d1f1-69f1-4c7b-9b9a-3ff526452567"}, "677b9409-3a7f-4435-bd3f-d5722301a839": {"doc_hash": "94ecd2066b831a3bcebf02f6e6211ed0e3068c8f4f09e79210efd970ae87d1c0", "ref_doc_id": "4542d1f1-69f1-4c7b-9b9a-3ff526452567"}, "7d37cc2a-905e-4764-9351-721059585ca3": {"doc_hash": "d8339f84dc77eaebc4b12d6ee160b5cf259bb9d09cb262a4f90da3f3c767a7ac", "ref_doc_id": "27f4fe09-6d1a-43b7-80d6-1141e649d833"}, "2105cbdb-2810-49a6-90bc-5ffda4b6f1db": {"doc_hash": "e5e68c3c6cd485364833945a6913ea1245fd880124a4be5682c90523013085bd", "ref_doc_id": "f8770bec-e174-49fe-b9f3-2e84fdcd9c29"}, "03723e5a-4b45-4c95-9d7e-8f4ba2a7a0f1": {"doc_hash": "cde794ccf3074e34b8e60f1b9ee13c7dcb0ce3a2c3ff3ac1da0e6b9ca70524ac", "ref_doc_id": "f8770bec-e174-49fe-b9f3-2e84fdcd9c29"}, "794b0645-5371-4c56-a4ae-cfa78c17c149": {"doc_hash": "ce00060813db9f1bfb3dd9802699aa40daf1620c814a5a1ef90a68b1ceccb01c", "ref_doc_id": "38c8a6e0-b8eb-4c17-b531-cf82f11fea38"}, "24931f73-8ee5-4e4f-9433-f1d7bb5cff63": {"doc_hash": "88da0c672923070a722f481e980567fa70cf5d8a5cfca65b33964323545ec5da", "ref_doc_id": "38c8a6e0-b8eb-4c17-b531-cf82f11fea38"}, "c2ba4600-ec5a-4568-b071-65b21b9e7596": {"doc_hash": "530196e30b5a8f21be09a2aa85a46be0baf692df4d7bf6cee7b9e4f909fc4229", "ref_doc_id": "f4452e4d-eadb-49ff-9224-5d250d10655b"}, "dfaee86c-16e2-41b9-a151-fe5f9475a467": {"doc_hash": "4e850983d59ec4e4617960ad5b4a7d564d31e05d1688259e073abc54139d6235", "ref_doc_id": "231bed0b-b454-4b9b-9e4d-dc51752a53b7"}, "adf07e33-c916-4d04-a6c2-26dbfa740520": {"doc_hash": "d39296b145cab687e2d52289d3415260d0070541feba3ac626010192c2a0fe8c", "ref_doc_id": "4141474b-5fb3-40f8-aa98-3e6384c57222"}, "c67b9127-6fd6-4e4f-9d5a-f86626a1eb62": {"doc_hash": "f1299110e32567d6b3333954f7a7cab26b1a3b659c1c424483e8b92c80bb5624", "ref_doc_id": "9b7f80e0-10cd-437f-a08e-c3b127dd1538"}, "c54351a0-be42-451e-90ca-b2fae7aea8e6": {"doc_hash": "b49e2b0fea5cd826b611f70a168ae1b259694c128bc9c8e28f78b176b02a9090", "ref_doc_id": "06d088b8-5eda-4b9e-ac62-da838d762942"}, "00cad084-ca58-4bfe-84ff-a039e455c537": {"doc_hash": "4f212090c23431b13ae7b2faa06101d10e753f65c0831afa322c0829e75cfed7", "ref_doc_id": "138366aa-9edc-4d4c-b373-19046c971eba"}, "3b50e938-f100-4101-a4a9-420338a87ae2": {"doc_hash": "f4501800084638b53312b2f1ac563ee00ab9504728f456c76de23064f1d73c77", "ref_doc_id": "138366aa-9edc-4d4c-b373-19046c971eba"}, "b066a024-a9e8-4584-bc9b-2d4aad98ea5e": {"doc_hash": "c6c5809e67a4d30130c434676ef99f7e8b0303c8294201ffa77799883962d7b5", "ref_doc_id": "8b4cc148-ac40-4b06-9f69-f4df8a60dd9d"}, "cfdcce89-47f0-4376-b11f-d27a9333a05e": {"doc_hash": "917b3fdabcb5e088fefd82b0c258f5a28fd7079f3c1707b6d8b5965e4312490d", "ref_doc_id": "39624053-dde6-4ea9-852c-443830e4ecf8"}, "40e857e9-7e1f-4237-bf85-5ebe11e153ca": {"doc_hash": "a6a5469cc8802a2e22573ce5537684ff6e1830be625574f577e9ea01f21190f7", "ref_doc_id": "2294b0a3-849d-45fb-a53b-2f76c832eee7"}, "6a4c5027-2aa1-4f19-aff8-ce1d6b46c1c4": {"doc_hash": "a856bf0eb6f4a37ea8ad18aa4722d93bf40ada1f629fac01c5ee92fee1d44512", "ref_doc_id": "5f6bde8d-81bb-4b6c-91b2-e8d863479e7d"}, "6c1c0966-944e-4b54-8827-f9ebd2ee5b1a": {"doc_hash": "7d8419fc837791ec023b3e7439830e7f89cf683e876cda46bcd5a8a46ab9222a", "ref_doc_id": "e04f264f-2e84-4943-a290-1fa8f433aeab"}, "5c67474e-eb78-4b68-b42b-2205f2906233": {"doc_hash": "2155f7c72605feaf249ae7edbb06a393298aee6b71b7d26d16c74078f124801b", "ref_doc_id": "c439521c-3007-4df2-8c24-b6059ff2cd3c"}, "aa8f3238-53ac-4bee-aaac-64ea9357eeb7": {"doc_hash": "29eb890857abb87299f85fcddcb556da8dbd96499ed6e2fce08d74e386b88a81", "ref_doc_id": "bed7d6f8-9979-401e-99db-b997782f4e41"}, "e78b5b2d-cd55-4f01-834c-7a9a9268a726": {"doc_hash": "7230b41acb70d0523813ab346946c94f509144dca8f13749967ccebb346143d7", "ref_doc_id": "e6a668b1-4ef3-41cc-8a07-4ba5588f5fe4"}, "1960534a-f49e-4367-bf42-5e927a959d3f": {"doc_hash": "e59c31318e583a2f0bbf4afa8b32879cc3d7d71173f82a17b9b2b83219f6e3f5", "ref_doc_id": "b77532fb-90c3-4db7-9740-9e258c503842"}, "48666463-7577-42fb-9f71-7a116602dacf": {"doc_hash": "6ac1de41b4cf0227be5cc5b59e3c2c27c66add979366afc49a7c69f8df485f61", "ref_doc_id": "03c5d1b1-39ca-48d9-b7a4-0f43551df237"}, "88c29aaf-290a-40df-bbfd-f69ba959fbed": {"doc_hash": "6b98c05aa1ba7a63478373f812afbe59beb9ea75fb339e324440caa3011f93e0", "ref_doc_id": "d706acd9-cfd8-47a6-99a9-b25a633422dc"}, "a5c267d1-2c12-41bd-b506-2c6e2b6af6cf": {"doc_hash": "04d06b69c97794c7291ec23be75d21c9fc9066d99b59ca1902879705ee89450f", "ref_doc_id": "0402fbb8-a671-40ea-b1b9-a9c000f70e84"}, "02d4e6fa-f78b-46b0-8c6f-7ee896e8de99": {"doc_hash": "06b67283d0c2dfaac55f89bfa7a735aff2a0733fe8a5df42ffa1b9d2387b4d9d", "ref_doc_id": "5a51cfb0-1f89-439b-84a4-a4cdeb44c35e"}, "f7df1e37-c117-445a-a4d3-c6150193f826": {"doc_hash": "5fabd387ce6986400b9cdc68ff7b1cfea5abfdfdb4ec9895734fc3b2a92dda7e", "ref_doc_id": "7b6b7623-3e92-486a-9246-e55bf4f57213"}, "677825e8-bf11-4653-bcfe-89b1a4c84e2a": {"doc_hash": "0cb7f45a3b411bf319bcaeff582d17f742d5d13a323329f240fdd6edbc1d0f81", "ref_doc_id": "5cfcc56d-1f80-4d9b-bfbc-7abf798b6e90"}, "9a544d0d-caee-4b41-b4a2-5a91456e5c66": {"doc_hash": "f66dc4cb59bccc330e7ff31aecd3919fe8e3c53ac2dc8aa202b03d80b841daf7", "ref_doc_id": "2cc29683-f6a1-4dd5-9462-306396fa44fb"}, "b182bec2-78e4-444e-8551-a20649c3c428": {"doc_hash": "74407abcb14d450f1378789fb3ee7fc92e3f616796137da0f70956515ebeee59", "ref_doc_id": "667d0d09-2139-45ed-80e1-35b4bd002b26"}, "47cb69ab-2357-4a7c-960b-1520f23edb67": {"doc_hash": "09e20099e36dca48682ee9db5e5fdb8162cf6e75a81eb59947de91b784914fe9", "ref_doc_id": "1431b9b7-8bd3-48b5-9831-67496411bc5b"}, "b551233f-62c0-4a48-a446-89b57e3c8512": {"doc_hash": "d8ad781421888edd81d6ac3cefa450523e93c9b3477bb89cccf90b952b06a43c", "ref_doc_id": "1431b9b7-8bd3-48b5-9831-67496411bc5b"}, "5815851b-147e-4713-8847-1e408c7dcaa0": {"doc_hash": "8b58f84cc98f41b204ec2a52fff5fa20d9adabdef719f45c058f199293aca36d", "ref_doc_id": "914b30a5-3038-470e-9154-5085e03e47da"}, "b16e11da-f73b-4e92-9a80-2fca1e986d78": {"doc_hash": "eb33d0c59d08fde2243e5991746c1530525d960929c90310bd1fa8786a1d49df", "ref_doc_id": "ac9d831f-0f21-4599-998d-b88c04e19f60"}, "ebaa0d0a-17c3-41e2-ba9b-4be7a1dd5ef7": {"doc_hash": "ec778be36f9741463e2ec110091a12339b0c5b2b313badfcd0e70f4d35691edd", "ref_doc_id": "bbb97f66-b566-4573-afc2-3bce0fa780e8"}, "bc5698c3-478c-42d7-93fc-4c756ba3126e": {"doc_hash": "71a7fe8bb09f9294fe5d72e9e943e5c723ab4eb48259e82bdde8a800b14a2223", "ref_doc_id": "b78b1b71-1564-4d20-aeb1-de79e1028204"}, "5de5f5e8-d376-4b8d-b85c-c45b81ebff82": {"doc_hash": "5d6bf31779080515db79d8177a62cfa00b166185248109f5f69e522b18ec344a", "ref_doc_id": "2481ee02-d949-4574-9175-9f9a58fecf88"}, "210a3a30-78af-428f-b853-f849a6b31f7d": {"doc_hash": "8815575c475f7697d933b4462ad6b3351029c2511abca4afc3556217cfa957f4", "ref_doc_id": "7735730c-373c-4966-b578-a1aba8ffb625"}, "eb26cbc3-ecc8-4d1d-a2f4-cea75a067825": {"doc_hash": "48134be1933aa1e88f310e20c3752d34ddd5f82e17e9ce500fb95a911b6d1afb", "ref_doc_id": "0bd10107-94d6-49b0-8764-6d1666bd7262"}, "fb31a447-ac53-4320-bc30-6985062b6212": {"doc_hash": "17eaa537555f7db2ad4b7c68a55ffad7383c00fd5c854e4caaa03ba227ccb403", "ref_doc_id": "984147b1-2e2e-4b79-9307-d44f5882b081"}, "60b2b8bf-e55c-4700-9d71-b84900a372e7": {"doc_hash": "e8ef10fbe8636c3ca875876130e5f36deb6b8b8b21b32a1f54430d05ef38f47c", "ref_doc_id": "4df5ac04-29ee-4501-83b6-fb56e12e3315"}, "656ad568-d8bc-4fa3-9167-16f2afee1e53": {"doc_hash": "17174fbbe59c3813982e7b13e269c4d59a1f6192b0627056eeb9d70bac260b22", "ref_doc_id": "4df5ac04-29ee-4501-83b6-fb56e12e3315"}, "42412901-bf61-49f2-8a6e-b65c7bb6fa8d": {"doc_hash": "41f6103843a195a4c598fdab045f7e973f62608fba31d9ad25a9f6bd1aa819ae", "ref_doc_id": "cd407ef0-0a81-49f4-adb6-70ff92f93364"}, "58cbd730-fac9-421b-9354-0c1d4a6d051c": {"doc_hash": "1d713ecf833791b6a269aa017f1de6d22b6c03f116b2ab0e12d27d6db3369d6e", "ref_doc_id": "cd407ef0-0a81-49f4-adb6-70ff92f93364"}, "194f391d-7bf1-4e44-be08-b20ff682ee8e": {"doc_hash": "758e789c17d32922f67114c6328de92ea6fc68ef56a7a4a720d8f0710c2343ac", "ref_doc_id": "1e237025-f9ab-488f-9524-e65599a2abf3"}, "48253f1f-a0e0-4bba-8b69-c512d0eafa35": {"doc_hash": "aec41271e4dfab27b173eca3456ce5e2f0ee871139846663c02b5b605fa4b81e", "ref_doc_id": "1e237025-f9ab-488f-9524-e65599a2abf3"}, "dbbc16f4-eccd-48ac-8ee3-259a52bc8041": {"doc_hash": "b760caae1bd5485fbb50c9ee94127bf0a2cfd614b2ab2394fcc2eb76c69dabf2", "ref_doc_id": "f87f2543-fba7-4ac7-8708-74fe89a048bd"}, "f7136211-c76a-4c72-85d4-ecb19ccc8b67": {"doc_hash": "451a1c3dcbd6c43ee2d5ddfa3826d200fa3bf535c73c284175b09e46cb1521b4", "ref_doc_id": "f87f2543-fba7-4ac7-8708-74fe89a048bd"}, "85926df1-0928-4352-adc6-43eb8ba6716f": {"doc_hash": "a772fbcc5a70d8807d37868c060b93cac67caa8c170d28868532db4bda54b238", "ref_doc_id": "7ee731ee-b2d7-417c-b7a2-6e3b3a714f03"}, "980a7cf9-d1dd-45a8-a546-508ebcdf4708": {"doc_hash": "9b405d5a453d3104be288577d0b9b1d93074c57a8e0ebc663bccd714357d5e4b", "ref_doc_id": "7ee731ee-b2d7-417c-b7a2-6e3b3a714f03"}, "0d2a969e-fc02-4aea-9f3c-f3f63450c235": {"doc_hash": "d6f9f25773c27a1d77af92f91ff58652665984846bd0c2d0ab524e2ed65c460f", "ref_doc_id": "8cad4573-a53f-4798-b58c-f0b612fd25b8"}, "2604b6be-5406-4f8d-84b2-7d51607a4fff": {"doc_hash": "7dbea140d854a8bc73715191647da4ac78758edd31ae69a36fc5d818e6fc5183", "ref_doc_id": "8cad4573-a53f-4798-b58c-f0b612fd25b8"}, "1b6c849b-9d4b-480b-bb94-97f9cf69a3fe": {"doc_hash": "41a105eadf19dbb89904afa088b20d4581a5d0848381f94b6175c9256cfded7f", "ref_doc_id": "a8380bcd-c579-4529-ab37-a2c9f5646886"}, "7c3b5900-8415-435e-a053-c8ddbb3b3fa3": {"doc_hash": "9f42e44194624b3ff5370e8fc1c17145c9afa3af0291603ddb103c79ebc26fd4", "ref_doc_id": "a8380bcd-c579-4529-ab37-a2c9f5646886"}, "6ddf4609-c938-4158-a79c-8ef107aca165": {"doc_hash": "7d15ae64d7dfe15878ac684588bebb1be79963216b090708342f05c7b5fbb265", "ref_doc_id": "6da4689f-7c37-45fe-8110-a8d6af0f0c09"}, "37557ac5-fd20-4d3d-a8bf-5278ffbcdadd": {"doc_hash": "80f1feefed06c1b386aa9b01899e0660afd621d0633ea22188b3bc6b08be1c33", "ref_doc_id": "6da4689f-7c37-45fe-8110-a8d6af0f0c09"}, "6ba8b328-5a4a-4bf1-af1b-7e47e1839e2e": {"doc_hash": "f76dfa6985f37c42d66742124898ad9af4ad0235f33e2d7f989fdaa76adcd5b2", "ref_doc_id": "f5d845b4-39c7-4b19-b4f7-77c73d3eaa08"}, "ebe1f4b2-1308-48ea-9816-f02cfdb7c340": {"doc_hash": "ddfc0d20a9b09049ad9dd31280f66aadec40725aa76f7d4f470f9d4ac12842b6", "ref_doc_id": "f5d845b4-39c7-4b19-b4f7-77c73d3eaa08"}, "32e65b58-2933-479d-9bf9-20c918fd0e98": {"doc_hash": "409cf5cd7f70fa86a552645660fbb7ad996fb3298fe4626a576730a1b7ca21da", "ref_doc_id": "8c2363f4-e830-4e7e-8505-f935c9e48962"}, "b035c99b-ed55-42eb-9234-c69e7ec4edae": {"doc_hash": "ba85c2d569605c348fa84ee3d2f48d16577d1a26eafc614b7c402fdc3c936750", "ref_doc_id": "8c2363f4-e830-4e7e-8505-f935c9e48962"}, "b48dd126-66bd-4a60-92fd-2706d071597c": {"doc_hash": "361a15d902a2b17b3a7eee9b363af2b9c044fe3d6eef56c3e1af59670f98a05e", "ref_doc_id": "2221a159-a76a-4119-90b6-b457ec229b33"}, "a7c1efb5-62ea-48a4-a27c-9bc9804dc05d": {"doc_hash": "3129301362e125c617d6538c1c25d718ef12d05cb264a6ff9a68243a5a29a8e8", "ref_doc_id": "2221a159-a76a-4119-90b6-b457ec229b33"}, "a71b6d0d-65e6-48c4-982d-94d3b8fcff83": {"doc_hash": "ab1eb6045822aee26ca5495475bbddab6eee2f4b45038a62a27c37d0691ad9e2", "ref_doc_id": "d6d03a67-9728-4ea9-b55b-ddda143197e8"}, "442bcb98-f995-4117-b069-83cebfec5df8": {"doc_hash": "381a20099f2a6e5bfc42cb5fd9a471856e3406f0893ae3761611d4bd84fd18dc", "ref_doc_id": "d6d03a67-9728-4ea9-b55b-ddda143197e8"}, "ca8b3601-21e2-49f3-aa52-371621e9e4b1": {"doc_hash": "a966e9c8de96f697fc9fae2a12fd8a7b543eea1404064b0a0dbb5c90e6858ab9", "ref_doc_id": "274f6e9a-bb3b-42f6-9eb3-eb72e83f3e96"}, "ba5d3d58-35e5-49d6-84ae-3f5cca2c346b": {"doc_hash": "6117024a4e204fe4cb3f6bcecce8446cd6d10fc17177235961196dd688177b33", "ref_doc_id": "274f6e9a-bb3b-42f6-9eb3-eb72e83f3e96"}, "bf6f85cf-e0ad-494d-a582-68a4ebc210db": {"doc_hash": "ed68ee878fdabd4c068855f39f12e039f95a7ef9307b43af992ed7b5be51859b", "ref_doc_id": "ea9614f7-ab93-4cf1-af04-549f91502c03"}, "6032904a-65bb-4a63-ba10-0cdb28f1e817": {"doc_hash": "f82195b1e444fad171dd5fa0f29590e57cab5e339f0bd825c3a0c28dcab19262", "ref_doc_id": "ea9614f7-ab93-4cf1-af04-549f91502c03"}, "864e301d-713f-4867-95a5-a0f6e9ee2020": {"doc_hash": "ae8cf0827f6a89d6ee6947ec13ee40a2b067924b5017bfcdbc764d63b19f60b5", "ref_doc_id": "8daebd54-b463-4982-a1d9-1c7d1cf947f0"}, "bebb5bd7-84ee-4517-a77b-5f8cd1773225": {"doc_hash": "2aa50bb88ba7cbc1511c756eecbf94b17707f02629d9c0a12a802b9ad39c4eae", "ref_doc_id": "8daebd54-b463-4982-a1d9-1c7d1cf947f0"}, "753f7fae-6a91-449d-8c0d-eeb6403b6166": {"doc_hash": "7a6a3759f3bc0f9e8bfe324172c064e6ee0e508dcfdd8e53bde19c8761f37ffd", "ref_doc_id": "323ca365-962f-4ade-9206-f8d36b1f015e"}, "c96dc8df-5a3f-46b8-999c-f462deb80a0a": {"doc_hash": "81c3edcc8d58f0b65b8f65aa6d7923ed834a839b06143ea9ff70d543bdcad21a", "ref_doc_id": "cd6b915b-9782-4c2d-bcf7-8cde5e59aec6"}, "3bc35726-2f44-4194-9b34-ce4fdcf862e2": {"doc_hash": "c356428d2ae75fe5c0f87e13fa7566869763844f3a15fd0b5a7569bb778c4e46", "ref_doc_id": "cd6b915b-9782-4c2d-bcf7-8cde5e59aec6"}, "7b135669-519e-4e2d-9cf4-cde3630957ca": {"doc_hash": "136f8d6c41a6650674ba6f54939c5e5aeff8657b7940e421414f993d0322c532", "ref_doc_id": "cf712e6d-2c81-4649-9a80-adeaa87cc907"}, "157efbf8-57ff-4a00-a7c5-3e8e30d49ac9": {"doc_hash": "8fe151582d6733f56c7ba92d3ed74ef2f721c03af8e8eb3db8ad3e4fba44982c", "ref_doc_id": "2d9e2197-d5f3-4c00-bde0-b01b92c9ed46"}, "6b669c2e-231d-4741-b03e-3e8944fda8b4": {"doc_hash": "8cd0ced94c3b509c1392426f2545a2ce7807ed530ec1d2c70d557084a1f32c48", "ref_doc_id": "2d9e2197-d5f3-4c00-bde0-b01b92c9ed46"}, "257ac7af-4f9c-4a66-945c-09eda6737d81": {"doc_hash": "331962fdfa0123ae343d232e1b1ce63974df4dcaa8fe73e09ec2b1896d93c82e", "ref_doc_id": "4b7ee8e3-6316-414f-a92f-6e308c079892"}, "e4ca3a22-fd73-4983-b6ab-727580b982a5": {"doc_hash": "2f0413bd7ef95507d24b94c089f3e9a9bc4824353a011ae132f4644e7603a3fe", "ref_doc_id": "11673d4b-a2ae-45f7-8a68-196580f007e8"}, "7709badd-9fa9-405b-8c02-9363a1e02f0f": {"doc_hash": "a1681206d317d8304375103c13d482d5401e1ba28520c036865f6f1fceeda5b7", "ref_doc_id": "3ea5e683-6a8d-4e5b-9ba6-824b60f08dfc"}, "21274e41-79a6-4097-9d76-8904db3110d0": {"doc_hash": "05700f5c7670bc99a14553a4b5f936a70166a492da1ca7cc4114091650951939", "ref_doc_id": "41d884ed-0515-4ce6-971f-748c36f2619f"}, "c3dd24e6-10cd-4e9c-9993-3dc2149dc6d5": {"doc_hash": "e820a3949009a3969f50112b9746a6072c252ec74a42a8bcded5063a0f2388ff", "ref_doc_id": "41d884ed-0515-4ce6-971f-748c36f2619f"}, "4ee37cd5-d04f-4630-9dcc-6d686857cc43": {"doc_hash": "d2eeae53653aac16af2740c2bc460a9f0cca0cb5f7274055e6d1a69ca6019b5d", "ref_doc_id": "41d884ed-0515-4ce6-971f-748c36f2619f"}, "8f80a1fa-a918-4809-bff7-92d2963f9c64": {"doc_hash": "035afe8810d785c7b574e1073f26c740ab1819a95fff7b51722949655b481a17", "ref_doc_id": "41d884ed-0515-4ce6-971f-748c36f2619f"}, "698fd051-49a7-462f-a8c3-9001653f41d2": {"doc_hash": "568b604ceeac675fc3438de858855d90f1a801bd23b5089b0f8b141beba58b70", "ref_doc_id": "a4dd0e45-3619-465c-9e70-09f5a399d0dc"}, "dcfda0f0-b0ba-4d04-a510-44c8a8ace777": {"doc_hash": "9d2e9083462125f5720d8da5e2a42f26e4a18155fab494228e54e958eb07b440", "ref_doc_id": "52d2dbb2-3c59-41e4-95f7-270c6048e0e4"}, "1d60c481-3056-4152-88a0-347c42d85629": {"doc_hash": "9bcea0a9ba0d490ff069bc05453f33bc885ae9083a6b6792f1f49b3461743775", "ref_doc_id": "2ae393ab-75a5-47b0-8ae2-798daeed29a2"}, "08f5b37a-d8b7-4b6f-9cdc-052afc6386b7": {"doc_hash": "ff9d2af23b4bef9c3c84e77e1dbeba91c3cf53751450940954913582f6fd424a", "ref_doc_id": "2ae393ab-75a5-47b0-8ae2-798daeed29a2"}, "d616ae66-39fd-46ac-8eae-773d00550042": {"doc_hash": "93bcdbedee426efe628cde152e7c2a021ad75749a15ff9e5f680e84eea9da7c9", "ref_doc_id": "b84781e5-6e46-4cd3-b5e4-c8ab50759d3a"}, "a50f9194-18ec-4267-8ed4-1056909b6157": {"doc_hash": "506d0c10122b8a627e4a2594282e364bb00dc630dcbdae2955348bcaedffdb15", "ref_doc_id": "2e153496-6d8e-47d5-a72f-3175d9336797"}, "5ad3a187-f6f7-42ed-acda-dc20e80753ee": {"doc_hash": "7a6f39a68c189fbead96c23fa2510bc694c950b43b9f316e1a260a9b1a995508", "ref_doc_id": "f37351e4-d2ae-43df-bfb4-c94794026b36"}, "202d9e73-2249-48b6-99e9-ae1e15070f22": {"doc_hash": "03671863c6e7cffd1875c4204fb2a6d009c8b46ff35134efafba744d36825cf5", "ref_doc_id": "d048a513-38aa-4ef4-944f-de2813d6ba70"}, "c57a8f3b-72d7-4606-aed8-01f1b89c1a31": {"doc_hash": "2c4c68394901e0c051d67ddcc49198b4fd9a271db36608ec41b9f23d9a84d36d", "ref_doc_id": "d048a513-38aa-4ef4-944f-de2813d6ba70"}, "79c7a385-fb6d-432d-8edf-3bd266c7760e": {"doc_hash": "55caee39dd4188f7f4d2207c659e314d1693d9f05cc06d5bd41884611a753a4d", "ref_doc_id": "4201f27a-ef95-4b58-a338-4d40632ce134"}, "663ece0b-ad9e-4c76-a9a5-cb2022eb1ca5": {"doc_hash": "c8cb860ad5a76cd1930072498833a93dccdbeb2ed978ef9a86ced6f7b4d829e6", "ref_doc_id": "4201f27a-ef95-4b58-a338-4d40632ce134"}, "69cb139c-0a9d-4185-88d8-ee805a5c84be": {"doc_hash": "c9027093711136c4646c0fd98f6611a16798a2af371c13cbb6206ae55a7418fd", "ref_doc_id": "4e369e70-bb2b-411a-8110-fca7beb518d2"}, "518949a8-0eb2-460e-b260-1a8c782d9218": {"doc_hash": "45e713729029616c182cbc221f11bb4ff119207777c05afb4e66ef8abb563a71", "ref_doc_id": "e1d11c65-d73c-4faf-a3c9-9e37af943cc2"}, "59f3691a-52cd-4544-8daa-1df5182376d9": {"doc_hash": "031e51ae468361b51ec700475902bdae7de5d4458b92d7c8ed6d549abe535c62", "ref_doc_id": "846b0ffc-7719-407a-9f67-a9f28b480b08"}, "45ebd30c-ff6e-4eb0-ab99-e22bb996115b": {"doc_hash": "3896c82aa8b7dc38f5abadad32cc94702e881d649402d9810ae122c8bb8fbb16", "ref_doc_id": "846b0ffc-7719-407a-9f67-a9f28b480b08"}, "a31d497e-abb8-47e4-8ef1-12fc1598a0c7": {"doc_hash": "e6cfdcdead5b4e2fd3ad27d9ca4cde77f63c84bf3be1d4473a1c0a39569b1be6", "ref_doc_id": "1e7bfadd-c783-4818-ae59-548fb6e549e3"}, "7f8a25b0-d2b9-4c43-8292-9f5bb4581b24": {"doc_hash": "6f5330f337511f79c922ec443d34320b01b30099242d8876b1b8418f1adf95f7", "ref_doc_id": "1e7bfadd-c783-4818-ae59-548fb6e549e3"}, "2dc26739-d5fe-47c3-9f52-1e8c52b7e448": {"doc_hash": "1d323d60d9d6f9142fee38feaf2d29b32a5e8cb36ff2644b7343f1c81be63107", "ref_doc_id": "30bc7272-d963-4c1c-892c-34eb0f595d63"}, "714e0fae-e10a-44d6-b9ed-ce675b165990": {"doc_hash": "6c1e6226f5332de74f7f8a985d17c66f717142f8479ffa6a45bd133f4b6aa270", "ref_doc_id": "9a75c85f-3a99-4a17-ad8d-da2a0e62d1eb"}, "adc54129-715d-4f5d-8b30-55d2de26eaec": {"doc_hash": "6a500357226fe3a73aded52623a32d9941f2957b398bf5cf0612a1fe62229359", "ref_doc_id": "0ad90e06-39aa-4342-8832-6ee4286c8db9"}, "1d2a554e-e164-47e6-891b-0360630b1770": {"doc_hash": "57a9adf58968ac5a737dfc493c534cb82d3940e983fa3e4bc528c2071861da5e", "ref_doc_id": "21df284e-7d94-4293-9f96-ac69368fe69a"}, "853a75e8-7956-4a3b-9d0c-b7e1536fe135": {"doc_hash": "1b90cabab72a6e3cee0dd99a60280183aa5c9a4f98bf03e00f84c24350ba37de", "ref_doc_id": "79ec8438-36b0-40cf-b80e-be587f4a64a2"}, "bd362fce-8ded-49a5-8abc-145d91ee15e2": {"doc_hash": "efe8e2297bf7a5e36b3238268a5421fd35c4aeff6d69a23bd431b74954118ab7", "ref_doc_id": "881cb765-d982-49fa-b600-0d81db28fc02"}, "4418b598-325d-48fb-a080-cd37465ead7e": {"doc_hash": "8c16e8f03341ceb8323ca011c9099ba3cebe26e240bee6b2b21fda309c27fa15", "ref_doc_id": "d4fe791f-3650-46d6-8c7b-18ba71594281"}, "10707d7c-f678-4d42-94b0-21e0488d93dc": {"doc_hash": "238433f6373a8847ab1b79c305d8aefbf897bf40c71900667cb52949837d7fe8", "ref_doc_id": "d4fe791f-3650-46d6-8c7b-18ba71594281"}, "2b0bdff0-0fc2-468a-8f1e-6d6c17081ad7": {"doc_hash": "f6a3251f2c7a1794ef994db9c8ed4aa29f0fcf0d9bd0c56b63f20b409c7bad7a", "ref_doc_id": "1c0c5472-3d1e-44ba-80db-6ffabc7e8951"}, "23e8cf71-ec95-4e9e-88a0-70c3539ea815": {"doc_hash": "1df2d1cc7e6205e87160634ca92246bd6ad2c56ad799e103bbadf96f41c1fac2", "ref_doc_id": "1c0c5472-3d1e-44ba-80db-6ffabc7e8951"}, "3886b414-eb71-4230-a54d-11f69a7c22a9": {"doc_hash": "857765831bda136f4e42054bc0a9b87f28f1b935bcec315aac3f31bb6a5757f7", "ref_doc_id": "6e7afc4c-a116-40e4-ae81-c34ed7dfaf16"}, "94658c9a-baf2-4a0f-9807-49086e380407": {"doc_hash": "9bc1f7d1fe35eced6a03c2b3cd161f5a831e88de1d180949179b86f1f9286104", "ref_doc_id": "6e7afc4c-a116-40e4-ae81-c34ed7dfaf16"}, "000db7ee-1c63-4351-b9e2-a79db2a1a294": {"doc_hash": "951d91e24289aa9861c691691e737b361ec334663bcd308d5e6b74054b68d0a3", "ref_doc_id": "accffe54-4dc0-4d46-9257-63ea5d7f77c2"}, "a7fd96e1-f067-48bf-862c-587908f9d7dc": {"doc_hash": "4ff2156dae3d0112a145d4d37f47b84d2fc3d7cfe29a17b6c27759675f9ecbd7", "ref_doc_id": "accffe54-4dc0-4d46-9257-63ea5d7f77c2"}, "34339bca-1c10-4566-ad77-9d0c172c112b": {"doc_hash": "58225db3875e68732e5dee578fec005361e0542760ae3773060a423ddf63643b", "ref_doc_id": "2884ad43-1757-4563-a2e4-ff52c6b9a0f8"}, "bce8a5f7-4c54-4fc9-90d5-4f23089fc9a2": {"doc_hash": "9881eb34296b2c55c898f28f1c7fbb086721d55a61defe8d856a8bbfefd2ec38", "ref_doc_id": "2884ad43-1757-4563-a2e4-ff52c6b9a0f8"}, "6467583b-0c5e-401d-abe2-c0f3885bb686": {"doc_hash": "1623da038572ed9478f2e83b9263a2512874217f254126395e3e3d84e415b6c0", "ref_doc_id": "6846c9f3-ff0f-4970-9650-043ebedc77a3"}, "1a1e7258-439c-4dba-adcc-049763b8ddd9": {"doc_hash": "586f246cea65fb638a2b3342c48c0c70f6cc732aa1494ee8a85400e235db2724", "ref_doc_id": "6846c9f3-ff0f-4970-9650-043ebedc77a3"}, "a23d32e9-2c46-4fec-8e99-4492ececc5cb": {"doc_hash": "a98d8ff17013f937a82a967e2a414eb947d5f4849f26f32db9b34732f1af6854", "ref_doc_id": "61cfebe8-7b9e-422a-9794-8b0275e58f81"}, "737e4f5c-c368-457b-8df1-65185af2f7ef": {"doc_hash": "277a62c7e7a43401abee5eceff4bbd440c79ba2c4187a23d42cfc02be8b609f2", "ref_doc_id": "61cfebe8-7b9e-422a-9794-8b0275e58f81"}, "82dd4d94-0cb0-40f1-9488-9f12514e30cc": {"doc_hash": "468196a43d40b2d63545985dbbe878b95a2475c00116bdece2c765579782bd31", "ref_doc_id": "a935f314-e0ce-4f7f-8f21-c0a4d92863b2"}, "eb6217c3-8ebd-4b4b-a85b-ff898e10acdb": {"doc_hash": "be288e33d9c0796fc2ec5211cf9d5de530e6be044cbb3b7bade40d80a6f1c0d9", "ref_doc_id": "a935f314-e0ce-4f7f-8f21-c0a4d92863b2"}, "636ab668-080e-4404-bb9f-211b7ec28ee2": {"doc_hash": "9916438563671f9ead4674d023a55da9c75d4c07db61577f2e52213f920fa741", "ref_doc_id": "bc0f2687-7615-4a54-b6d1-e7092477b375"}, "effde21b-e291-47c4-8b0e-3a4242621443": {"doc_hash": "eb9699b68fc8c06bb4fc93d388638777d79fdec474c40149d6e9e657b27962a5", "ref_doc_id": "bc0f2687-7615-4a54-b6d1-e7092477b375"}, "659eeaf9-2df9-4e15-8632-eac9784343ea": {"doc_hash": "c02d0a91656691ef71e7617291e2e8e7fbb9569b55f6135ed021ac1b80b7f437", "ref_doc_id": "5b8cbb35-81d7-455b-a84d-4336f9016ef4"}, "78a2cec4-f030-4841-847e-5e2706346121": {"doc_hash": "eb74263992ef9a39929ae442d023a659c5a3c0d8c30df305a4c4c8c23e75932c", "ref_doc_id": "5b8cbb35-81d7-455b-a84d-4336f9016ef4"}, "c6ea6663-da5e-44aa-a4c0-ed63166f4572": {"doc_hash": "652f4d2d6745707a3eb8cb436b7d29bb1044d17965b7e56d95b8ff843e9af804", "ref_doc_id": "330a0a2c-53c9-47a5-a2bd-7d1e76673699"}, "fe335d52-b751-4d9f-a311-949b83022ae1": {"doc_hash": "10e3430897ac37073d71093e7a4ba01fda19ea1e3ec06cd91a530bbe909154cf", "ref_doc_id": "330a0a2c-53c9-47a5-a2bd-7d1e76673699"}, "24fefd06-8554-491e-9b36-9a075eda7dd8": {"doc_hash": "548f3498fc5932083cf5b99fca09cea3ca7457fbf2b01894f0df3be76c3ded39", "ref_doc_id": "ed5e741b-fc69-400d-97ec-bb53e292a299"}, "d7cd0638-93dc-4673-8f14-6b60ee708a94": {"doc_hash": "3ce51866173489b020ac30d898273fba8f6b3648dc18b7c375ce3ab192dd0666", "ref_doc_id": "737902c8-e286-48ad-a69d-033f5ed61b44"}, "efd49475-6132-42e6-86d6-e64a095aed80": {"doc_hash": "b303f2ae072725ac1aceb3c2d9482bb0be75676e2b58288ad849192584249075", "ref_doc_id": "737902c8-e286-48ad-a69d-033f5ed61b44"}, "c954c509-66e1-4109-8d82-7ab6400aa5c0": {"doc_hash": "2aaa6b605391c8df089b1ead0a66b5f8960673702876e57bcad7d3919e673513", "ref_doc_id": "d4c26588-708d-4aca-837e-52b92ad098f4"}, "a51ddbfc-06ee-4dd2-bde7-e5a99c9ed51e": {"doc_hash": "752af43898bed1d295cbbae89ab13b112c317edb81bbd17201b21d2a35b3b78a", "ref_doc_id": "f88d11df-ec28-4033-8018-80718239ae2b"}, "5d744f19-b622-4dd0-be9b-c24ae1d89bd2": {"doc_hash": "266771e442e0d5f11b17341b07e7ae34756bac3128be3f328724c1e9eec76f2a", "ref_doc_id": "12a5d0ac-dac5-4548-aa38-6be3fdaa41ef"}, "dbf2961b-dea5-475d-b77b-d42b8ca2b448": {"doc_hash": "9fa68205e76169e780fdf5439ecaa618803bd99fec523297d49088c09b6c17a9", "ref_doc_id": "3de3963b-5b0e-4560-8920-a6af9e3ba92a"}, "e242c8ec-5d9b-43b4-a9cd-e846578f51ee": {"doc_hash": "4d21b601c9408f8291386384af7bb5bf10e136dec53e06b87281e45760658fb1", "ref_doc_id": "3de3963b-5b0e-4560-8920-a6af9e3ba92a"}, "5494eb4a-28ca-408d-956b-3df7f4ae0bf2": {"doc_hash": "53c5bc55b3b1af3d8f3069174ebdae88799bf1032f0a1a1ea2d20b4e0028c4b1", "ref_doc_id": "60bd711d-98a0-4cb9-90db-ec803541e88a"}, "a16ada22-3f59-4794-9e30-989e297336c0": {"doc_hash": "2185ea2c5f101fec7e0d60746639848fb3176b27c1a5d06540ceb93cbb7eba52", "ref_doc_id": "60bd711d-98a0-4cb9-90db-ec803541e88a"}, "45566cc9-905d-4609-b80d-4054a59e87c2": {"doc_hash": "878e3fd5c17790457ecc9ad98158061b36733fe526e03420b54de26da08ce434", "ref_doc_id": "d8e0f6f7-bb8b-4c93-b8d1-cf74fc7a924f"}, "20a50737-1a2d-4022-a005-42d7dbaf94a3": {"doc_hash": "76e9eb82b10aca15cc51d4a6917777bcf86f4fe0fb421648575549fa57b4495b", "ref_doc_id": "91b9a779-53c5-427d-b575-9f520f1a4b15"}, "9cfee261-7a1f-4931-b18f-7b911109358b": {"doc_hash": "e349c9f485ca02ac14f8aa59565b8651d985dfef413c5f36e7a1d580e3d28397", "ref_doc_id": "2a684c09-42b9-48cf-bb90-6ea90cbcb765"}, "34a14ea5-ec15-40d9-91c9-e50e6bd7002f": {"doc_hash": "fe4bcc11e762cea6f3770c460b6d4889e16078f4329d046fb98551f1a0a3e5cf", "ref_doc_id": "2a684c09-42b9-48cf-bb90-6ea90cbcb765"}, "5590af85-534e-4ed9-82ad-8a4c1ff14c6a": {"doc_hash": "7cec42c07a81e81eb4a3f8e7b2e7babf02a49fdf662328038c8bceeda4803850", "ref_doc_id": "1f97e834-e51a-4773-890b-e1839fb3889e"}, "7d220af9-81e8-453c-a6fa-4cf1377ac13a": {"doc_hash": "7fb3abd7c0dd29f9e6507205e07a06b784a0a18f7c4049df4bf3ba65b16f2370", "ref_doc_id": "a8717b08-1164-4898-8d05-18b7f9226ec1"}, "82b5f605-7631-429d-b473-4b7e5bbf34f6": {"doc_hash": "f05efa927271d2dfc05973c1b8ca9849cfd4e7086cf4872409ba0bcbf01863ed", "ref_doc_id": "71e7147a-33d9-4090-9e41-417f6f1d7e72"}, "06e37c82-0f0e-41d9-82fe-106c47ce7ff9": {"doc_hash": "3fee9ce59d7e779ff736d15c328c67215c9aecb3f073db0d10c8565b8e08c856", "ref_doc_id": "ec9fccdc-a752-4178-a26e-b00c8f66be44"}, "702e9027-4239-4bd9-846c-f18c602aa819": {"doc_hash": "a7c18b1f38594caa99d1ee056915698773980495fe844958d046d5f9dadf044d", "ref_doc_id": "ec9fccdc-a752-4178-a26e-b00c8f66be44"}, "6b6e2167-6e71-461b-bb21-775fc9bb8ada": {"doc_hash": "335fc44ae5b7ac70e2e7cc5822294c3add1b846e25de17b051b16e1ba56a1b57", "ref_doc_id": "7727deff-3681-4e2a-90cb-6429bc578b9b"}, "e5c05947-ee92-47ce-8cd7-dd074eb597d8": {"doc_hash": "09bee7cb4fc5dde0dfe0cfff93927e408f29ef374d29130c5ffb4549ceae066e", "ref_doc_id": "40740a4f-274d-41dc-86c0-0c33d00605dc"}, "eacca254-e9a9-4fda-b115-4dcbccff0f04": {"doc_hash": "1c887532fdb97f833b753acc673ae6e39b32d656e8806f0d3b2f55e738ab4f91", "ref_doc_id": "201fca88-45d6-4f71-92e6-2ec0cc8b9c0b"}, "3f612715-a809-4ae5-959f-4b135b2a84d7": {"doc_hash": "0ec4588614c790a0aef42460b9b9ece0728fe1f840c000effda9a56aa75431ba", "ref_doc_id": "201fca88-45d6-4f71-92e6-2ec0cc8b9c0b"}, "7597db72-44af-4eda-923e-bd09a4dc373a": {"doc_hash": "bd0ec63970011397a126471b4ac28db9f45ca990730c3f47d2a72e68e2dfa12a", "ref_doc_id": "4215217f-2d80-4f77-a3b8-8a185a5bca01"}, "10328131-02c8-4ff9-8864-7c90b56a9d00": {"doc_hash": "e1d854f0b06de7627612af23eebd48bd0044d93b576ca719369d6d485b61959d", "ref_doc_id": "4215217f-2d80-4f77-a3b8-8a185a5bca01"}, "6b8d8c4e-d684-4327-a79a-2d0ec778cebd": {"doc_hash": "cddfec46b984a1fd7d0c8cf06be3ce8230cfbd69aac220408fb6bad86651e596", "ref_doc_id": "7a5b009d-73ef-44de-9256-6404be516951"}, "3a24f00a-23fd-4cc6-91d5-82f4a54c9e21": {"doc_hash": "b0d2a82fcfd5fba69dcc10a61caa86d73ba507c29b4f164095cd3fd3c1c5b89d", "ref_doc_id": "7a5b009d-73ef-44de-9256-6404be516951"}, "85f8d3bd-5a8c-46fb-8498-e96258343397": {"doc_hash": "fcfa8e89d9629534d70bedba360d8d131ff5e757406fbc47b7b63efaaf60b16a", "ref_doc_id": "3ffe9e80-3fef-45ac-85a3-c278a1531c18"}, "2df93a6d-3c5d-46fa-bb28-a3111e91efeb": {"doc_hash": "515a02c83eea791deff074e26d40319966d0712fc4f3973a58a72d2e1edd377c", "ref_doc_id": "3ffe9e80-3fef-45ac-85a3-c278a1531c18"}, "97ceeb8d-77d9-40a0-9aad-0a5f4811586a": {"doc_hash": "8808921242c800e400d4e64a799e97dbf040095faf51c7aa51b130ed65de890a", "ref_doc_id": "cc50cd5e-571b-4bd1-9669-58dd68716c74"}, "9e39fd88-aa12-4bac-9fa2-ad224dc2de43": {"doc_hash": "fb0ecdf46ff9492e9cb782d1544a3e27fff0b6031da5aac9979dee9b6e3fa08e", "ref_doc_id": "a994c212-2d13-48c7-ae31-c5ca9f70eb42"}, "49c287eb-c6d6-48d2-b54f-b113468e861e": {"doc_hash": "05e5fccb5b27e3242f2d8e14c76a68145108eca11b4059cbd17eb5ff2b98a4fc", "ref_doc_id": "66a2d553-7e1b-4216-a0be-1048ec53a9ee"}, "cc99699c-b9c5-480b-98db-9de59ee17649": {"doc_hash": "c811238f781f45c22413d9f455722dbdfcb7aa35e306f10831250c9f4e71bae2", "ref_doc_id": "9be0f482-850b-4d49-b4ce-efd96bcf646e"}, "1339d356-d314-40fe-bd7a-e81a4138a3db": {"doc_hash": "bd200a476e709741e27949073edd7001837ee8a2dd7968044db13fb35e8ba356", "ref_doc_id": "4fb0113f-4dd4-48d9-882a-3ed50046463e"}, "5e00101a-ce99-4c5d-848a-66e1544ffff9": {"doc_hash": "853ccbd4ed389d96821426c53f623e8a1941c734955b6ef65eb850e61c077411", "ref_doc_id": "ca4eed97-c081-421d-ad92-f813d13c6692"}, "bb3d898b-b154-4fc5-a5c5-6cb50decd35f": {"doc_hash": "415eb0c3a19d1eb5bea5a07e0ddfe71b97f85ce9c0661081d91c98273c7686df", "ref_doc_id": "06c1c216-46a7-435d-91b6-b77e69b2e663"}, "9660e8ee-e313-4fb3-b4e5-69e9adde059d": {"doc_hash": "b6a49824bb4a6e6b61db79353a0707b31c6099b741c412adf95a3dfce23e0a03", "ref_doc_id": "4261a0f0-9abd-4ccb-8a42-2bf93f779a81"}, "4a4a011d-1333-47e3-b677-9f750cc69e44": {"doc_hash": "48ebe68b12ce9111ef418b0f839c1b3d9a542dbffba0c97e6c0b94c891752c3f", "ref_doc_id": "9c5a6ef9-f171-4a62-b1a1-1110ea63f69a"}, "cf7e3e5c-ca6f-40af-9294-4b1b0770bcb4": {"doc_hash": "6fd2853e0393c5a9a03ee2a193e14f1d92d63ea1d4e8c537afc7da52e5624d2c", "ref_doc_id": "5365b0d7-9626-40cf-9a2b-90ef407e45e7"}, "c757947d-3479-400e-a599-b42f962d6ad8": {"doc_hash": "ce49370a6ef89f91c34fbc93f4b084ea1820b7e63f2450b4ea8f2099c9632c3a", "ref_doc_id": "a44ea320-629c-4d26-a9c9-d62bf41962ed"}, "8f9fc708-17f7-418b-bed2-defb4bb913dd": {"doc_hash": "cc65df719bced6322f726227dbd6e18e6d0623f387487dfdbc8cb673675a6265", "ref_doc_id": "935dc53e-b916-4faa-a741-7a3fa240a804"}, "86ef48ad-5ce4-4111-b54c-f28f00787f58": {"doc_hash": "d93093e2bfb5d502be189bdbb6a63cdf1f6b044ee890cf836ef891b7947add39", "ref_doc_id": "abc1ee0c-61cf-4f23-b569-9d2dc0a8e7db"}, "8a883018-321b-493a-9e9c-f9f850849985": {"doc_hash": "2aa6ac65ade9e3b37e21e102dd4886d725d84b0d1009a1ffb956c51fbd681b23", "ref_doc_id": "46e2d04d-a4d8-4764-849d-f4c6c04a7fc9"}, "3374e65d-34f9-4dad-9846-a0707b6493d9": {"doc_hash": "b20fd51fa52c8c4aa77ab49bd856778b20fef154d159c8d4913f7840ab28c691", "ref_doc_id": "30d464d1-866e-418c-b789-9e7801de44b9"}, "837013c2-af58-4471-8a21-7d197142b5e9": {"doc_hash": "c15620c4ab1908b9bc69582d3f2f7d45f5208a8736eeccf1b7eb19645576a1a0", "ref_doc_id": "157debf0-ea7e-4062-a6eb-1b79508a6a8d"}, "9f5e46b8-6ee8-4d40-8251-558523b170c5": {"doc_hash": "5e8e31912ca1578c1d05e63a84ccc487ee805834c8e16ba2f1654f2aaa0c3e9b", "ref_doc_id": "338628df-1354-44fe-bece-70aad2d91871"}, "8ecbd0a1-1be3-47ee-abbe-c451c638cb3a": {"doc_hash": "8a7ab2bb2cd8e4f96e10cd9ed41873ff7d3240d598e69a55f5ed42607ef74fe0", "ref_doc_id": "34989f6d-d8f1-4dae-8666-a07c3565abae"}, "c3998a18-6e3e-4b62-a639-a87e97294ad3": {"doc_hash": "010babd0807ed89bba17aacb09167f758b4ec3f9f41d20a7a87fb91d62e8b661", "ref_doc_id": "ecda3c23-1fe5-4285-a605-24743c348ae0"}, "22d92bfe-3715-4df7-9989-1338642062cb": {"doc_hash": "f61cd0caca23db98a02b1cc1c87afa549d88b09e527c2677c15a1e32c396e517", "ref_doc_id": "ecda3c23-1fe5-4285-a605-24743c348ae0"}, "76f8a797-6448-4fcf-a515-d2ed0423cffd": {"doc_hash": "f98c2e744690412248756b94e52ce1e2cf0bfc83f4bc42e689c3654affd73ef8", "ref_doc_id": "0dad4af6-36f9-4ec4-9181-2623b16be04a"}, "37e7e5eb-edbe-4884-9474-6130e84e1892": {"doc_hash": "184d84d2ca6b723b07f3eb893cd6b7bb621758ad5e0b7cc25631e458000d9538", "ref_doc_id": "0dad4af6-36f9-4ec4-9181-2623b16be04a"}, "ebd8058c-dd91-4a2f-8310-9d6846805619": {"doc_hash": "5115e0f60ee5585f2d0bcd4606354fdf4dae32e7bb05fd41d6366d0b168bbbc8", "ref_doc_id": "bcbd7fe7-5488-40ae-8c34-56cce0cdef21"}, "305d8671-e7d0-4663-86c1-b80ea387e109": {"doc_hash": "147eb45f28b55f3c8620441305b1e6feefc18478a7234436bf1847b3f6904383", "ref_doc_id": "bcbd7fe7-5488-40ae-8c34-56cce0cdef21"}, "7278749d-4e5f-40e4-9860-9b76656faa68": {"doc_hash": "295c186eafaf659f2710eb87e20c8cd77ed5ebceab546bfea35e5ae03a95a826", "ref_doc_id": "08f71681-5cea-4347-a5fd-e5e3fb8c0ac6"}, "ce5718e8-92da-4b87-a507-aa54f47e632c": {"doc_hash": "88befc5ef9f8e04f712f54a7c5b76938bab4a301a04da51419e69e816acfaf21", "ref_doc_id": "08f71681-5cea-4347-a5fd-e5e3fb8c0ac6"}, "69a66ba7-3a0e-494e-88f2-b03dae9bb4ad": {"doc_hash": "0caa0214318b0beff8da411a9dde50189522e5f8a3fd026d98f364b17be588d7", "ref_doc_id": "c200a591-8def-4aa6-9f1c-1dbeeedf89a3"}, "056f7daa-3310-4458-bb84-56ab023cda78": {"doc_hash": "bc7d9011be1f11b7f6053905157ddc59c0b68693a7c618d700547f8c23ceb379", "ref_doc_id": "c200a591-8def-4aa6-9f1c-1dbeeedf89a3"}, "06ddd3eb-81d8-4353-8e05-b7b6cbb48088": {"doc_hash": "ae5bb07b94d9dace877f9a4106aee508eaad0fda66e38e811921b3d001280a6e", "ref_doc_id": "3ec61808-230a-4b6f-9917-87e45308fc81"}, "df052885-c49a-4c8b-b09f-d01134df263b": {"doc_hash": "5d79c579f855916f353ed1214aa2bbd70dcc458a1cc94e9db1ffc6e5b7588c97", "ref_doc_id": "3ec61808-230a-4b6f-9917-87e45308fc81"}, "f53251b1-4d89-4bdf-bf3c-2bb3f0c5e7ce": {"doc_hash": "4cfc415ce1ec9d8c601af304de7dd0d3278d8d5da750e081cf91ab71df8af093", "ref_doc_id": "40e43c23-3dfe-4f9f-b36b-673633c9ad2f"}, "6738e4ff-3ef2-47ac-89d7-fe274ef3c042": {"doc_hash": "7f2060d5029a0f252fddf6fc7fe111370b0f664c590d7a092973368225d198cb", "ref_doc_id": "40e43c23-3dfe-4f9f-b36b-673633c9ad2f"}, "c022a9d1-56f8-4001-b032-56706ce8cb5e": {"doc_hash": "45ec507fcc4a8bad72cdd73b2dbacc3fde15128d0ac4a6855bcb9180c9a14994", "ref_doc_id": "62bdfbf7-2d85-4125-af36-9ac687fa789a"}, "695ddcaf-c48e-4976-a1b5-6715e5d74101": {"doc_hash": "ed77b3ef07b14d0491f185f7e57d44a662c6a1e21f163c17debf9a97566c14a5", "ref_doc_id": "531e2299-5003-43d4-97ab-3305b6a7e6bf"}, "af4db8cd-9b93-4948-bcdf-a1005c19f6fe": {"doc_hash": "e2d3f15c5bdfb77d6de319c7193af3856643a183fd694c77dedfaa41c77bc080", "ref_doc_id": "b08f4748-544d-4530-86b0-acb6bd9f2ae4"}, "66ab1f86-14d9-45c1-995a-d9fa68212ba2": {"doc_hash": "ca8d564f95a8a9f22bdb4670de59ab6a7b5324411b20aeb9f3d18641ec100637", "ref_doc_id": "c81fcd7d-f5a6-49e5-9ef7-266b2205171b"}, "1b0612ad-35f7-44d0-a827-7157b5c0f0d8": {"doc_hash": "c5e22a4f89d845fafe60204531d65621c93ef9cf9430eb55cf40f5e426b6ab07", "ref_doc_id": "c81fcd7d-f5a6-49e5-9ef7-266b2205171b"}, "98355748-83a3-40a8-87db-ada94f9cece0": {"doc_hash": "7c2d555a54938fc32755fd9e378d5b75c2ca42205a4801c5cfdcd5def4445aac", "ref_doc_id": "56119e97-408d-43be-8433-5fcde63abbc9"}, "15e57bf5-1352-4fc1-a31c-0a9280808957": {"doc_hash": "0f34b121b8ec252b75a98e84526320248230ed7e5dc73549935f1fe961167aa5", "ref_doc_id": "bb11912f-41bb-4f7b-9328-ea892513e3bc"}, "4ac20016-45ed-473d-8287-07cf88570733": {"doc_hash": "e2421515181b510b6b56c7c27cbd0827be787af469e9d9e435f0630f556f9bc5", "ref_doc_id": "bb11912f-41bb-4f7b-9328-ea892513e3bc"}, "56a53584-835b-418d-9e7a-67bb78d6111b": {"doc_hash": "a55bb73cf2adf8b9c919e1fde1ab9b5a69d3e2268bcf0863b29a2ed0686db022", "ref_doc_id": "d06b8f31-9819-4fa3-a618-95a782d11fa1"}, "76e1b885-8869-4162-b58f-d349b606e121": {"doc_hash": "c210599c82efcfb0941a4e63e1be3e9e35ca301993458382ba3b9e3fd5e0ef82", "ref_doc_id": "7149e6a7-531d-41d9-9437-233b59a8b686"}, "7199d6f1-eb79-4cc0-836b-95fff52d11d1": {"doc_hash": "32d7bd9dcefe8ad5230da9303cd5223c847872c74e79758041b44572b561f1ce", "ref_doc_id": "7149e6a7-531d-41d9-9437-233b59a8b686"}, "2ac679ba-6e65-4e28-9c76-eb0f7885a603": {"doc_hash": "ab8653869a6c41284c4c2a068f84858b30fac3b93bcd37e2cd48cbe271db7733", "ref_doc_id": "10abe83b-9c38-48f9-9a3a-e3cb73e52724"}, "b8140036-043b-4f30-860e-8fa4d03ff327": {"doc_hash": "cc775197e8c2583088b81cbc40e9a88cf87b0daac845a5d21f29888f86721ed6", "ref_doc_id": "10abe83b-9c38-48f9-9a3a-e3cb73e52724"}, "e571ddec-c04d-43d6-9062-d8903d16ab4b": {"doc_hash": "459ec11e52819f0651843a7aeb49190e6682838f42c5f1f23189eb2860a705ae", "ref_doc_id": "fa2e441f-0455-454f-b46c-5c7bedec8131"}, "f35b6fa9-8a02-4fed-ad86-db2b73ae8398": {"doc_hash": "e8798eee928723607d7737cd5124a88b73544cf91de9bd0a200484d42dc5cbe5", "ref_doc_id": "fa2e441f-0455-454f-b46c-5c7bedec8131"}, "1c0e0d68-42da-43dd-b59d-9ce6075f1b80": {"doc_hash": "0737d82e4f46aaa2e9db0a36ac570c8f29cbc9dbba84dad77f657e20d7a811c2", "ref_doc_id": "8713fa42-15a5-4ac5-8c66-28b1ff130f52"}, "ffa66238-965c-4c00-b0d9-52fceab5c612": {"doc_hash": "01d0f21aef1ec1fc5459c9902075806a32fccae42b0dab9d45884b6bcfaa77ca", "ref_doc_id": "8829a23e-e3d8-48a3-a27a-83d910e08765"}, "d65c5c95-8e11-42b4-98c8-bd5e29fd1299": {"doc_hash": "64973a6b98d67b779607feff4097cc570fff958239201e59b8edc720d467a1de", "ref_doc_id": "74cd4ef2-e295-40df-81b6-e75b10d8727b"}, "dc83c643-b5a3-477a-b6cb-1e439d845885": {"doc_hash": "c9574cf77f5e891db849a05c351e9a83794ef859a9a0c29203722ec02f47943c", "ref_doc_id": "89af7da1-d7fa-44d3-be3f-79a9a7533a1b"}, "ea86f588-f74c-416f-b4b8-16bbf98f863d": {"doc_hash": "9e684114d17aabada9173035462c0fb6d8be26d47a09c871a6725c1a468639e7", "ref_doc_id": "89af7da1-d7fa-44d3-be3f-79a9a7533a1b"}, "6757028e-e15b-43ac-bf0c-24a14ff3be3a": {"doc_hash": "86b36469408cb89d8c2d7c0d0a54b92c03a977afca6ffe4a131e8c3968d51652", "ref_doc_id": "7f12ccd9-3bcc-4199-9ee0-bcd3146cb88a"}, "c503547b-7ec9-49e2-a8d8-8b8681bf335f": {"doc_hash": "5b7339112c591bc671a04e2d2b86afea84465f98dcd13eefd4c30f9700fbaf96", "ref_doc_id": "7f12ccd9-3bcc-4199-9ee0-bcd3146cb88a"}, "7c873f19-8722-43cf-b07b-ff14d286120f": {"doc_hash": "9dc3e599d61bb559db22b53cade5074eb132bc78745c5f00d9e77879acfa9cb4", "ref_doc_id": "5e364d65-4c36-48ae-9925-4e244c0066b3"}, "62d60eb9-9c59-4d35-ab76-5ac97ba09e7c": {"doc_hash": "4ffcade3975ad210f145bdbbe82513479e556affec2a2c71fa4065b2c8fbc813", "ref_doc_id": "4983977d-78bf-49bb-a0c5-75c3f7bdfce8"}, "ba91977e-5db3-49d1-b622-0492eeb4d153": {"doc_hash": "6725eeae540219dd29b523ceefcb53f0eb734515ddb131ff2af559a2f7a23946", "ref_doc_id": "df932b3f-cc3b-4ace-8670-dd32e92af638"}, "b4b8efdf-2b68-4cd2-af11-48f5da910a63": {"doc_hash": "099b18064f676c403b2c4ddef8f89103079ddc0cc2dfa973f40d062d72c8872a", "ref_doc_id": "df932b3f-cc3b-4ace-8670-dd32e92af638"}, "8f7f2756-94a9-4400-9382-de7b06b42644": {"doc_hash": "3109f971081e2a0f93db3bcac7cf4241cab1b93807fd9d38419df93bb46b9dcd", "ref_doc_id": "79b9ad7d-6921-41e3-9f85-5ce9fce13116"}, "6a31181d-d2f4-460d-aaf4-efe6bcc7c5a6": {"doc_hash": "544211cdfea19e049f598be9d5a6243863a40f2272764000e8bebf0ff27ac830", "ref_doc_id": "79b9ad7d-6921-41e3-9f85-5ce9fce13116"}, "b04189d5-13cb-443e-9e95-f5b0e74f400a": {"doc_hash": "6241fbc51d7af3422020cc5ad7e490f1196dd880a92733db6acb4853bd29d488", "ref_doc_id": "0f4c4639-d618-4a73-86ea-c46f2d9aa921"}, "f8938a62-277f-4ab6-ab7b-8834546eb727": {"doc_hash": "5fb0360e4a570db4a467069852f275d76ca1b64fbb2d5afc7fb3bad4403bb46f", "ref_doc_id": "b80e9edb-4c3a-4660-82d6-6d3cc258b82e"}, "f88f3f7f-7677-4f38-b7ad-0b343f57deba": {"doc_hash": "c6d3693c2cf61dc9d7034dcaa295285daecdd474b189138a84edbef502019983", "ref_doc_id": "b80e9edb-4c3a-4660-82d6-6d3cc258b82e"}, "746de1b2-5dbd-42f4-940c-dca15ca3ce3d": {"doc_hash": "d5eb28a91536e7b2237727e19e4e30a1792c4b14147090e89a5fc1229e89a40b", "ref_doc_id": "a33e732d-be5a-4593-921f-33d103453654"}, "bced8172-7c3c-4699-bd87-150d5861883c": {"doc_hash": "91f5c93bce2e998166dc08cc03392e45e74b3cb379c19dc0de57b32946bbc8cf", "ref_doc_id": "a33e732d-be5a-4593-921f-33d103453654"}, "bddfb8b1-5354-4d1e-8cf4-3a179b0179fc": {"doc_hash": "8ae9f8243adc8b045d46243c41c8e1d14f11eb400611462a8edc9156dc8b3d81", "ref_doc_id": "a18f9b68-e101-4e5a-89fa-cde23ec2bcf8"}, "f1a92455-61b4-4d25-a199-0a7f10200560": {"doc_hash": "e309b9de7e82e4f4a65992827964c6385313342c102cc838f218ce5beb3b8364", "ref_doc_id": "a18f9b68-e101-4e5a-89fa-cde23ec2bcf8"}, "fc188010-42c1-418b-9b20-49ecc2f6a77f": {"doc_hash": "07fd51956c7b4a3cc1643c984b15fa04815773d6282476448095b204e4472031", "ref_doc_id": "8ce3dc78-dd2b-4865-bc38-f1e4807b8ca0"}, "bbd7d86d-a94c-4f9b-8fb2-491e6b6120a9": {"doc_hash": "c84b1182f6eab6b3a54916223390f6e6e87b91b031735fd01911f61973d87bc7", "ref_doc_id": "8ce3dc78-dd2b-4865-bc38-f1e4807b8ca0"}, "588507ff-f57f-4c02-a9c6-2821cccc0da0": {"doc_hash": "87ae7db80994bcc257d186e0c5b088bb05cda19eaec43a1f80403e088ba34691", "ref_doc_id": "d6e7e157-53e0-4e1a-af8e-d43314ec5c61"}, "f83839aa-80a7-4635-8c87-03fdbd40de6d": {"doc_hash": "febeb442537c49f085898db2735b7733cbb2a608f3abb5972184b24d8d02827c", "ref_doc_id": "ea5f0cbd-f5fc-48f6-9699-6f5bd18497ae"}, "c30bc458-d118-41ab-bbce-d626cd73e0a5": {"doc_hash": "32fc46210c0190c3005bfebeb2593e303798ae579bba771b256e7bbdd79d9198", "ref_doc_id": "ea5f0cbd-f5fc-48f6-9699-6f5bd18497ae"}, "388f8350-0d9b-410e-bc92-ec73d3156d50": {"doc_hash": "719199b99ef6f6902a1550de51517f4c6bad634ee7be182df0d6d25117a24be4", "ref_doc_id": "9fb68e17-fe10-489d-9d2b-c69ddcfdcbc9"}, "c42699fd-fd78-4534-9774-5cb54ba0759a": {"doc_hash": "785c5e2417c23c0c4329763366729fddf5dd2a709cebfddeba28b8eaf6c5af1e", "ref_doc_id": "9fb68e17-fe10-489d-9d2b-c69ddcfdcbc9"}, "2be73afa-b526-40e8-aa2b-c36d12996363": {"doc_hash": "7df8339b3822cbc31988f137f62784738a1c9ebbf21fc37a3453d0af7f1e896f", "ref_doc_id": "d1e6a54f-08a6-44cb-9a0e-7958bcbbb3f3"}, "904ee4a3-1080-48ab-b999-805bf1f3f820": {"doc_hash": "8ccd834d38ed59050f1f7a8d83253903e420c1519715866edb9d37d3a992a343", "ref_doc_id": "d1e6a54f-08a6-44cb-9a0e-7958bcbbb3f3"}, "7bf479fc-1a49-4dda-82f4-6ceec803b7f0": {"doc_hash": "aa2cf5b6962fe899d727d144b7a68d3ed649cd951cce0ba80d22cbafb5dc21e1", "ref_doc_id": "246fb635-a3d6-42c5-b4a8-a0258b96bc82"}, "76775e1a-091f-4a5e-8404-245822957910": {"doc_hash": "597fff5ddee4cffac493aa85fe4606e8f252187460261f46c725f29d10e8f4b3", "ref_doc_id": "246fb635-a3d6-42c5-b4a8-a0258b96bc82"}, "07c03225-ec5e-4907-8ab2-7ac763aa0500": {"doc_hash": "b601b00873b8278f4d45f416052d4d1f22974381c265ffcbc785b041de075065", "ref_doc_id": "84859801-425a-4208-96c8-ff12c482a9fd"}, "a870b9da-692d-49a4-abc8-b48468789b24": {"doc_hash": "9ff928c82245f94a022bc40e1345d671c95ef49979d7263fb8a7e90edd7a663f", "ref_doc_id": "02457e77-2805-41c7-9b76-6f54ceb3d8b8"}, "f178ea09-239a-4e8e-94ef-8216481c64e4": {"doc_hash": "3648204a900e48a8fedca38359bc7cbd20db12b5e01c41913aac914560d91f19", "ref_doc_id": "cb069b1b-c21a-4a17-8942-d93fb785bce2"}, "4aff74fd-04bf-4ac6-ba4b-a05b3b399fef": {"doc_hash": "d21d6ee56910b6a5ba70d4965afcfe66f6590e3637ad9169303b849814979f53", "ref_doc_id": "cb069b1b-c21a-4a17-8942-d93fb785bce2"}, "266141a6-e08f-493f-93df-f4cb325c0847": {"doc_hash": "325158a3eec723435a06b3e1393fc87c674e06ace24a9eb50f28b6bcc09f966b", "ref_doc_id": "db0357ce-f2c1-4ffd-a2e4-11fde0aa692a"}, "fd3fa58b-645d-4e6e-ae69-00155f77fd8a": {"doc_hash": "b9c553aa888873be452e674a607858b1d1b9d0fcd5806a4fa8bb7b48116bd955", "ref_doc_id": "63514ab1-bc97-44ef-8ecb-3644ca44d06f"}, "f74fdbf3-a1b8-48cc-9502-f36532412764": {"doc_hash": "b32e032a010c5ffa146160ef19aa197c87cb3da2c71b814c9f2bdda2aeedb615", "ref_doc_id": "fa19d8b5-b9d5-4e68-9c91-1e3793da50c2"}, "2ecab720-d49c-44ad-9a56-ffd6f5ef7282": {"doc_hash": "2a1cba05709c95e3e1fe34fd496cffeb74d62831c14ead21f469844504b7b0b4", "ref_doc_id": "e5e4d27a-e300-4b12-914f-e187c7c3625d"}, "b1a081da-1e92-4ccf-9980-393505d382ce": {"doc_hash": "3fc8d2a8b9f2f3170faaef1a8e66842d4b2b3deb67c198ebaf627d241750890b", "ref_doc_id": "e5e4d27a-e300-4b12-914f-e187c7c3625d"}, "8f48b068-fbea-4f29-a882-67c1d2f050c7": {"doc_hash": "e4fa6c31b6ccd38cd51615d755cd04c4e9566b660839a43d07c442e4becd16b6", "ref_doc_id": "25253ec0-9c70-46d6-94cb-048378c8c666"}, "ab7c3898-57b9-48c6-87d1-609924cb2de3": {"doc_hash": "54247f8941974daf77f277872ed2cda3fa05b02e693d327d4fc4ca551c19e1a0", "ref_doc_id": "df869a45-fd74-47b9-ad50-4d0a86e34c52"}, "595e5644-799a-4d11-bb9c-feba8a980e7f": {"doc_hash": "9061343711b1c5c2255dca23edf76c368d519ec44ee779fcaf7d361f7a23514f", "ref_doc_id": "df869a45-fd74-47b9-ad50-4d0a86e34c52"}, "ea73bc01-5572-4775-b4fe-08d357ba5018": {"doc_hash": "9bea1cdb717b607091d772522cf5ceecdb425028a40027ebc6904a7302bd232d", "ref_doc_id": "46c365f3-dc6e-484b-840f-5b3e6ddcbf3e"}, "22e9a799-7af1-46d9-838b-79e1c1171d32": {"doc_hash": "5782855d57a9f4261e3ed34e9b4dc37634836d75bef8fc39a6a674e343228ebc", "ref_doc_id": "1293e67f-3115-43f3-a907-bcda77482792"}, "654b27a3-a1c2-48e9-8c41-b0185d9a3246": {"doc_hash": "a8ca2b7b4b2444ae4c30325700cbe84dd3cfd064e7b03cfc0f82ce83557d2c0a", "ref_doc_id": "1293e67f-3115-43f3-a907-bcda77482792"}, "ed07785b-4301-4dcc-88d9-389de7f798c9": {"doc_hash": "6d0339b711fc9958dbbf88f6753a3d2ec6e6167f0a8a0d28beccb58e4c4c54b2", "ref_doc_id": "b61d2b8f-918b-48f6-a64b-79f29dd658e1"}, "281995ad-adce-4242-a5f3-018ac229d06f": {"doc_hash": "2343d952bcc4414d385078fbf6934dfc8b0115c5c7b920c5c7b46ded83335c9a", "ref_doc_id": "d87e31e2-84dc-44de-b814-206f0884f4d4"}, "024c07a1-84bc-473c-a65c-b548359ec66a": {"doc_hash": "4619c7fe7cba7fcdd474dbd0fee06aec7873ec3f6e6d5ec9d5fcc10c756dce00", "ref_doc_id": "e776003c-e220-4638-b7a5-da9c6f13d5aa"}, "1df982b2-08c7-4636-8cb3-408c9687798a": {"doc_hash": "9e1a184ba9a0625ffcedf8f70d6258d84515ed3148c09ec2cce13a17c6a6ad56", "ref_doc_id": "bb668445-6997-46c7-90f0-ef3f83e1b62a"}, "be29d1d5-1d8b-4140-8400-f1f9b5b61ec6": {"doc_hash": "67608a1ccad563fe238d7f87a0eeeeac7de46afc40987b9a79dee64cfc106401", "ref_doc_id": "a40950c0-eae1-417d-9e54-51b368975165"}, "0f7a1538-2be8-4b43-a787-124c0062c5ad": {"doc_hash": "09b3587f514325afee39b45d9375a51a87e5b0090ece858d5b65a716ba2ce06a", "ref_doc_id": "e92e8db5-6032-4ca4-8d54-adb8fc6b145c"}, "9353b2ce-4c17-4b53-a915-69cbf3ce1aa4": {"doc_hash": "611b876faa738055b714bfba49538a94df8810113401682aa58753877a0b5d50", "ref_doc_id": "901e01c0-2e10-40bf-88f6-dc8f32aae36e"}, "03332007-40bf-4656-867c-0be4f56ff8db": {"doc_hash": "408a67c97c92beda819ccca08d831743037325e7bd0af137079cf0c3f955e81f", "ref_doc_id": "69c41b84-56b2-4880-b79a-94cb7687707a"}, "a5dcb08b-edf9-47e6-b7c6-a7ec657bcb08": {"doc_hash": "7769c32f0bf1d8193422ae013b6a7c9d86199453ed483300936dadbc66b2760c", "ref_doc_id": "419a0728-958b-42f4-b58b-72ad9d01987a"}, "c9fd0bc0-2886-4809-a38b-b9838661ef5f": {"doc_hash": "b13622bc6539d48d5f35a73f5535cb4b845e148f7dc56621bd9f8228502d709a", "ref_doc_id": "d4a4302b-7d5d-427d-aad4-2b6e947028bf"}, "2578b658-aab8-429b-aea6-98f54ee436ea": {"doc_hash": "ef231db6921b35d3fb9f179c41d39027571d57a809b06928b39f35b019d8c910", "ref_doc_id": "51aebedf-63d5-4a7d-bb06-b251620c30fd"}, "4ccecc8f-8da3-4d78-b638-4c8f9c91accd": {"doc_hash": "a75470acad5a677b38d572def033ca7e268e0f2bc9b8b9262a34f2f268b46fba", "ref_doc_id": "eeadb1ad-4504-4af1-bfbe-e6421ee02e94"}, "1dacbeca-5c0e-4522-ae61-82e8a78b44e5": {"doc_hash": "b7e316275034180eb5676e737be1236b12b7b6478dc7944e02688c79afdbfdd6", "ref_doc_id": "d5578a06-afc6-4310-9811-dfa11f1d3f80"}, "9857e94f-3d1b-4bea-97ce-9d7e0878ea37": {"doc_hash": "83f5fa1f9bd704dfa1ee32068a3972da5b5eb9f4678784b75b971c7ab5affdb1", "ref_doc_id": "d5578a06-afc6-4310-9811-dfa11f1d3f80"}, "7d55cb85-ebe4-4d0c-80eb-6c976f46f698": {"doc_hash": "c7efa97fca38b765e1f65d679dee4063c411ddb70764c06f021893783b0a4c4a", "ref_doc_id": "46b4e596-a3af-4322-9644-bdb7a8456ffc"}, "27d9f8b5-4a68-4d5b-91ed-5f2e5a203996": {"doc_hash": "0e3d95ab610be2e78ffe8b30ee72f0a0578ae04a8bf6722d6b1b8255aed885f6", "ref_doc_id": "46b4e596-a3af-4322-9644-bdb7a8456ffc"}, "e18eba1e-8023-4e14-931d-812b61583525": {"doc_hash": "dec1ef41b765d8e22ace4ab13a84daa40ba41095c0a07857779cebfb0e13a62d", "ref_doc_id": "67b4751c-ad18-4e27-8e5a-1eff6afbd4bf"}, "882be096-b2df-49e5-83df-fc113064203f": {"doc_hash": "934b574e8e396568765cd9cff109012393b301a34539d67a9bc29bfd383052f9", "ref_doc_id": "67b4751c-ad18-4e27-8e5a-1eff6afbd4bf"}, "6e489381-4e92-44e0-b3ca-10f21815c176": {"doc_hash": "44507d18225e85f3dc292386a3ed5eceec62730a1b0d00a942e391442d1ec6f2", "ref_doc_id": "a2d2562e-a2f4-459b-8289-f4c6a244106f"}, "c5d6d49a-40c9-4a79-a8bc-33f33d7efded": {"doc_hash": "069b64123d200a02525fe24e5e4b71194e3c971a39c5d3401eb44cef7c62ed20", "ref_doc_id": "0f6e9222-24be-4e11-bc8e-d26ec5b80b46"}, "31a41d2d-77ba-4166-8cd4-9205060a3fb2": {"doc_hash": "7180f3ab0db72a2398923c90b9bd09fafb38f3c617695de7f89182655bd257e4", "ref_doc_id": "0fc8f12d-a3d0-4915-a3d3-f813f7bc0285"}, "2e5aa4fe-e593-4f73-9851-b762bf7a0350": {"doc_hash": "e9b92c24dc2c54f49200fce5faac0b437761eb79746b61c63c8f99c567510be8", "ref_doc_id": "0fc8f12d-a3d0-4915-a3d3-f813f7bc0285"}, "757db6bb-1eee-418d-8105-c91b21967451": {"doc_hash": "b3627e8c08b473857464925f5d837339c769a845033e292e5d5f3e2ddf34ba08", "ref_doc_id": "84c8b8d3-b090-4b82-b5d0-e71ea38f13b1"}, "60ee3206-e49f-4079-8873-ce827640b302": {"doc_hash": "2f934ea7487914a00b47b0e7b7ade0d3803eac26023b4d9cdf3b07ff13a5e8e2", "ref_doc_id": "84c8b8d3-b090-4b82-b5d0-e71ea38f13b1"}, "20e48498-0e13-481b-b7f6-d105ca7fbf1e": {"doc_hash": "fb35af2ba1349c58daa3bd991734c22543b70574de86ae6ce6f4492c9840676b", "ref_doc_id": "eafd62c9-fa7e-4668-aa3a-c6480e5bbee9"}, "5524e0c6-a72e-46ad-ae4f-7ece3d18b69c": {"doc_hash": "e598bbb0fbb833869f3ce549a4b6c6515d82cee1f7de169f4730e7ed728ba2f1", "ref_doc_id": "65efad3b-d97f-46bb-8549-1ad86fa424ce"}, "3f231b3d-1d05-41e1-ad10-9b1aaa6ebe90": {"doc_hash": "086de99f7c87ffcf94ed713c3cc60cfb88ac09448578cc2bfdb3830d8254bd60", "ref_doc_id": "1375e909-94e2-44c1-9dd6-1554fe0ed9d0"}, "8da7edaf-250f-4aa0-a7ea-f518bb233fb7": {"doc_hash": "1d0a10d25c12cf1c0b26baae79b1d3bc18ef40bedfd67900b4f0f8fe3f466c24", "ref_doc_id": "1375e909-94e2-44c1-9dd6-1554fe0ed9d0"}, "1e585f73-32bc-414a-b0d7-0943bfb67b47": {"doc_hash": "812d8ebcf5425cb8af28bd6b12d2c2efa8a797aabbb3da3818080b1b1a12f53e", "ref_doc_id": "2c39ec53-9359-47a4-a215-8a631188dfdc"}, "03039334-257f-4bd3-a212-7f2040ccb085": {"doc_hash": "0b62f715befe657c262212b36f11d1b419f7fc18cb2117414a08230c9b497939", "ref_doc_id": "2c39ec53-9359-47a4-a215-8a631188dfdc"}, "ee81dec9-9a33-4e60-9324-241aff9a8cd3": {"doc_hash": "df880e3768af66df7557d2ad178d7364b8de8cdd6f333bd800131cf439bbd1cb", "ref_doc_id": "63e7e542-6cde-4e91-a0f4-abd22140a1d3"}, "54ead7d6-5c39-4830-aec3-c01bedf19cc6": {"doc_hash": "128a334a9edff4181baa8ae9ce9d4b549774bde8213fe444568890ee6bc6c693", "ref_doc_id": "9846989c-e9f6-4203-9f43-ead78b93b056"}, "2493352c-6848-4ce7-b76f-1450631db509": {"doc_hash": "d0c2e9dc40aa7cf5ca7f50210ebc2b385c005232b490ae2710c00f815fd24097", "ref_doc_id": "0625e51f-9931-491b-a8bb-38e09c44314d"}, "5a8948c0-755f-4bdc-83c9-20aca4c3cb7e": {"doc_hash": "b6299f15065d47fa608a4ddf139e2f02fdcd5236dec3adbd3849834c997eda99", "ref_doc_id": "9df3aaf0-a362-413c-90fb-5dffc961dacf"}, "da300fd9-7c05-4819-87ec-063376be7192": {"doc_hash": "243bcac1355e6a90bacae553c063dd4152581ca5cbbfd1cfa09c29175177840e", "ref_doc_id": "d7528921-b73d-466c-99e0-ea1a798b6e4e"}, "271fa919-b825-42a1-ae10-ae8a7bf2dbc9": {"doc_hash": "1244fdca6565a3bd2f704215f9f2df877f1732ee7ba5ca3a82238b2dbe34d23f", "ref_doc_id": "d7528921-b73d-466c-99e0-ea1a798b6e4e"}, "90868e43-9681-45b6-ad81-92bf72a12554": {"doc_hash": "0f12d221bc6ec0796b5c83aa6e4d820bd983306990754efd47a5998a8c067de3", "ref_doc_id": "1a29522c-4f4a-4bf2-81e6-1d74ca488d43"}, "5d112a2f-6ed3-4c45-95ba-f10e26bd8409": {"doc_hash": "d72abc90baf509c6e2c426fbe8bb459c25be46a64e3fde9fb1714c1d57b7fdc2", "ref_doc_id": "1a29522c-4f4a-4bf2-81e6-1d74ca488d43"}, "2340776b-533e-4ffc-9040-e45adeacaab5": {"doc_hash": "6c513b46abc855d3cabe7255799e65d8335d3d9de1d3106e666c15e97db4211e", "ref_doc_id": "da37ecfb-be3a-4872-8984-87093d4332c5"}, "53651072-ed77-4deb-8333-ec2620554703": {"doc_hash": "870af400d299b7eb70722794c6e1993ab2cfd8c0e4d2b08156b46998e0c20ec3", "ref_doc_id": "7ab61080-3d7b-45a2-9eab-9deb73de4042"}, "3e181833-f600-48ca-b018-52ee0db53259": {"doc_hash": "8cf9e743a3b904311f67f0fccb41d7e7eb97543ea7adb8ffa1fb4484d03eeb75", "ref_doc_id": "7ab61080-3d7b-45a2-9eab-9deb73de4042"}, "4bccd09d-fae4-4a62-b413-40373f96d9ea": {"doc_hash": "c46fba80176f50f1a34873413f3585623e2f72323096eccaf62f4d436f6f9aba", "ref_doc_id": "aecdd0ca-fe22-4f21-9364-de299f1bb421"}, "86c49bd4-c39f-4b8f-8f00-436dd2266263": {"doc_hash": "97702b7cc11e07d6cdb8c88b9f06a4bc16d3e7629319f01870bee44e30a01ea1", "ref_doc_id": "aecdd0ca-fe22-4f21-9364-de299f1bb421"}, "6818b224-146d-4893-8a9a-51be06871611": {"doc_hash": "99f08b7ca198425111f5cdc9f2cc98b5b9f701d756fc480d4b62905fd16d0fa1", "ref_doc_id": "2a786d27-e161-4888-8a09-dbe8ac79cd75"}, "7b99001b-ac79-4314-96fa-4678826312f7": {"doc_hash": "aa3c29b283bb3a976dea2a502118189dde29ec54daa8a88a5a0fbae141f4b88f", "ref_doc_id": "4a078511-b5f0-4ff3-90db-2eb2cd1df1e8"}, "9e792912-93da-49d4-a338-dc8248c92cd4": {"doc_hash": "3758845215e03be793afe141060bbff9733ddcacf3fc87c4689543ccf8e72862", "ref_doc_id": "db27aafa-5ffe-437c-80fe-782cb013c41b"}, "a5ab681b-0bb0-427b-8004-e4a8dafb00cf": {"doc_hash": "f0326b9797360ca76d05dceee486a1baaa683893ea944dc3a1382c3f09c8fec2", "ref_doc_id": "47f69e3e-d902-436e-8701-98897f601fe8"}, "76efa852-28d6-4caa-bb09-43345c43484c": {"doc_hash": "3d8458330ec5154456cfe88f8021245608a1111ecd721f0c06cf2e0a4896af77", "ref_doc_id": "3d28dc3a-4b54-4ba9-af63-f6b372141d05"}, "f924c019-fbc0-42be-9995-e5c026033e13": {"doc_hash": "83008f90605a83e35cd9e70b7c8f077f590f432bb176dee70a815a8aa05c2795", "ref_doc_id": "f1a34d79-00b3-46a1-9166-473f2b292e8c"}, "3f4a9a77-6c7b-4007-bbd0-22571a327dbc": {"doc_hash": "7cfd2c4e5ce139f1a39ce4b3af6f3cb07e12d2f0eeafbe87530cf4378e4b60af", "ref_doc_id": "d0efa5e7-736c-4f30-b3f8-a2e1e61a1817"}, "010343b2-35f0-400b-96fd-9b8272d9028b": {"doc_hash": "5f051a2f5f0a1250e5f9ebddd6ab7ddc90f776e98dd3321532829cb38d753afa", "ref_doc_id": "6ffc809a-9fa2-45bc-a00a-f46dd7a7f53a"}, "56eb98a5-c3b5-44eb-8eec-1119a9804f8b": {"doc_hash": "33c078a2bb845305f882820c6d3fc03cba9d146a45cc025723ae8097f09ac756", "ref_doc_id": "00c53246-49c5-480b-9396-3ef42bb4f090"}, "c81d2f9e-1d17-477c-9af8-e454710c3c53": {"doc_hash": "e22becfa7fc7b5348be80701a214f0cbea0c230c7a5417d2ed4f746ab8144e83", "ref_doc_id": "bf5098dc-6d7a-4671-b3c8-f7f8e6972b63"}, "ee888244-2d6a-4c42-9d75-2f0f25c19be8": {"doc_hash": "f1d204dfc43dc953be90dad053e7db984a29eb653aada820ece9518654e9b4a3", "ref_doc_id": "2d773a3f-33a0-4638-ac85-00b61f5e71f8"}, "1e54d55a-5c24-445d-8b62-21e117c8b33e": {"doc_hash": "37e441e8fef293787b97b1095dd8d8f4aa46e5f80f3295e3066474efef1e4977", "ref_doc_id": "8538221a-6eaf-465d-8c19-1bcddb3035ef"}, "c1a84491-5209-44d4-96d4-95d57a6881c7": {"doc_hash": "41a3a88c3ba57c52c21916dd16b4fe4ff57c13b19ecd72a9d696deb1948e3d90", "ref_doc_id": "e5c9c990-88d5-48d9-9f41-eb9c2c7143b8"}, "dc8a4c5d-8f1d-475a-8c66-d799263e379f": {"doc_hash": "3cd847179a9bee08323d340da1a773387d4152eeaad765541ee121bedd72db4e", "ref_doc_id": "e78bb85f-ebb4-405b-87cd-18a2f5dd63df"}, "e2c8ad2d-8030-45ff-ae04-aac9b9a5cae6": {"doc_hash": "42a17c6b670336878f0233397730fc83aeea35d7fcf2a8063115143a24e973f0", "ref_doc_id": "6629e62c-cc7b-4d80-8ea7-101f50d01dad"}, "e92f9ed7-3778-4e28-939b-87f513caa73c": {"doc_hash": "3bcf27b51c97afafe34f22b06998081c5af8628a809fd39ff69f31e3a4a34942", "ref_doc_id": "15f33460-1672-40f0-9c38-0bc561f4bc5c"}, "085f21eb-e129-4552-9fbe-9207eb073414": {"doc_hash": "5e4b020b43303c179cc9ede3a8efe48823dcf7daeb1ef015992e55f5010569cc", "ref_doc_id": "dee80d4d-37ba-428b-b5ad-70ead45bf44d"}, "6efd25f5-0e2e-4e41-9133-419240ae7b64": {"doc_hash": "19bf9d62b72d28aad5d9ff1c1073e464efa16e295f8a9598f7c48297899042b3", "ref_doc_id": "5765a62e-fcdf-4246-b636-4f91b6cc1b68"}, "b94095fb-a0b3-4e45-970f-cccead0f6757": {"doc_hash": "890018c8d9f56bf55a3ca1d9639cc1c2e3b3ee969121ab7bb8f8989bd45b77fe", "ref_doc_id": "fa6ab322-8b62-4268-a9b0-5841763f99e1"}, "418638b6-4c00-4c2a-822a-fb5cfdf37540": {"doc_hash": "50182c7f53007dd05ef5b64f7abde0628640d4fbac684d03e13389a23e31e1d9", "ref_doc_id": "25c5489c-61fd-436a-be49-d9374ed3b412"}, "86ab90b9-fc34-4269-b36d-1aad4eeeb386": {"doc_hash": "ad841b315d40bcf9cc400839e385907a7a4fe12289285289d5ff1efa81ebda88", "ref_doc_id": "bba78ebc-a9bf-4312-a3d9-269b18e25020"}, "64c1be50-1b0a-47e2-9bd4-b070e02d9637": {"doc_hash": "4a98a3721dcd64aa870c707721bb0dea6bbb49a2509492021f4c77b2d423e0e4", "ref_doc_id": "3c927528-dca5-4559-bf82-53e132977f9d"}, "d054ebab-121d-43d2-8bc9-6e4b28e49f10": {"doc_hash": "1643d8d1194edbfbe4b1880eaca978b0cc24d25fcd74f46730692d3e8d2addb0", "ref_doc_id": "26a88d0d-72df-4257-a412-51b92921486a"}, "62fe18bd-0f14-46e7-8949-1baa4c90880f": {"doc_hash": "bf2446a2d9e19e8f73618d71f5dc9e74572ea1048d1edfa7218968b3d0d8c625", "ref_doc_id": "b2985ad3-ffc4-4d25-9afd-db2268e34fd7"}, "1711f4c4-eb63-4321-802d-5538678ed38b": {"doc_hash": "511f7449fac00049e03ea0f936a7ba608e7d9ad0055c4d0a8bce330a23b9613b", "ref_doc_id": "76794624-b8cd-4fdd-a907-0620c2ad634f"}, "6bb5da42-40e2-4735-90cc-44e4fbd399a6": {"doc_hash": "3f3aa15431f507b15b947bea24972a4d4c78090f85c278195b617a7a58b9325e", "ref_doc_id": "e1b31a7a-ebb2-477d-ad0a-d0a09799ea61"}, "c13c30a4-925f-403b-b71a-602e02f50803": {"doc_hash": "b603f6cab24af5b3714238d75e51ce3e8f6c5eac609b73a481cdae1c2eb95f36", "ref_doc_id": "fab0a071-c1ac-4cc9-b04d-072e0e5abe4f"}, "46bc2a51-03d5-487d-b4cd-4c6ef1dd0bbc": {"doc_hash": "19ef9dd1986e42211b8119da6f160a86a72c79f04a1192192e2a1667f606b5eb", "ref_doc_id": "fab0a071-c1ac-4cc9-b04d-072e0e5abe4f"}, "34d0a42d-c31b-4fc5-9ef9-986fa7a67967": {"doc_hash": "ee3238fa0e26cdc8a7b5a193c8f80d75b3bcb622e3ce0e0a7279d5fc0e9e6f44", "ref_doc_id": "35c748ce-0f7f-487a-94cc-c304e78cb68e"}, "3dbaee3f-d706-44d9-9d9f-cc962b61915f": {"doc_hash": "4f8997dc4faaeb00f8873876817642c94051555e85dc2e4f32a59553a0b15117", "ref_doc_id": "35c748ce-0f7f-487a-94cc-c304e78cb68e"}, "70334453-1c6a-4f22-b184-a111fc664994": {"doc_hash": "1854b472ed677b2f764bffdb019d94b9fddcec6d83f9ae07b27a902b325c1abb", "ref_doc_id": "b56ffd43-7fb5-4423-aac7-b1ed99005058"}, "2ee89903-3efd-476f-8d84-187d3403d68f": {"doc_hash": "97890ae768a4065621a3a587dc8bb84a7bad926a07017b711271803a4ed23566", "ref_doc_id": "b56ffd43-7fb5-4423-aac7-b1ed99005058"}, "a2153e1b-3877-49c8-b864-d81259977824": {"doc_hash": "422b02cc477285c1c27f68cd0de035dfaa4647fa9c2db205a406de7a06a09ccd", "ref_doc_id": "b470c5e4-e2fa-408b-acc0-7610283b9f46"}, "01e27d7e-880b-415b-b5e9-db02c081ea98": {"doc_hash": "ac8e7a23a31709b9e0454a0c2c5b6c3a26dd0d13a521d844e060e429698937ed", "ref_doc_id": "b470c5e4-e2fa-408b-acc0-7610283b9f46"}, "e2425432-1cfa-4edd-9995-d7fa1ceb8742": {"doc_hash": "ab88fe1d5eb2ab030c807714f878da57782d38467f09dc766a03f81ce51ee8e1", "ref_doc_id": "765a50f0-f35a-4c48-84b1-4b78e745372a"}, "1ce24e59-defd-4e20-8208-478c10b06c05": {"doc_hash": "a74ad87aea6db3f7ec2c78e2a2ed9eb789a1ad995d59aa33c179eafda8e0b59a", "ref_doc_id": "765a50f0-f35a-4c48-84b1-4b78e745372a"}, "fd0140ba-e784-464f-a3e8-325ae055f634": {"doc_hash": "16081a9aafd19815b4a2842ce9d5459f3d0f7550966f4a6f1b4fb0324cafb93f", "ref_doc_id": "169a72c6-f20e-45eb-805b-7529751a4caa"}, "a3aae84e-2c2b-4d55-b71c-e0869acd272b": {"doc_hash": "cca1c0e096d6c4c2dd9b06852d358c7a7b5341bf5dc87293a9738727d4a9bf68", "ref_doc_id": "169a72c6-f20e-45eb-805b-7529751a4caa"}, "41244adf-b664-42fb-8461-a091d86489e5": {"doc_hash": "356b0493cbfebc8a3bed99f97da8a9eca61bc2c03f732f842ec02c6f48269589", "ref_doc_id": "72fa8667-c192-4edf-bd23-0de79807fa4e"}, "a6c1492f-7a77-4eb9-80b9-3a7dbf7ced63": {"doc_hash": "1f6aaf3d2c2ef7be9ae674f154c6b32e8ff412004048f452985a52da624c48c0", "ref_doc_id": "2efd9480-141a-4edb-a4f8-e407d6af6ab9"}, "40b36c25-2ce5-4b3b-ac15-41680f568264": {"doc_hash": "3a8b755b22c81f8ff724803fc5191be82c81691142cace3e591f604a5d98e393", "ref_doc_id": "19d95850-8684-4093-ac2d-c7d4db626f66"}, "ca856611-5a4a-42e4-af87-5c4341730378": {"doc_hash": "8bb6edef6b96faa511a3a99c2806066683399de7af0a0a33f674a773c85fa5e6", "ref_doc_id": "19d95850-8684-4093-ac2d-c7d4db626f66"}, "b18fabbb-b720-4165-9734-d6c183767ac5": {"doc_hash": "1a0d92841b26f76f9124d0f25fec27fdc19a6d2120aa4a73a716578028ed8011", "ref_doc_id": "d69cf9d1-e3c3-4a64-8666-a9929c425f97"}, "6835e32b-f647-4142-a00f-c0a4cb605e1a": {"doc_hash": "ee8c1cf535b565c2d323de29925408fc9b99e566f6f29b674881462154cf7ab1", "ref_doc_id": "d69cf9d1-e3c3-4a64-8666-a9929c425f97"}, "bdbaa196-f7b1-4c32-81bd-6677e2055287": {"doc_hash": "d605fdf0cbcaa800aa078a8b0ac86de0729c0d4618f5b35ccc3894e6694108bf", "ref_doc_id": "55251ba9-1e8e-447d-81fc-9ddf6f66fd9c"}, "37f09b6e-8f7a-45bb-92f8-cd651dc6955d": {"doc_hash": "7fbc3f0c0567e543479213cdc08e64354aaaaa87f6909d42f4347054fef38229", "ref_doc_id": "55251ba9-1e8e-447d-81fc-9ddf6f66fd9c"}, "f8b5b9e1-65a9-4a75-9402-1bb2eb76759b": {"doc_hash": "99b233fc7fcb90a4e0b834168c1ca05e56664724d52a8699a362163a8297a07b", "ref_doc_id": "127ce84c-719d-489e-a1cc-ec0023b7fb41"}, "c9e16bfa-6ff1-412b-a219-26e41094b863": {"doc_hash": "d43551c7269ca45db43a699f3c06835f0d52ef10642e6bb4d8b9c673624fd4d6", "ref_doc_id": "2b5c7e98-453c-4cd5-a436-8804597000b7"}, "6b632007-29f6-4c4a-ab9b-df7fb2fc7f2f": {"doc_hash": "58bb135515deabfb3fc77a1f76d23a4a1feceb3d6ad48cc414ce7f7677dbfc31", "ref_doc_id": "d9fcba23-6d54-4844-aada-75d373dfc994"}, "cd1b9809-1187-4b37-b695-226b8753187f": {"doc_hash": "05dfaa32ab129308b56bca2672d07aa5b5c6b274fc550e403155ec2dbf45a921", "ref_doc_id": "73f6ac98-d20a-43b7-8ac7-7b35280b7bb3"}, "44872cca-5aeb-4d05-bd39-e2d8fa15dfc1": {"doc_hash": "3e60bbe954057ed150c2ff5a06ab1f5d485c770bf043be96b393066023066ec5", "ref_doc_id": "46f34564-ba2e-4b64-bdd4-5e80b316fb0b"}, "2d21c534-1c21-4588-8486-e1414ba6fb6d": {"doc_hash": "d97f16c5414e43f9e223c1d5c51871b710dbcb51c08df20fcbf0d5a6684bcfd8", "ref_doc_id": "fe92000c-6b6c-4e23-a8e2-48fc992ed62a"}, "65575090-8554-4378-8906-8b6ef9985d0c": {"doc_hash": "3a7b647be017e981fc8f159dfd2ae22fd75d4d0cb9f9cdcee4ffdc2eb2ec5d56", "ref_doc_id": "290d33d1-2795-4809-ae6b-ed770523a435"}, "0818c7c5-6ea4-4fdc-bca2-acebe99e22f2": {"doc_hash": "65826f5060ba2907d5818a69cf2f5841812b50c16653579acaf76638b7ac96ec", "ref_doc_id": "c85e02ff-d6f9-48d0-beb1-6fa5e01c785b"}, "6ffba7fa-8efb-4e07-9797-44509cd7458e": {"doc_hash": "46dc2081e7d38497786e20283d6babc72a2a3fe88de9be98b33de6da4e72b494", "ref_doc_id": "51b18c4a-6a26-4173-a5ce-5e4555842d51"}, "111c05e6-44f0-44cf-a6af-6cea987aac7f": {"doc_hash": "ee122525db48fd729673c7402c2ab325591bbad337d83abafcaa978064abf427", "ref_doc_id": "44f75753-6a5f-4f89-bf50-200e23cf3644"}, "b45f3c7a-0296-48ef-8104-e174fbde26eb": {"doc_hash": "bac37de03a6962f81edee879bfcf70ffee8b7e4e11fb6a7393e858bbdb2f5b0e", "ref_doc_id": "b65e1569-a524-4fd5-9d6d-6f16c45e7a2b"}, "c69f20ce-6e76-416d-a04c-9c6f934a0620": {"doc_hash": "186ef598d862092025717c4ed175ac38c7b3aaa7d874421225558a96c84ab7a8", "ref_doc_id": "0df19d52-f2b4-433d-8dba-194eaac0bcd3"}, "5f300f12-bf91-480f-a794-216e5fa9113c": {"doc_hash": "1bef806dc7b12d368c3f9a2595e2b4f27ed6087ae58d726a7a8f327f7826716d", "ref_doc_id": "7d744064-fca6-4808-a94f-5d0f4b60f93c"}, "fd7c38b6-41fe-4639-86ec-40fcf698edfc": {"doc_hash": "7192a54763e64041b2aef322c77948b9575df0ab4f77b5166a7d5faa1416d023", "ref_doc_id": "41487428-7b98-4c4e-b2aa-db8956dee02e"}, "3300171a-ef73-48e9-966d-8608aebeeed6": {"doc_hash": "ce769292cdb1116f5f23661a3ce11fb22a25d4fbbd0d203ace2536537f47ed1d", "ref_doc_id": "41487428-7b98-4c4e-b2aa-db8956dee02e"}, "eb3446c2-5d33-4251-b31d-ae34f1bc7002": {"doc_hash": "a2fda93715758d8c7a57a7e6c6b5c2a433c5b6b15e4721ceec02e41ca5858313", "ref_doc_id": "74d5a7d6-5102-4cd9-a7bc-b2f946a04eca"}, "ec12159e-dc79-4a62-870a-cc72bca972f7": {"doc_hash": "0a097741ebbb66e83e8cccce21877cd784eccac804faa908d296b5409aecb250", "ref_doc_id": "081dc218-d569-4b78-afcc-03d5ae64639b"}, "e518d1a9-ccdf-41f2-b3aa-bd36b0be071c": {"doc_hash": "229f0c30979f226cc88a2bc78f501c190cb61c733662595b12b2d8ee8a6627c5", "ref_doc_id": "4b1fe07d-f576-430e-a5a5-5609bac620d8"}, "9bd5149a-c217-41cc-a14f-66074114e956": {"doc_hash": "06be0ec0eaf6870398e53b70ba5af46c71e5ab8adf6280926618d17ec01fd6fe", "ref_doc_id": "6f0be2ca-a6e1-48d1-8a09-803b0ac6aadc"}, "806188d5-d066-4b7b-9627-2a65735ec556": {"doc_hash": "c2b87c4cdbe72b5224f92fb8cd0a8e85cc84198aeca16a9c9ec698d63042264b", "ref_doc_id": "5d86632e-4098-49ef-9bce-5941abfaa41b"}, "69d33f2e-cac0-4591-b5f8-5f7675c7c8c1": {"doc_hash": "0126f78e5ddb3cde527d3aa3b2e7392bd517964b05da36948e3abeab8d326503", "ref_doc_id": "31bab90b-6577-4f76-becd-d41b6ef67ca6"}, "45c99848-eece-4f2d-a205-ca5264b05725": {"doc_hash": "fbb8f6db6b2111fe654a90d6a21cc684d7ab7a9a28044c6d54114f470ec485ee", "ref_doc_id": "37402106-d5de-44b2-ac3a-42a2c3335742"}, "550f8478-b7f7-42a0-9ed8-be8b03dba448": {"doc_hash": "3f23d6ab08377753fc53e48792ee3edffcbd7039bd3bbcf995a7e69d02a88b3a", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "4a3fc04c-4c55-4a83-8c8c-09c3ddd93e2c": {"doc_hash": "5ebcbe5987cb722ebcad3c784104e1aa5b5bc8819f7fed2aff8d9a16c0abb561", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "e915169e-4c4e-43ae-8baa-fd82556feb0b": {"doc_hash": "f12742d7939d56a891c5743cc41fa22aed0b1bc87e5a91495eac36eb437543eb", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "a6148072-b16a-4f17-95c8-178842c9b272": {"doc_hash": "f34fd2e3669432cde1adbb21945fd55b2ab8b7d19c0a75e58cb06e089047d608", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "2b014450-7c46-4201-bccc-8fe19366e485": {"doc_hash": "eb736e73fdfe4e909974826163631e6ddecb3c324e0a529cd07995c2f2de0e41", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "6fd1218a-1024-4718-b57d-f6521a0ec07d": {"doc_hash": "fa413707499ca077f4245bdb3603ac3f8e1984ebb9a294cd37a7e50a073772b1", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "30cccc45-b025-430e-984f-08eca27b6aed": {"doc_hash": "35571840d48e2b9b574b56fe03f5a68bbdd3fd0d1873fb4d0fdd20ba1ab82301", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "3bbdc785-3fa7-4868-8d42-98e455bdfe10": {"doc_hash": "1b98cf9b1da81bfdb73bbdc5e2f50aa528d72206839fea6ed9eea5b08c568047", "ref_doc_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0"}, "67e7574a-b270-4ee7-9456-786b5e28f468": {"doc_hash": "b612691c1b78b67e65f2c5ae8a735e943471d9e0fd9e2b3857dcc984d3d13aaf", "ref_doc_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec"}, "fdf24518-2885-4c5a-8179-83a0cfa9112b": {"doc_hash": "dd5c862f1edb13fedcff15e26411adcf49149f7735b814c13d2e69945fad0094", "ref_doc_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec"}, "f0083e35-acf4-43d7-a656-3b29f2bb7f21": {"doc_hash": "c714302767d1d9115b4629dfa129c1392656b9da375f1d445d3eb3c336f916fc", "ref_doc_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec"}, "91ecc5fe-dbba-4360-b3f3-911840d92d0a": {"doc_hash": "f6c1de6dacbdce1c80b5d9addf2c0304d84101629a874958f50f2d2be804479c", "ref_doc_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec"}, "5b95ed1c-5c5b-4c9d-a133-46ef888b63d0": {"doc_hash": "ee1b46fb0a0d5c35b4dff79c7891c87d4c5018c40f8f119a48848ffd7053e69a", "ref_doc_id": "964ccec2-45cb-479b-994f-2efb4ddd721f"}, "cf9a4a53-65b0-4c02-bcba-cfad0aa8dbcd": {"doc_hash": "7edce2428da10e7647b7a3695d797cbba81f08d98d5b633db9c12f17d3d28831", "ref_doc_id": "9fbda692-108f-459c-908b-7a1e6e7c315b"}, "e2f8856d-0c04-494e-a2ad-fe6fd9593674": {"doc_hash": "e7bd88c2b0870e03cecd33738a8cb5347f539bf657267a3e0d3cf9d1c7ee40b5", "ref_doc_id": "9fbda692-108f-459c-908b-7a1e6e7c315b"}, "cd536435-76ca-450c-9c80-2db2199fffc3": {"doc_hash": "d85ab0db43af395400b3ca32dce113469a8de0f0994da4244c58c5529b2f4d8a", "ref_doc_id": "4a87b7d5-cc94-47d2-bbc1-fb14e2720577"}, "db00f74b-808f-4e94-aaea-2d2c35730878": {"doc_hash": "8794e950082fafa689c24b45435f4c8d1e1e918dbe6867aa26bbf60b66187fb1", "ref_doc_id": "4a87b7d5-cc94-47d2-bbc1-fb14e2720577"}, "f42d2590-74aa-4743-bf27-3da25faeba26": {"doc_hash": "53c35d06b8172ab4c7cf56e96e4dcff200e584ec03b15866c9109980c9494efa", "ref_doc_id": "fe550835-8566-4978-9bf7-dda40e2b3bb3"}, "9c74e65e-8133-4256-96c8-cb380760aa08": {"doc_hash": "677d22140aaf94bc7c8dd6602cce48d5b930e06c001a456c2d65d94739bd3032", "ref_doc_id": "fe550835-8566-4978-9bf7-dda40e2b3bb3"}, "d5b246e6-0989-450f-8c13-c66b7f5d5f11": {"doc_hash": "5c6871a5498c6e96ee898bda12edfc81e967b1d2e5481cd78b667ad7c66931b5", "ref_doc_id": "cbc66741-005e-4e68-9e5f-d4861015a063"}, "e74c06dd-c210-4c89-9545-c968ad577785": {"doc_hash": "0aa48ce5825102fac2d197bda61d0ddba2e3c62af4b5f757ab7a8fec69c7c5bc", "ref_doc_id": "cbc66741-005e-4e68-9e5f-d4861015a063"}, "f2610876-7f81-49f4-a38f-6159563de431": {"doc_hash": "da2ad08a270546e984e214a6a53c1a04245bef4ec541875e08f8dbb13fa4354d", "ref_doc_id": "b6b75af7-ba77-46cb-8dcc-4a14b1b4a579"}, "f04ca86c-77d5-4068-821e-e839b3ec2a6a": {"doc_hash": "b4d37a849f8ea242f0dfe407e7846b8399dbc6d058628a64c5ec75d86777d18e", "ref_doc_id": "b6b75af7-ba77-46cb-8dcc-4a14b1b4a579"}, "6b95984b-a08c-4bda-878d-da8bae1d48d1": {"doc_hash": "f8f61e5ba00a133b362fcd8d87516f3d71d5a7c6f7506e40d2a5ec8b08403672", "ref_doc_id": "d7368dbb-4b16-4821-a93b-f086b2ac0bd3"}, "9fa8765b-2fa5-4aea-bf2d-00fa9621de19": {"doc_hash": "d6bdcbb509e4f431a994f8c768931957441781d507ee76799878be4452552edb", "ref_doc_id": "d7368dbb-4b16-4821-a93b-f086b2ac0bd3"}, "c63eb826-e8a0-484f-88e2-9b26d5587079": {"doc_hash": "bdf86c736d85db006313c235e55958b10cd89fc975222339dcde66753f70a665", "ref_doc_id": "85991672-7277-4561-809c-642434fefec2"}, "c02d0a5d-544e-4159-a655-f149336a4cbf": {"doc_hash": "8ca852cc00d08f677cd1a59610a56e80f93b53c5cfa41f9312cf4f00c4858007", "ref_doc_id": "85991672-7277-4561-809c-642434fefec2"}, "ae23b968-f18b-41f1-a4c5-408c77268c0e": {"doc_hash": "84e822610c8a2f4c44b304b4a72201479ab153156e899315901174f278212c62", "ref_doc_id": "f951dd04-b13e-4a67-8333-0828e8420835"}, "b818340f-eac2-4270-9f60-4bb0d6f1346f": {"doc_hash": "c225f13a319c4eddb92cc7b87ce09d718b8b27ffc653330067aebc5478976d56", "ref_doc_id": "f951dd04-b13e-4a67-8333-0828e8420835"}, "7f4ba791-5a5a-4728-a735-8208fa00ebbd": {"doc_hash": "64f5ce0c56bb79c6614c168a1e06ebac23a35430ccd2673b4c4e26065f2ed9b7", "ref_doc_id": "097d0a30-4c1d-4e9a-8e0a-78fa3a40bdaa"}, "6dbc07b1-2a4b-4964-a714-2b868d248041": {"doc_hash": "9b48f7c279b351dcb9a8eb6be99e45178b7d3e60a34e25bb410213e020a05d3b", "ref_doc_id": "1246465d-6e2d-4544-b407-84a9a1652ebb"}, "180de058-3c24-4192-9eaa-ff4f3b44a64a": {"doc_hash": "162cbd0387c0b0c8416a2b522acf51254f8ba809cec1f78e22237a3064539e4d", "ref_doc_id": "75729f0b-27fe-4751-ad50-dd5a1a24779e"}, "e1f883d3-3e01-4189-a9cb-c3d819efabce": {"doc_hash": "7b972b4a520af78085d10cb76d45bd89bdc2a44b54d2ace06d7c0ddab8e98685", "ref_doc_id": "75729f0b-27fe-4751-ad50-dd5a1a24779e"}, "ac247e9c-fb42-48b9-ae91-6f3c97c3feb6": {"doc_hash": "061945f3f8cd24bd4ef85e344076873df3bb4e2fc5112fa7308c095e752cc998", "ref_doc_id": "d8a68a8d-8160-4632-85a0-5771f9c59475"}, "db8fbd06-1ca9-47bd-94ab-567be42f71f3": {"doc_hash": "92993a863c3ed02a8b8d2e22c6897e01cc5fba8617e8fd5d12bae737d7a1dfd3", "ref_doc_id": "cc393f62-a902-44e6-808e-d7f8c16b6c4f"}, "5b8ced69-a224-40b4-bfeb-54825d65166f": {"doc_hash": "88657b633c4c98ce1e70286d54f8835dd6536fd113c5f47702950bee53171b57", "ref_doc_id": "92a62af3-d928-4365-af5b-a53c78692b78"}, "72e4257f-3b54-4d69-ba5e-b193bea61493": {"doc_hash": "20387ea87ee81602a2b6d95e5b5ba855351332984061c111ea0141aec5d10155", "ref_doc_id": "92a62af3-d928-4365-af5b-a53c78692b78"}, "b7ee6a1b-726f-44ef-b183-313db6ecbb9f": {"doc_hash": "11d6bf4014a83ccb352dbe19735166f3f449f8a1a4ffa085b0565083e4ba9675", "ref_doc_id": "c0869ce3-4467-4487-a5ec-5e285d44a0ed"}, "2c97bcd6-29da-4385-9f05-ad1f9641c326": {"doc_hash": "66f5f74295132674726f2043afc83d34139cdb30212b7a1d2f1ffcba3cb8d25b", "ref_doc_id": "c0869ce3-4467-4487-a5ec-5e285d44a0ed"}, "bd69ea8e-9768-4950-b3da-d66371154db5": {"doc_hash": "d9ec640079d2556df307e958a31419755701cd23818202d6e59eb7f1f11ee998", "ref_doc_id": "b2c519fe-18bb-473f-9883-092ac3946401"}, "5ce94892-c4ab-4376-81fe-d465f57051f8": {"doc_hash": "1414ff239823cd982b250fad177b44fe4c549e6aaadff691d88e236a7fccf451", "ref_doc_id": "b2c519fe-18bb-473f-9883-092ac3946401"}, "2a465e95-2188-43aa-8bce-fb3d0869f27f": {"doc_hash": "43010ef633cc5f625253ebff34cdc5c140d970632235c33ab9bb78b01f2bea51", "ref_doc_id": "ecfcc254-dc58-4cdb-a5a9-25a68d1cc523"}, "47320d82-9459-4f0b-8e33-736486fa077c": {"doc_hash": "e4578a471cdb06166733b607e6ba6b3ae608107878dcfa2139bb54e049922ce3", "ref_doc_id": "ecfcc254-dc58-4cdb-a5a9-25a68d1cc523"}, "6d2392be-f487-41f7-a543-37c1998ee372": {"doc_hash": "5a7a2118b85d3009a33dadfb58b465dafeda67c2d5961933e66f5638cda4c694", "ref_doc_id": "8e26dbcb-1217-41a6-9162-602b56510796"}, "cc25314a-1ece-4ea9-a79d-f5ef7d4fcccf": {"doc_hash": "624c4d9137a8bd276d91293209223d573a2c3bc036c30a3c9d08110dd2b1fa33", "ref_doc_id": "f1d5aeb3-c4c4-41df-a150-c313644ee1ae"}, "11c1b280-2e77-4edc-a668-9d8365c8d72b": {"doc_hash": "9b566f4b9ded4c19d5702cfb63fde723b2a56487d8e546c1b5d20222a622b601", "ref_doc_id": "d3ad39fa-e5c4-420a-bfe6-1d890e2db942"}, "2d87e85b-da3c-4927-872b-a0413cc7932d": {"doc_hash": "b04d20b89f6e76ccd089a0fd1d2d90acd593a9acd40231bd84c4d11208b61402", "ref_doc_id": "d3ad39fa-e5c4-420a-bfe6-1d890e2db942"}, "6771a3f1-af1f-4899-a947-c06cbebc62a9": {"doc_hash": "4f46425c4807a534feb285def7c95a3ed8e561ce0ec444af6753aaf81372d545", "ref_doc_id": "e34da759-c53d-4e2e-8e45-ad9067ea6b07"}, "9e07832d-d775-4c9c-9490-106dab31ea79": {"doc_hash": "0776435f3f8dc87e68660161390280168ec9ccb4ce9efec292213227722d1c34", "ref_doc_id": "e34da759-c53d-4e2e-8e45-ad9067ea6b07"}, "e0de3564-2693-4092-8829-31e7138af7fd": {"doc_hash": "4daef9ba8c7966a7ab7515f984627a93db133e451e63de1a4e379af83146a549", "ref_doc_id": "22c61995-6340-4dc5-a3f8-9e39b1234199"}, "da69ed1c-f13d-425a-96ae-be2a0f72abb8": {"doc_hash": "e8061041c7e12e3adbc0626a65c60885cb65041b0b54c3456cd577a659068f45", "ref_doc_id": "22c61995-6340-4dc5-a3f8-9e39b1234199"}, "ad270ec0-39cc-4bae-8997-d536b0f84bbc": {"doc_hash": "d100888344e008e38580d22e00e0d2c78f31fc108f680a40375cffa45b68be73", "ref_doc_id": "22c61995-6340-4dc5-a3f8-9e39b1234199"}, "a8636c90-fe85-484f-9a95-c2d2a34bef32": {"doc_hash": "0b57db45b619e13a55d5f0fd0941a49c3685c74304a3296a4fce7e4f358f5d7c", "ref_doc_id": "f4ffa069-fe6c-44d8-9c11-c97461e72a94"}, "ecfc48bc-2d47-45ff-9ecb-bdbb373890e0": {"doc_hash": "595328b75084908d801ed64dae830bae2af29dc2801dad6371ba27d4ab94c5ad", "ref_doc_id": "424a0381-3eba-4d92-8a5c-e471aedd99cc"}, "64db4296-a7d4-4a34-ab19-87a96cb8651b": {"doc_hash": "8d9f89738e673b67e57b432e0eba5a48475a54fde894abfdc1908a04a14ed7c7", "ref_doc_id": "424a0381-3eba-4d92-8a5c-e471aedd99cc"}, "5356ae6a-f47f-4a75-8cb7-bbbc5ec974a6": {"doc_hash": "7348f6811a29b9ce664f6ad7313ca79677b918dfac4ac37b8e74ea9d91ad77bc", "ref_doc_id": "f133e66e-f4aa-4101-a91d-77e6716d0b94"}, "05c47fe6-4733-414e-9172-ddc8e36c2cc6": {"doc_hash": "fcfee8a414e1ddaaa1b9a608911be77442d506d7a153a4a30006ff0f286cac16", "ref_doc_id": "e33c8ee3-0f3c-40c2-9ec1-709a23f9b896"}, "40b58cc2-8717-4d94-8304-fd02b8c90750": {"doc_hash": "bccbcbed428d2887e21e19b97f1636798f486e3be062492473f48e4a33ec6231", "ref_doc_id": "e33c8ee3-0f3c-40c2-9ec1-709a23f9b896"}, "a1aee49c-8635-4809-81cc-22dd1a028d73": {"doc_hash": "a84df77bdc2f36f8afe05f2ab6bfe2b670f4edf7e2c39fd9d7fee26053819f5a", "ref_doc_id": "6ab9d980-17e1-4252-afb4-078c8c8b7a47"}, "44a127ec-4b89-4524-a47d-9cdd238217ba": {"doc_hash": "4a8488190add8d175bd2cd496c81a935076f7b677bf1cd7e4e162565b550fea0", "ref_doc_id": "6ab9d980-17e1-4252-afb4-078c8c8b7a47"}, "59630cb9-efe4-4e7d-be74-295cb786981b": {"doc_hash": "df83dffb28f31e5014a9725dc570fff42da4be09cb7ae9252250674e84683b21", "ref_doc_id": "d865c805-9711-4514-9cf6-da476d623707"}, "90940378-913a-4d2e-b443-ebdd668c0032": {"doc_hash": "57c8f36d57db4ad2de263fe847c9bc406369c156b512704d21a2a41ada3dcf97", "ref_doc_id": "d865c805-9711-4514-9cf6-da476d623707"}, "b0079c2e-5a31-4a93-b44e-e814a8579c5a": {"doc_hash": "394b95fdf414a8245812b6a9632593f0bafa36e0d9981a0439c77331d739fcfb", "ref_doc_id": "c90da06b-2d2d-4da1-a71b-54219707de7e"}, "987566c4-5808-45aa-9c51-d7d0453c83a1": {"doc_hash": "76eae6c2b1d0b7dffb720e56f372f0658b72cc0888bed4c30ac10bbfb40281cc", "ref_doc_id": "c90da06b-2d2d-4da1-a71b-54219707de7e"}, "119826e8-fda6-4eaa-ba29-da12ec42d13f": {"doc_hash": "5e85830fcb4460a2bd6b07f1407562bc064ed67e0fc948094880136f5cb6c355", "ref_doc_id": "c11b7070-63c7-4487-801f-b89d4cab6f3b"}, "76b27637-f500-4fc1-ab75-4e0fb567ef0a": {"doc_hash": "94ea1cf28c2615a6c3b7e50e85cc8db155745dd20f9109b78610f7a4c266d12c", "ref_doc_id": "3c2aa291-1b69-43f9-b095-0f6ec4d4ee0d"}, "f7acd1cb-bb39-44f1-aac6-aec3e0ca7ee6": {"doc_hash": "753df5891031133877ed917fec013ec5858879740dcafb6755975071a9006ce9", "ref_doc_id": "3c2aa291-1b69-43f9-b095-0f6ec4d4ee0d"}, "7df97166-f136-48f7-97a4-e2edafdcbc89": {"doc_hash": "0ea93346b818e89cec3693cdf3345f8a7a46c1320a4141a2c8b8eed957c3f7dc", "ref_doc_id": "26543aee-e965-4abc-a05d-c989100253ff"}, "722967fe-b1a7-406b-bf44-c42fadd1c7b7": {"doc_hash": "a7334fd0ceef27c181d6de59f275c9683f18dadfa3c7b8a789c4321e88ba1b9d", "ref_doc_id": "1191fc05-c2d6-46fb-a40e-a81d11a95925"}, "142ff1e5-a27f-4899-a52d-e70db747ee1d": {"doc_hash": "63b0816c3ae9e182b7c39f3a92842743c0e68ed1f19956c9b415f679c7b8ba22", "ref_doc_id": "1191fc05-c2d6-46fb-a40e-a81d11a95925"}, "0ab92e01-67b2-408b-b8b5-cd0a393d2c73": {"doc_hash": "ec77ccffdb168442057e02b4658a12876e481f55ab472748898afd8fc33c3294", "ref_doc_id": "1f1b73a8-4a72-476a-83b3-25cbf32febde"}, "1aca5e72-ac85-4913-9ed8-ca5ccc87c076": {"doc_hash": "8aa1b354411643da3a67ed3f865d5877dff1f5345fe3f8d096a76135c9ae96f1", "ref_doc_id": "1f1b73a8-4a72-476a-83b3-25cbf32febde"}, "8d7c20dd-3f8e-4a6d-b2db-f784daa11d75": {"doc_hash": "89c072af792bf337e3b10c09f0be07e8425c9c56510022b640b95ff1ef3d565f", "ref_doc_id": "4ac14750-0d08-4951-8f28-d995dbcba3f4"}, "e2afc2f7-55e6-440e-b676-51c17f1e7ded": {"doc_hash": "7abc71d2bf4f893deed940f1c54c1d15af0044220bbc2bb80c5ae6a0b3e9a35b", "ref_doc_id": "4ac14750-0d08-4951-8f28-d995dbcba3f4"}, "b1ecda72-4777-4a19-8cdf-31b1e1766383": {"doc_hash": "bb6ac5b463f9961a42ccb74bbefcfe9b845367bb6e9fa563193d64b1d6de0b53", "ref_doc_id": "34e1d986-db11-4c4f-8889-68d952cc3892"}, "2183f7a0-acc2-43dd-bfca-10f97cbe1364": {"doc_hash": "f0d7cc733840aef143660b7202a4c3fa5f55a0993156bf1c1ed900cd58f5e2b1", "ref_doc_id": "34e1d986-db11-4c4f-8889-68d952cc3892"}, "952cd26b-0cdb-4a83-8e06-58ad9daa563c": {"doc_hash": "96898024952b9aaef8ebcb4f4ce215b19deeee02af892785c25f6be3257f75bb", "ref_doc_id": "ab60f9b5-6375-4c31-a59b-b11006df4710"}, "401d0d83-8cbe-4aa2-a355-8eb10b3a8ec3": {"doc_hash": "9e2c66c75da8c20054d2ea980f65fa2b8fcc19a385bb17c88f4596061d059dc3", "ref_doc_id": "ab60f9b5-6375-4c31-a59b-b11006df4710"}, "3f6d7066-2be9-4bb8-9262-2c53a65e9939": {"doc_hash": "7b96460882de1d283b9b25a8d26b744f7de1c3e431358589e0d3c8590c0e9ef9", "ref_doc_id": "9c3b73f3-b95f-4239-a4e3-a33368aa8338"}, "d0212669-b3cf-4647-a546-c85de5ce91cf": {"doc_hash": "6750c766344e8079107104dd1eb35cb0384fd8283fe67113e12f172fea12daff", "ref_doc_id": "9c3b73f3-b95f-4239-a4e3-a33368aa8338"}, "4f9bd468-fc1c-4e3c-84ad-5a7a628ad6bb": {"doc_hash": "938256d3fa9e3523e85a191a482b993bf7038ae1c79726b07fea436758c4142f", "ref_doc_id": "f8ca75fb-fdee-4bfa-8228-3c57d2058e06"}, "e0bc1994-653e-4058-a205-def9a563a6cb": {"doc_hash": "cd3d62ce13732a02ac6958fecf198a601dedc25bb559bfc0e626f29c2af95c3e", "ref_doc_id": "f8ca75fb-fdee-4bfa-8228-3c57d2058e06"}, "f625d375-b0c1-4a77-936f-5ed05a143ef6": {"doc_hash": "974381d4c16878a7a04acb1996242cc087c2c469fe516f25b5a18b373ea8fbad", "ref_doc_id": "6e46d55e-fc88-4337-873e-5873ebbd5d54"}, "0bd790fc-c625-45a4-afea-6503d8571a99": {"doc_hash": "00e30676295c6535ca96558db050ea42376a4acc0ac9d70e082941abd1dc32bd", "ref_doc_id": "6e46d55e-fc88-4337-873e-5873ebbd5d54"}, "c2f44b76-8417-4eda-a0d9-cf490ff4e0b9": {"doc_hash": "8781289d1c74f19bee149ddfb6f086fdb99dac985be4c2d2e004f05287e908b1", "ref_doc_id": "1f581f06-c46f-4585-bec3-f839e5343a0d"}, "642e63ac-55b7-4753-96a2-a769555b8701": {"doc_hash": "f1f507e7890b54e1aaa9bc9e280e633987e668143234cf23dbcf192b9df6f479", "ref_doc_id": "be32d9b0-fb6e-40a4-be5d-7d81e285062e"}, "f23ab101-85fd-47d4-8d21-dd621587b676": {"doc_hash": "f97c871f4a0de1c68558df838f387eaa2af8780224323ed0e15225d0c5add916", "ref_doc_id": "f57dd00b-449d-41bb-af66-7b2bbff74d00"}, "3b3786fd-1528-4f30-b0a5-3e64106df9b5": {"doc_hash": "d978f1a8c3b217e16fa23d01389f94d575d96c719170e17c38f948ebe2018efa", "ref_doc_id": "93175af7-25b4-4800-abab-f0827d0f9f29"}, "99d0505e-c2a0-4be5-9dd3-83d3f4d92094": {"doc_hash": "4bcd4c63eec965b73233aa1826c7ed1622acb594f182c737466a7ef8f8464216", "ref_doc_id": "93175af7-25b4-4800-abab-f0827d0f9f29"}, "65c7ec7b-c283-4417-8a8b-3c763a75f1e9": {"doc_hash": "51648784a590147cc59680cf706b73ff04f6430c5830692a992b821d2417c0e0", "ref_doc_id": "6438614d-0a54-4fd4-b0a9-0065fa40a2e1"}, "5e14d646-fc98-4fad-bbd2-c5a01464e7d5": {"doc_hash": "3de7336a9c050fe3bf00200a6c2fc11246f94fb15142475a95838e90c38703ad", "ref_doc_id": "6438614d-0a54-4fd4-b0a9-0065fa40a2e1"}}, "docstore/data": {"dfbd86a8-4fb6-49dc-b7b5-5f7bf0044228": {"__data__": {"id_": "dfbd86a8-4fb6-49dc-b7b5-5f7bf0044228", "embedding": null, "metadata": {"page_label": "1", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "306c842f-5201-4c69-bde4-51e163f6a8d3", "node_type": "4", "metadata": {"page_label": "1", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "93fc36583d7c3243aadb2488d10baeccfcae79f1bd91f1421e8ca9b81aff0cc9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFINETUNING TEXT-TO-IMAGE DIFFUSION MODELS\nFOR FAIRNESS\nXudong Shen\u22171, Chao Du\u20202, Tianyu Pang\u20202, Min Lin2\nYongkang Wong3, Mohan Kankanhalli\u20203\n1ISEP programme, NUS Graduate School, National University of Singapore\n2Sea AI Lab, Singapore\n3School of Computing, National University of Singapore\nxudong.shen@u.nus.edu; {duchao, tianyupang, linmin }@sea.com;\nyongkang.wong@nus.edu.sg; mohan@comp.nus.edu.sg\nABSTRACT\nThe rapid adoption of text-to-image diffusion models in society underscores an ur-\ngent need to address their biases. Without interventions, these biases could prop-\nagate a skewed worldview and restrict opportunities for minority groups. In this\nwork, we frame fairness as a distributional alignment problem. Our solution con-\nsists of two main technical contributions: (1) a distributional alignment loss that\nsteers specific characteristics of the generated images towards a user-defined tar-\nget distribution, and (2) adjusted direct finetuning of diffusion model\u2019s sampling\nprocess (adjusted DFT), which leverages an adjusted gradient to directly opti-\nmize losses defined on the generated images. Empirically, our method markedly\nreduces gender, racial, and their intersectional biases for occupational prompts.\nGender bias is significantly reduced even when finetuning just five soft tokens.\nCrucially, our method supports diverse perspectives of fairness beyond absolute\nequality, which is demonstrated by controlling age to a 75% young and 25% old\ndistribution while simultaneously debiasing gender and race. Finally, our method\nis scalable: it can debias multiple concepts at once by simply including these\nprompts in the finetuning data. We share code and various fair diffusion model\nadaptors at https://sail-sg.github.io/finetune-fair-diffusion/.\n1 I NTRODUCTION\nText-to-image (T2I) diffusion models (Nichol et al., 2021; Saharia et al., 2022) have witnessed an\naccelerated adoption by corporations and individuals alike. The scale of images generated by these\nmodels is staggering. To provide a perspective, DALL-E 2 (Ramesh et al., 2022) is used by over one\nmillion users (Bastian, 2022), while the open-access Stable Diffusion (SD) (Rombach et al., 2022)\nis utilized by over ten million users (Fatunde & Tse, 2022). These figures will continue to rise.\nHowever, this influx of content from diffusion models into society underscores an urgent need to\naddress their biases. Recent scholarship has demonstrated the existence of occupational biases (Se-\nshadri et al., 2023), a concentrated spectrum of skin tones (Cho et al., 2023), and stereotypical\nassociations (Schramowski et al., 2023) within diffusion models. While existing diffusion debiasing\nmethods (Friedrich et al., 2023; Bansal et al., 2022; Chuang et al., 2023; Orgad et al., 2023) offer\nsome advantages, such as being lightweight, they struggle to adapt to a wide range of prompts. Fur-\nthermore, they only approximately remove the biased associations but do not offer a way to control\nthe distribution of generated images. This is concerning because perceptions of fairness can vary\nacross specific issues and contexts; absolute equality might not always be the ideal outcome.\nWe frame fairness as a distributional alignment problem, where the objective is to align particular at-\ntributes of the generated images, such as gender, with a user-defined target distribution. Our solution\nconsists of two main technical contributions. First, we design a loss function that steers the gener-\nated images towards the desired distribution while preserving image semantics. A key component\nis the distributional alignment loss (DAL). For a batch of generated images, DAL uses pre-trained\nclassifiers to estimate class probabilities ( e.g., male and female probabilities) and dynamically gen-\n\u2217Work done during internship at Sea AI Lab.\u2020Corresponding authors.\n1", "start_char_idx": 0, "end_char_idx": 3885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b1a973e-8068-4232-80f2-27d58c512046": {"__data__": {"id_": "1b1a973e-8068-4232-80f2-27d58c512046", "embedding": null, "metadata": {"page_label": "2", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c4638a3-09fe-4150-82da-08358b86148d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ff0a6ac8bc4fbab66a851955bfd48a81791bcea9d2eb287d8579ae5a81747470", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa9b9ad1-a17f-403e-ae50-4edaa65e1699", "node_type": "1", "metadata": {}, "hash": "07c43c202fd0dd0a34d2964f471d2a38061e3c9720a72bb6ecc32f319e197cce", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nerates target classes that match the target distribution and have the minimum transport distance. To\npreserve image semantics, we regularize CLIP (Radford et al., 2021) and DINO (Oquab et al., 2023)\nsimilarities between images generated by the original and finetuned models.\nSecond, we propose adjusted direct finetuning of diffusion models, adjusted DFT for short, illus-\ntrated in Fig. 2. While most diffusion finetuning methods (Gal et al., 2023; Zhang & Agrawala,\n2023; Brooks et al., 2023; Dai et al., 2023) use the same denoising diffusion loss from pre-training,\nDFT aims to directly finetune the diffusion model\u2019s sampling process to minimize any loss defined\non the generated images, such as ours. However, we show the exact gradient of the sampling pro-\ncess has exploding norm and variance, rendering the naive DFT ineffective (illustrated in Fig. 1).\nAdjusted DFT leverages an adjusted gradient to overcome these issues. It opens venues for more\nrefined and targeted diffusion model finetuning and can be applied for objectives beyond fairness.\nEmpirically, we show our method markedly reduces gender, racial, and their intersectional biases for\noccupational prompts. The debiasing is effective even for prompts with unseen styles and contexts,\nsuch as \u201c A philosopher reading. Oil painting \u201d and \u201c bartender at willard intercontinental makes mint\njulep \u201d (Fig. 3). Our method is adaptable to any component of the diffusion model being finetuned.\nAblation study shows that finetuning the text encoder while keeping the U-Net unchanged hits a\nsweet spot that effectively mitigates biases and lessens potential negative effects on image quality.\nSurprisingly, finetuning as few as five soft tokens as a prompt prefix is able to largely reduces gender\nbias, demonstrating the effectiveness of soft prompt tuning (Lester et al., 2021; Li & Liang, 2021)\nfor fairness. These results underscore the robustness of our method and the efficacy of debiasing T2I\ndiffusion models by finetuning their language understanding components.\nA salient feature of our method is its flexibility, allowing users to specify the desired target distri-\nbution. For example, we can effectively adjust the age distribution to achieve a 75% young and\n25% old ratio (Fig. 4) while simultaneously debiasing gender and race (Tab. 5). We also show the\nscalability of our method. It can debias multiple concepts at once, such as occupations, sports, and\npersonal descriptors, by expanding the set of prompts used for finetuning.\nGenerative AI is set to profoundly influence society. It is well-recognized that LLMs require social\nalignment finetuning post pre-training (Christiano et al., 2017; Bai et al., 2022). However, this\nanalogous process has received less attention for T2I models, or multimedia generative AI overall.\nBiases and stereotypes can manifest more subtly within visual outputs. Yet, their influence on\nhuman perception and behavior is substantial and long-lasting (Goff et al., 2008). We hope our\nwork inspire further development in promoting social alignment across multimedia generative AI.\n2 R ELATED WORK\nBias in diffusion models. T2I diffusion models are known to produce biased and stereotypical\nimages from neutral prompts. Cho et al. (2023) observe that Stable Diffusion (SD) has an overall\ntendency to generate males when prompted with occupations and the generated skin tone is con-\ncentrated on the center few tones from the Monk Skin Tone Scale (Monk, 2023). Seshadri et al.\n(2023) observe SD amplifies gender-occupation biases from its training data. Besides occupations,\nBianchi et al. (2023) find simple prompts containing character traits and other descriptors also gen-\nerate stereotypical images. Luccioni et al. (2023) develop a tool to compare collections of generated\nimages with varying gender and ethnicity. Wang et al. (2023a) propose a text-to-image association\ntest and find SD associates females more with family and males more with career.\nBias mitigation in diffusion models. Existing techniques for mitigating bias in T2I diffusion mod-\nels remain limited and predominantly focus on prompting. Friedrich et al.", "start_char_idx": 0, "end_char_idx": 4184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa9b9ad1-a17f-403e-ae50-4edaa65e1699": {"__data__": {"id_": "aa9b9ad1-a17f-403e-ae50-4edaa65e1699", "embedding": null, "metadata": {"page_label": "2", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c4638a3-09fe-4150-82da-08358b86148d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ff0a6ac8bc4fbab66a851955bfd48a81791bcea9d2eb287d8579ae5a81747470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b1a973e-8068-4232-80f2-27d58c512046", "node_type": "1", "metadata": {"page_label": "2", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b8582cbe303b26472826dc6871ba6a26e79714cd00cf22f6f26093234f618e53", "class_name": "RelatedNodeInfo"}}, "text": "Seshadri et al.\n(2023) observe SD amplifies gender-occupation biases from its training data. Besides occupations,\nBianchi et al. (2023) find simple prompts containing character traits and other descriptors also gen-\nerate stereotypical images. Luccioni et al. (2023) develop a tool to compare collections of generated\nimages with varying gender and ethnicity. Wang et al. (2023a) propose a text-to-image association\ntest and find SD associates females more with family and males more with career.\nBias mitigation in diffusion models. Existing techniques for mitigating bias in T2I diffusion mod-\nels remain limited and predominantly focus on prompting. Friedrich et al. (2023) propose to ran-\ndomly include additional text cues like \u201c male \u201d or \u201c female \u201d if a known occupation is detected in the\nprompts, to generate images with a more balanced gender distribution. However, this approach is\nineffective for debiasing occupations that are not known in advance. Bansal et al. (2022) suggest\nincorporating ethical interventions into the prompts, such as appending \u201c if all individuals can be a\nlawyer irrespective of their gender \u201d to \u201c a photo of a lawyer \u201d. Kim et al. (2023) propose to optimize\na soft token \u201c V*\u201d such that the prompt \u201c V* a photo of a doctor \u201d generates doctor images with a bal-\nanced gender distribution. Nevertheless, the efficacy of their method lacks robust validation, as they\nonly train the soft token for one specific occupation and test it on two unseen ones. Besides prompt-\ning, debiasVL (Chuang et al., 2023) proposes to project out biased directions in text embeddings.\nConcept Algebra (Wang et al., 2023b) projects out biased directions in the score predictions. The\n2", "start_char_idx": 3515, "end_char_idx": 5217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97d85b92-91d7-4b08-b9b3-17b801c27740": {"__data__": {"id_": "97d85b92-91d7-4b08-b9b3-17b801c27740", "embedding": null, "metadata": {"page_label": "3", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d845450-949d-4be6-a703-0fb85bddcc88", "node_type": "4", "metadata": {"page_label": "3", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a9da393206056fa1527403a47b7d01a8c105ae91a55768bbf99bdbca32d5d754", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1f0289d-46b1-42b0-b194-03028f8d6844", "node_type": "1", "metadata": {}, "hash": "cdc77839d836c6280a3a4f8c7058729015a2fb32bb9b2d031a066c922f0a6dd6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTIME (Orgad et al., 2023) and UCE methods (Gandikota et al., 2023), which modify the attention\nweight, can also be used for debiasing. Similar issues of fairness and distributional control have also\nbeen explored in other image generative models (Wu et al., 2022).\nFinetuning diffusion models. Finetuning is a powerful way to enhance a pre-trained diffusion\nmodel\u2019s specific capabilities, such as adaptability (Gal et al., 2023), controllability (Zhang &\nAgrawala, 2023), instruction following (Brooks et al., 2023), and image aesthetics (Dai et al., 2023).\nConcurrent works (Clark et al., 2023; Wallace et al., 2023) also explore the direct finetuning of dif-\nfusion models, albeit with goals diverging from fairness and solutions different from ours. Adjusted\nDFT complements them because we identify and address shared challenges inherent in DFT.\n3 B ACKGROUND ON DIFFUSION MODELS\nDiffusion models (Ho et al., 2020) assume a forward diffusion process that gradually injects Gaus-\nsian noise to a data distribution q(x0)according to a variance schedule \u03b21, . . . , \u03b2 T:\nq(x1:T|x0) =YT\nt=1q(xt|xt\u22121), q(xt|xt\u22121) =N(xt|p\n1\u2212\u03b2txt\u22121, \u03b2tI), (1)\nwhere Tis a predefined total number of steps (typically 1000 ). The schedule {\u03b2t}t\u2208[T]is chosen\nsuch that the data distribution q(x0)is gradually transformed into an approximately Gaussian dis-\ntribution qT(xT)\u2248 N(xT|0,I). Diffusion models then learn to approximate the data distribution\nby reversing such diffusion process, starting from a Gaussian distribution p(xT) =N(xT|0,I):\np\u03b8(x0:T) =p(xT)YT\nt=1p\u03b8(xt\u22121|xt), p\u03b8(xt\u22121|xt) =N(xt\u22121|\u00b5\u03b8(xt, t), \u03c3tI), (2)\nwhere \u00b5\u03b8(xt, t)is parameterized using a noise prediction network \u03f5\u03b8(xt, t)with\u00b5\u03b8(xt, t) =\n1\u221a\u03b1t(xt\u2212\u03b2t\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(xt, t)),\u03b1t= 1\u2212\u03b2t,\u00af\u03b1t=Qt\ns=1\u03b1s, and{\u03c3t}t\u2208[T]are pre-determined noise\nvariances. After training, generating from diffusion models involves sampling from the reverse\nprocess p\u03b8(x0:T), which begins by sampling a noise variable xT\u223cp(xT), and then proceeds to\nobtain x0as follows:\nxt\u22121=1\u221a\u03b1t(xt\u2212\u03b2t\u221a1\u2212\u00af\u03b1t\u03f5\u03b8(xt, t)) +\u03c3twt,wt\u223c N(0,I). (3)\nLatent diffusion models. Rombach et al. (2022) introduce latent diffusion models (LDM), whose\nforward/reverse diffusion processes are defined in the latent space. With image encoder fEncand de-\ncoder fDec, LDMs are trained on latent representations z0=fEnc(x0). To generate an image, LDMs\nfirst sample a latent noise zT, run the reverse process to obtain z0, and decode it with x0=fDec(z0).\nText-to-image diffusion models. In T2I diffusion models, the noise prediction network \u03f5\u03b8accepts\nan additional text prompt P, i.e.,\u03f5\u03b8(g\u03d5(P),xt, t), where g\u03d5represents a pretrained text encoder\nparameterized by \u03d5. Most T2I models, including Stable Diffusion (Rombach et al., 2022), further\nemploy LDM and thus use a text-conditional noise prediction model in the latent space, denoted\nas\u03f5\u03b8(g\u03d5(P),zt, t), which serves as the central focus of our work. Sampling from T2I diffusion\nmodels additionally utilizes the classifier-free guidance technique (Ho & Salimans, 2021).", "start_char_idx": 0, "end_char_idx": 3038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1f0289d-46b1-42b0-b194-03028f8d6844": {"__data__": {"id_": "f1f0289d-46b1-42b0-b194-03028f8d6844", "embedding": null, "metadata": {"page_label": "3", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d845450-949d-4be6-a703-0fb85bddcc88", "node_type": "4", "metadata": {"page_label": "3", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a9da393206056fa1527403a47b7d01a8c105ae91a55768bbf99bdbca32d5d754", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97d85b92-91d7-4b08-b9b3-17b801c27740", "node_type": "1", "metadata": {"page_label": "3", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7d83811c88e2022563aabc8e502bce850720094d6f7ee06077dc7cc15b20e695", "class_name": "RelatedNodeInfo"}}, "text": "To generate an image, LDMs\nfirst sample a latent noise zT, run the reverse process to obtain z0, and decode it with x0=fDec(z0).\nText-to-image diffusion models. In T2I diffusion models, the noise prediction network \u03f5\u03b8accepts\nan additional text prompt P, i.e.,\u03f5\u03b8(g\u03d5(P),xt, t), where g\u03d5represents a pretrained text encoder\nparameterized by \u03d5. Most T2I models, including Stable Diffusion (Rombach et al., 2022), further\nemploy LDM and thus use a text-conditional noise prediction model in the latent space, denoted\nas\u03f5\u03b8(g\u03d5(P),zt, t), which serves as the central focus of our work. Sampling from T2I diffusion\nmodels additionally utilizes the classifier-free guidance technique (Ho & Salimans, 2021).\n4 M ETHOD\nOur method consists of ( i) a loss design that steers specific attributes of the generated images towards\na target distribution while preserving image semantics, and ( ii) adjusted direct finetuning of the\ndiffusion model\u2019s sampling process.\n4.1 L OSS DESIGN\nGeneral case For a clearer introduction, we first present the loss design for a general case, which\nconsists of the distributional alignment loss Lalignand the image semantics preserving loss Limg. We\nstart with the distributional alignment loss (DAL) Lalign. Suppose we want to control a categorical\nattribute of the generated images that has Kclasses and align it towards a target distribution D. Each\nclass is represented as a one-hot vector of length KandDis a discrete distribution over these classes.\nWe first generate a batch of images I={x(i)}i\u2208[N]using the diffusion model being finetuned and\nsome prompt P. For every generated image x(i), we use a pre-trained classifier hto produce a class\nprobability vector p(i)= [p(i)\n1,\u00b7\u00b7\u00b7, p(i)\nK] =h(x(i)), with p(i)\nkdenoting the estimated probability\n3", "start_char_idx": 2342, "end_char_idx": 4112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e70d5e86-7c5c-4c13-ac53-f84b4be6dc72": {"__data__": {"id_": "e70d5e86-7c5c-4c13-ac53-f84b4be6dc72", "embedding": null, "metadata": {"page_label": "4", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4858a74b-9d67-419d-875c-ebc25cb4769d", "node_type": "4", "metadata": {"page_label": "4", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "db9b32fe5548bbd1c25fa4b2ea275549f38bda73d1c2d818aa8e4bbaac185ad3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1d408ab-8634-4fd0-af31-95bcb98d370a", "node_type": "1", "metadata": {}, "hash": "343dd726afcd14899a0deb87ea55b04405e540a3a077e6b1964bb27ba5343d49", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nthatx(i)is from class k. Assume we have another set of vectors {u(i)}i\u2208[N]that represents the\ntarget distribution and where every u(i)is a one-hot vector representing a class, we can compute the\noptimal transport (OT) (Monge, 1781) from {p(i)}i\u2208[N]to{u(i)}i\u2208[N]:\n\u03c3\u2217= argmin\n\u03c3\u2208SNXN\ni=1|p(i)\u2212u(\u03c3i)|2, (4)\nwhere SNdenotes all permutations of [N],\u03c3= [\u03c31,\u00b7\u00b7\u00b7, \u03c3N], and \u03c3i\u2208[N]. Intuitively, \u03c3\u2217finds,\nin the class probability space, the most efficient modification of the current images to match the\ntarget distribution. We construct {u(i)}i\u2208[N]to be i.i.d. samples from the target distribution and\ncompute the expectation of OT:\nq(i)=Eu(1),\u00b7\u00b7\u00b7,u(N)\u223cD[u(\u03c3\u2217\ni)],\u2200i\u2208[N]. (5)\nq(i)is a probability vector where the k-th element is the probability that image x(i)should\nhave target class k, had the batch of generated images indeed followed the target distribution D.\nThe expectation of OT can be computed analytically when the number of classes Kis small or\napproximated by empirical average when Kincreases. We note one can also construct a fixed set of\n{u(i)}i\u2208[N], for example half male and half female to represent a balanced gender distribution. But\na fixed split poses a stronger finite-sample alignment objective and neglects the sensitivity of OT.\nFinally, we generate target classes {y(i)}i\u2208[N]and confidence of these targets {c(i)}i\u2208[N]by:\ny(i)= arg max( q(i))\u2208[K], c(i)= max( q(i))\u2208[0,1],\u2200i\u2208[N]. We define DAL as the\ncross-entropy loss w.r.t. these dynamically generated targets, with a confidence threshold C,\nLalign=1\nNXN\ni=11[c(i)\u2265C]LCE(h(x(i)), y(i)). (6)\nWe also use an image semantics preserving loss Limg. We keep a copy of the frozen, not finetuned\ndiffusion model and penalize the image dissiminarity measured by CLIP and DINO:\nLimg=1\nNXN\ni=1h\n(1\u2212cos(CLIP (x(i)),CLIP (o(i)))) + (1 \u2212cos(DINO (x(i)),DINO (o(i))))i\n, (7)\nwhere I\u2032={o(i)}i\u2208[N]is the batch of images generated by the frozen model using the same\nprompt P. We call them original images. We require every pair of finetuned image x(i)and original\nimage o(i)are generated using the same initial noise. We use both CLIP and DINO because CLIP\nis pretrained with text supervision and DINO is pretrained with image self-supervision. In imple-\nmentation, we use the laion/CLIP-ViT-H-14-laion2B-s32B-b79K and the dinov2-vitb14 (Oquab\net al., 2023). We caution that CLIP and DINO can have their own biases (Wolfe et al., 2023).\nAdaptation for face-centric attributes In this work, we focus on face-centric attributes such as\ngender, race, and age. We find the following adaptation from the general case yields the best results.\nFirst, we use a face detector dfaceto retrieve the face region dface(x(i))from every generated image\nx(i). We apply the classifier hand the DAL Lalignonly on the face regions. Second, we introduce\nanother face realism preserving loss Lface, which penalize the dissimilarity between the generated\nfacedface(x(i))and the closest face from a set of external real faces DF,\nLface=1\nN(1\u2212min\nF\u2208DFcos(emb(dface(x(i))),emb(F)), (8)\nwhere emb (\u00b7)is a face embedding model. Lfacehelps retain realism of the faces, which can be\nsubstantially edited by the DAL.", "start_char_idx": 0, "end_char_idx": 3182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1d408ab-8634-4fd0-af31-95bcb98d370a": {"__data__": {"id_": "d1d408ab-8634-4fd0-af31-95bcb98d370a", "embedding": null, "metadata": {"page_label": "4", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4858a74b-9d67-419d-875c-ebc25cb4769d", "node_type": "4", "metadata": {"page_label": "4", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "db9b32fe5548bbd1c25fa4b2ea275549f38bda73d1c2d818aa8e4bbaac185ad3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e70d5e86-7c5c-4c13-ac53-f84b4be6dc72", "node_type": "1", "metadata": {"page_label": "4", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ec66ab2d148102a1c3aae1bf622326c5fe8842fdbb67c77547feafdb9d98022e", "class_name": "RelatedNodeInfo"}}, "text": "Adaptation for face-centric attributes In this work, we focus on face-centric attributes such as\ngender, race, and age. We find the following adaptation from the general case yields the best results.\nFirst, we use a face detector dfaceto retrieve the face region dface(x(i))from every generated image\nx(i). We apply the classifier hand the DAL Lalignonly on the face regions. Second, we introduce\nanother face realism preserving loss Lface, which penalize the dissimilarity between the generated\nfacedface(x(i))and the closest face from a set of external real faces DF,\nLface=1\nN(1\u2212min\nF\u2208DFcos(emb(dface(x(i))),emb(F)), (8)\nwhere emb (\u00b7)is a face embedding model. Lfacehelps retain realism of the faces, which can be\nsubstantially edited by the DAL. In our implementation, we use the CelebA (Liu et al., 2015) and\nthe FairFace dataset (Karkkainen & Joo, 2021) as external faces. We use the SFNet-20 (Wen et al.,\n2022) as the face embedding model.\nOur final loss Lis a weighted sum: L=Lalign+\u03bbimgLimg+\u03bbfaceLface. Notably, we use a dynamic\nweight \u03bbimg. We use a larger \u03bbimg,1if the generated image x(i)\u2019s target class y(i)agrees with the\noriginal image o(i)\u2019s class h(dface(o(i))). Intuitively, we encourage minimal change between x(i)\nando(i)if the original image o(i)already satisfies the distributional alignment objective. For other\nimages x(i)whose target class y(i)does not agree with the corresponding original image o(i)\u2019s class\nh(dface(o(i))), we use a smaller weight \u03bbimg,2for the non-face region and the smallest weight \u03bbimg,3\nfor the face region. Intuitively, these images do require editing, particularly on the face regions.\nFinally, if an image does not contain any face, we only apply Limgbut not LalignandLface. If an\nimage contains multiple faces, we focus on the one occupying the largest area.\n4", "start_char_idx": 2433, "end_char_idx": 4246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50eeb5d3-27b1-40e8-929f-c86167b8f268": {"__data__": {"id_": "50eeb5d3-27b1-40e8-929f-c86167b8f268", "embedding": null, "metadata": {"page_label": "5", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99baeb5b-9dde-474b-9e5b-5491b3377a67", "node_type": "4", "metadata": {"page_label": "5", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3058b7a501be5d2f0c866a69d01275bf3b02089df7d133a6180cb2a5d96013d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19d1a6e7-c1d7-4d61-ad31-1ac9dc7ebb93", "node_type": "1", "metadata": {}, "hash": "68c79f5ad4d90e7d60d6bf1b36bb391a0aa2f7023dc7265648097d735ebbbf39", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n0 200 400 600 800 100000.20.40.60.81\ntraininig iterationtraining loss\n(a)\n1000 800 600 400 200 00.1110100100010k (b)\nFigure 1: The left figure plots the training loss during direct fine-tuning, w/ three distinct gradients.\nEach reported w/ 3 random runs. The right figure estimates the scale of these gradients at different\ntime steps. Mean and 90% CI are computed from 20 random runs. Read Section 4.2 for details.\n4.2 A DJUSTED DIRECT FINETUNING OF DIFFUSION MODEL \u2019S SAMPLING PROCESS\nConsider that the T2I diffusion model generates an image x0=fDec(z0)using a prompt Pand\nan initial noise zT. Our goal is to finetune the diffusion model to minimize a differentiable loss\nL(x0). We begin by considering the naive DFT, which computes the exact gradient of L(x0)in the\nsampling process, followed by gradient-based optimization. To see if naive DFT works, we test it\nfor the image semantics preserving loss Limgusing a fixed image as the target and optimize a soft\nprompt. This resembles a textual inversion task (Gal et al., 2023). Fig. 1a shows the training loss\nhas no decrease after 1000 iterations. It suggests the naive DFT of diffusion models is not effective.\nBy explicitly writing down the gradient, we are able to detect why the naive DFT fails. To simplify\nthe presentation, we analyze the gradient w.r.t. the U-Net parameter,dL(x0)\nd\u03b8. But the same issue\narises when finetuning the text encoder or the prompt.dL(x0)\nd\u03b8=dL(x0)\ndx0dx0\ndz0dz0\nd\u03b8, withdz0\nd\u03b8=\n\u22121\u221a\u00af\u03b11\u03b21\u221a1\u2212\u00af\u03b11|{z}\nA1I|{z}\nB1\u2202\u03f5(1)\n\u2202\u03b8\u2212XT\nt=2\u00121\u221a\u00af\u03b1t\u03b2t\u221a1\u2212\u00af\u03b1t|{z}\nAt\u0012Yt\u22121\ns=1\u0012\n1\u2212\u03b2s\u221a1\u2212\u00af\u03b1s\u2202\u03f5(s)\n\u2202zs\u0013\u0013\n| {z }\nBt\u2202\u03f5(t)\n\u2202\u03b8\u0013\n, (9)\nwhere \u03f5(t)denotes the U-Net function \u03f5\u03b8(g\u03d5(P),zt, t)evaluated at time step t. Importantly, the\nrecurrent evaluations of U-Net in the reverse diffusion process lead to a factor Btthat scales expo-\nnentially in t. It leads to two issues. First,dz0\nd\u03b8becomes dominated by the components AtBt\u2202\u03f5(t)\n\u2202\u03b8for values of tclose to T= 1000 . Second, due to the fact that Btencompasses allpossible products\nbetween {\u2202\u03f5(s)\n\u2202zs}s\u2264t\u22121, this coupling between partial gradients of different time steps introduces sub-\nstantial variance todz0\nd\u03b8. Fig. 2a illustrates this issue. We empirically show these problems indeed\nexist in naive DFT. Since directly computing the Jacobian matrices\u2202\u03f5(t)\n\u2202\u03b8and\u2202\u03f5(s)\n\u2202zsis too expen-\nsive, we assumedL(x0)\ndx0dx0\ndz0is a random Gaussian matrix R\u223c N(0,10\u22124\u00d7I)and plot the values\nof|RAtBt\u2202\u03f5(t)\n\u2202\u03b8|,|RAt\u2202\u03f5(t)\n\u2202\u03b8|, and|R\u2202\u03f5(t)\n\u2202\u03b8|in Fig. 1b. It is apparent both the scale and variance of\f\f\fRAtBt\u2202\u03f5(t)\n\u2202\u03b8\f\f\fexplodes as t\u21921000 , but neither\f\f\fRAt\u2202\u03f5(t)\n\u2202\u03b8\f\f\fnor\f\f\fR\u2202\u03f5(t)\n\u2202\u03b8\f\f\fdo.", "start_char_idx": 0, "end_char_idx": 2624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19d1a6e7-c1d7-4d61-ad31-1ac9dc7ebb93": {"__data__": {"id_": "19d1a6e7-c1d7-4d61-ad31-1ac9dc7ebb93", "embedding": null, "metadata": {"page_label": "5", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99baeb5b-9dde-474b-9e5b-5491b3377a67", "node_type": "4", "metadata": {"page_label": "5", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3058b7a501be5d2f0c866a69d01275bf3b02089df7d133a6180cb2a5d96013d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50eeb5d3-27b1-40e8-929f-c86167b8f268", "node_type": "1", "metadata": {"page_label": "5", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3458ee54750f9bf72bca5830074974f2ecc72993cc4cb5c84b33c1de4f609b66", "class_name": "RelatedNodeInfo"}}, "text": "Since directly computing the Jacobian matrices\u2202\u03f5(t)\n\u2202\u03b8and\u2202\u03f5(s)\n\u2202zsis too expen-\nsive, we assumedL(x0)\ndx0dx0\ndz0is a random Gaussian matrix R\u223c N(0,10\u22124\u00d7I)and plot the values\nof|RAtBt\u2202\u03f5(t)\n\u2202\u03b8|,|RAt\u2202\u03f5(t)\n\u2202\u03b8|, and|R\u2202\u03f5(t)\n\u2202\u03b8|in Fig. 1b. It is apparent both the scale and variance of\f\f\fRAtBt\u2202\u03f5(t)\n\u2202\u03b8\f\f\fexplodes as t\u21921000 , but neither\f\f\fRAt\u2202\u03f5(t)\n\u2202\u03b8\f\f\fnor\f\f\fR\u2202\u03f5(t)\n\u2202\u03b8\f\f\fdo.\nHaving detected the cause of the issue, we propose adjusted DFT, which uses an adjusted gradient\nthat sets At= 1 andBt=I:(dz0\nd\u03b8)adjusted =\u2212PT\nt=1\u2202\u03f5(t)\n\u03b8. It is motivated from the unrolled\nexpression of the reverse process:\nz0=\u2212XT\nt=1At\u03f5\u03b8(g\u03d5(P),zt, t) +1\u221a\u00af\u03b1TzT+XT\nt=21\u221a\u00af\u03b1t\u22121wt,wt\u223c N(0,I). (10)\nWhen we set Bt=I, we are essentially considering ztas an external variable and independent\nof the U-Net parameters \u03b8, rather than recursively dependent on \u03b8. Otherwise, by the chain rule, it\ngenerates all the coupling between partial gradients of different time steps in Bt. But setting Bt=I\ndoes preserve all uncoupled gradients, i.e.,\u2202\u03f5(t)\n\u2202\u03b8,\u2200t\u2208[T]. When we set At= 1, we standardize the\ninfluence of \u03f5\u03b8(g\u03d5(P),zt, t)from different time steps tinz0. It is known that weighting different\ntime steps properly can accelerate diffusion training (Ho et al., 2020; Hang et al., 2023). Finally, we\nimplement adjusted DFT in Appendix Algorithm A.1. Fig. 2b provides a schematic illustration.\nWe test the proposed adjusted gradient and a variant that does not standardize Aifor the same\nimage semantics preserving loss w/ the same fixed target image. The results are shown in Fig. 1a.\n5", "start_char_idx": 2257, "end_char_idx": 3798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d876b1b9-7372-4a6a-9d6b-21aa0b6862c5": {"__data__": {"id_": "d876b1b9-7372-4a6a-9d6b-21aa0b6862c5", "embedding": null, "metadata": {"page_label": "6", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4626d870-ea0c-443c-a936-f1eaa3f26a07", "node_type": "4", "metadata": {"page_label": "6", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8e25e4cd9da96eb5c940387e0bc87a6738f3da21c3a1075a36b0efb1c0ed44eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9534790-348b-4e19-8ccc-e82228affa98", "node_type": "1", "metadata": {}, "hash": "d389e441338680c1be789ded6e66f73758520f7b1147a4348543327e6d1f96f9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n= uncoupled grads: \n+ coupled grads: \n(a) Naive DFT.\n (b) Adjusted DFT, which also standardize Aito 1.\nFigure 2: Comparison of naive and adjusted direct finetuning (DFT) of the diffusion model. Gray\nsolid lines denote the sampling process. Red dashed lines highlight the gradient computation w.r.t.\nthe model parameter ( \u03b8). Variables ztand\u03f5(t)represent data and noise prediction at time step t.\nDiand I idenote the direct and indirect gradient paths between adjacent time steps. For instance,\natt= 3, naive DFT computes the exact gradient \u2212A3B3\u2202\u03f5(3)\n\u2202\u03b8(defined in Eq. 9), which involve\nother time step\u2019s noise predictions (through the gradient paths I 1I2I3I4I5, I1I2D2I5, and D 1I3I4I5).\nAdjusted DFT leverages an adjusted gradient, which removes the coupling with other time steps and\nstandardizes Aito 1, for more effective finetuning. Read Section 4.2 for details.\nWe find that both adjusted gradients effectively reduce the training loss, suggesting Biis indeed\nthe underlying issue. Moreover, standardizing Aifurther stabilizes the optimization process. We\nnote that, to reduce the memory footprint, in all experiments we ( i) quantize the diffusion model to\nfloat16 , (ii) apply gradient checkpointing (Chen et al., 2016), and ( iii) use DPM-Solver++ (Lu\net al., 2022) as the diffusion scheduler, which only requires around 20 steps for T2I generations.\n5 E XPERIMENTS\n5.1 M ITIGATING GENDER ,RACIAL ,AND THEIR INTERSECTIONAL BIASES\nWe apply our method to runwayml/stable-diffusion-v1-5 (SD for short), a T2I diffusion model\nopenly accessible from Hugging Face, to reduce gender, racial, and their intersectional biases. We\nconsider binary gender and recognize its limitations. Enhancing the representation of non-binary\nidentities faces additional challenges from the intricacies of visually representing non-binary iden-\ntities and the lack of public datasets, which are beyond the scope of this work. We adopt the eight\nrace categories from the FairFace dataset but find trained classifiers struggle to distinguish between\ncertain categories. Therefore, we consolidate them into four broader classes: WMELH= {White,\nMiddle Eastern, Latino Hispanic }, Asian= {East Asian, Southeast Asian }, Black, and Indian. The\ngender and race classifiers used in DAL are trained on the CelebA or FairFace datasets. We consider\na uniform distribution over gender, race, or their intersection as the target distribution. We employ\nthe prompt template \u201c a photo of the face of a {occupation }, a person \u201d and use 1000/50 occupations\nfor training/test. For the main experiments and except otherwise stated, we finetune LoRA (Hu et al.,\n2021) with rank 50 applied on the text encoder. Appendix A.2 provides other experiment details.\nEvaluation. We train separate gender and race classifiers for evaluation. We generate 60, 80, or 160\nimages for each prompt to evaluate gender, racial, or intersectional biases, respectively. For every\nprompt P, we compute the following metric: bias (P) =1\nK(K\u22121)/2P\ni,j\u2208[K]:i<j|freq(i)\u2212freq(j)|,\nwhere freq (i)is group i\u2019s frequency in the generated images. The number of groups Kis 2/4/8 for\ngender/race/their intersection. The classification of an image into a specific group is based on the\nface that covers the largest area. This bias metric considers a perfectly balanced target distribution. It\nmeasures the disparity of different groups\u2019 representations, averaged across all contrasting groups.\nWe also report ( i) CLIP-T, the CLIP similarity between the generated image and the prompt, ( ii)\nCLIP-I, the CLIP similarity between the generated image and the original SD\u2019s generation for the\nsame prompt and noise, and ( iii) DINO, which parallels CLIP-I but uses DINO features. We use\nCLIP-ViT-bigG-14 and DINOv2 vit-g/14 for evaluation, which differ from the ones used for training.\nResults.", "start_char_idx": 0, "end_char_idx": 3869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9534790-348b-4e19-8ccc-e82228affa98": {"__data__": {"id_": "c9534790-348b-4e19-8ccc-e82228affa98", "embedding": null, "metadata": {"page_label": "6", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4626d870-ea0c-443c-a936-f1eaa3f26a07", "node_type": "4", "metadata": {"page_label": "6", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8e25e4cd9da96eb5c940387e0bc87a6738f3da21c3a1075a36b0efb1c0ed44eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d876b1b9-7372-4a6a-9d6b-21aa0b6862c5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5a66684b834e86f69446da132a273d4c4cfe2ae49330f1980d7acb628f88374b", "class_name": "RelatedNodeInfo"}}, "text": "The number of groups Kis 2/4/8 for\ngender/race/their intersection. The classification of an image into a specific group is based on the\nface that covers the largest area. This bias metric considers a perfectly balanced target distribution. It\nmeasures the disparity of different groups\u2019 representations, averaged across all contrasting groups.\nWe also report ( i) CLIP-T, the CLIP similarity between the generated image and the prompt, ( ii)\nCLIP-I, the CLIP similarity between the generated image and the original SD\u2019s generation for the\nsame prompt and noise, and ( iii) DINO, which parallels CLIP-I but uses DINO features. We use\nCLIP-ViT-bigG-14 and DINOv2 vit-g/14 for evaluation, which differ from the ones used for training.\nResults. Table 1 reports the efficacy of our method in comparison to existing works. Evaluation\ndetails of debiasVL and UCE are reported in Appendix A.7. Our method consistently achieves the\nlowest bias across all three scenarios. While Concept Algebra and UCE excel in preserving visual\nsimilarity to images generated by the original SD, they are significantly less effective for debiasing.\nThis is because some of these visual alterations are essential for enhancing the representation of\nminority groups. Furthermore, our method still maintains a strong alignment with the text prompt.\nFig. 3a shows generated images using the unseen occupation \u201celectrical and electronics repairer\u201d\n6", "start_char_idx": 3129, "end_char_idx": 4548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fbbaab6-57d1-44d2-a124-522b61c76309": {"__data__": {"id_": "9fbbaab6-57d1-44d2-a124-522b61c76309", "embedding": null, "metadata": {"page_label": "7", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91e7a275-998f-4f10-9179-50adf7b60c7f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0d328aef75b6c83db339a414ac1076c8f630559c31d43d20e94be027340d43bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbc7c63e-5ef5-490e-acc5-fd2f6c0ebf5b", "node_type": "1", "metadata": {}, "hash": "3b87f80201a23498ea1a0c32e3af236dc0656280a00a99610a6f7fc2b94ec65e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 1: Comparison with debiasVL (Chuang et al., 2023), Ethical Intervention (Bansal et al., 2022),\nConcept Algebra (Wang et al., 2023b), and Unified Concept Editing (UCE) (Gandikota et al., 2023).\nDebias:MethodBias\u2193 Semantics Preservation \u2191\nGender Race G. \u00d7R. CLIP-T CLIP-I DINO\nOriginal SD .67\u00b1.29 .42\u00b1.06 .21\u00b1.03 .39\u00b1.05 \u2014 \u2014GenderdebiasVL .98\u00b1.10 .49\u00b1.04 .24\u00b1.02 .36\u00b1.05 .63\u00b1.14 .53\u00b1.21\nUCE .59\u00b1.33 .43\u00b1.06 .21\u00b1.03 .38\u00b1.05 .83\u00b1.15 .78\u00b1.21\nEthical Int. .56\u00b1.32 .37\u00b1.08 .19\u00b1.04 .37\u00b1.05 .68\u00b1.19 .60\u00b1.25\nC. Algebra .47\u00b1.31 .41\u00b1.06 .20\u00b1.02 .39\u00b1.05 .84\u00b1.14 .79\u00b1.20\nOurs .23\u00b1.16 .44\u00b1.06 .20\u00b1.03 .39\u00b1.05 .77\u00b1.15 .70\u00b1.22RacedebiasVL .84\u00b1.18 .38\u00b1.04 .21\u00b1.01 .36\u00b1.04 .50\u00b1.12 .36\u00b1.19\nUCE .62\u00b1.33 .40\u00b1.08 .20\u00b1.04 .38\u00b1.05 .79\u00b1.15 .73\u00b1.22\nEthical Int. .58\u00b1.28 .37\u00b1.06 .19\u00b1.03 .36\u00b1.05 .65\u00b1.18 .57\u00b1.25\nOurs .74\u00b1.27 .12\u00b1.05 .14\u00b1.03 .39\u00b1.04 .73\u00b1.15 .67\u00b1.21G.\u00d7R.debiasVL .99\u00b1.04 .47\u00b1.04 .24\u00b1.01 .35\u00b1.05 .63\u00b1.18 .49\u00b1.20\nUCE .72\u00b1.28 .36\u00b1.09 .20\u00b1.03 .38\u00b1.05 .79\u00b1.16 .74\u00b1.22\nEthical Int. .55\u00b1.31 .35\u00b1.07 .18\u00b1.03 .36\u00b1.05 .63\u00b1.18 .55\u00b1.24\nOurs .16\u00b1.13 .09\u00b1.04 .06\u00b1.02 .39\u00b1.05 .67\u00b1.15 .58\u00b1.22\nfrom the test set. The original SD generates predominantly white male, marginalizing many other\nidentities, including female, Black, Indian, Asian, and their intersections. Our debiased SD greatly\nimproves the representation of minorities. Appendix Fig. A.1 shows the training curve. We plot the\ngender and race representations by each occupation in Appendix Fig. A.2, A.3, A.4, and A.5.\nTable 2: Eval w/ non-templated prompts.\nMethodBias\u2193 S. P.\u2191\nGender Race G. \u00d7R. CLIP-T\nSD.60\u00b1.28.39\u00b1.09.19\u00b1.05.41\u00b1.05\ndeb.VL .84\u00b1.18.42\u00b1.07.22\u00b1.03.37\u00b1.06\nEth. Int. .51\u00b1.29.38\u00b1.07.18\u00b1.03.40\u00b1.05\nUCE .51\u00b1.31.36\u00b1.08.18\u00b1.04.40\u00b1.05\nOurs .32\u00b1.22.30\u00b1.08.14\u00b1.04.40\u00b1.05Generalization to non-templated prompts. We\nobtain 40 occupation-related prompts from the LAION-\nAesthetics V2 dataset (Schuhmann et al., 2022), listed\nin Appendix A.6. These prompts feature more nuanced\nstyle and contextual elements, such as \u201c A philoso-\npher reading. Oil painting. \u201d (Fig. 3b) or \u201c bartender at\nwillard intercontinental makes mint julep \u201d (Fig. 3c). Our\nevaluations, reported in Table 2, indicate that although\nwe debias only templated prompts, the debiasing effect\ngeneralizes to more complex non-templated prompts as well.", "start_char_idx": 0, "end_char_idx": 2302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbc7c63e-5ef5-490e-acc5-fd2f6c0ebf5b": {"__data__": {"id_": "dbc7c63e-5ef5-490e-acc5-fd2f6c0ebf5b", "embedding": null, "metadata": {"page_label": "7", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91e7a275-998f-4f10-9179-50adf7b60c7f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0d328aef75b6c83db339a414ac1076c8f630559c31d43d20e94be027340d43bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fbbaab6-57d1-44d2-a124-522b61c76309", "node_type": "1", "metadata": {"page_label": "7", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ec10328886f74f504c062354c6778be5c60a57ccff403c23b28ae52f7c27bd75", "class_name": "RelatedNodeInfo"}}, "text": "We\nobtain 40 occupation-related prompts from the LAION-\nAesthetics V2 dataset (Schuhmann et al., 2022), listed\nin Appendix A.6. These prompts feature more nuanced\nstyle and contextual elements, such as \u201c A philoso-\npher reading. Oil painting. \u201d (Fig. 3b) or \u201c bartender at\nwillard intercontinental makes mint julep \u201d (Fig. 3c). Our\nevaluations, reported in Table 2, indicate that although\nwe debias only templated prompts, the debiasing effect\ngeneralizes to more complex non-templated prompts as well. The debiasing effect can be more\napparently seen from Fig. 3b and 3c. In Appendix Section A.5, we analyze the debiased SD\u2019s gener-\nations for general, non-occupational prompts as well as potential negative impacts on image quality.\nTable 3: Eval on multi-face image generation.\nPro-\nmptModelBias (single face) \u2193 Bias (all faces) \u2193 S. P.\u2191\nGender Race G. \u00d7R. Gender Race G. \u00d7R. CLIP-T\nTwo\npplSD.46\u00b1.28.45\u00b1.04.21\u00b1.02.43\u00b1.26.45\u00b1.03.21\u00b1.02.40\u00b1.04\nOurs .26\u00b1.16.14\u00b1.06.09\u00b1.03.18\u00b1.14.15\u00b1.06.08\u00b1.03.41\u00b1.04\nThree\npplSD.48\u00b1.32.44\u00b1.05.21\u00b1.02.42\u00b1.28.44\u00b1.04.21\u00b1.02.40\u00b1.05\nOurs .26\u00b1.17.16\u00b1.05.09\u00b1.02.16\u00b1.14.19\u00b1.06.10\u00b1.02.41\u00b1.05Generalization to multi-face im-\nage generation. We change the test\nprompt template to \u201cA photo of the\nfaces of two/three {occupation },\ntwo/three people\u201d to generate im-\nages of two or three individuals,\nwith results reported in Tab. 3. We\nadditionally report the bias metric\ncalculated for all faces in the generated images. The results show the debiasing effect generalizes to\nmulti-face image generations as well. We show generated images in Appendix Figs A.16 and A.17.\nTable 4: Finetuning different SD components.\nFor prompt prefix, five soft tokens are finetuned.\nFor others, LoRA w/ rank 50 is finetuned.\nFinetued\nComponentBias\u2193Semantics Preservation \u2191\nGender CLIP-T CLIP-I DINO\nOriginal SD .67\u00b1.29.39\u00b1.05 \u2014 \u2014\nPrompt Prefix .24\u00b1.19.39\u00b1.05.70\u00b1.15.62\u00b1.22\nText Encoder .23\u00b1.16.39\u00b1.05.77\u00b1.15.70\u00b1.22\nU-Net .22\u00b1.14.39\u00b1.05.90\u00b1.09.87\u00b1.13\nT.E. & U-Net .17\u00b1.13.40\u00b1.04.80\u00b1.14.74\u00b1.20Ablation on different components to finetune.\nWe finetune various components of SD to re-\nduce gender bias, with results reported in Ta-\nble 4. First, our method proves highly robust to\nthe number of parameters finetuned. By optimiz-\ning merely five soft tokens as prompt prefix, gen-\nder bias can already be significantly mitigated.\nFig. A.18 provides a comparison of the generated\nimages. Second, while Table 4 suggests finetun-\ning both the text encoder and U-Net is the most\neffective, Fig. 5 reveals two adverse effects of\nfinetuning U-Net for debiasing purposes. It can deteriorate image quality w.r.t. facial skin textual and\nthe model becomes more capable at misleading the classifier into predicting an image as one gender,\ndespite the image\u2019s perceptual resemblance to another gender. Our findings shed light on the de-\ncisions regarding which components to finetune when debiasing diffusion models. We recommend\n7", "start_char_idx": 1800, "end_char_idx": 4718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "706cd728-283b-4a79-bd2c-b3e9aed8768e": {"__data__": {"id_": "706cd728-283b-4a79-bd2c-b3e9aed8768e", "embedding": null, "metadata": {"page_label": "8", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a6ce8f6-7443-4759-892a-29effe63279e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9460b32339598bdc118b1ef51ec20b280614b846536f96adb08aa3da2fc46737", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt with unseen occupation: \u201ca photo of the face of a electrical and electronics repairer, a person\u201d.\nGender bias: 0.84 (original) \u21920.11 (debiased). Racial bias: 0.48 \u21920.10. Gender \u00d7Race bias: 0.24 \u21920.06.\n(b) Prompt with unseen style: \u201cA philosopher reading. Oil painting.\u201d. Gender bias: 0.80 (original) \u21920.23\n(debiased). Racial bias: 0.45 \u21920.31. Gender \u00d7Race bias: 0.22 \u21920.15.\n(c) Prompt with unseen context: \u201cbartender at willard intercontinental makes mint julep\u201d. Gender bias: 0.87\n(original) \u21920.17 (debiased). Racial bias: 0.49 \u21920.33. Gender \u00d7Race bias: 0.24 \u21920.15.\nFigure 3: Images generated from the original SD (left) and the SD jointly debiased for gender and\nrace (right). The model is debiased using the prompt template \u201ca photo of the face of a {occupation },\na person\u201d. For every image, the first color-coded bar denotes the predicted gender: male or female.\nThe second denotes race: WMELH, Asian, Black, or Indian. Bar height represents prediction confi-\ndence. Bounding boxes denote detected faces. For the same prompt, images with the same number\nlabel are generated using the same noise. More images in Appendix Figs A.12, A.13, A.14, A.15.\nthe prioritization of finetuning the language understanding components, including the prompt and\nthe text encoder. By doing so, we encourage the model to maintain a holistic visual representation\nof gender and racial identities, rather than manipulating low-level pixels to signal gender and race.\n5.2 D ISTRIBUTIONAL ALIGNMENT OF AGE\n00.20.40.60.81 senator\ncustodian\nbutcher\ninventor\ncitizen\nFigure 4: Freq. of Age=old from gener-\nated images. X-axis denotes occupations.\nGreen horizontal line (25%) is the target.We demonstrate our method can align the age distri-\nbution to a non-uniform distribution, specifically 75%\nyoung and 25% old, for every occupational prompt\nwhile simultaneously debiasing gender and race. Uti-\nlizing the age attribute from the FairFace dataset,\nyoung is defined as ages 0-39 and old encompasses\nages 39 and above. To avoid the pitfall that the model\nconsistently generating images of young white females\nand old black males, we finetune with a stronger DAL\nthat aligns age toward the target distribution condi-\ntional on gender and race . Similar to gender and race, we evaluate using an independently trained\nage classifier. We report other experiment details in Appendix Section A.11.\nTable 5 reveals that our distributional alignment of age is highly accurate at the overall level, yield-\ning a 24.8% representation of old individuals on average. It neither undermines the efficiency of\n8", "start_char_idx": 0, "end_char_idx": 2632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afae5beb-35b5-481b-a8a1-c4fed277fa88": {"__data__": {"id_": "afae5beb-35b5-481b-a8a1-c4fed277fa88", "embedding": null, "metadata": {"page_label": "9", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4542d1f1-69f1-4c7b-9b9a-3ff526452567", "node_type": "4", "metadata": {"page_label": "9", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ecddab5e803b68b0a75460dd6478254f7457fc41dfeca66eb2446de688ab0e7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "677b9409-3a7f-4435-bd3f-d5722301a839", "node_type": "1", "metadata": {}, "hash": "7925128cf2625ebaddb1261bda21d60e150581b6616ae28c5398d924906ec748", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFinetune Prompt Prefix Finetune T ext Encoder Finetune U-Net Finetune T .E. & U-Net\n Original SD\n(a)\nSergeant\nFinetune T .E. & U-Net\n Finetune U-Net Finetune T ext Encoder Finetune Prompt Prefix Original SD (b)\nFigure 5: The left figure (a) shows finetuning U-Net may deteriorate image quality regarding facial\nskin texture. The right figure (b) shows it may generate images whose predicted gender does not\nagree with human perception, i.e., overfitting. Color bar has same semantic as in Fig. 3. Figs. A.19\nand A.20 report more examples.\nTable 5: Aligning age distribution to 75% young and 25% old besides debiasing gender & race.\nMethodBias\u2193 Freq. Semantics Preservation \u2191\nGender Race G. \u00d7R. Age=old CLIP-T CLIP-I DINO\nOriginal SD .67\u00b1.29.42\u00b1.06.21\u00b1.03.202\u00b1.263 .39\u00b1.05 \u2014 \u2014\nDebias G. \u00d7R..16\u00b1.13.09\u00b1.04.06\u00b1.02.147\u00b1.216 .39\u00b1.05.67\u00b1.15.58\u00b1.22\nDebias G. \u00d7R. & Align Age. .15\u00b1.12.09\u00b1.04.06\u00b1.02.248\u00b1.091 .38\u00b1.05.66\u00b1.16.58\u00b1.23\nTable 6: Debiasing gender, racial, and intersectional biases for multiple concepts at once.\nOccupations Sports Occ. w/ style & context Personal descriptors\nBias\u2193 S. P.\u2191 Bias\u2193 S. P.\u2191 Bias\u2193 S. P.\u2191 Bias\u2193 S. P.\u2191\nG. R. G. \u00d7R. CLIP-T G. R. G. \u00d7R. CLIP-T G. R. G. \u00d7R. CLIP-T G. R. G. \u00d7R. CLIP-T\nSD.67 .42 .21 .38 .56 .38 .19 .35 .41 .37 .18 .43 .37 .36 .17 .41\n\u00b1.29\u00b1.06\u00b1.03 \u00b1.05\u00b1.28\u00b1.05\u00b1.03 \u00b1.06\u00b1.26\u00b1.08\u00b1.03 \u00b1.05\u00b1.26\u00b1.06\u00b1.03 \u00b1.04\nOurs.23 .10 .07 .38 .37 .11 .08 .35 .31 .19 .11 .42 .18 .13 .07 .41\n\u00b1.18\u00b1.04\u00b1.02 \u00b1.05\u00b1.23\u00b1.06\u00b1.04 \u00b1.05\u00b1.20\u00b1.07\u00b1.03 \u00b1.05\u00b1.17\u00b1.06\u00b1.03 \u00b1.04\ndebiasing gender and race nor negatively impacts the quality of the generated images. Fig. 4 further\ndemonstrates that the original SD displays marked occupational age bias. For example, it associates\n\u201csenator\u201d solely with old individuals, followed by custodian, butcher, and inventor. While the distri-\nbutional alignment is noisier at the individual prompt level, our method achieves approximately 25%\nrepresentation of old individuals for most occupations. We show generated images in Fig. A.21.\n5.3 D EBIASING MULTIPLE CONCEPTS AT ONCE\nFinally, we show our method is scalable. It can debias multiple concepts at once by simply includ-\ning these prompts in the finetuning data. We now debias SD using a mixture of the following four\nclasses of prompts: (1) occupational prompts : formulated with the template \u201c a photo of the face\nof a{occupation }, a person \u201d. We utilize the same 1000/50 occupations as in Section 5.1 for train-\ning/testing. (2) sports prompts : formulated with the template \u201c a person playing {sport}\u201d. We use\n250/50 sports activities for training/testing, such as \u201c yoga \u201d, \u201ckickboxing \u201d, and \u201c ninjutsu \u201d. (3) Occu-\npational prompts with style & context : these are non-templated prompts that specify occupations\nwith diverse styles or contexts. We train/test on 150/19 such prompts obtained from the captions\nin the LAION-AESTHETICS dataset.", "start_char_idx": 0, "end_char_idx": 2894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "677b9409-3a7f-4435-bd3f-d5722301a839": {"__data__": {"id_": "677b9409-3a7f-4435-bd3f-d5722301a839", "embedding": null, "metadata": {"page_label": "9", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4542d1f1-69f1-4c7b-9b9a-3ff526452567", "node_type": "4", "metadata": {"page_label": "9", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ecddab5e803b68b0a75460dd6478254f7457fc41dfeca66eb2446de688ab0e7c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afae5beb-35b5-481b-a8a1-c4fed277fa88", "node_type": "1", "metadata": {"page_label": "9", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "148d94b5c750eaff83364835e9a481a91e6b322dce0381cc0803319c15598dd1", "class_name": "RelatedNodeInfo"}}, "text": "We now debias SD using a mixture of the following four\nclasses of prompts: (1) occupational prompts : formulated with the template \u201c a photo of the face\nof a{occupation }, a person \u201d. We utilize the same 1000/50 occupations as in Section 5.1 for train-\ning/testing. (2) sports prompts : formulated with the template \u201c a person playing {sport}\u201d. We use\n250/50 sports activities for training/testing, such as \u201c yoga \u201d, \u201ckickboxing \u201d, and \u201c ninjutsu \u201d. (3) Occu-\npational prompts with style & context : these are non-templated prompts that specify occupations\nwith diverse styles or contexts. We train/test on 150/19 such prompts obtained from the captions\nin the LAION-AESTHETICS dataset. For instance, one example reads, \u201c a aesthetic portrait of a\nmagician working on ancient machines to do magic, concept art \u201d. And finally, (4) personal de-\nscriptors : these prompts describe individual(s). We use 40/10 such prompts for training/testing.\nExamples include \u201c hot personal trainer \u201d and \u201c Oil painting of a person wearing colorful fabric \u201d.\nWe provide details of the prompts in Appendix A.12.\nTable 6 reports the evaluation. The debiased SD reduces gender, racial, and intersectional biases\nfor all four concepts without degrading prompt-image alignment. We show generated images in\nAppendix A.13. We did not notice a significant decrease in image quality. However, it does appear\nto increase the probability of generating images that blend male and female characteristics compared\nwith single concept debiasing.\n6 C ONCLUSION\nThis work considers fairness in text-to-image (T2I) diffusion models as a distributional alignment\nproblem. It proposes a supervised finetuning method, consisting of the distributional alignment loss\nand the adjusted direct finetuning of the diffusion model\u2019s sampling process. The study contributes\nmeaningfully to the ethical use of T2I diffusion models and highlights the importance of social\nalignment for multimedia generative AI.\n9", "start_char_idx": 2208, "end_char_idx": 4172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d37cc2a-905e-4764-9351-721059585ca3": {"__data__": {"id_": "7d37cc2a-905e-4764-9351-721059585ca3", "embedding": null, "metadata": {"page_label": "10", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27f4fe09-6d1a-43b7-80d6-1141e649d833", "node_type": "4", "metadata": {"page_label": "10", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d8339f84dc77eaebc4b12d6ee160b5cf259bb9d09cb262a4f90da3f3c767a7ac", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nThis research is partly supported by the National Research Foundation, Singapore under its Strategic\nCapability Research Centres Funding Initiative. Any opinions, findings and conclusions or recom-\nmendations expressed in this material are those of the author(s) and do not reflect the views of\nNational Research Foundation, Singapore. The computational work was partially performed on\nresources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).\nETHICS STATEMENT\nThis work contributes meaningfully to the ethical use of text-to-image diffusion models by ensuring\na more balanced (or socially desired) representation of different protected groups in the generated\nimages. The problem we studied is frequently overlooked but carries enduring consequences, such\nas perpetuating a skewed worldview and restricting opportunities for minority groups. We discuss\nthe ethical concerns of our work below.\nFirstly, attempts to mitigate biases in T2I generative models, including our own, face the fundamen-\ntal challenge of defining what visually represents different protected groups, including male, female,\nBlack, and Asian individuals. Our method involves using classifiers trained on face images to define\nthese groups. This approach, however, may neglect non-facial characteristics and risk marginal-\nizing individuals with atypical facial features. Alternative approaches exist, such as utilizing the\ngenerative model\u2019s own understanding of different protected groups. Yet, it\u2019s uncertain whether the\nalternative approach is devoid of stereotypes and more effectively addresses issues of marginaliza-\ntion. We acknowledge the complexity of this issue and the absence of a completely satisfactory\nresolution.\nSecondly, our method treats the attributes to be debiased as categorical. Consequently, it falls short\nin improving the representation of individuals who do not conform to traditional social categories,\nsuch as those with non-binary gender identities or mixed racial backgrounds. As we acknowledged\nearlier in our discussion of binary gender, this represents a significant research question that our\ncurrent work does not address and requires future research.\nFinally, our method does not address more nuanced forms of cultural biases. For example, the\nneutral prompt \u201dappetizing food\u201d may predominantly generate food images of Western cuisine. We\nrecognize the significance of tackling these cultural biases in addition to biases centered around\nhumans. We look forward to future research addressing this issue.\nREPRODUCIBILITY STATEMENT\nWe provide our source code and various trained fair adaptors for runwayml/stable-diffusion-v1-5\ninhttps://github.com/sail-sg/finetune-fair-diffusion . The pseudocode for\nthe proposed adjusted DFT is shown in Appendix Algorithm A.1. Experiment details are reported\nin Section 5.1 in the main paper, as well as Appendix A.2, A.5, A.6, A.7, A.12.\nREFERENCES\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,\n2022.\nHritik Bansal, Da Yin, Masoud Monajatipoor, and Kai-Wei Chang. How well can text-to-image\ngenerative models understand ethical natural language interventions? In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing , pp. 1358\u20131370, 2022.\nMatthias Bastian. Dall-e 2 has more than one million users, new\nfeature released, 9 2022. URL https://the-decoder.com/\ndall-e-2-has-one-million-users-new-feature-rolls-out .\nFederico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza,\nTatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily accessible text-to-\n10", "start_char_idx": 0, "end_char_idx": 3888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2105cbdb-2810-49a6-90bc-5ffda4b6f1db": {"__data__": {"id_": "2105cbdb-2810-49a6-90bc-5ffda4b6f1db", "embedding": null, "metadata": {"page_label": "11", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8770bec-e174-49fe-b9f3-2e84fdcd9c29", "node_type": "4", "metadata": {"page_label": "11", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "55ced52a93176b7b321d8933de39244427dde9319e049eebed76100a71797bed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03723e5a-4b45-4c95-9d7e-8f4ba2a7a0f1", "node_type": "1", "metadata": {}, "hash": "81bd9f3df27f93e2fe8a7e65931bf482c7aca3468a1722fd0e44443a9a0db42f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nimage generation amplifies demographic stereotypes at large scale. In Proceedings of the 2023\nACM Conference on Fairness, Accountability, and Transparency , pp. 1493\u20131504, 2023.\nTim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image\nediting instructions. In CVPR , 2023.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174 , 2016.\nJaemin Cho, Abhay Zala, and Mohit Bansal. DALL-Eval: Probing the reasoning skills and social\nbiases of text-to-image generative transformers. In International Conference of Computer Vision ,\n2023.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in Neural Information Processing\nSystems , 30, 2017.\nChing-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka. Debias-\ning vision-language models via biased prompts. arXiv preprint arXiv:2302.00070 , 2023.\nKevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet. Directly fine-tuning diffusion models\non differentiable rewards. arXiv preprint arXiv:2309.17400 , 2023.\nXiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon\nVandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Rade-\nnovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran\nMotwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Va-\njda, and Devi Parikh. Emu: Enhancing image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807 , 2023.\nMureji Fatunde and Crystal Tse. Digital Media Company Stabil-\nity AI Raises Funds at $1 Billion Value, December 2022. URL\nhttps://www.bloomberg.com/news/articles/2022-10-17/\ndigital-media-firm-stability-ai-raises-funds-at-1-billion-value .\nFelix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha\nLuccioni, and Kristian Kersting. Fair diffusion: Instructing text-to-image generation models on\nfairness. arXiv preprint arXiv:2302.10893 , 2023.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and\nDaniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using\nTextual Inversion. In International Conference on Learning Representations , 2023.\nRohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzy \u00b4nska, and David Bau. Unified\nconcept editing in diffusion models. arXiv preprint arXiv:2308.14761 , 2023.\nPhillip Atiba Goff, Jennifer L Eberhardt, Melissa J Williams, and Matthew Christian Jackson. Not\nyet human: implicit knowledge, historical dehumanization, and contemporary consequences.\nJournal of personality and social psychology , 94(2):292, 2008.\nTiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Bain-\ning Guo. Efficient Diffusion Training via Min-SNR Weighting Strategy. arXiv preprint\narXiv:2303.09556 , 2023.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep\nGenerative Models and Downstream Applications , 2021.", "start_char_idx": 0, "end_char_idx": 3244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03723e5a-4b45-4c95-9d7e-8f4ba2a7a0f1": {"__data__": {"id_": "03723e5a-4b45-4c95-9d7e-8f4ba2a7a0f1", "embedding": null, "metadata": {"page_label": "11", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8770bec-e174-49fe-b9f3-2e84fdcd9c29", "node_type": "4", "metadata": {"page_label": "11", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "55ced52a93176b7b321d8933de39244427dde9319e049eebed76100a71797bed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2105cbdb-2810-49a6-90bc-5ffda4b6f1db", "node_type": "1", "metadata": {"page_label": "11", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e5e68c3c6cd485364833945a6913ea1245fd880124a4be5682c90523013085bd", "class_name": "RelatedNodeInfo"}}, "text": "Unified\nconcept editing in diffusion models. arXiv preprint arXiv:2308.14761 , 2023.\nPhillip Atiba Goff, Jennifer L Eberhardt, Melissa J Williams, and Matthew Christian Jackson. Not\nyet human: implicit knowledge, historical dehumanization, and contemporary consequences.\nJournal of personality and social psychology , 94(2):292, 2008.\nTiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Bain-\ning Guo. Efficient Diffusion Training via Min-SNR Weighting Strategy. arXiv preprint\narXiv:2303.09556 , 2023.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep\nGenerative Models and Downstream Applications , 2021.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nNeural Information Processing Systems , 33:6840\u20136851, 2020.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations , 2021.\n11", "start_char_idx": 2565, "end_char_idx": 3620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "794b0645-5371-4c56-a4ae-cfa78c17c149": {"__data__": {"id_": "794b0645-5371-4c56-a4ae-cfa78c17c149", "embedding": null, "metadata": {"page_label": "12", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38c8a6e0-b8eb-4c17-b531-cf82f11fea38", "node_type": "4", "metadata": {"page_label": "12", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "54ea74847895bcc37dbafc88b8f73fe45270b7ca8c1ff4b9aad132430c157be9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24931f73-8ee5-4e4f-9433-f1d7bb5cff63", "node_type": "1", "metadata": {}, "hash": "ebfe5952887ae2174c4539bc1a81954c88c882188be147c65f9be74177107984", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nKimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender,\nand age for bias measurement and mitigation. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision , pp. 1548\u20131558, 2021.\nEunji Kim, Siwon Kim, Chaehun Shin, and Sungroh Yoon. De-stereotyping text-to-image models\nthrough prompt tuning. In ICML Workshop on Challenges in Deployable Generative AI , 2023.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-\ncessing , pp. 3045\u20133059, 2021.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) ,\npp. 4582\u20134597, 2021.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\nInProceedings of International Conference on Computer Vision , pp. 3730\u20133738, 2015.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver++: Fast\nSolver for Guided Sampling of Diffusion Probabilistic Models. arXiv preprint arXiv:2211.01095 ,\n2022.\nAlexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias:\nAnalyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408 , 2023.\nGaspard Monge. M \u00b4emoire sur la th \u00b4eorie des d \u00b4eblais et des remblais. Mem. Math. Phys. Acad. Royale\nSci., pp. 666\u2013704, 1781.\nEllis Monk. The monk skin tone scale. 2023.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.\nMaxime Oquab, Timoth \u00b4ee Darcet, Theo Moutakanni, Huy V . V o, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao\nHuang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand\nJoulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision.\narXiv:2304.07193 , 2023.\nHadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing implicit assumptions in text-to-image\ndiffusion models. arXiv preprint arXiv:2303.08084 , 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In Proceedings of the 38th International Conference\non Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 8748\u2013\n8763. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents.", "start_char_idx": 0, "end_char_idx": 3157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24931f73-8ee5-4e4f-9433-f1d7bb5cff63": {"__data__": {"id_": "24931f73-8ee5-4e4f-9433-f1d7bb5cff63", "embedding": null, "metadata": {"page_label": "12", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38c8a6e0-b8eb-4c17-b531-cf82f11fea38", "node_type": "4", "metadata": {"page_label": "12", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "54ea74847895bcc37dbafc88b8f73fe45270b7ca8c1ff4b9aad132430c157be9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "794b0645-5371-4c56-a4ae-cfa78c17c149", "node_type": "1", "metadata": {"page_label": "12", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ce00060813db9f1bfb3dd9802699aa40daf1620c814a5a1ef90a68b1ceccb01c", "class_name": "RelatedNodeInfo"}}, "text": "Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing implicit assumptions in text-to-image\ndiffusion models. arXiv preprint arXiv:2303.08084 , 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In Proceedings of the 38th International Conference\non Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 8748\u2013\n8763. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj \u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pp. 10684\u201310695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems , 35:36479\u201336494, 2022.\n12", "start_char_idx": 2480, "end_char_idx": 3782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2ba4600-ec5a-4568-b071-65b21b9e7596": {"__data__": {"id_": "c2ba4600-ec5a-4568-b071-65b21b9e7596", "embedding": null, "metadata": {"page_label": "13", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4452e4d-eadb-49ff-9224-5d250d10655b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "530196e30b5a8f21be09a2aa85a46be0baf692df4d7bf6cee7b9e4f909fc4229", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nPatrick Schramowski, Manuel Brack, Bj \u00a8orn Deiseroth, and Kristian Kersting. Safe latent diffusion:\nMitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 22522\u201322531, 2023.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems , 35:25278\u201325294, 2022.\nPreethi Seshadri, Sameer Singh, and Yanai Elazar. The bias amplification paradox in text-to-image\ngeneration. arXiv preprint arXiv:2308.00755 , 2023.\nBram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent opti-\nmization improves classifier guidance. arXiv preprint arXiv:2303.13703 , 2023.\nJialu Wang, Xinyue Liu, Zonglin Di, Yang Liu, and Xin Wang. T2IAT: Measuring valence and\nstereotypical biases in text-to-image generation. In Findings of the Association for Computational\nLinguistics , pp. 2560\u20132574, 2023a.\nZihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for score-based con-\nditional model. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative\nModeling , 2023b.\nZijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng\nChau. Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models.\narXiv preprint arXiv:2210.14896 , 2022.\nYandong Wen, Weiyang Liu, Adrian Weller, Bhiksha Raj, and Rita Singh. SphereFace2: Binary\nClassification is All You Need for Deep Face Recognition. In International Conference on Learn-\ning Representations , 2022.\nRobert Wolfe, Yiwei Yang, Bill Howe, and Aylin Caliskan. Contrastive language-vision ai models\npretrained on web-scraped multimodal data exhibit sexual objectification bias. In Proceedings of\nthe 2023 ACM Conference on Fairness, Accountability, and Transparency , pp. 1174\u20131185, 2023.\nChen Henry Wu, Saman Motamed, Shaunak Srivastava, and Fernando D De la Torre. Generative\nvisual prompt: Unifying distributional control of pre-trained generative models. Advances in\nNeural Information Processing Systems , 35:22422\u201322437, 2022.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543 , 2023.\n13", "start_char_idx": 0, "end_char_idx": 2490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfaee86c-16e2-41b9-a151-fe5f9475a467": {"__data__": {"id_": "dfaee86c-16e2-41b9-a151-fe5f9475a467", "embedding": null, "metadata": {"page_label": "14", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "231bed0b-b454-4b9b-9e4d-dc51752a53b7", "node_type": "4", "metadata": {"page_label": "14", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4e850983d59ec4e4617960ad5b4a7d564d31e05d1688259e073abc54139d6235", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA A PPENDIX\nA.1 A DJUSTED DFT\nWe show implementation of adjusted DFT in Algorithm A.1. In our experiments, we use DPM-\nSolver++ (Lu et al., 2022) as the diffusion scheduler. We randomly sample total inference time\nstepSfrom{19,20,21,22,23 }to avoid overfitting. We provide code in https://github.com/\nsail-sg/finetune-fair-diffusion .\nAlgorithm A.1: Adjusted DFT of diffusion model\ninput : text encoder g\u03d5; U-Net \u03f5\u03b8; image decoder fDec; loss function L; variance schedule\n{\u03b2t\u2208(0,1)}t\u2208[T]and corresponding {\u03b1t}t\u2208[T],{\u00af\u03b1t}t\u2208[T]; prompt P; diffusion\nscheduler scheduler ; inference time step schedule t1=T, t2,\u00b7\u00b7\u00b7, tS= 0.\n/*Prepare grad coefficients. */\nfori= 1,2,\u00b7\u00b7\u00b7, S\u22121do\nCi\u21901/(1\u221a\n\u00af\u03b1ti\u03b2ti\u221a\n1\u2212\u00af\u03b1ti);\nend\nC\u2190[C1,\u00b7\u00b7\u00b7, CS\u22121]/(QK\u22121\nt=1Ct)1/(S\u22121);\n/*T2I w/ adjusted gradient */\nzt\u2190zT\u223c N(0,1);\nfori= 1,2,\u00b7\u00b7\u00b7, S\u22121do\nt, tprev\u2190ti, ti+1;\nz\u2032\nt\u2190detach (zt);\n\u03f5t\u2190f\u03b8(g\u03d5(P),z\u2032\nt, tt);\n\u03f5\u2032\nt\u2190\u03f5t.grad hook( g:g\u00d7C[i]);\nztprev\u2190scheduler (zt, \u03f5\u2032\nt, t, tprev);\nend\nx0\u2190fDec(z0);\nBackpropagate gradientdL(x0)\ndx0from generated image x0to U-Net \u03b8, text encoder \u03d5, or prompt\nP\nA.2 E XPERIMENT DETAILS\nWe do not list training occupations here due to their large quantity. The test occupa-\ntions are [\u2019senator\u2019, \u2019violinist\u2019, \u2019ticket taker\u2019, \u2019electrical and\nelectronics repairer\u2019, \u2019citizen\u2019, \u2019geologist\u2019, \u2019food cooking\nmachine operator\u2019, \u2019community and social service specialist\u2019,\n\u2019manufactured building and mobile home installer\u2019, \u2019behavioral\ndisorder counselor\u2019, \u2019sewer\u2019, \u2019roustabout\u2019, \u2019researcher\u2019,\n\u2019operations research analyst\u2019, \u2019fence erector\u2019, \u2019construction\nand related worker\u2019, \u2019legal secretary\u2019, \u2019correspondence clerk\u2019,\n\u2019narrator\u2019, \u2019marriage and family therapist\u2019, \u2019clinical laboratory\ntechnician\u2019, \u2019gas compressor and gas pumping station operator\u2019,\n\u2019cosmetologist\u2019, \u2019stocker\u2019, \u2019machine offbearer\u2019, \u2019salesperson\u2019,\n\u2019administrative services manager\u2019, \u2019mail machine operator\u2019,\n\u2019veterinary technician\u2019, \u2019surveying and mapping technician\u2019,\n\u2019signal and track switch repairer\u2019, \u2019industrial machinery\nmechanic\u2019, \u2019inventor\u2019, \u2019public safety telecommunicator\u2019,\n\u2019ophthalmic medical technician\u2019, \u2019promoter\u2019, \u2019interior designer\u2019,\n\u2019blaster\u2019, \u2019general internal medicine physician\u2019, \u2019butcher\u2019, \u2019farm\nequipment service technician\u2019, \u2019associate dean\u2019, \u2019accountants\nand auditor\u2019, \u2019custodian\u2019, \u2019sergeant\u2019, \u2019executive assistant\u2019,\n\u2019administrator\u2019, \u2019physical science technician\u2019, \u2019health\ntechnician\u2019, \u2019cardiologist\u2019] . We have another 10 occupations used for vali-\ndation: [\"housekeeping cleaner\", \"freelance writer\", \"lieutenant\",\n\"fine artist\", \"administrative law judge\", \"librarian\", \"sale\",\n\"anesthesiologist\", \"secondary school teacher\", \"dancer\"] .\n14", "start_char_idx": 0, "end_char_idx": 2654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adf07e33-c916-4d04-a6c2-26dbfa740520": {"__data__": {"id_": "adf07e33-c916-4d04-a6c2-26dbfa740520", "embedding": null, "metadata": {"page_label": "15", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4141474b-5fb3-40f8-aa98-3e6384c57222", "node_type": "4", "metadata": {"page_label": "15", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d39296b145cab687e2d52289d3415260d0070541feba3ac626010192c2a0fe8c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFor the gender debiasing experiment, we train a gender classifier using the CelebA dataset. We use\nCelebA faces as external faces for the face realism preserving loss. We set \u03bbface= 1,\u03bbimg,1= 8,\n\u03bbimg,2= 0.2\u00d7\u03bbimg,1, and \u03bbimg,3= 0.2\u00d7\u03bbimg,2. We use batch size N= 24 and set the confidence\nthreshold for the distributional alignment loss C= 0.8. We train for 10k iterations using AdamW\noptimizer with learning rate 5e-5. We checkpoint every 200 iterations and report the best checkpoint.\nThe finetuning takes around 48 hours on 8 NVIDIA A100 GPUs.\nFor the race debiasing experiment, we train a race classifier using the FairFace dataset. We use\nFairFace faces as external faces for the face realism preserving loss. We set \u03bbface= 0.1,\u03bbimg,1= 6,\n\u03bbimg,2= 0.6\u00d7\u03bbimg,1, and \u03bbimg,3= 0.3\u00d7\u03bbimg,2. We use batch size N= 32 and set the confidence\nthreshold for the distributional alignment loss C= 0.8. We train for 12k iterations using AdamW\noptimizer with learning rate 5e-5. We checkpoint every 200 iterations and report the best checkpoint.\nThe finetuning takes around 48 hours on 8 NVIDIA A100 GPUs.\nFor the experiment that debiases gender and race jointly, we train a classifier that classifies both\ngender and race using the FairFace dataset. We use FairFace faces as external faces for the face\nrealism preserving loss. We set \u03bbface= 0.1andWimg,1= 8. For the gender attribute, we use\n\u03bbimg,2= 0.2\u00d7\u03bbimg,1, and\u03bbimg,3= 0.2\u00d7\u03bbimg,2. For the race attribute, we use \u03bbimg,2= 0.6\u00d7\u03bbimg,1\nand\u03bbimg,3= 0.3\u00d7\u03bbimg,2. We use batch size N= 32 and set the confidence threshold for the\ndistributional alignment loss C= 0.6. We train for 14k iterations using AdamW optimizer with\nlearning rate 5e-5. We checkpoint every 200 iterations and report the best checkpoint. The finetuning\ntakes around 48 hours on 8 NVIDIA A100 GPUs.\nA.3 T RAINING LOSS VISUALIZATION\n0 1k 2k 3k 5k 10k00.511.5\n0 1k 2k 3k 5k 10k00.20.40.60.8\n0 1k 2k 3k 5k 10k00.20.40.6\n0 1k 2k 3k 5k 10k00.10.20.30.40.5\n0 1k 2k 3k 5k 10k00.20.40.60.8\nFigure A.1: Training and validation losses in the gender debiasing experiment. X-axis denotes\ntraining iterations. We trained for 10k iterations, which took around 48 hours on 8 NVIDIA A100\nGPUs. The first four plots show different training losses, where gray lines denote the losses and\nblack lines show 10-point moving averages.\nA.4 R EPRESENTATION PLOT\nWe plot the gender and race representations for every occupational prompt in Fig. A.2, A.3, A.4,\nand A.5. These results provide a more detailed analysis than those presented in Tab. 1.\n15", "start_char_idx": 0, "end_char_idx": 2569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c67b9127-6fd6-4e4f-9d5a-f86626a1eb62": {"__data__": {"id_": "c67b9127-6fd6-4e4f-9d5a-f86626a1eb62", "embedding": null, "metadata": {"page_label": "16", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b7f80e0-10cd-437f-a08e-c3b127dd1538", "node_type": "4", "metadata": {"page_label": "16", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f1299110e32567d6b3333954f7a7cab26b1a3b659c1c424483e8b92c80bb5624", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n00.20.40.60.81\nOccupationsfreq. of gender=male\n00.20.40.60.81\nOccupationsfreq. of gender=male\n00.20.40.60.81\nOccupationsfreq. of gender=male\n00.20.40.60.81\nOccupationsfreq. of gender=male\n00.20.40.60.81\nOccupationsfreq. of gender=male\nFigure A.2: Comparison of gender representation in images generated using different occupational\nprompts. Green horizontal line denotes the target ( 50% male and female, respectively). These\nfigures correspond to the gender debiasing experiments in Tab. 1. Every plot represents a different\ndebiasing method. Prompt template is \u201ca photo of the face of a {occupation }, a person\u201d.\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\nFigure A.3: Comparison of race representation in images generated using different occupational\nprompts. Green horizontal line denotes the target ( 25% for WMELH, Asian, Black, or Indian,\nrespectively). These figures correspond to the race debiasing experiments in Tab. 1. Every row\nrepresents a different debiasing method. Prompt template is \u201ca photo of the face of a {occupation },\na person\u201d.\n16", "start_char_idx": 0, "end_char_idx": 1796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c54351a0-be42-451e-90ca-b2fae7aea8e6": {"__data__": {"id_": "c54351a0-be42-451e-90ca-b2fae7aea8e6", "embedding": null, "metadata": {"page_label": "17", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06d088b8-5eda-4b9e-ac62-da838d762942", "node_type": "4", "metadata": {"page_label": "17", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b49e2b0fea5cd826b611f70a168ae1b259694c128bc9c8e28f78b176b02a9090", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n00.20.40.60.81\nOccupationsfreq. of gender=male\n00.20.40.60.81\nOccupationsfreq. of gender=male\n00.20.40.60.81\nOccupationsfreq. of gender=male\n00.20.40.60.81\nOccupationsfreq. of gender=male\nFigure A.4: Comparison of gender representation in images generated using different occupational\nprompts. Green horizontal line denotes the target ( 50% for male and female, respectively). These\nfigures correspond to the gender \u00d7race debiasing experiments in Tab. 1. Every plot represents a\ndifferent debiasing method. Prompt template is \u201ca photo of the face of a {occupation }, a person\u201d.\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\n00.20.40.60.81\nOccupationsfreq. of race=WMELH\n00.20.40.60.81\nOccupationsfreq. of race=Black\n00.20.40.60.81\nOccupationsfreq. of race=Indian\n00.20.40.60.81\nOccupationsfreq. of race=Asian\nFigure A.5: Comparison of race representation in images generated using different occupational\nprompts. Green horizontal line denotes the target ( 25% for WMELH, Asian, Black, or Indian,\nrespectively). These figures correspond to the gender \u00d7race debiasing experiments in Tab. 1. Ev-\nery row represents a different debiasing method. Prompt template is \u201ca photo of the face of a\n{occupation }, a person\u201d.\n17", "start_char_idx": 0, "end_char_idx": 1769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00cad084-ca58-4bfe-84ff-a039e455c537": {"__data__": {"id_": "00cad084-ca58-4bfe-84ff-a039e455c537", "embedding": null, "metadata": {"page_label": "18", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138366aa-9edc-4d4c-b373-19046c971eba", "node_type": "4", "metadata": {"page_label": "18", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "54f50a9ae46fc7d9c217a8843aee3947aff973865ed66612c363011f2250c39c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b50e938-f100-4101-a4a9-420338a87ae2", "node_type": "1", "metadata": {}, "hash": "44678ce52f12f04bd0e2f1784302801813a30cf4c3395e5597cc4602ef4ddcd0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA.5 E VALUATION ON GENERAL PROMPTS\nThis section aims to analyze the impact of our debiasing method on image generations for general\nprompts that are not necessarily occupational. We randomly select 19 prompts from the DiffusionDB\ndataset (Wang et al., 2022). These prompts were written by real users and collected from the official\nStable Diffusion Discord server. For each prompt, we generate six images using both the original\nSD and the SD debiased for gender and racial biases in occupational prompts, with the same set of\nnoises. The generated images are shown in Figs A.6, A.7, A.8, & A.9.\nWe find the images generated by the debiased SD closely resemble those from the original SD and\nmaintain a strong alignment with the textual prompts. The debiased SD still has a good under-\nstanding of various concepts: celebrities such as \u201cjuice wrld\u201d (Fig. A.6a) and \u201celizabeth olsen\u201d\n(Fig. A.6b), animals such as \u201csquirrel\u201d (Fig. A.10a), carton figures such as \u201cgarfield gnome\u201d\n(Fig. A.9d), and styles such as \u201cthe style of stephen gammel and lisa frank\u201d (Fig. A.9b), \u201c3d\u201d\n(Fig. A.9c), and \u201cthe style of Mona Lisa\u201d (Fig. A.10a). Moreover, it remains instructionable and\ncapable of performing creative image hallucination. For example, the debiased SD is still able to\ndepict dinosaur in NYC streets in Fig. A.8b and draw squirrel in the style of Mona Lisa in Fig. A.10a.\nUpon closely examining the generated images, we observe that the most significant adverse effect\nof our debiasing finetuning is it sometimes reduces naturalness and smoothness of the generated\nimages. We have identified some instances of such. Firstly, the fourth column of Fig. A.6c exhibits\nan unnatural texture on the face post-debiasing. Secondly, the generated cartoon illustrations may\nbecome noisier, as seen in Figs A.9a, A.9b, and to a lesser degree in Fig. A.9c. Besides natural-\nness and smoothness, Fig. A.6d seems to indicate that debiasing finetuning diminishes the model\u2019s\nability to accurately represent the named entity \u201cSnoop Dogg\u201d. It is important to note that these\nobservations are based on the lead author\u2019s subjective assessment of a limited set of images and may\nnot generalize. Furthermore, evaluating the impact of our debiasing finetuning is complicated by\nthe fact that the original SD can also occasionally produce unnatural images, as observed in the last\ncolumn of Fig. A.7a.\nFinally, Fig. A.11, which is an expanded version of Fig. A.7c, displays additional images generated\nfrom the prompt \u201dA beautiful painting of woman by Mandy Jurgens, Trending on artstation.\u201d This\nprovides an example of how the debiasing effect generalizes to general prompts. We note that the\ndebiased SD evaluated here was finetuned for gender and racial biases for templated occupational\nprompts. it has not been debiased with respect to the term \u201dwoman\u201d. First, the debiased SD effec-\ntively maintains gender accuracy by still recognizing the term \u201dwoman\u201d and not generating images\nof man. This shows the debiased SD does not exhibit overfitting in this respect. Second, while\nthe debiased SD increases the representation of Asian women in the generated images, it does not\nsimilarly increase the presence of Black and Indian women. This suggests that the debiasing effect\u2019s\ngeneralization is somewhat limited. To achieve fairer outcomes for general prompts, debiasing fine-\ntuning w.r.t. a wider range of prompts is necessary.\nA.6 P ROMPTS FROM LAION-A ESTHETICS V2\nWe obtain the following 40 prompts from LAION-Aesthetics V2 dataset: [\u2019A census\nworker speaks to a New York City tenant\u2019, \u2019A chef in a white coat\nleans on a table\u2019, \u2019A painting of a chef wearing white chef hat\nand jacket\u2019, \u2019A philosopher reading.", "start_char_idx": 0, "end_char_idx": 3737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b50e938-f100-4101-a4a9-420338a87ae2": {"__data__": {"id_": "3b50e938-f100-4101-a4a9-420338a87ae2", "embedding": null, "metadata": {"page_label": "18", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "138366aa-9edc-4d4c-b373-19046c971eba", "node_type": "4", "metadata": {"page_label": "18", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "54f50a9ae46fc7d9c217a8843aee3947aff973865ed66612c363011f2250c39c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00cad084-ca58-4bfe-84ff-a039e455c537", "node_type": "1", "metadata": {"page_label": "18", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4f212090c23431b13ae7b2faa06101d10e753f65c0831afa322c0829e75cfed7", "class_name": "RelatedNodeInfo"}}, "text": "This shows the debiased SD does not exhibit overfitting in this respect. Second, while\nthe debiased SD increases the representation of Asian women in the generated images, it does not\nsimilarly increase the presence of Black and Indian women. This suggests that the debiasing effect\u2019s\ngeneralization is somewhat limited. To achieve fairer outcomes for general prompts, debiasing fine-\ntuning w.r.t. a wider range of prompts is necessary.\nA.6 P ROMPTS FROM LAION-A ESTHETICS V2\nWe obtain the following 40 prompts from LAION-Aesthetics V2 dataset: [\u2019A census\nworker speaks to a New York City tenant\u2019, \u2019A chef in a white coat\nleans on a table\u2019, \u2019A painting of a chef wearing white chef hat\nand jacket\u2019, \u2019A philosopher reading. Oil painting.\u2019, \u2019A sailor\nblows a trumpet in Times Square\u2019, \u2019A young factory worker at work\non an industrial lathe\u2019, \u2019A young marine officer\u2019, \u2019Afterparty\nbartender\u2019, \u2019Attractive steampunk singer with microphone photo\u2019,\n\u2019Beautiful model at cafe terrasse Paris\u2019, \u2019Beautiful young model\ndrinking cocktail in casino Stock\u2019, \u2019Buddhist teaching younger\nstudent\u2019, \u2019English writer and essayist\u2019, \u2019Olympic swimmer\u2019,\n\u2019Portrait of pilot in winter overal\u2019, \u2019The playwright and\nnovelist\u2019, \u2019Unification minister attends peace forum\u2019, \u2019Vintage\nphoto of rancher\u2019, \u2019bartender at willard intercontinental makes\nmint julep\u2019, \u2019beautiful flamenco dancer posing on a studio\nbackground\u2019, \u2019fashionable young model in elegant dress on boat\nat the lake\u2019, \u2019gifted young artist\u2019, \u2019haircut royal air force\n18", "start_char_idx": 3014, "end_char_idx": 4518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b066a024-a9e8-4584-bc9b-2d4aad98ea5e": {"__data__": {"id_": "b066a024-a9e8-4584-bc9b-2d4aad98ea5e", "embedding": null, "metadata": {"page_label": "19", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b4cc148-ac40-4b06-9f69-f4df8a60dd9d", "node_type": "4", "metadata": {"page_label": "19", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c6c5809e67a4d30130c434676ef99f7e8b0303c8294201ffa77799883962d7b5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt: \u201cjuice wrld in Vikings 4k quality super realistic\u201d.\n(b) Prompt: \u201canime key visual of elizabeth olsen police officer, cyberpunk, futuristic, stunning features, perfect\nface, high detailed, digital painting, artstation, smooth, soft focus, illustration, art by artgerm and greg rutkowski\nand alphonse mucha \u201d.\n(c) Prompt: \u201cjohn cena!! [ in cyberpunk attire ]!!, made of wires and metallic materials!!, portrait!!, digital\nart, afrofuturism, tarot card, 4 k, digital art, illustrated by greg rutkowski, max hay, rajmund kanelba, cgsociety\ncontest winner \u201d.\n(d) Prompt: \u201csnoop doog as ridley scott alien, highly detailed, concept art, art by wlop and artgerm and greg\nrutkowski, masterpiece, trending on artstation, 8 k \u201d.\nFigure A.6: Images generated using general prompts. For every subfigure, top row is generated\nusing the original SD and bottom row is generated using the SD debiased for both gender and race.\nThe pair of images at the same column are generated using the same noise.\n19", "start_char_idx": 0, "end_char_idx": 1044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfdcce89-47f0-4376-b11f-d27a9333a05e": {"__data__": {"id_": "cfdcce89-47f0-4376-b11f-d27a9333a05e", "embedding": null, "metadata": {"page_label": "20", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39624053-dde6-4ea9-852c-443830e4ecf8", "node_type": "4", "metadata": {"page_label": "20", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "917b3fdabcb5e088fefd82b0c258f5a28fd7079f3c1707b6d8b5965e4312490d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt: \u201cmovie still close-up portrait of skinny cheerful Alicia Vikander in a wedding dress kissing a groom\nwho is a morbidly obese and bearded nerd, by David Bailey, Cinestill 800t 50mm eastmancolor, heavy grainy\npicture, very detailed, high quality, 4k, HD criterion, precise texture and facial expression\u201d.\n(b) Prompt: \u201ca 1 0 year old boy is climbing a hollow log. the boy has large ears sticking straight out. standing\nin the foreground is an obese italian man clapping his hands furiously. \u201d.\n(c) Prompt: \u201cA beautiful painting of woman by Mandy Jurgens, Trending on artstation\u201d.\n(d) Prompt: \u201ca dwarf man wearing horns, diffuse lighting, fantasy, intricate, elegant, highly detailed, lifelike,\nphotorealistic, digital painting, artstation, illustration, concept art, smooth, sharp focus, naturalism, trending on\nbyron\u2019s - muse, by greg rutkowski and greg staples \u201d.\nFigure A.7: Images generated using general prompts. For every subfigure, top row is generated\nusing the original SD and bottom row is generated using the SD debiased for both gender and race.\nThe pair of images at the same column are generated using the same noise.\n20", "start_char_idx": 0, "end_char_idx": 1188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40e857e9-7e1f-4237-bf85-5ebe11e153ca": {"__data__": {"id_": "40e857e9-7e1f-4237-bf85-5ebe11e153ca", "embedding": null, "metadata": {"page_label": "21", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2294b0a3-849d-45fb-a53b-2f76c832eee7", "node_type": "4", "metadata": {"page_label": "21", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a6a5469cc8802a2e22573ce5537684ff6e1830be625574f577e9ea01f21190f7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt: \u201cAncient natural pool overgrown with moss, surrounded by lush plants, vines hanging from the tall\ntrees, pine trees, detailed, digital art, trending on Artstation, atmospheric, volumetric lighting, hyper-realistic,\nUnreal Engine, sharp\u201d.\n(b) Prompt: \u201cdinosaur kaiju in nyc street, destroyed buildings, 1990s, photographic, kodak portra 400, 8k\u201d.\n(c) Prompt: \u201ca dark exterior landscape shot of jabba\u2019s palace at night, rusty mri machine star wars maschinen\nkrieger, ilm, beeple, star citizen halo, mass effect, starship troopers, iron smelting pits, high tech industrial,\nwarm saturated colours, dramatic space sky\u201d.\n(d) Prompt: \u201cPointillism Painting of a Victorian manor at dusk soft glow HDR\u201d.\nFigure A.8: Images generated using general prompts. For every subfigure, top row is generated\nusing the original SD and bottom row is generated using the SD debiased for both gender and race.\nThe pair of images at the same column are generated using the same noise.\n21", "start_char_idx": 0, "end_char_idx": 1020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a4c5027-2aa1-4f19-aff8-ce1d6b46c1c4": {"__data__": {"id_": "6a4c5027-2aa1-4f19-aff8-ce1d6b46c1c4", "embedding": null, "metadata": {"page_label": "22", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f6bde8d-81bb-4b6c-91b2-e8d863479e7d", "node_type": "4", "metadata": {"page_label": "22", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a856bf0eb6f4a37ea8ad18aa4722d93bf40ada1f629fac01c5ee92fee1d44512", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt: \u201ca close up illustration of cthulhu as a cute kindergarten age monster playing with toys by artist\njess bradley, concept art, digital art, gaudy colors \u201d.\n(b) Prompt: \u201cdeep focus shot of a crisp really angry squidward in the style of stephen gammel and lisa frank,\ndark psychedelica, psychedelic, anger, 8 k, award - winning art \u201d.\n(c) Prompt: \u201c3d octane render style glowing eyes 3d anime child model brown skin beautiful Afro hair 3d video\ngame animal crossing background cinematic 8K\u201d.\n(d) Prompt: \u201c \\u201c garfield gnome \\u201d \u201d.\nFigure A.9: Images generated using general prompts. For every subfigure, top row is generated\nusing the original SD and bottom row is generated using the SD debiased for both gender and race.\nThe pair of images at the same column are generated using the same noise.\n22", "start_char_idx": 0, "end_char_idx": 860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c1c0966-944e-4b54-8827-f9ebd2ee5b1a": {"__data__": {"id_": "6c1c0966-944e-4b54-8827-f9ebd2ee5b1a", "embedding": null, "metadata": {"page_label": "23", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e04f264f-2e84-4943-a290-1fa8f433aeab", "node_type": "4", "metadata": {"page_label": "23", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7d8419fc837791ec023b3e7439830e7f89cf683e876cda46bcd5a8a46ab9222a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt: \u201cPainting of a squirrel in the style of Mona Lisa\u201d.\n(b) Prompt: \u201cmacro shot top view cute yellow rabbit mascot with oversized eyes and ears, logo colored drawing\nsticker \u201d.\n(c) Prompt: \u201ca complex machine that pours coffee into your mouth when you wake up\u201d.\nFigure A.10: Images generated using general prompts. For every subfigure, top row is generated\nusing the original SD and bottom row is generated using the SD debiased for both gender and race.\nThe image pairs at the same column are generated using the same noise.\n23", "start_char_idx": 0, "end_char_idx": 580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c67474e-eb78-4b68-b42b-2205f2906233": {"__data__": {"id_": "5c67474e-eb78-4b68-b42b-2205f2906233", "embedding": null, "metadata": {"page_label": "24", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c439521c-3007-4df2-8c24-b6059ff2cd3c", "node_type": "4", "metadata": {"page_label": "24", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2155f7c72605feaf249ae7edbb06a393298aee6b71b7d26d16c74078f124801b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFigure A.11: Images generated from the prompt: \u201cA beautiful painting of woman by Mandy Jurgens,\nTrending on artstation\u201d. Top/bottom 6 \u00d76 images are generated by the original/debiased SD. For\nevery image, the first color-coded bar denotes the predicted gender: male or female. The second\ndenotes race: WMELH, Asian, Black, or Indian. Bounding boxes denote detected faces. Bar height\nrepresents prediction confidence. The pair of images with the same number label are generated using\nthe same noise.\n24", "start_char_idx": 0, "end_char_idx": 545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa8f3238-53ac-4bee-aaac-64ea9357eeb7": {"__data__": {"id_": "aa8f3238-53ac-4bee-aaac-64ea9357eeb7", "embedding": null, "metadata": {"page_label": "25", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bed7d6f8-9979-401e-99db-b997782f4e41", "node_type": "4", "metadata": {"page_label": "25", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "29eb890857abb87299f85fcddcb556da8dbd96499ed6e2fce08d74e386b88a81", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\npilot\u2019, \u2019jazz pianist\u2019, \u2019magician in the desert\u2019, \u2019model with\nloose wavy curls\u2019, \u2019model with short hair and wavy side bangs\u2019,\n\u2019naval officer\u2019, \u2019painting of a nurse in a white uniform\u2019, \u2019paris\nphotographer\u2019, \u2019portrait of a flamenco dancer\u2019, \u2019scientist in his\nlab working with chemicals\u2019, \u2019singapore wedding photographer\u2019,\n\u2019student with globe\u2019, \u2019the guitarist in Custom Picture Frame\u2019,\n\u2019top chef Seattle\u2019, \u2019wedding in venice photographer\u2019, \u2019wedding\nphotographer amsterdam\u2019, \u2019wedding photographer in Sydney\u2019,\n\u2019wedding photographer in tuscany\u2019] .\nA.7 E VALUATION OF DEBIAS VL AND UCE\nDebiasVL (Chuang et al., 2023) debiases vision-language models by projecting out biased directions\nin the text embeddings. Empirically, the authors apply it on Stable Diffusion v2-11using the prompt\ntemplate \u201c A photo of a {occupation }.\u201d. They use 80 occupations for training and 20 for\ntesting.\nFor the results reported in Table 1, we apply debiasVL on Stable Diffusion v1-5 using the prompt\ntemplate \u201c a photo of the of a {occupation }, a person \u201d. To debias gender or race\nindividually, we use 1000 occupations for training and 50 for testing. To debias gender and race\njointly, we use 500 occupations for training due to memory limit, and the same 50 occupations for\ntesting. We use the same hyperparameter \u03bb= 500 as in their paper.\nWe test this method for gender bias, with different diffusion models, training occupations, and\nprompt templates. Results are reported in Table 7. We find this method sensitive to both the dif-\nfusion model and the prompt. It generally works better for SD v2-1 than SD v1-5. Using a larger\nset of occupations for training might or might not be helpful. For some combinations, this method\nexacerbates rather than mitigates gender bias. We note that the failure of debiasVL is also observed\nin Kim et al. (2023).\nFor unified concept editing (UCE) (Gandikota et al., 2023) reported in Table 1, we use the same 37\noccupations as from their paper and two templates, \u201c {occupation }\u201d and \u201ca photo of a {occupation }\u201d,\nfor training.\nPrompt Template ModelOccupationsGender Bias \u2193Train Eval\nA photo of a {occupation }.SD v2-1 - ours 0.66\u00b10.27\nDebiased SD v2-1 original ours 0.52\u00b10.30\nDebiased SD v2-1 ours ours 0.78\u00b10.21\na photo of the face of a\n{occupation }, a personSD v2-1 - ours 0.67\u00b10.31\nDebiased SD v2-1 original ours 0.49\u00b10.28\nDebiased SD v2-1 ours ours 0.49\u00b10.26\nA photo of a {occupation }.SD v1-5 - ours 0.61\u00b10.26\nDebiased SD v1-5 original ours 0.92\u00b10.12\nDebiased SD v1-5 ours ours 0.38\u00b10.27\na photo of the face of a\n{occupation }, a personSD v1-5 - ours 0.67\u00b10.29\nDebiased SD v1-5 original ours 0.99\u00b10.04\nDebiased SD v1-5 ours ours 0.98\u00b10.10\nTable 7: Evaluating debiasVL (Chuang et al., 2023) with different diffusion models, training occu-\npations, and prompt templates.\n1https://huggingface.co/stabilityai/stable-diffusion-2\n25", "start_char_idx": 0, "end_char_idx": 2894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e78b5b2d-cd55-4f01-834c-7a9a9268a726": {"__data__": {"id_": "e78b5b2d-cd55-4f01-834c-7a9a9268a726", "embedding": null, "metadata": {"page_label": "26", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6a668b1-4ef3-41cc-8a07-4ba5588f5fe4", "node_type": "4", "metadata": {"page_label": "26", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7230b41acb70d0523813ab346946c94f509144dca8f13749967ccebb346143d7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA.8 E XPANDED VERSION OF FIG3FROM MAIN TEXT\n(a) Prompt w/ unseen occupation: \u201ca photo of the face of a electrical and electronics repairer, a person\u201d. Gender\nbias: 0.84 (original) \u21920.11 (debiased). Racial bias: 0.48 \u21920.10. Gender \u00d7Race bias: 0.24 \u21920.06.\n(b) Prompt w/ unseen occupation: \u201ca photo of the face of a cardiologist, a person\u201d. Gender bias: 0 (original)\n\u21920.10 (debiased). Racial bias: 0.44 \u21920.08. Gender \u00d7Race bias: 0.19 \u21920.06.\n(c) Prompt w/ unseen occupation: \u201ca photo of the face of a farm equipment service technician, a person\u201d.\nGender bias: 0.95 (original) \u21920.10 (debiased). Racial bias: 0.48 \u21920.11. Gender \u00d7Race bias: 0.24 \u21920.06.\nFigure A.12: Images generated from the original model (left) and the model jointly debiased for\ngender and race (right). The model is debiased using the prompt template \u201ca photo of the face of a\n{occupation }, a person\u201d. For every image, the first color-coded bar denotes the predicted gender:\nmale or female. The second denotes race: WMELH, Asian, Black, or Indian. Bounding boxes\ndenote detected faces. Bar height represents prediction confidence. For the same prompt, images\nwith the same number label are generated using the same noise.\n26", "start_char_idx": 0, "end_char_idx": 1234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1960534a-f49e-4367-bf42-5e927a959d3f": {"__data__": {"id_": "1960534a-f49e-4367-bf42-5e927a959d3f", "embedding": null, "metadata": {"page_label": "27", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b77532fb-90c3-4db7-9740-9e258c503842", "node_type": "4", "metadata": {"page_label": "27", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e59c31318e583a2f0bbf4afa8b32879cc3d7d71173f82a17b9b2b83219f6e3f5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt w/ unseen occupation: \u201ca photo of the face of a gas compressor and gas pumping station operator, a\nperson\u201d. Gender bias: 0.81 (original) \u21920.14 (debiased). Racial bias: 0.46 \u21920.06. Gender \u00d7Race bias: 0.23\n\u21920.05.\n(b) Prompt w/ unseen occupation: \u201ca photo of the face of a geologist, a person\u201d. Gender bias: 0.23 (original)\n\u21920.01 (debiased). Racial bias: 0.47 \u21920.11. Gender \u00d7Race bias: 0.21 \u21920.06.\n(c) Prompt w/ unseen occupation: \u201ca photo of the face of a senator, a person\u201d. Gender bias: 0.87 (original) \u2192\n0.17 (debiased). Racial bias: 0.49 \u21920.33. Gender \u00d7Race bias: 0.24 \u21920.15.\nFigure A.13: Images generated from the original model (left) and the model jointly debiased for\ngender and race (right). The model is debiased using the prompt template \u201ca photo of the face of a\n{occupation }, a person\u201d. For every image, the first color-coded bar denotes the predicted gender:\nmale or female. The second denotes race: WMELH, Asian, Black, or Indian. Bounding boxes\ndenote detected faces. Bar height represents prediction confidence. For the same prompt, images\nwith the same number label are generated using the same noise.\n27", "start_char_idx": 0, "end_char_idx": 1177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48666463-7577-42fb-9f71-7a116602dacf": {"__data__": {"id_": "48666463-7577-42fb-9f71-7a116602dacf", "embedding": null, "metadata": {"page_label": "28", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03c5d1b1-39ca-48d9-b7a4-0f43551df237", "node_type": "4", "metadata": {"page_label": "28", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6ac1de41b4cf0227be5cc5b59e3c2c27c66add979366afc49a7c69f8df485f61", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt w/ unseen style or context: \u201cEnglish writer and essayist\u201d. Gender bias: 0.86 (original) \u21920.37\n(debiased). Racial bias: 0.50 \u21920.38. Gender \u00d7Race bias: 0.24 \u21920.18.\n(b) Prompt w/ unseen style or context: \u201cA philosopher reading. Oil painting.\u201d. Gender bias: 0.80 (original) \u2192\n0.23 (debiased). Racial bias: 0.45 \u21920.31. Gender \u00d7Race bias: 0.22 \u21920.15.\n(c) Prompt w/ unseen style or context: \u201cbartender at willard intercontinental makes mint julep\u201d. Gender bias:\n0.87 (original) \u21920.17 (debiased). Racial bias: 0.49 \u21920.33. Gender \u00d7Race bias: 0.24 \u21920.15.\nFigure A.14: Images generated from the original model (left) and the model jointly debiased for\ngender and race (right). The model is debiased using the prompt template \u201ca photo of the face of a\n{occupation }, a person\u201d. For every image, the first color-coded bar denotes the predicted gender:\nmale or female. The second denotes race: WMELH, Asian, Black, or Indian. Bounding boxes\ndenote detected faces. Bar height represents prediction confidence. For the same prompt, images\nwith the same number label are generated using the same noise.\n28", "start_char_idx": 0, "end_char_idx": 1144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88c29aaf-290a-40df-bbfd-f69ba959fbed": {"__data__": {"id_": "88c29aaf-290a-40df-bbfd-f69ba959fbed", "embedding": null, "metadata": {"page_label": "29", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d706acd9-cfd8-47a6-99a9-b25a633422dc", "node_type": "4", "metadata": {"page_label": "29", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6b98c05aa1ba7a63478373f812afbe59beb9ea75fb339e324440caa3011f93e0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Prompt w/ unseen occupation: \u201cA chef in a white coat leans on a table\u201d. Gender bias: 0.84 (original) \u2192\n0.03 (debiased). Racial bias: 0.45 \u21920.32. Gender \u00d7Race bias: 0.22 \u21920.14.\n(b) Prompt w/ unseen style: \u201cportrait of a flamenco dancer\u201d. Gender bias: 0.76 (original) \u21920.59 (debiased).\nRacial bias: 0.47 \u21920.38. Gender \u00d7Race bias: 0.23 \u21920.18.\n(c) Prompt w/ unseen context: \u201cstudent with globe\u201d. Gender bias: 0.23 (original) \u21920.04 (debiased). Racial\nbias: 0.33 \u21920.21. Gender \u00d7Race bias: 0.15 \u21920.09.\nFigure A.15: Images generated from the original model (left) and the model jointly debiased for\ngender and race (right). The model is debiased using the prompt template \u201ca photo of the face of a\n{occupation }, a person\u201d. For every image, the first color-coded bar denotes the predicted gender:\nmale or female. The second denotes race: WMELH, Asian, Black, or Indian. Bounding boxes\ndenote detected faces. Bar height represents prediction confidence. For the same prompt, images\nwith the same number label are generated using the same noise.\n29", "start_char_idx": 0, "end_char_idx": 1087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5c267d1-2c12-41bd-b506-2c6e2b6af6cf": {"__data__": {"id_": "a5c267d1-2c12-41bd-b506-2c6e2b6af6cf", "embedding": null, "metadata": {"page_label": "30", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0402fbb8-a671-40ea-b1b9-a9c000f70e84", "node_type": "4", "metadata": {"page_label": "30", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "04d06b69c97794c7291ec23be75d21c9fc9066d99b59ca1902879705ee89450f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA.9 M ULTI -FACE IMAGE GENERATIONS\nOriginal SDAdministrativeServiceManager\nOurs (Debias Gender  Race)\nCardiologist\nElectricalandElectronicsRepairer\nFarmEquipmentServiceTechnician\nFenceErector\nIndustrialMachineryMechanic\nFigure A.16: Examples of image generation featuring two individuals. Images are generated using\nthe prompt \u201cA photo of the faces of two {occupation }, two people\u201d. The occupation is shown at\nthe left. For every occupation, images with the same number are generated using the same noise.\nFor every image, the first color-coded bar denotes the predicted gender: male or female. The sec-\nond denotes race: WMELH, Asian, Black, or Indian. Bar height represents prediction confidence.\nBounding boxes denote the faces that covers the largest area in every image.\n30", "start_char_idx": 0, "end_char_idx": 824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02d4e6fa-f78b-46b0-8c6f-7ee896e8de99": {"__data__": {"id_": "02d4e6fa-f78b-46b0-8c6f-7ee896e8de99", "embedding": null, "metadata": {"page_label": "31", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a51cfb0-1f89-439b-84a4-a4cdeb44c35e", "node_type": "4", "metadata": {"page_label": "31", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "06b67283d0c2dfaac55f89bfa7a735aff2a0733fe8a5df42ffa1b9d2387b4d9d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nOriginal SDAdministrativeServiceManager\nOurs (Debias Gender  Race)Cardiologist\nElectricalandElectronicsRepairer\nFarmEquipmentServiceTechnician\nFenceErector\nIndustrialMachineryMechanic\nFigure A.17: Examples of image generation featuring three individuals. Images are generated using\nthe prompt \u201cA photo of the faces of three {occupation }, three people\u201d. The occupation is shown at\nthe left. For every occupation, images with the same number are generated using the same noise.\nFor every image, the first color-coded bar denotes the predicted gender: male or female. The sec-\nond denotes race: WMELH, Asian, Black, or Indian. Bar height represents prediction confidence.\nBounding boxes denote the faces that covers the largest area in every image.\n31", "start_char_idx": 0, "end_char_idx": 794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7df1e37-c117-445a-a4d3-c6150193f826": {"__data__": {"id_": "f7df1e37-c117-445a-a4d3-c6150193f826", "embedding": null, "metadata": {"page_label": "32", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b6b7623-3e92-486a-9246-e55bf4f57213", "node_type": "4", "metadata": {"page_label": "32", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5fabd387ce6986400b9cdc68ff7b1cfea5abfdfdb4ec9895734fc3b2a92dda7e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA.10 C OMPARING DIFFERENT FINETUNED COMPONENTS\nFig. A.18, A.19, A.20 compare generated images from finetuning different components of the dif-\nfusion model.\nFenceErectorIndustrialMachineryMechanicFarmEquipmentServiceTechnicianElectrical andElectronicsRepairerCardiologistAdministrativeServiceManager\nOriginal SDFinetune Prompt PrefixFinetune Text Encoder\nFigure A.18: Examples of generated images when different components of diffusion model are fine-\ntuned. Images are generated using the prompt \u201ca photo of the face of a {occupation }, a person\u201d. The\noccupation is shown at the left. For every occupation, images with the same number are generated\nusing the same noise.\n32", "start_char_idx": 0, "end_char_idx": 719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "677825e8-bf11-4653-bcfe-89b1a4c84e2a": {"__data__": {"id_": "677825e8-bf11-4653-bcfe-89b1a4c84e2a", "embedding": null, "metadata": {"page_label": "33", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5cfcc56d-1f80-4d9b-bfbc-7abf798b6e90", "node_type": "4", "metadata": {"page_label": "33", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0cb7f45a3b411bf319bcaeff582d17f742d5d13a323329f240fdd6edbc1d0f81", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFinetunePrompt PrefixFinetune TextEncoderFinetuneU-NetFinetune TextEncoder & U-NetAdministrativeServiceManagerAdministrator\nCosmetologistExecutiveAssistantGeologist\nOriginal SD\nFigure A.19: Examples showing how finetuning U-Net may deteriorate image quality regarding\nfacial skin texture. Images are generated using the prompt \u201c a photo of the face of a\n{occupation }, a person \u201d. The occupation is shown at the left side of every row. Every row\nof images are generated using the same noise.\n33", "start_char_idx": 0, "end_char_idx": 539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a544d0d-caee-4b41-b4a2-5a91456e5c66": {"__data__": {"id_": "9a544d0d-caee-4b41-b4a2-5a91456e5c66", "embedding": null, "metadata": {"page_label": "34", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cc29683-f6a1-4dd5-9462-306396fa44fb", "node_type": "4", "metadata": {"page_label": "34", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f66dc4cb59bccc330e7ff31aecd3919fe8e3c53ac2dc8aa202b03d80b841daf7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFinetune Text EncoderSenator\nSergeantConstructtion and Related Worker\nFinetune U-Net\nFigure A.20: Examples showing how finetuning U-Net may generate images whose predicted gen-\nder according to the classifier does not agree with human perception, i.e., overfitting. Each image\nis accompanied by a color-coded bar on the left to indicate the predicted gender: blue for male and\nred for female. The height of the bar represents the classifier\u2019s prediction confidence. The lead\nauthor reviewed these generated images. Those where the predicted gender didn\u2019t match their per-\nception are highlighted in yellow boxes. Images are generated using the prompt \u201c a photo of\nthe face of a {occupation }, a person \u201d. The occupation is shown at the left. For ev-\nery occupation, images with the same number are generated using the same noise.\n34", "start_char_idx": 0, "end_char_idx": 877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b182bec2-78e4-444e-8551-a20649c3c428": {"__data__": {"id_": "b182bec2-78e4-444e-8551-a20649c3c428", "embedding": null, "metadata": {"page_label": "35", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "667d0d09-2139-45ed-80e1-35b4bd002b26", "node_type": "4", "metadata": {"page_label": "35", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "74407abcb14d450f1378789fb3ee7fc92e3f616796137da0f70956515ebeee59", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA.11 A GEALIGNMENT\nFor the experiment that aligns age distribution while simultaneously debiasing gender and race, we\ntrain a classifier that classifies gender, race, and age using the FairFace dataset. We use FairFace\nfaces as external faces for the face realism preserving loss. We set \u03bbface= 0.1and\u03bbimg,1= 8. For\nthe gender attribute, we use \u03bbimg,2= 0.2\u00d7\u03bbimg,1, and \u03bbimg,3= 0.2\u00d7\u03bbimg,2. For race and age,\nwe use \u03bbimg,2= 0.3\u00d7\u03bbimg,1, and \u03bbimg,3= 0.3\u00d7\u03bbimg,2. We use batch size N= 40 and set the\nconfidence threshold for the distributional alignment loss C= 0.7. We train for 14k iterations using\nAdamW optimizer with learning rate 5e-5. We checkpoint every 200 iterations and report the best\ncheckpoint. The finetuning takes around 56 hours on 8 NVIDIA A100 GPUs.\nDebias G.R. & Align AgeDebias G.R.Original SDAdministratorAssociateDean\nBlasterIndustrialMachineryMechanic\nFigure A.21: Examples showing the effect of aligning age to 75% young and 25% old besides jointly\ndebiasing gender and race. In this figure, the color-coded bar denotes age: red is yound and blue is\nold. We do not annotate gender and race for visual clarity. Images are generated using the prompt\n\u201ca photo of the face of a {occupation }, a person \u201d. The occupation is shown\nat the left. For every occupation, images with the same number are generated using the same noise.\nA.12 D EBIASING MULTIPLE CONCEPTS AT ONCE\nBelow we list the prompts used for training and testing.\n(1) Occupational prompts: formulated with the template \u201ca photo of the face of a {occupation },\na person\u201d. We utilize the same 1000/50 occupations as in Section 5.1 for training/testing. The\noccupations are listed in Sec. A.2.\n(2) Sports prompts: formulated with the template \u201ca person playing {sport}\u201d. We\nuse 250/50 sports activities for training/testing. Training sports include: [\u2019ulama\u2019,\n\u2019casting (fishing)\u2019, \u2019futsal\u2019, \u2019freestyle slalom skating\u2019,\n\u2019figure skating\u2019, \u2019dinghy sailing\u2019, \u2019skipping rope\u2019, \u2019kickboxing\u2019,\n\u2019cross-country equestrianism\u2019, \u2019limited overs cricket\u2019, \u2019eskrima\u2019,\n\u2019equestrian vaulting\u2019, \u2019creeking\u2019, \u2019sledding\u2019, \u2019capoeira\u2019,\n\u2019enduro\u2019, \u2019ringette\u2019, \u2019bodyboarding\u2019, \u2019sumo\u2019, \u2019valencian pilota\u2019,\n\u2019hunting\u2019, \u2019jetsprint\u2019, \u2019fives\u2019, \u2019laser tag\u2019, \u00b7\u00b7\u00b7]. Test sports\nare [\u2019pommel horse\u2019, \u2019riverboarding\u2019, \u2019hurdles\u2019, \u2019underwater\n35", "start_char_idx": 0, "end_char_idx": 2325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47cb69ab-2357-4a7c-960b-1520f23edb67": {"__data__": {"id_": "47cb69ab-2357-4a7c-960b-1520f23edb67", "embedding": null, "metadata": {"page_label": "36", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1431b9b7-8bd3-48b5-9831-67496411bc5b", "node_type": "4", "metadata": {"page_label": "36", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fbba88a92beeb77bf833ba2ea0acc2ec708d1b35caf22a99137039ae8d68fff1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b551233f-62c0-4a48-a446-89b57e3c8512", "node_type": "1", "metadata": {}, "hash": "44c0148a61569043525301a6cbf63a0dd3e22ff332c5dc37608ad52b49f4f61a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nhockey\u2019, \u2019broomball\u2019, \u2019running\u2019, \u2019vovinam\u2019, \u2019rock fishing\u2019,\n\u2019barrel racing\u2019, \u2019cross-country cycling\u2019, \u2019silat\u2019, \u2019canoeing\u2019,\n\u2019cowboy action shooting\u2019, \u2019telemark skiing\u2019, \u2019adventure racing\u2019,\n\u2019olympic weightlifting\u2019, \u2019wiffle ball\u2019, \u2019toboggan\u2019, \u2019rhythmic\ngymnastics\u2019, \u2019english pleasure\u2019, \u2019northern praying mantis (martial\nart)\u2019, \u2019aggressive inline skating\u2019, \u2019arena football\u2019, \u2019australian\nrules football\u2019, \u2019beach tennis\u2019, \u2019haidong gumdo\u2019, \u2019trial\u2019,\n\u2019bandy\u2019, \u2019ball (rhythmic gymnastics)\u2019, \u2019bujinkan\u2019, \u2019freestyle\nfootball\u2019, \u2019gaelic football\u2019, \u2019horseball\u2019, \u2019okinawan kobud \u00afo\u2019,\n\u2019slamball\u2019, \u2019pankration\u2019, \u2019fox hunting\u2019, \u2019street football\u2019,\n\u2019juggling club\u2019, \u2019land sailing\u2019, \u2019ultimate (sport)\u2019, \u2019skibobbing\u2019,\n\u2019test cricket\u2019, \u2019bikejoring\u2019, \u2019tang soo do\u2019, \u2019sambo (martial\nart)\u2019, \u2019wing chun\u2019, \u2019synchronized swimming\u2019, \u2019rink bandy\u2019,\n\u2019beach handball\u2019, \u2019cyclo-cross\u2019, \u2019harness racing\u2019, \u2019jujutsu\u2019,\n\u2019slacklining\u2019, \u2019polo\u2019, \u2019rugby\u2019, \u2019association football\u2019, \u2019medley\nswimming\u2019, \u2019big-game fishing\u2019, \u2019demolition derby\u2019, \u2019rope (rhythmic\ngymnastics)\u2019, \u2019taekwondo\u2019, \u2019team handball\u2019, \u2019cross-country\nskiing\u2019, \u2019rundown\u2019, \u2019schutzhund\u2019, \u2019canoe polo\u2019, \u2019archery\u2019, \u2019squash\n(sport)\u2019, \u2019snooker\u2019, \u2019wing tsun\u2019, \u2019jai alai\u2019, \u2019streetball\u2019,\n\u2019sea kayak\u2019, \u2019muay thai\u2019, \u2019lure coursing\u2019, \u2019calisthenics\u2019, \u2019krav\nmaga\u2019, \u2019wheelchair basketball\u2019, \u2019trampolining\u2019, \u2019indoor american\nfootball\u2019, \u2019speed skating\u2019, \u2019amateur wrestling\u2019, \u2019rugby sevens\u2019,\n\u2019frontenis\u2019] .\n(3) Occupational prompts with style or context: these are non-templated prompts that specify\noccupations with diverse styles or contexts. We train/test on 150/19 such prompts obtained\nfrom the captions in the LAION-AESTHETICS dataset. The training prompts include: [\"a\nepic hero adventurer holding a torch in a dark cave, artgerm,\nrealistic, cryengine, symmetric\", \"salvador dali the painter\nbecame super saiyan, dragon ball style, cinematic lighting,\nhighly detailed, concept art, art by wlop and artgerm and greg\nrutkowski, masterpiece, trending on artstation, 8 k\", \"concept\nart of scientist with scifi accessories by jama jurabaev, brush\nstroke,, trending on artstation, upper half portrait, symmetry,\nhigh quality, extremely detailed\", \"detective falling through\nthe sky, city, by peter mohrbacher, artgerm, karol bak, loish,\nayami kojima, james stokoe, highly detailed, ultra detailed, ultra\nrealistic, trending on artstation\", \"concept art of agent 4 7,\nvector art, by cristiano siqueira, brush hard, highly detailed,\nartstation, high quality\", \"nightbringer yasuo slashing, ultra\ndetails background trending on artstation digital painting\nsplashart drawn by a professional artist\", \"portrait of a middle\n- aged writer with a beard, he is smoking a cigarette, style\nof greg rutkowski\", \"cute star trek officer lady gaga, natural\nlighting, path traced, \u00b7\u00b7\u00b7].", "start_char_idx": 0, "end_char_idx": 2813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b551233f-62c0-4a48-a446-89b57e3c8512": {"__data__": {"id_": "b551233f-62c0-4a48-a446-89b57e3c8512", "embedding": null, "metadata": {"page_label": "36", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1431b9b7-8bd3-48b5-9831-67496411bc5b", "node_type": "4", "metadata": {"page_label": "36", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fbba88a92beeb77bf833ba2ea0acc2ec708d1b35caf22a99137039ae8d68fff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47cb69ab-2357-4a7c-960b-1520f23edb67", "node_type": "1", "metadata": {"page_label": "36", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "09e20099e36dca48682ee9db5e5fdb8162cf6e75a81eb59947de91b784914fe9", "class_name": "RelatedNodeInfo"}}, "text": "The test prompts are: [\"concept art of elite\nscientist by jama jurabaev, emperor secret society, cinematic\nshot, trending on artstation, high quality, brush stroke\",\n\"cyborg scientist by jama jurabaev, cinematic shot, extremely\ndetailed, trending on artstation, high quality, brush stroke\", \"a\nhaggard detective in a trenchcoat scanning a crimescene, sketchy\nartstyle, digital art, dramatic, thick lines, rough lines, line\nart, cinematic, trending on artstation\", \"computer scientist\nwho served as an intel systems engineer, full-body shot, digital\npainting, smooth, elegant, hd, art by WLOP and Artgerm and Greg\nRutkowski and Alphonse Mucha\", \"a painting so beautiful and\nuniversally loved it creates peace on earth, profound epiphany,\ntrending on artstation, by john singer sargent\", \"a portrait of\nfish magician in glass armor releasing spell, full height, moving\nforward, concept art, trending on artstation, highly detailed,\n36", "start_char_idx": 2814, "end_char_idx": 3746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5815851b-147e-4713-8847-1e408c7dcaa0": {"__data__": {"id_": "5815851b-147e-4713-8847-1e408c7dcaa0", "embedding": null, "metadata": {"page_label": "37", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "914b30a5-3038-470e-9154-5085e03e47da", "node_type": "4", "metadata": {"page_label": "37", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8b58f84cc98f41b204ec2a52fff5fa20d9adabdef719f45c058f199293aca36d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nintricate, sharp focus, digital art, 8 k\", \"blonde sailor moon\nas aeon flux, by Stanley Artgerm Lau, greg rutkowski, Craig\nmullins, Peter chung, thomas kindkade, alphonse mucha, loish,\",\n\"a aesthetic portrait of a magician working on ancient machines\nto do magic, concept art\", \"portrait old barbarian warrior with\ntrucker mustache and short hair, 8 k, trending on art station,\nby tooth wu and greg rutkowski\", \"High fantasy detective with\nwhips with crab companion, RPG Scene, Oil Painting, octane render,\nTrending on Artstation, Insanely Detailed, 8k, UHD\", \"selfie\nof a space soldier by louis daguerre, cinematic, high quality,\ncgsociety, artgerm, 4 k, uhd, 5 0 mm, trending on artstation\",\n\"a beautiful model in crop top, by guweiz and wlop and ilya\nkuvshinov and artgerm, symmetrical eyes, aesthetic, gorgeous,\nstunning, alluring, attractive, artstation, deviantart, pinterest,\ndigital art\", \"a mad scientist mutating into a monster because of\nspilled chemicals in the laboratory, wlop, trending on artstation,\ndeviantart, anime key visual, official media, professional art,\n8 k uhd\", \"portrait of a mutant wrestler with posing in front of\nmuscle truck, with a spray painted makrel on it, dystopic, dust,\nintricate, highly detailed, concept art, Octane render\", \"portrait\nof a vicotrian doctor in suit with helmet by darek zabrocki and\ngreg ruthkowski, alphonse mucha, simon stalenhag and cinematic and\natmospheric, concept art, artstation, trending on artstation\",\n\"concept art of portrait ofcyborg scientist by jama jurabaev,\nextremely detailed, trending on artstation, high quality, brush\nstroke\", \"a beautiful masterpiece painting of a clothed artist by\njuan gimenez, award winning, trending on artstation,\", \"comic book\nboss fight, highly detailed, professional digital painting, Unreal\nEngine 5, Photorealism, HD quality, 8k resolution, cinema 4d, 3D,\ncinematic, art by artgerm and greg rutkowski\", \"magician shuffling\ncards, cards, fantasy, digital art, soft lighting, concept art, 8\nk\"].\n(4) personal descriptors: these prompts describe individual(s). We use 40/10 such prompts for\ntraining/testing. The training prompts are: [\"Business person looking at wall with\nlight tunnel opening\", \"person sitting on rock on body of water\",\n\"person standing on rocky cliff\", \"Oil painting of a person on\na horse\", \"Cleaning service person avatar cartoon character\", \"A\nperson sits against a wall in Wuhan, China.\", \"person riding on\nteal dutch bicycle\", \"Most interesting person\", \"person standing\nbeside another person holding fire poi\", \"Youngest person reach\nSouth Pole\", \"A mural of a person holding a camera.\", \"painting\nof two dancing persons\", \"elderly personal care maryland\", \"person\ndoing fire dancing\", \"three persons standing near the edge of a\ncliff during day\", \"person throwing fish net while standing on\nboat\", \"person in black and white shirt lying on yellow bed\",\n\"A person playing music.\", \"person dances in door way with a\nview of the taj mahal\", \"person drinking out of a lake with\nlifestraw\", \"painting of a person standing outside a cottage\",\n\"person standing on mountaintop arms spread\", \"a person standing\nin front of a large city landscape\", \"person standing in front\nof waterfall during daytime\", \"person lying on red hammock\",\n\"Front view of person on railroad track between valley\", \"A\nperson flying through the air on a rock\", \"A person cycling to\nwork through the city\", \"person with colorful balloons\", \"person\ndecorating a wedding cake\", \"person standing in front of Torii\nGate\", \"person wearing the headphones on the street\", \"person\nsitting on a rock\", \"Colourful stage set with person in costume\",\n\"person standing beside trees during winter season\", \"person on\n37", "start_char_idx": 0, "end_char_idx": 3748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b16e11da-f73b-4e92-9a80-2fca1e986d78": {"__data__": {"id_": "b16e11da-f73b-4e92-9a80-2fca1e986d78", "embedding": null, "metadata": {"page_label": "38", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac9d831f-0f21-4599-998d-b88c04e19f60", "node_type": "4", "metadata": {"page_label": "38", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "eb33d0c59d08fde2243e5991746c1530525d960929c90310bd1fa8786a1d49df", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\na mountain top\", \"An image of an elderly person painting\", \"Day\n6: An old person\", \"iluminated tent with a person sitting out\nfront\", \"person sitting alone at a street stall eating soup\"] .\nThe test prompts are: [\"bird\u2019s eyeview photo of person lying on green\ngrass\", \"A person holding a picture in front of a desert.\", \"A\npainting of a person in a garage.\", \"steel wool photography of\nperson in room\", \"individual photo shoot in Prague\", \"Oil painting\nof a person wearing colorful fabric\", \"person standing in front\nof cave\", \"person in cold weather in a tent\", \"A person sitting\non dry barren dirt.\", \"a person standing next to a vase of flowers\non a table\", \"hot personal trainer\", \"a person lying on a dog\",\n\"Image may contain: person, flower and sunflower\", \"person in\nwater throwing guitar\", \"person standing at a forge holding a\nsledge hammer\", \"image of a homeless person sitting on the side\nof a building\", \"H&M spokesperson: \u2019Our models are too thin\u2019\",\n\"Biohazard cleaning persons\", \"A close up of a person wearing a\nhat\", \"photo of person covered by red headscarf\"] .\nA.13 G ENERATED IMAGES FROM DEBIASING MULTIPLE CONCEPTS\nIn Figure A.22, we first compare the generated images for occupational prompts by the original\nSD, the SD debiased for single concept, and the SD debiased for multiple concepts. We find that\nmulti-concept debiasing seems to increase the likelihood of generating images that blend male and\nfemale characteristics. Notable examples include: Salesperson images No. 6, 12, and 15, which\nappear male but have hairstyles typically associated with females; violinist images No. 1, 8, and\n2, perceived as male but dressed in clothing typically associated with females; and community and\nsocial service specialist image No. 10, which is perceptually male but with pink lipstick, a feature\ncommonly associated with females. We did not observe other significant impacts on image quality\nresulting from the debiasing of multiple concepts.\nWe show generated images for occupational prompts in Figure A.23, for sports prompts in Fig-\nure A.24, for occupational prompts with style or context in Figure A.25, and for personal descriptors\nin Figure A.26.\n38", "start_char_idx": 0, "end_char_idx": 2220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebaa0d0a-17c3-41e2-ba9b-4be7a1dd5ef7": {"__data__": {"id_": "ebaa0d0a-17c3-41e2-ba9b-4be7a1dd5ef7", "embedding": null, "metadata": {"page_label": "39", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbb97f66-b566-4573-afc2-3bce0fa780e8", "node_type": "4", "metadata": {"page_label": "39", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ec778be36f9741463e2ec110091a12339b0c5b2b313badfcd0e70f4d35691edd", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nOriginal SDOurs (Debias Gender  Race, multiple concepts)AssociateDean\nOurs (Debias Gender  Race, single concept)\nCommunityand SocialServiceSpecialist\nIndustrialMachineryMechanic\nViolinist\nSalesperson\nFigure A.22: Comparison of generated images for occupational prompts between original SD, the\nSD debiased for single concept, and the SD debiased for multiple concepts. Images are generated\nusing the prompt template \u201c a photo of the face of a {occupation }, a person \u201d.\nThe occupation is shown at the left. For every occupation, images with the same number are gener-\nated using the same noise.\n39", "start_char_idx": 0, "end_char_idx": 642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc5698c3-478c-42d7-93fc-4c756ba3126e": {"__data__": {"id_": "bc5698c3-478c-42d7-93fc-4c756ba3126e", "embedding": null, "metadata": {"page_label": "40", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b78b1b71-1564-4d20-aeb1-de79e1028204", "node_type": "4", "metadata": {"page_label": "40", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "71a7fe8bb09f9294fe5d72e9e943e5c723ab4eb48259e82bdde8a800b14a2223", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nOriginal SDOurs (Debias Gender  Race, multiple concepts)AssociateDean\nCommunityand SocialServiceSpecialist\nIndustrialMachineryMechanic\nViolinist\nSalesperson\nFigure A.23: Comparison of images generated for occupational prompts by original SD and the SD\ndebiased for multiple concepts. Images are generated using the prompt template \u201c a photo of\nthe face of a {occupation }, a person \u201d. The occupation is shown at the left. For\nevery prompt, images with the same number are generated using the same noise.\n40", "start_char_idx": 0, "end_char_idx": 551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5de5f5e8-d376-4b8d-b85c-c45b81ebff82": {"__data__": {"id_": "5de5f5e8-d376-4b8d-b85c-c45b81ebff82", "embedding": null, "metadata": {"page_label": "41", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2481ee02-d949-4574-9175-9f9a58fecf88", "node_type": "4", "metadata": {"page_label": "41", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5d6bf31779080515db79d8177a62cfa00b166185248109f5f69e522b18ec344a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nOriginal SDOurs (Debias Gender  Race, multiple concepts)rock fishingslamballtelemark skiingsambo (martialart)olympicweightlifting\nFigure A.24: Comparison of images generated for sports prompts by original SD and the SD\ndebiased for multiple concepts. Images are generated using the prompt template \u201c a person\nplaying {sport }\u201d. The sport is shown at the left. For every prompt, images with the same\nnumber are generated using the same noise.\n41", "start_char_idx": 0, "end_char_idx": 489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "210a3a30-78af-428f-b853-f849a6b31f7d": {"__data__": {"id_": "210a3a30-78af-428f-b853-f849a6b31f7d", "embedding": null, "metadata": {"page_label": "42", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7735730c-373c-4966-b578-a1aba8ffb625", "node_type": "4", "metadata": {"page_label": "42", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8815575c475f7697d933b4462ad6b3351029c2511abca4afc3556217cfa957f4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nOriginal SDOurs (Debias Gender  Race, multiple concepts)a aestheticportrait of amagicianworking onancient machinesto do magic,concept art\nconcept art ofelite scientist byjama jurabaev,emperor secretsociety,cinematic shot,trending onartstation, highquality, brushstroke\na painting sobeautiful anduniversally lovedit creates peaceon earth,profoundepiphany,trending onartstation, byjohn singersargent\nselfie of a spacesoldier by louisdaguerre,cinematic, highquality,cgsociety,artgerm, 4 k, uhd,5 0 mm, trendingon artstationHigh fantasydetective withwhips with crabcompanion, RPGScene, OilPainting, octanerender, Trendingon Artstation,InsanelyDetailed, 8k, UHD\nFigure A.25: Comparison of images generated for occupational prompts with style or context by\noriginal SD and the SD debiased for multiple concepts. The prompts are shown at the left. For every\nprompt, images with the same number are generated using the same noise.\n42", "start_char_idx": 0, "end_char_idx": 970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb26cbc3-ecc8-4d1d-a2f4-cea75a067825": {"__data__": {"id_": "eb26cbc3-ecc8-4d1d-a2f4-cea75a067825", "embedding": null, "metadata": {"page_label": "43", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bd10107-94d6-49b0-8764-6d1666bd7262", "node_type": "4", "metadata": {"page_label": "43", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "48134be1933aa1e88f310e20c3752d34ddd5f82e17e9ce500fb95a911b6d1afb", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nOriginal SDOurs (Debias Gender  Race, multiple concepts)person in coldweather in a tentImage maycontain/ person,flower andsunflowerhot personaltrainer.H&Mspokesperson/'Our models aretoo thin'.a personstanding next toa vase of flowerson a table\nFigure A.26: Comparison of images generated for personal descriptors by original SD and the SD\ndebiased for multiple concepts. The prompts are shown at the left. For every prompt, images with\nthe same number are generated using the same noise.\n43", "start_char_idx": 0, "end_char_idx": 535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb31a447-ac53-4320-bc30-6985062b6212": {"__data__": {"id_": "fb31a447-ac53-4320-bc30-6985062b6212", "embedding": null, "metadata": {"page_label": "1", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "984147b1-2e2e-4b79-9307-d44f5882b081", "node_type": "4", "metadata": {"page_label": "1", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "17eaa537555f7db2ad4b7c68a55ffad7383c00fd5c854e4caaa03ba227ccb403", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nKNOWLEDGE CARD: FILLING LLM S\u2019 KNOWLEDGE GAPS\nWITH PLUG-INSPECIALIZED LANGUAGE MODELS\nShangbin Feng1Weijia Shi1Yuyang Bai2\nVidhisha Balachandran3Tianxing He1Yulia Tsvetkov1\n1University of Washington2Xi\u2019an Jiaotong University3Carnegie Mellon University\nshangbin@cs.washington.edu\nABSTRACT\nBy design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end, we\npropose KNOWLEDGE CARD, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce knowledge\ncards \u2014specialized language models trained on corpora from specific domains\nand sources. Knowledge cards serve as parametric repositories that are selected\nat inference time to generate background knowledge for the base LLM. We then\npropose three content selectors to dynamically select and retain information in\ndocuments generated by knowledge cards, specifically controlling for relevance ,\nbrevity , and factuality of outputs. Finally, we propose two complementary integra-\ntion approaches to augment the base LLM with the (relevant, factual) knowledge\ncurated from the specialized LMs. Through extensive experiments, we demonstrate\nthatKNOWLEDGE CARD achieves state-of-the-art performance on six benchmark\ndatasets. Ultimately, KNOWLEDGE CARD framework enables dynamic synthesis\nand updates of knowledge from diverse domains. Its modularity will ensure that\nrelevant knowledge can be continuously updated through the collective efforts of\nthe research community.1\n1 I NTRODUCTION\nLarge language models (LLMs) have demonstrated an impressive ability to encode world knowledge\nin model parameters (Petroni et al., 2019; Roberts et al., 2020). However, they still face various\nchallenges in knowledge-intensive tasks and contexts: they suffer from hallucination (Kry \u00b4sci\u00b4nski et al.,\n2020; Pagnoni et al., 2021; Ji et al., 2023), struggle to encode long-tail facts (Kandpal et al., 2023;\nMallen et al., 2023), and could not be easily updated with new and emerging knowledge (De Cao\net al., 2021; Hase et al., 2021). Existing works propose addressing these limitations through retrieval\naugmentation or generated knowledge prompting. Retrieval-augmented LMs (Guu et al., 2020;\nBorgeaud et al., 2022; Shi et al., 2023) employ retrieval systems to fetch relevant documents from a\ngeneral and fixed retrieval corpus (e.g., Wikipedia or the Pile (Gao et al., 2020)), leveraging external\nknowledge from non-parametric sources to aid LLM generation. Generated knowledge prompting\napproaches (Shin et al., 2020; Liu et al., 2022a; Sun et al., 2022) prompt LLMs to incorporate and\ngenerate contextual documents to encourage knowledge-aware generation.\nWhile the two lines of work have achieved some success, these existing systems struggle to reflect\ntwo key properties of knowledge. Knowledge is modular (Stuckenschmidt et al., 2009): it is an\n\u201carchipelago\u201d rather than a single \u201ccontinent\u201d, encapsulating information that exists in diversified\nforms, domains, sources, perspectives, and more. The lack of knowledge modularity has made gener-\nalization to new domains and targeted updates of knowledge stored in LMs difficult. Knowledge is\ncollaborative (Cayzer, 2004): LLMs should be able to represent and incorporate diverse and evolving\nknowledge, from multi-faceted sources and perspectives, while enabling collaborative contribution\nfrom various stakeholders. Community-driven knowledge could aggregate new knowledge from\ndomain experts and enable the development of specialized LLMs, tailored to specific industries or\napplications. That being said, existing approaches and systems did not employ modular orcollab-\norative knowledge sources that enable the plug-and-play updates and contributions from various\nstakeholders. While approaches such as retrieval augmentation could be extended for modularity,\n1Resources are available at https://github.com/BunsenFeng/Knowledge Card.\n1", "start_char_idx": 0, "end_char_idx": 4178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60b2b8bf-e55c-4700-9d71-b84900a372e7": {"__data__": {"id_": "60b2b8bf-e55c-4700-9d71-b84900a372e7", "embedding": null, "metadata": {"page_label": "2", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4df5ac04-29ee-4501-83b6-fb56e12e3315", "node_type": "4", "metadata": {"page_label": "2", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a4b71426dea1072024b0f80527e342463351a5f0b6ddbd01b588d5daf616234e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "656ad568-d8bc-4fa3-9167-16f2afee1e53", "node_type": "1", "metadata": {}, "hash": "9ebf833fd6a6ccb90ce937d853340f599578a2378a17082e4eb3cb3b9423325e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nthey are hardly compatible with the current landscape of model sharing (Wolf et al., 2019) and do not\nfacilitate community-driven efforts to fill in LLMs\u2019 knowledge gaps.\nTo this end, we propose KNOWLEDGE CARD, a novel framework to empower general-purpose\nLLMs with modular and collaboratively-sourced knowledge through the integration of smaller, but\nspecialized language models. As an increasing amount of powerful LLMs are released behind API\ncalls, not directly accessible, and are prohibitively expensive to train or adapt, KNOWLEDGE CARD\nspecifically focuses on augmenting black-box LLMs to enrich their knowledge capabilities. We first\ncurate specialized LMs, knowledge cards , trained on corpora from diverse sources and domains to\nserve as modular knowledge repositories ( \u00a72.1). Compared to existing approaches, knowledge cards\nenable flexible and targeted information access, searching over domains, and employing private and\npersonalized knowledge sources. These specialized LMs are later prompted to generate background\ninformation to support general-purpose LLMs. We then propose three levels of knowledge selectors to\ndynamically select and refine generated documents and control for topic relevance, document brevity,\nand knowledge factuality ( \u00a72.2). Finally, we propose bottom-up andtop-down \u2014two approaches\nto empower general-purpose LLMs by integrating outputs from specialized LMs (i.e.,plugging in\nknowledge cards into the LLM) ( \u00a72.3). Specifically, the bottom-up approach starts by prompting all\nknowledge cards to generate multiple documents, then performs selection with the three knowledge\nselectors, while concatenating the final knowledge paragraph with the query for LLM generation.\nWhile the bottom-up approach uniquely enables multi-domain knowledge synthesis, it also presents\nthe risk of presenting irrelevant information to LLM in contexts where external information is not\nneeded. This motivates us to propose the top-down approach, where the general-purpose LLM itself\ndecides whether external knowledge is necessary for the given query, then relevant knowledge cards\nare selectively activated for knowledge integration; this process is repeated until the general-purpose\nLLM has enough confidence to generate a response.\nExtensive experiments demonstrate that KNOWLEDGE CARD outperforms vanilla LLMs, retrieval-\naugmented LMs, and generated prompting approaches on three tasks across six datasets. For\ngeneral-purpose knowledge QA ,KNOWLEDGE CARD improves Codex performance by 6.6% on\nMMLU and even outperforms the 3-times larger Flan-PaLM. For misinformation analysis that tests\nmulti-domain knowledge integration, KNOWLEDGE CARD outperforms all baseline approaches by\nat least 15.8% and 10.0% balanced accuracy scores on two- and four-way classification settings. In\nthe third task, to evaluate the ability to update the knowledge of general-purpose LLMs, we curate\nMIDTERM QA, a QA dataset focusing on the 2022 U.S. midterm elections while the knowledge\ncutoff of LLMs is generally 2021 or earlier. Experiments demonstrate that KNOWLEDGE CARD\noutperforms all baselines by at least 55.6% on exact match scores, showcasing the ability for temporal\nknowledge update while only adding one knowledge card trained on midterm election news with 100x\nfewer parameters than the general-purpose LLM. Our findings demonstrate the potential of filling in\nthe knowledge gaps of general-purpose LLMs by integrating modular and collaborative knowledge\nfrom small, independently trained, and specialized LMs. We envision KNOWLEDGE CARD as an\ninitiative to encourage LM developers to collaborate in expanding the knowledge of large language\nmodels while reducing the carbon footprint from retraining gigantic LMs from scratch.\n2 M ETHODOLOGY\nWe introduce KNOWLEDGE CARD, a novel framework to empower general-purpose LLMs with\nmodular and collaborative knowledge (Figure 1). We train various knowledge cards , LMs trained on\nspecialized knowledge corpora from diversified domains and sources ( \u00a72.1). We then use them to\nproduce background knowledge for the general-purpose LLMs, while employing three knowledge\nselectors to ensure quality in knowledge synthesis ( \u00a72.2). Finally, we propose bottom-up andtop-\ndown , two approaches to condition the LLM on the content sourced from knowledge cards and\npost-processed using the knowledge selectors (\u00a72.3).", "start_char_idx": 0, "end_char_idx": 4421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "656ad568-d8bc-4fa3-9167-16f2afee1e53": {"__data__": {"id_": "656ad568-d8bc-4fa3-9167-16f2afee1e53", "embedding": null, "metadata": {"page_label": "2", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4df5ac04-29ee-4501-83b6-fb56e12e3315", "node_type": "4", "metadata": {"page_label": "2", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a4b71426dea1072024b0f80527e342463351a5f0b6ddbd01b588d5daf616234e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60b2b8bf-e55c-4700-9d71-b84900a372e7", "node_type": "1", "metadata": {"page_label": "2", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e8ef10fbe8636c3ca875876130e5f36deb6b8b8b21b32a1f54430d05ef38f47c", "class_name": "RelatedNodeInfo"}}, "text": "We envision KNOWLEDGE CARD as an\ninitiative to encourage LM developers to collaborate in expanding the knowledge of large language\nmodels while reducing the carbon footprint from retraining gigantic LMs from scratch.\n2 M ETHODOLOGY\nWe introduce KNOWLEDGE CARD, a novel framework to empower general-purpose LLMs with\nmodular and collaborative knowledge (Figure 1). We train various knowledge cards , LMs trained on\nspecialized knowledge corpora from diversified domains and sources ( \u00a72.1). We then use them to\nproduce background knowledge for the general-purpose LLMs, while employing three knowledge\nselectors to ensure quality in knowledge synthesis ( \u00a72.2). Finally, we propose bottom-up andtop-\ndown , two approaches to condition the LLM on the content sourced from knowledge cards and\npost-processed using the knowledge selectors (\u00a72.3).\n2.1 K NOWLEDGE CARDS\nWhile existing approaches rely on one fixed source of knowledge to improve LLMs (one retrieval\ncorpus (Guu et al., 2020; Borgeaud et al., 2022; Shi et al., 2023), one knowledge graph (Wang et al.,\n2021; Zhang et al., 2021; Feng et al., 2023c), or one pretrained LLM itself (Shin et al., 2020; Liu\net al., 2022a; Sun et al., 2022)), we hypothesize that since knowledge is modular, general-purpose\nLLMs should be augmented with modular plug-and-play knowledge repositories that allow users to\ncollaboratively add, remove, edit, or update information. In addition, different communities might\n2", "start_char_idx": 3579, "end_char_idx": 5034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42412901-bf61-49f2-8a6e-b65c7bb6fa8d": {"__data__": {"id_": "42412901-bf61-49f2-8a6e-b65c7bb6fa8d", "embedding": null, "metadata": {"page_label": "3", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd407ef0-0a81-49f4-adb6-70ff92f93364", "node_type": "4", "metadata": {"page_label": "3", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cbffbc589d85b93aab14f0a9cf94bbd66157239b23dd408023871ba3beae4bb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58cbd730-fac9-421b-9354-0c1d4a6d051c", "node_type": "1", "metadata": {}, "hash": "6740936866b9c20aff2c2d770f54f1c8644a1ecf066640a5a4d8a8960611ba51", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nRelevance  SelectorPruning  SelectorFactuality SelectorAnswer:\nRelevance SelectorDo you need more information? \nYes or No:Q: Who is the senior senator of \nTom Brady \u2019s birthplace ?\nsports\nFactuality Selector\nKnowledge : Tom Brady returned to his hometown of San Mateo, CA . . .Yes No\nauto selection exp selection\ngeneral -purpose LLM\nBottom -Up Bottom -Up Top-Down Top-DownChoose an information \nsource from the \nfollowing:               ,   \n                                       ,\n                       ,\n                         .         sports\nbiomedical literature\nNLP papers\nbook corpusChoose an information \nsource from the \nfollowing:               ,   \n                                       ,\n                       ,\n                         .         sports\nbiomedical literature\nNLP papers\nbook corpus\nsportsWhat kind of \ninformation do you \nneed?\nPlease provide the \nstate Tom Brady is \nfrom.What kind of \ninformation do you \nneed?\nPlease provide the \nstate Tom Brady is \nfrom.What kind of \ninformation do you \nneed?\nPlease provide the \nstate Tom Brady is \nfrom.\nAnswer : Answer : Knowledge: Tom \nBrady returned to \nhis hometown of \nSan Mateo, CA . . .\nQuestion : Who is \nthe senior senator \nof Tom Brady \u2019s \nbirthplace ? N\nB\nGA\nP\nCH\nS\n...N\nB\nGA\nP\nCH\nS\n...\nDianne Feinstein\nDianne Feinstein\nknowledge cards\n San Mateo is \nlocated in the \nnorthwest of \nCalifornia ...Dianne Feinstein, \nthe senior senator \nfrom California, is \nrumored to retire ... Tom Brady returned \nto his hometown of \nSan Mateo ...general -purpose LLM\nQ: Who is the senior senator of Tom Brady \u2019s birthplace ? San Mateo is located in the northwest of California ...\nDianne Feinstein, the senior senator from California ... Tom BradyKnowledge:\nreturned to his hometown of San Mateo ...\nQuestion : Who is the senior senator of Tom Brady \u2019s birthplace ? \nAnswer :  San Mateo is located in the northwest of California ...\nDianne Feinstein, the senior senator from California ... Tom BradyKnowledge:\nreturned to his hometown of San Mateo ...\nQuestion : Who is the senior senator of Tom Brady \u2019s birthplace ? \nAnswer : Dianne Feinstein\nDianne Feinstein\nknowledge documents\nnews biomedgeogra\nphyNLP \npaperspoliticsConcept\nNetart \nhistorysports ... news biomedgeogra\nphyNLP \npaperspoliticsConcept\nNetart \nhistorysports ...knowledge cardsbegin\nknowledge documentsknowledge documents\nFigure 1: Overview of KNOWLEDGE CARD. We train knowledge cards on various knowledge\ndomains and employ three knowledge selectors for quality control. We propose bottom-up and\ntop-down to integrate general-purpose LLMs with modular and specialized LMs for multi-domain\nknowledge synthesis ( bottom-up ) and proactively seeking external knowledge ( top-down ).\nhave different definitions and requirements for knowledge. Wikipedia factoids, biomedical literature,\nmathematical formulae, and commonsense knowledge graphs are all valuable knowledge components\nin various contexts, thus LLMs should be able to represent and incorporate knowledge contributed by\nstakeholders across multi-faceted domains and industries.\nTo this end, we propose to curate knowledge cards , specialized LMs that are much smaller than\nblack-box LLMs, trained on diversified knowledge corpora from a wide range of domains and sources.\nConcretely, we obtain nknowledge cards C={c1,c2,\u00b7\u00b7\u00b7,cn}, each starting from an existing LM\ncheckpoint and further trained on a specific knowledge corpora Diwith the causal language modeling\nobjective. Given a query to the LLM, these knowledge cards are selectively activated and used with\nprompted generation. Formally, given query q, specialized LM cdefines a mapping c(q) :q\u2192dq\nwhere qis used as prompt to generate a continuation as the knowledge document dq, which are later\nprepended into the context of general-purpose LLMs through various mechanisms (\u00a72.3).\nIn this way, the modularity of knowledge is demonstrated through the effortless addition, removal, or\nselective activation of various knowledge cards during the LLM generation process. Similarly, the\ncollaborative nature of knowledge is reflected by enabling individuals to contribute trained knowledge\ncards on their desired knowledge source to KNOWLEDGE CARD, expanding the knowledge of\ngeneral-purpose LLMs through community-driven efforts.", "start_char_idx": 0, "end_char_idx": 4320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58cbd730-fac9-421b-9354-0c1d4a6d051c": {"__data__": {"id_": "58cbd730-fac9-421b-9354-0c1d4a6d051c", "embedding": null, "metadata": {"page_label": "3", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd407ef0-0a81-49f4-adb6-70ff92f93364", "node_type": "4", "metadata": {"page_label": "3", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cbffbc589d85b93aab14f0a9cf94bbd66157239b23dd408023871ba3beae4bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42412901-bf61-49f2-8a6e-b65c7bb6fa8d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "41f6103843a195a4c598fdab045f7e973f62608fba31d9ad25a9f6bd1aa819ae", "class_name": "RelatedNodeInfo"}}, "text": "Given a query to the LLM, these knowledge cards are selectively activated and used with\nprompted generation. Formally, given query q, specialized LM cdefines a mapping c(q) :q\u2192dq\nwhere qis used as prompt to generate a continuation as the knowledge document dq, which are later\nprepended into the context of general-purpose LLMs through various mechanisms (\u00a72.3).\nIn this way, the modularity of knowledge is demonstrated through the effortless addition, removal, or\nselective activation of various knowledge cards during the LLM generation process. Similarly, the\ncollaborative nature of knowledge is reflected by enabling individuals to contribute trained knowledge\ncards on their desired knowledge source to KNOWLEDGE CARD, expanding the knowledge of\ngeneral-purpose LLMs through community-driven efforts.\n2.2 K NOWLEDGE SELECTORS\nWhile it is possible to directly adopt dqas relevant knowledge, we identify three key challenges in\nthe successful integration of knowledge cards and general-purpose LLMs: relevance, brevity, and\nfactuality. We design three respective selectors to control for such factors.\nRelevance Selector While we expect knowledge cards to generate background information that is\nrelevant and helpful to the query q, LMs sometimes deviate from the query (Holtzman et al., 2019).\nFurthermore, only a handful of knowledge cards would be relevant for a given query. To this end, we\npropose to select and retain knowledge documents based on relevance. Concretely, given a set of m\ngenerated documents {d1,\u00b7\u00b7\u00b7,dm}and the query q, we aim to retain the top- krelevant documents\nand discard irrelevant information. We adopt a separate encoder-based LM enc(\u00b7)that maps a token\nsequence to a feature vector and cosine similarity sim(\u00b7,\u00b7)to measure relevance. Formally, we retain\ndiifi\u2208top-kj(sim(enc( dj),enc(q)))where top-k is the top- kargmax operation.\nPruning Selector Existing works mostly integrate one piece of external knowledge into LLMs (Sun\net al., 2022; Shi et al., 2023), while tasks requiring integration of multiple domains of information,\n3", "start_char_idx": 3514, "end_char_idx": 5580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "194f391d-7bf1-4e44-be08-b20ff682ee8e": {"__data__": {"id_": "194f391d-7bf1-4e44-be08-b20ff682ee8e", "embedding": null, "metadata": {"page_label": "4", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e237025-f9ab-488f-9524-e65599a2abf3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3c21c37aa81254d9601be9ed4d98eaac17c79ace52472a7a377c29f43fc2b461", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48253f1f-a0e0-4bba-8b69-c512d0eafa35", "node_type": "1", "metadata": {}, "hash": "19042d8572ff4540bb05805b147a1f9a244e056bfd1a52f4037932d7cd8181e4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nsuch as misinformation detection (Karimi et al., 2018) and multi-hop QA (Nishida et al., 2019), are not\nwell supported by existing paradigms. To effectively incorporate generated documents from multiple\nLMs while fitting into the LLM context length limit, we propose to prune knowledge documents.\nFormally, given mdocuments {d1,\u00b7\u00b7\u00b7,dm}, we adopt a pruning model prune( \u00b7), operationalized\nmost simply as a summarization system (Zhang et al., 2020; Liu et al., 2022b), to obtain the condensed\nversions separately {\u02dcd1,\u00b7\u00b7\u00b7,\u02dcdm}. This pruning method allows for the integration into the main\nLLM of information from multiple domains while preserving space for in-context learning.\nFactuality Selector Language models are prone to hallucination (Ji et al., 2023) and the knowledge\ncards are no exception. Given a set of mpruned knowledge documents {\u02dcd1,\u00b7\u00b7\u00b7,\u02dcdm}, their original\nversions {d1,\u00b7\u00b7\u00b7,dm}, and the query q, we filter out the non-factual knowledge and retain \u2113\ndocuments. Specifically, we evaluate the factuality of knowledge documents with two measures.\nWe first evaluate summarization factuality , ensuring that the pruned version \u02dcdifactually captures the\nimportant points in the original di. Concretely, we adopt factuality evaluation models (Kry \u00b4sci\u00b4nski\net al., 2020; Feng et al., 2023a) as a scoring function sum-fact( \u00b7,\u00b7), where each knowledge document\ndis assigned a summarization factuality score ssum\nd= sum-fact( \u02dcd|d)\u2208[0,1].\nWe then propose to evaluate whether the generated knowledge document is well-supported by\nreal-world knowledge through retrieval-augmented fact checking . Specifically, given a knowledge\ndocument d, we retrieve kdocuments from a retrieval corpus t1, . . . , tk, then employ a fact-checking\nmodel (Schuster et al., 2021) as a scoring function fact-check( \u00b7,\u00b7). We then assign a fact-checked\nfactuality score to each dbased on the retrieved document that most supports d, formally sfact\nd=\nmax 1\u2264i\u2264kfact-check( d|ti)\u2208[0,1]. We then average the summarization factuality score and the\nfact-checking score for each document to obtain sd.\nWhile it is straightforward to greedily select \u2113knowledge documents with the highest sdscores,\nnew and more recent knowledge might not be well-supported by existing fact-checking tools. As a\nresult, we propose top-kfactuality sampling to allow for flexibility while remaining stringent towards\nknowledge documents that are clearly wrong. Formally, we first obtain Dkas the set of knowledge\ndocuments with the top- kfactuality scores where k > \u2113 is a hyperparameter. We then define a\nsampling probability distribution over all mknowledge documents:\np(\u02dcdi|q) =(\nexp(sdi)/P\ndj\u2208Dkexp(sdj),if\u02dcdi\u2208 Dk.\n0, if\u02dcdi/\u2208 Dk.\nWe sample \u2113knowledge documents from {\u02dcd1,\u00b7\u00b7\u00b7,\u02dcdm}with probabilities {p(\u02dcd1|q),\u00b7\u00b7\u00b7, p(\u02dcdm|\nq)}. In this way, knowledge documents with very low factuality scores are strictly removed while\nflexibility is built in through sampling from the knowledge with factuality scores near the top.\n2.3 K NOWLEDGE INTEGRATION\nAfter defining the modular components in KNOWLEDGE CARD (a general-purpose LLM, knowledge\ncards, and knowledge selectors), we propose two approaches, bottom-up andtop-down , to integrate\nthe general-purpose LLM with external knowledge sources, which are selected outputs of knowledge\ncards. Specifically, bottom-up activates all available knowledge cards at once and employs the three\nknowledge selectors to control for knowledge quality. Bottom-up enables multi-domain knowledge\nsynthesis across all available sources, but these might occasionally introduce irrelevant information\nwhich may adversely impact LLM inference. We additionally propose a top-down approach, in\nwhich the LLM proactively seeks external information from selected knowledge cards. top-down is\nadvantageous in tasks and domains where external knowledge is not always necessary.", "start_char_idx": 0, "end_char_idx": 3895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48253f1f-a0e0-4bba-8b69-c512d0eafa35": {"__data__": {"id_": "48253f1f-a0e0-4bba-8b69-c512d0eafa35", "embedding": null, "metadata": {"page_label": "4", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e237025-f9ab-488f-9524-e65599a2abf3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3c21c37aa81254d9601be9ed4d98eaac17c79ace52472a7a377c29f43fc2b461", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "194f391d-7bf1-4e44-be08-b20ff682ee8e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "758e789c17d32922f67114c6328de92ea6fc68ef56a7a4a720d8f0710c2343ac", "class_name": "RelatedNodeInfo"}}, "text": "2.3 K NOWLEDGE INTEGRATION\nAfter defining the modular components in KNOWLEDGE CARD (a general-purpose LLM, knowledge\ncards, and knowledge selectors), we propose two approaches, bottom-up andtop-down , to integrate\nthe general-purpose LLM with external knowledge sources, which are selected outputs of knowledge\ncards. Specifically, bottom-up activates all available knowledge cards at once and employs the three\nknowledge selectors to control for knowledge quality. Bottom-up enables multi-domain knowledge\nsynthesis across all available sources, but these might occasionally introduce irrelevant information\nwhich may adversely impact LLM inference. We additionally propose a top-down approach, in\nwhich the LLM proactively seeks external information from selected knowledge cards. top-down is\nadvantageous in tasks and domains where external knowledge is not always necessary.\nBottom-Up Approach Bottom-up starts by prompting available knowledge cards, then progres-\nsively goes through the three knowledge selectors, and these outputs are incorporated into the LLM\nvia the prompt context. Formally, given nknowledge cards C={c1,\u00b7\u00b7\u00b7,cn}and the query q,\nwe generate n1documents with each knowledge card through temperature sampling (Holtzman\net al., 2019) to obtain {d1,\u00b7\u00b7\u00b7,dn\u00d7n1}. We first apply the relevance selector to retain n2most\nrelevant documents {d1,\u00b7\u00b7\u00b7,dn2}, then conduct knowledge pruning through the pruning selector\n{\u02dcd1,\u00b7\u00b7\u00b7,\u02dcdn2}, and finally leverage the factuality selector to obtain n3high-quality knowledge\ndocuments {\u02dcd1,\u00b7\u00b7\u00b7,\u02dcdn3}.\n4", "start_char_idx": 3017, "end_char_idx": 4571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbbc16f4-eccd-48ac-8ee3-259a52bc8041": {"__data__": {"id_": "dbbc16f4-eccd-48ac-8ee3-259a52bc8041", "embedding": null, "metadata": {"page_label": "5", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f87f2543-fba7-4ac7-8708-74fe89a048bd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9513cdb1df2c0ddb64f73fc2be8e4a4e3572d2fc4a1c2a7d358ffe7a251a7a60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7136211-c76a-4c72-85d4-ecb19ccc8b67", "node_type": "1", "metadata": {}, "hash": "553ac5d9415a50f3dc6cff928871f51a470ffa22100577e015a954515bbc264f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nThe final prompt for the LLM is a concatenation of knowledge documents and the query, formally\n[\u201cKnowledge: \u201d\u2225\u02dcd1\u2225\u02dcd2\u2225 \u00b7\u00b7\u00b7 \u2225 \u02dcdn3\u2225q] where \u2225denotes concatenation. We expect the bottom-\nup approach to be strong in multi-domain knowledge synthesis since multiple knowledge cards\ncould be activated at once to provide background knowledge from diverse perspectives. In addition,\nhyperparameters n1,n2, andn3enable fine-grained control over the knowledge synthesis process.\nTop-Down Approach Inbottom-up , we assume that every query would benefit from external\nknowledge generated by knowledge cards. However, this could introduce unnecessary information in\nthe LLM\u2019s prompt context (Zhao et al., 2023). Following Kadavath et al. (2022), who showed that\nLLMs possess preliminary abilities to identify their inherent knowledge limitations, we propose the\ntop-down approach, putting the LLM in charge to iteratively identify whether external knowledge is\nneeded and selectively activate relevant knowledge cards through various strategies.\nConcretely, for the nknowledge cards C={c1,\u00b7\u00b7\u00b7,cn}, we also ask the knowledge card contributors\nto submit a textual description of LMs S={s1,\u00b7\u00b7\u00b7,sn}such as \u201c biomedical literature \u201d, \u201ccollege\ncalculus \u201d, or \u201c commonsense knowledge graph \u201d. We first ask the LLM a yes/no question to determine\nwhether external knowledge is needed for the given query q, specifically \u201c Do you need more\ninformation? (Yes or No) \u201d. We encourage better-calibrated answers to the yes/no question through\nin-context learning (Wei et al.; Press et al., 2022): specifically, we introduce a set of in-context\nlearning examples that encompass two distinct categories of questions posed to the LLM. The first\ncategory consists of questions that the LLM is capable of answering accurately without the need for\nany extra information. For these questions, the response to the query \u201cDo you need more information\n(Yes or No)?\u201d is \u201cNo.\u201d The second category comprises questions that the LLM cannot answer\ncorrectly without the provision of additional information. In this case, the corresponding output\nlabel for the query is \u201cYes.\u201d In this way, we prompt the LLM to learn to request external knowledge\nthrough in-context learning; we analyze the effectiveness of this approach in Section 5. If the LLM\nanswers \u201c No\u201d, we directly prompt the LLM to generate based on the query, without resorting to\nknowledge cards. If the LLM requests external knowledge by answering \u201c Yes\u201d, we employ two\nstrategies (Algoithm 2) to select a relevant knowledge card and generate background knowledge.\n\u2022Automatic Selection (AUTO ) We further prompt the LLM with \u201c What kind of information do you\nneed? \u201d and select one knowledge card based on its response rq. Concretely, we identify which\nLM description {s1,\u00b7\u00b7\u00b7,sn}is most relevant to rqwith the relevance selector ( \u00a72.2) and activate\nthe corresponding LM to generate multiple knowledge documents, then select one with the highest\nfactuality score based on the factuality selector (\u00a72.2) to obtain d.\n\u2022Explicit Selection (EXP) Alternatively, we ask the LLM to directly select one knowledge card\nby prompting with \u201c Choose an information source from the following: s1, . . . , sn\u201d. If the LLM\nresponds with si, we activate the corresponding knowledge card cito generate multiple knowledge\ndocuments and select one with the factuality selector (\u00a72.2) to obtain d.\nUpon obtaining the document, we append \u201c Knowledge: d\u201d to the LLM context. We then iteratively\nask \u201c Do you need more information? (Yes or No) \u201d again, repeat the above process, until the LLM\nanswers \u201c No\u201d and generates a knowledge-informed response. We expect top-down to perform better\nwhen external knowledge is not always necessary. In this way, the top-down approach enables LLMs\nto take charge in identifying their inherent knowledge limitations and seeking help from external\nknowledge cards proactively. We provide prompt examples in Tables 10 and 11 in the Appendix.", "start_char_idx": 0, "end_char_idx": 4016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7136211-c76a-4c72-85d4-ecb19ccc8b67": {"__data__": {"id_": "f7136211-c76a-4c72-85d4-ecb19ccc8b67", "embedding": null, "metadata": {"page_label": "5", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f87f2543-fba7-4ac7-8708-74fe89a048bd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9513cdb1df2c0ddb64f73fc2be8e4a4e3572d2fc4a1c2a7d358ffe7a251a7a60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbbc16f4-eccd-48ac-8ee3-259a52bc8041", "node_type": "1", "metadata": {"page_label": "5", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b760caae1bd5485fbb50c9ee94127bf0a2cfd614b2ab2394fcc2eb76c69dabf2", "class_name": "RelatedNodeInfo"}}, "text": ". . , sn\u201d. If the LLM\nresponds with si, we activate the corresponding knowledge card cito generate multiple knowledge\ndocuments and select one with the factuality selector (\u00a72.2) to obtain d.\nUpon obtaining the document, we append \u201c Knowledge: d\u201d to the LLM context. We then iteratively\nask \u201c Do you need more information? (Yes or No) \u201d again, repeat the above process, until the LLM\nanswers \u201c No\u201d and generates a knowledge-informed response. We expect top-down to perform better\nwhen external knowledge is not always necessary. In this way, the top-down approach enables LLMs\nto take charge in identifying their inherent knowledge limitations and seeking help from external\nknowledge cards proactively. We provide prompt examples in Tables 10 and 11 in the Appendix.\n3 E XPERIMENT SETTINGS\nImplementation Forknowledge cards , we use OPT-1.3B (Zhang et al., 2022) as the starting point\nand separately train 25 specialized LMs on a wide range of knowledge sources and domains, including\ncorpora in the Pile (Gao et al., 2020), branch-train-merge (Li et al., 2022), knowledge graphs (Speer\net al., 2017; West et al., 2022; Vrande \u02c7ci\u00b4c & Kr \u00a8otzsch, 2014; Pellissier Tanon et al., 2020; Feng et al.,\n2021; Zhang et al., 2021), news and social media (Liu et al., 2022c; Feng et al., 2023b), and more.\n(Appendix E) We use MPNet (Song et al., 2020) as the encoder in the relevance selector , Pegasus\n(Zhang et al., 2020) as the summarization model in the pruning selector , the WikiSearch API as the\nretrieval system in the factuality selector , and FactKB (Feng et al., 2023a) and VitaminC (Schuster\net al., 2021) as the summarization and fact-checking factuality scoring functions. We use Codex\n(CODE -DAVINCI -002) (Chen et al., 2021) as the default, general-purpose, black-box LLM.\n5", "start_char_idx": 3249, "end_char_idx": 5031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85926df1-0928-4352-adc6-43eb8ba6716f": {"__data__": {"id_": "85926df1-0928-4352-adc6-43eb8ba6716f", "embedding": null, "metadata": {"page_label": "6", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ee731ee-b2d7-417c-b7a2-6e3b3a714f03", "node_type": "4", "metadata": {"page_label": "6", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bc1e91dd9f330fc860a57a485b51901bf77ea2cd04795ed1d60a77b6e8c2874a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "980a7cf9-d1dd-45a8-a546-508ebcdf4708", "node_type": "1", "metadata": {}, "hash": "e5e81295c84074bf0aaadb52b0d34ad26c7b3d29ad1fae0b6deb8aa2993d72e1", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nType Model Human. Social STEM Other All\nVanilla LMCODEX 74.2 76.9 57.8 70.1 68.3\nPALM 77.0 81.0 55.6 69.6 69.3\nFLAN-PALM - - - - 72.2\nRetrievalATLAS 46.1 54.6 38.8 52.8 47.9\nREPLUG 76.0 79.7 58.8 72.1 71.4\nREPLUG LSR 76.5 79.9 58.9 73.2 71.8\nGenerateGKP 73.3 74.5 59.5 71.4 70.0\nRECITATION 76.9 78.1 59.0 74.0 71.9\nKNOWLEDGE CARDBOTTOM -UP 77.2 76.7 57.9 72.2 70.7\nTOP-DOWN AUTO 77.7 78.9 59.2 73.0 72.0\nTOP-DOWN EXP 78.6 80.9 59.6 74.3 72.8\nTable 1: Model performance on the MMLU Bench-\nmark. KNOWLEDGE CARD improves Codex by at\nleast 3.5% while top-down outperforms all baselines.Type ModelTwo-Way Four-Way\nBAcc MaF BAcc MaF\nVanilla LM CODEX 65.6 51.0 52.8 44.0\nRetrievalREPLUG 78.8 67.8 55.8 53.0\nREPLUG LSR 78.8 68.5 57.5 54.4\nGenerateGKP 73.5 60.3 61.1 46.3\nRECITATION 65.0 47.7 64.2 48.6\nGRTR 66.1 49.1 51.6 36.9\nKNOWLEDGE CARDBOTTOM -UP 89.8 87.3 70.6 67.3\nTOP-DOWN AUTO 86.4 78.7 63.0 60.2\nTOP-DOWN EXP 91.3 86.0 69.4 65.5\nTable 2: Performance on misinformation de-\ntection. BAcc and MaF are balanced accu-\nracy and macro F1. bottom-up performs best\ndue to multi-domain knowledge integration.\nType ModelOpen-Book Multiple-Choice\nEM F1 2-way 4-way\nVanilla LM CODEX 55.1 57.9 90.9 60.8\nRetrievalREPLUG 44.8 - 85.7 62.8\nREPLUG LSR 37.2 - 86.9 65.3\nSI ET AL . 52.1 54.5 84.7 61.4\nGenerateGKP 45.0 46.9 89.1 53.5\nRECITATION 44.4 46.4 89.3 52.3\nGRTR 55.6 58.4 77.4 59.0\nKNOWLEDGE CARDBOTTOM -UP 83.6 85.6 81.6 64.5\nTOP-DOWN AUTO 87.5 89.3 89.5 63.0\nTOP-DOWN EXP 75.3 75.7 91.9 67.6\nTable 3: Performance on MidtermQA. KNOWL -\nEDGE CARD successfully updates the knowledge\nof Codex by adding a single knowledge card.Tasks and Datasets 1) For general-purpose\nQA, we adopt MMLU (Hendrycks et al., 2020),\na multiple-choice QA dataset covering 57 tasks\nin humanities, STEM, social sciences, and oth-\ners. Following previous works (Si et al., 2022;\nShi et al., 2023), we adopt a 5-shot in-context\nlearning setting. 2) To evaluate multi-domain\nknowledge synthesis , we adopt misinformation\ndetection, since news articles often encompass\nfacts and opinions at the intersection of differ-\nent domains and perspectives.", "start_char_idx": 0, "end_char_idx": 2154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "980a7cf9-d1dd-45a8-a546-508ebcdf4708": {"__data__": {"id_": "980a7cf9-d1dd-45a8-a546-508ebcdf4708", "embedding": null, "metadata": {"page_label": "6", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ee731ee-b2d7-417c-b7a2-6e3b3a714f03", "node_type": "4", "metadata": {"page_label": "6", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bc1e91dd9f330fc860a57a485b51901bf77ea2cd04795ed1d60a77b6e8c2874a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85926df1-0928-4352-adc6-43eb8ba6716f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a772fbcc5a70d8807d37868c060b93cac67caa8c170d28868532db4bda54b238", "class_name": "RelatedNodeInfo"}}, "text": "KNOWL -\nEDGE CARD successfully updates the knowledge\nof Codex by adding a single knowledge card.Tasks and Datasets 1) For general-purpose\nQA, we adopt MMLU (Hendrycks et al., 2020),\na multiple-choice QA dataset covering 57 tasks\nin humanities, STEM, social sciences, and oth-\ners. Following previous works (Si et al., 2022;\nShi et al., 2023), we adopt a 5-shot in-context\nlearning setting. 2) To evaluate multi-domain\nknowledge synthesis , we adopt misinformation\ndetection, since news articles often encompass\nfacts and opinions at the intersection of differ-\nent domains and perspectives. We leverage the\nwidely adopted LUN misinformation detection\ndataset (Rashkin et al., 2017) with both 2-way\nand 4-way classification settings. All models are evaluated based on 16-shot in-context learning. 3)\nTo evaluate temporal knowledge update , we curate MIDTERM QA, a QA benchmark focusing on the\n2022 U.S. midterm elections since the knowledge cutoff of black-box LLMs is often 2021 or earlier.\nMIDTERM QApresents three evaluation datasets and settings: open-book, 2-way, and 4-way multiple\nchoice. 5-shot in-context learning is adopted to evaluate KNOWLEDGE CARD and baselines. We did\nnot consider existing temporal QA datasets (Jang et al., 2021; Dhingra et al., 2022; Kasai et al., 2022)\nsince they do not focus on any specific event or knowledge domain.\nBaselines We compare KNOWLEDGE CARD with a wide range of baseline methods in three\ncategories. 1) vanilla black-box LLMs: Codex (Chen et al., 2021), PaLM (Chowdhery et al., 2022),\nand Flan-PaLM (Chung et al., 2022); 2) generated knowledge prompting approaches: GKP (Liu et al.,\n2022a), recitation (Sun et al., 2022), GRTR (Yu et al., 2022) (Note that we apply these methods to\nthe same LLM Codex (Chen et al., 2021) for a fair comparison); 3) retrieval-augmented language\nmodels: Atlas (Izacard et al., 2022), Si et al. (2022), RePlug, and RePlug LSR (Shi et al., 2023).\n4 R ESULTS\nMMLU For general-purpose knowledge QA, we use the MMLU benchmark (Hendrycks et al.,\n2020). As shown in Table 1, all three configurations of KNOWLEDGE CARD significantly improve\nvanilla Codex. Among them, the top-down approach with explicit selection performs best, improving\nCodex by 6.6% overall accuracy. Concurrently, top-down approaches surpass all baselines, including\nFlan-PaLM with a few hundred billion more parameters. These results suggest that we present an\neffective approach for making general-purpose LLMs better in knowledge-intensive contexts. In\naddition, top-down generally outperforms bottom-up likely because MMLU contains math-related\nquestions that do not necessitate external knowledge. This observation suggests that top-down\napproaches are better at tasks where external knowledge is not always necessary.\nMisinformation Detection To examine whether KNOWLEDGE CARD successfully integrates multi-\nfaceted knowledge from diversified sources, we adopt the LUN misinformation dataset (Rashkin et al.,\n2017) with two- and four-way classification settings. Table 2 demonstrates that KNOWLEDGE CARD\nsignificantly improves Codex by at least 31.7% and 19.4% in balanced accuracy scores for both\nsettings. In addition, bottom-up outperforms both variants of top-down , thanks to its methodology to\njointly activate knowledge cards from various domains and enable multi-domain knowledge synthesis.\n6", "start_char_idx": 1564, "end_char_idx": 4913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d2a969e-fc02-4aea-9f3c-f3f63450c235": {"__data__": {"id_": "0d2a969e-fc02-4aea-9f3c-f3f63450c235", "embedding": null, "metadata": {"page_label": "7", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cad4573-a53f-4798-b58c-f0b612fd25b8", "node_type": "4", "metadata": {"page_label": "7", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "eb286a3a0a8ba2cb9462e35986c27f3d8cfd831fd56a2fec6c52b11fc1391c4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2604b6be-5406-4f8d-84b2-7d51607a4fff", "node_type": "1", "metadata": {}, "hash": "2a7aa6602562560d2708b444edb60c784340b781dab4b153ea5141fd5b5a82d3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n+ BookCorpus + PubMed + IMDB + News + Wikipedia+1.0+2.0+3.0ACC\n+ BookCorpus + PubMed + IMDB + News + Wikipedia+1.0+2.0Balanced ACC\nFigure 2: Performance on misinformation de-\ntection when each knowledge card is separately\nadded. KNOWLEDGE CARD enables modular\npatching of LLMs while in-domain knowledge\ncards help the most.\nACC Macro F185.087.590.092.595.0\n-1.4\n-0.6-1.8\n-1.0-3.3\n-2.5full model\nw/o pruningw/o relevance\nw/o factualityFigure 3: Ablation study of the three knowledge\nselectors on misinformation detection. While\nthe three selectors all contribute to model perfor-\nmance, the factuality selector is most crucial.\nMidtermQA To examine whether KNOWLEDGE CARD could update the parametric knowledge\nof LLMs, we train an additional knowledge card on news articles regarding the 2022 U.S. midterm\nelections and plug it into KNOWLEDGE CARD. We present model performance on MidtermQA in\nTable 3, which demonstrates that KNOWLEDGE CARD substantially outperforms all baselines in\nthe open-book setting by as much as 57.3% in exact match scores (EM). This indicates that one\nknowledge card with 1.3B parameters successfully updates the parametric knowledge of the 175B\nCodex through KNOWLEDGE CARD. In addition, top-down outperforms bottom-up , indicating that\nthe selective activation of knowledge cards is better when there is a specific knowledge card tied to\nthe task domain. KNOWLEDGE CARD also outperforms SI ET AL .(Codex + Contriever) that uses\nthe same midterm election news as retrieval corpora. In addition, generated knowledge prompting\napproaches (GKP, recitation, GRTR) underperform vanilla Codex, showing that probing LLMs for\nexplicit knowledge is counterproductive when internal LLM knowledge is outdated or wrong.\n5 A NALYSIS\nPatching LLM Knowledge When general-purpose LLMs struggle at tasks due to knowledge\nlimitations, KNOWLEDGE CARD could serve as an efficient approach to patch LLM weaknesses by\nadding specialized language models. To this end, we evaluate the change in performance when five\nknowledge cards are separately added to augment Codex with the top-down approach. Results in\nFigure 2 demonstrate that patching the LLM with all five LMs leads to various levels of performance\ngains on misinformation detection, while the most in-domain LMs (Wikipedia and news) lead to\ngreater improvements. This suggests that when LLMs perform poorly on knowledge-intensive tasks,\nan additional knowledge card trained on in-domain corpora could help with K NOWLEDGE CARD.\nModelTwo-Way Four-Way\nBAcc MaF BAcc MaF\nREPLUG 78.8 67.8 55.8 53.0\nREPLUG LSR 78.8 68.5 57.5 54.4\nBOTTOM -UP 90.0 87.0 65.3 63.3\nTOP-DOWN AUTO 80.7 70.9 60.1 56.8\nTOP-DOWN EXP 80.6 70.0 59.7 56.5\nTable 4: KNOWLEDGE CARD outperforms\nretrieval LM R EPLUG in the Wikipedia-only\nsetting, suggesting that modular LMs present\na better knowledge repository than retrieval.Knowledge Selector Study In Section 2.2, we pro-\npose three levels of knowledge selectors to control\nfor various factors and ensure knowledge quality. We\nconduct ablation studies to remove each knowledge\nselector in the bottom-up approach and re-evaluate\non misinformation detection. Figure 3 demonstrates\nthat while all three knowledge selectors are helpful,\nthe factuality selector contributes most to model per-\nformance and thus plays a crucial role in ensuring the\nquality of generated knowledge documents.\nRetrieval vs. Specialized LMs In order to assess\nthe effectiveness of modular specialized LMs as compared to non-parametric sources like retrieval,\nwe exclusively use the Wikipedia LM in KNOWLEDGE CARD and compare with the state-of-the-art\nretrieval LM REPLUG that also uses Wikipedia as the retrieval knowledge source.", "start_char_idx": 0, "end_char_idx": 3734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2604b6be-5406-4f8d-84b2-7d51607a4fff": {"__data__": {"id_": "2604b6be-5406-4f8d-84b2-7d51607a4fff", "embedding": null, "metadata": {"page_label": "7", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cad4573-a53f-4798-b58c-f0b612fd25b8", "node_type": "4", "metadata": {"page_label": "7", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "eb286a3a0a8ba2cb9462e35986c27f3d8cfd831fd56a2fec6c52b11fc1391c4f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d2a969e-fc02-4aea-9f3c-f3f63450c235", "node_type": "1", "metadata": {"page_label": "7", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d6f9f25773c27a1d77af92f91ff58652665984846bd0c2d0ab524e2ed65c460f", "class_name": "RelatedNodeInfo"}}, "text": "We\nconduct ablation studies to remove each knowledge\nselector in the bottom-up approach and re-evaluate\non misinformation detection. Figure 3 demonstrates\nthat while all three knowledge selectors are helpful,\nthe factuality selector contributes most to model per-\nformance and thus plays a crucial role in ensuring the\nquality of generated knowledge documents.\nRetrieval vs. Specialized LMs In order to assess\nthe effectiveness of modular specialized LMs as compared to non-parametric sources like retrieval,\nwe exclusively use the Wikipedia LM in KNOWLEDGE CARD and compare with the state-of-the-art\nretrieval LM REPLUG that also uses Wikipedia as the retrieval knowledge source. Table 4 demon-\nstrates that KNOWLEDGE CARD outperforms REPLUG on both settings of misinformation detection,\nsuggesting that knowledge cards present a better knowledge repository. Note that KNOWLEDGE\nCARD is also compatible with multiple knowledge formats (e.g. retrieval and search engine) while\nthey could be complementary (Appendix A).\nKnowledge Stream Analysis Inbottom-up , three hyperparameters (\u00a72.3) govern the \u201cknowledge\nstream\u201d from knowledge cards to the general-purpose LLMs. Specifically, n1controls how many\n7", "start_char_idx": 3054, "end_char_idx": 4257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b6c849b-9d4b-480b-bb94-97f9cf69a3fe": {"__data__": {"id_": "1b6c849b-9d4b-480b-bb94-97f9cf69a3fe", "embedding": null, "metadata": {"page_label": "8", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8380bcd-c579-4529-ab37-a2c9f5646886", "node_type": "4", "metadata": {"page_label": "8", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b7bdc70d7fb259027ed3e52ff637fe3b5e903d23e2a5f2915b028ce18520f816", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c3b5900-8415-435e-a053-c8ddbb3b3fa3", "node_type": "1", "metadata": {}, "hash": "cfa0c20397732c2e488b44de54f883a208bdff51a38e00786cec5a35c9152c30", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n12345\nn1 (n2 = 5, n3 = 3)0.850.90\n35 81012\nn2 (n1 = 3, n3 = 3)0.850.90\n12345\nn3 (n1 = 3, n2 = 5)0.850.90\nACC\nMacro F1\nFigure 4: Investigating the impact of n1,n2, and n3, which\ngovern the knowledge stream from modular knowledge cards\nto general-purpose LLMs. These hyperparameters enable fine-\ngrained control over the knowledge synthesis process.\ndavinci w/ bottom-up w/ autocard w/ expcard70758085\n+10.0+12.0\n+9.6\nturbo w/ bottom-up w/ autocard w/ expcard707580\n+5.0\n+2.2 +2.6Figure 5: KNOWLEDGE CARD\nis compatible with other LLMs,\nspecifically TEXT -DAVINCI -003\nand GPT-3.5- TURBO .\ndocuments each LM generates, n2controls how many are retained after the three knowledge selectors,\nandn3controls how many are put into the context of LLMs. We investigate these control measures\nand report performance in Figure 4. It is illustrated that: 1) n1has a marginal impact, suggesting\nthat knowledge cards generate largely homogeneous knowledge even with temperature sampling\n(Caccia et al., 2018); 2) larger n2leads to performance drops, suggesting that the three knowledge\nselectors ensure knowledge quality; 3) n3 = 1 , where only one knowledge document is adopted at a\ntime (as in previous works (Sun et al., 2022; Shi et al., 2023)) is worse than larger values, showing\nthe advantage of multi-domain knowledge synthesis uniquely enabled by K NOWLEDGE CARD.\nyes no1,290 498\n1,140 71auto: 2-way\n314 171\n1,576 938auto: 4-way\ncorrect incorrectyes no1,004 42\n1,642 311exp: 2-way\ncorrect incorrect364 68\n1,718 849exp: 4-way\nFigure 6: Confusion matri-\nces of yes/no and correctness\nintop-down , enabling fine-\ngrained error analysis.LLM Compatibility While we follow previous works (Sun et al.,\n2022; Shi et al., 2023) and adopt Codex as the default black-box\nLLM, KNOWLEDGE CARD is compatible with different models. We\nadditionally evaluate KNOWLEDGE CARD with two other LLMs,\nTEXT -DAVINCI -003 and GPT-3.5- TURBO , and present results in Fig-\nure 5. Both bottom-up andtop-down consistently improve different\nLLMs across various datasets and evaluation metrics.\nYes/No in Top-Down Intop-down (\u00a72.3), we begin by asking\nLLMs if they might need external knowledge for the given query and\nadopt in-context examples to encourage well-calibrated answers. We\nillustrate LLM responses along with the correctness of their answer\nin Figure 6. The vast majority of queries are mapped to the \u201cyes,\ncorrect\u201d and \u201cno, correct\u201d categories, suggesting that LLMs have\npreliminary abilities to \u201cknow what they know\u201d and seek external\ninformation if necessary. However, this ability is far from perfect,\nevident in the non-negligible category of \u201cno, incorrect\u201d, suggesting that prompting LLMs to ac-\nknowledge knowledge limitations requires further research (Kadavath et al., 2022; Zhao et al., 2023),\nwhile new approaches to abstain could be easily integrated into KNOWLEDGE CARD. In addition,\nthe \u201cyes, incorrect\u201d categories suggest that specialized LMs occasionally fail to provide enough\ninformation. These confusion matrices provide fine-grained error analysis and guidance as to whether\nthe general-purpose LLM, the yes/no question, or knowledge cards require further improvements.", "start_char_idx": 0, "end_char_idx": 3210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c3b5900-8415-435e-a053-c8ddbb3b3fa3": {"__data__": {"id_": "7c3b5900-8415-435e-a053-c8ddbb3b3fa3", "embedding": null, "metadata": {"page_label": "8", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8380bcd-c579-4529-ab37-a2c9f5646886", "node_type": "4", "metadata": {"page_label": "8", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b7bdc70d7fb259027ed3e52ff637fe3b5e903d23e2a5f2915b028ce18520f816", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b6c849b-9d4b-480b-bb94-97f9cf69a3fe", "node_type": "1", "metadata": {"page_label": "8", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "41a105eadf19dbb89904afa088b20d4581a5d0848381f94b6175c9256cfded7f", "class_name": "RelatedNodeInfo"}}, "text": "The vast majority of queries are mapped to the \u201cyes,\ncorrect\u201d and \u201cno, correct\u201d categories, suggesting that LLMs have\npreliminary abilities to \u201cknow what they know\u201d and seek external\ninformation if necessary. However, this ability is far from perfect,\nevident in the non-negligible category of \u201cno, incorrect\u201d, suggesting that prompting LLMs to ac-\nknowledge knowledge limitations requires further research (Kadavath et al., 2022; Zhao et al., 2023),\nwhile new approaches to abstain could be easily integrated into KNOWLEDGE CARD. In addition,\nthe \u201cyes, incorrect\u201d categories suggest that specialized LMs occasionally fail to provide enough\ninformation. These confusion matrices provide fine-grained error analysis and guidance as to whether\nthe general-purpose LLM, the yes/no question, or knowledge cards require further improvements.\nRace Codex K NOWLEDGE CARD\nAL, senate Doug Jones \u2717 Katie Britt \u2713\nPA, senate Bob Casey \u2717 John Fetterman \u2713\nCA, 3rd Mike Thompson \u2717 Kevin Kiley \u2713\nIN, 2nd Jackie Walorski \u2717 Jim Banks \u2717\nNV , governor Steve Sisolak \u2717 Joe Lombardo \u2713\nTable 5: While vanilla Codex falsely claims\nthat these incumbents won again in the 2022\nelections, KNOWLEDGE CARD successfully\nupdates the knowledge of black-box LLMs.Qualitative Analysis We curated MIDTERM QAto\nevaluate whether KNOWLEDGE CARD enables effi-\ncient knowledge update. We examine the 88 races\nwhere the incumbent was not re-elected: Codex an-\nswered 1 out of the 88 questions correctly, while\nbottom-up andtop-down with automatic and explicit\nselection answered 63, 77, and 42 correctly. Table 5\nshows that Codex states the incumbents would win\nagain in 2022, while KNOWLEDGE CARD success-\nfully updates LLMs with 100x more parameters.\n8", "start_char_idx": 2374, "end_char_idx": 4087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ddf4609-c938-4158-a79c-8ef107aca165": {"__data__": {"id_": "6ddf4609-c938-4158-a79c-8ef107aca165", "embedding": null, "metadata": {"page_label": "9", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6da4689f-7c37-45fe-8110-a8d6af0f0c09", "node_type": "4", "metadata": {"page_label": "9", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cf5a4db887f1a878ebe5b895318df58321455b58538aed76595ff5f8e4207451", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37557ac5-fd20-4d3d-a8bf-5278ffbcdadd", "node_type": "1", "metadata": {}, "hash": "73b59f5b45e12e18b825728d1e81194cfcc73e3b877ef873be53d0c5fdf94d0a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n6 R ELATED WORK\nRetrieval-Augmented Language Models Augmenting language models with retrieval has ad-\nvanced the state-of-the-art in open-domain QA (Guu et al., 2020; Izacard et al., 2022; Lewis et al.,\n2020; Hu et al., 2022), text classification (Zhao et al., 2023), and language modeling (Hu et al., 2022;\nBorgeaud et al., 2022; Min et al., 2023). The retrieval system could be integrated into encoder-decoder\n(Izacard et al., 2022) and decoder-only models (Borgeaud et al., 2022; Shi et al., 2022; Rubin et al.,\n2022), or leveraged to interpolate the next token probability distributions (Khandelwal et al., 2019;\nZhong et al., 2022). Recent advances incorporated frozen (Mallen et al., 2023; Si et al., 2022; Khattab\net al., 2022) and trainable retrievers (Shi et al., 2023) as well as search engines (Press et al., 2022)\nto augment LLMs. Compared to retrieval models and search engines, KNOWLEDGE CARD enables\nflexible information seeking, searching over knowledge domains, and employing private knowledge\nsources. In addition, these works often leverage only oneretrieval corpora and assume that it\u2019s\n\u201comniscient\u201d while suffering from various issues such as domain coverage and knowledge update.\nIn contrast, we propose to reflect the modularity and community-driven nature of knowledge by\nintegrating plug-and-play knowledge cards with general-purpose LLMs.\nGenerated Knowledge Prompting LMs acquire knowledge through training on gargantuan textual\ncorpora (Petroni et al., 2019; Dhingra et al., 2022; He et al., 2021). Generated knowledge prompting\n(Liu et al., 2022a) is one of the early approaches to tap into the parametric knowledge of LLMs by\nprompting them to generate background information and re-using it for QA. Related works also\npropose to use LM parametric knowledge for retrieval (Tay et al., 2022), answer commonsense\nquestions with self-talk (Shwartz et al., 2020), generate queries (Wang et al., 2022; Zhuang et al.,\n2022) or token sequences (Bevilacqua et al., 2022) for document augmentation. In addition, recitation-\naugmented language models (Sun et al., 2022) propose to augment QA examples with diversified\nknowledge recitations, while (Yu et al., 2022) shows that generated knowledge is, under certain\ncircumstances, better than retrieval. However, this line of work assumes that the encoded knowledge\nin LLM parameters is all we need, while LLM knowledge suffers from hallucination (Ji et al., 2023),\nstruggles to encode long-tail facts (Mallen et al., 2023), and can not be efficiently updated (De Cao\net al., 2021). While recent works propose to edit LLM knowledge (Meng et al., 2022; Hernandez et al.,\n2023), they are hardly compatible with black-box LLMs. In addition, parametric knowledge in LLMs\nis far from modular and collaborative, while LMs should be able to incorporate knowledge contributed\nby all stakeholders in LLM research and applications. To this end, we propose KNOWLEDGE CARD\nas a community-driven initiative to empower general-purpose LLMs with modular and collaborative\nknowledge through the sharing and re-using of knowledge cards.\nModular LMs Mixture-of-Experts (MoE) (Masoudnia & Ebrahimpour, 2014) aims to activate one\nexpert based on the input instance, which has been adopted in language model research (Gururangan\net al., 2022; Roller et al., 2021; Lewis et al., 2021; Kudugunta et al., 2021; Pfeiffer et al., 2022).\nAdapters are also proposed for task transfer and parameter-efficient fine-tuning (Houlsby et al., 2019;\nPfeiffer et al., 2020; Zaken et al., 2022).", "start_char_idx": 0, "end_char_idx": 3571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37557ac5-fd20-4d3d-a8bf-5278ffbcdadd": {"__data__": {"id_": "37557ac5-fd20-4d3d-a8bf-5278ffbcdadd", "embedding": null, "metadata": {"page_label": "9", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6da4689f-7c37-45fe-8110-a8d6af0f0c09", "node_type": "4", "metadata": {"page_label": "9", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cf5a4db887f1a878ebe5b895318df58321455b58538aed76595ff5f8e4207451", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ddf4609-c938-4158-a79c-8ef107aca165", "node_type": "1", "metadata": {"page_label": "9", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7d15ae64d7dfe15878ac684588bebb1be79963216b090708342f05c7b5fbb265", "class_name": "RelatedNodeInfo"}}, "text": "To this end, we propose KNOWLEDGE CARD\nas a community-driven initiative to empower general-purpose LLMs with modular and collaborative\nknowledge through the sharing and re-using of knowledge cards.\nModular LMs Mixture-of-Experts (MoE) (Masoudnia & Ebrahimpour, 2014) aims to activate one\nexpert based on the input instance, which has been adopted in language model research (Gururangan\net al., 2022; Roller et al., 2021; Lewis et al., 2021; Kudugunta et al., 2021; Pfeiffer et al., 2022).\nAdapters are also proposed for task transfer and parameter-efficient fine-tuning (Houlsby et al., 2019;\nPfeiffer et al., 2020; Zaken et al., 2022). In addition, parameter averaging (Matena & Raffel, 2022;\nMcMahan et al., 2017; Izmailov et al., 2018; Wortsman et al., 2022; Li et al., 2022; Gururangan et al.,\n2023), model fusion (Don-Yehiya et al., 2022; Borzunov et al., 2022), continual learning (Jang et al.,\n2021; Qin et al., 2022; Ke et al., 2022; Qin et al., 2023), and other collaborative approaches (K \u00a8opf\net al., 2023; Sha, 2023; Luo et al., 2023) have also shed light on the possibility of distributed LM\ntraining. However, existing modular LMs mostly operate in the white-box setting, i.e.assuming\naccess to the model parameters, token probabilities, and more. Since the most prominent LLMs\nare only released behind API calls, we propose KNOWLEDGE CARD with the aim of empowering\nblack-box general-purpose LLMs with community-driven and collaborative knowledge.\n7 C ONCLUSION\nWe propose KNOWLEDGE CARD, a novel framework to empower general-purpose LLMs with\nmodular and collaborative knowledge. We first present knowledge cards, specialized LMs trained\non various domains and sources of knowledge, and propose three knowledge selectors to ensure\nknowledge quality. We then propose bottom-up andtop-down approaches to integrate knowledge\ncards with general-purpose LLMs to enable multi-domain knowledge synthesis and grounding in\nexternal information when necessary. Extensive experiments demonstrate that KNOWLEDGE CARD\noutperforms vanilla LLMs, retrieval LMs, and generated knowledge prompting approaches across\nthree tasks and six datasets, showcasing its ability to integrate multiple sources of information,\nefficiently update LLM\u2019s knowledge, and more. We envision KNOWLEDGE CARD as a community-\ndriven initiative to empower general-purpose LLMs with modular and collaborative knowledge.\n9", "start_char_idx": 2935, "end_char_idx": 5330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ba8b328-5a4a-4bf1-af1b-7e47e1839e2e": {"__data__": {"id_": "6ba8b328-5a4a-4bf1-af1b-7e47e1839e2e", "embedding": null, "metadata": {"page_label": "10", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5d845b4-39c7-4b19-b4f7-77c73d3eaa08", "node_type": "4", "metadata": {"page_label": "10", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "72f122cc835e1eacc331076eb15368d8aa953d8e29a7886923284847158b92ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebe1f4b2-1308-48ea-9816-f02cfdb7c340", "node_type": "1", "metadata": {}, "hash": "60a01cabc4132a20dba356471a499af216e1c497d43cfe254f03925f6cc9ca08", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nWe thank the reviewers, the area chair, members of Tsvetshop, and the UW NLP Group for their feed-\nback. This research is supported in part by the Office of the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-\n22072200004. This material is also funded by the DARPA Grant under Contract No. HR001120C0124.\nWe also gratefully acknowledge support from NSF CAREER Grant No. IIS2142739, NSF Grants\nNo. IIS2125201, IIS2203097, and the Alfred P. Sloan Foundation Fellowship. The views and\nconclusions contained herein are those of the authors and should not be interpreted as necessarily rep-\nresenting the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government.\nThe U.S. Government is authorized to reproduce and distribute reprints for governmental purposes\nnotwithstanding any copyright annotation therein.\nREFERENCES\nSharegpt. https://sharegpt.com/ , 2023. Accessed: 2023-09-27.\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies , pp. 3554\u20133565, 2021.\nEugene Bagdasaryan and Vitaly Shmatikov. Spinning language models: Risks of propaganda-as-a-\nservice and countermeasures. In 2022 IEEE Symposium on Security and Privacy (SP) , pp. 769\u2013786.\nIEEE, 2022.\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio\nPetroni. Autoregressive search engines: Generating substrings as document identifiers. Advances\nin Neural Information Processing Systems , 35:31668\u201331683, 2022.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on\nmachine learning , pp. 2206\u20132240. PMLR, 2022.\nAlexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem\nChumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning\nof large models. In Workshop on Broadening Research Collaborations 2022 , 2022.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin.\nLanguage gans falling short. In International Conference on Learning Representations , 2018.\nSteve Cayzer. Semantic blogging and decentralized knowledge management. Communications of the\nACM , 47(12):47\u201352, 2004.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005 , 2013.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\nWenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. Kgpt: Knowledge-grounded pre-training\nfor data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pp. 8635\u20138648, 2020.", "start_char_idx": 0, "end_char_idx": 3470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebe1f4b2-1308-48ea-9816-f02cfdb7c340": {"__data__": {"id_": "ebe1f4b2-1308-48ea-9816-f02cfdb7c340", "embedding": null, "metadata": {"page_label": "10", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5d845b4-39c7-4b19-b4f7-77c73d3eaa08", "node_type": "4", "metadata": {"page_label": "10", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "72f122cc835e1eacc331076eb15368d8aa953d8e29a7886923284847158b92ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ba8b328-5a4a-4bf1-af1b-7e47e1839e2e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f76dfa6985f37c42d66742124898ad9af4ad0235f33e2d7f989fdaa76adcd5b2", "class_name": "RelatedNodeInfo"}}, "text": "One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005 , 2013.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\nWenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. Kgpt: Knowledge-grounded pre-training\nfor data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pp. 8635\u20138648, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n10", "start_char_idx": 2852, "end_char_idx": 3727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32e65b58-2933-479d-9bf9-20c918fd0e98": {"__data__": {"id_": "32e65b58-2933-479d-9bf9-20c918fd0e98", "embedding": null, "metadata": {"page_label": "11", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c2363f4-e830-4e7e-8505-f935c9e48962", "node_type": "4", "metadata": {"page_label": "11", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a9098a194f59c6503dd2ffe43ea741a9958fa841f1f3a09bd44ead3760b7cf57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b035c99b-ed55-42eb-9234-c69e7ec4edae", "node_type": "1", "metadata": {}, "hash": "7099ec6ddfd612495f54fe395a1b3b0463dcff238c554325c31edecfc5b5ebfc", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416 , 2022.\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp.\n6491\u20136506, 2021.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. Time-aware language models as temporal knowledge bases. Transactions of\nthe Association for Computational Linguistics , 10:257\u2013273, 2022.\nShachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem\nChoshen. Cold fusion: Collaborative descent for distributed multitask finetuning. arXiv preprint\narXiv:2212.01378 , 2022.\nShangbin Feng, Zilong Chen, Wenqian Zhang, Qingyao Li, Qinghua Zheng, Xiaojun Chang, and\nMinnan Luo. Kgap: Knowledge graph augmented political perspective detection in news media.\narXiv preprint arXiv:2108.03861 , 2021.\nShangbin Feng, Zhaoxuan Tan, Herun Wan, Ningnan Wang, Zilong Chen, Binchi Zhang, Qinghua\nZheng, Wenqian Zhang, Zhenyu Lei, Shujie Yang, et al. Twibot-22: Towards graph-based twitter\nbot detection. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track , 2022.\nShangbin Feng, Vidhisha Balachandran, Yuyang Bai, and Yulia Tsvetkov. FactKB: Generaliz-\nable factuality evaluation using language models enhanced with factual knowledge. In Houda\nBouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Em-\npirical Methods in Natural Language Processing , pp. 933\u2013952, Singapore, December 2023a.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.59. URL\nhttps://aclanthology.org/2023.emnlp-main.59 .\nShangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. From pretraining data to language\nmodels to downstream tasks: Tracking the trails of political biases leading to unfair NLP models. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) , pp. 11737\u201311762, July 2023b.\nShangbin Feng, Zhaoxuan Tan, Wenqian Zhang, Zhenyu Lei, and Yulia Tsvetkov. KALM: Knowledge-\naware integration of local, document, and global contexts for long document understanding. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) , pp. 2116\u20132138, July 2023c.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027 , 2020.\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, and Luke Zettlemoyer. Demix layers:\nDisentangling domains for modular language modeling. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies , pp. 5557\u20135576, 2022.\nSuchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, and Luke\nZettlemoyer. Scaling expert language models with unsupervised domain discovery.", "start_char_idx": 0, "end_char_idx": 3350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b035c99b-ed55-42eb-9234-c69e7ec4edae": {"__data__": {"id_": "b035c99b-ed55-42eb-9234-c69e7ec4edae", "embedding": null, "metadata": {"page_label": "11", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c2363f4-e830-4e7e-8505-f935c9e48962", "node_type": "4", "metadata": {"page_label": "11", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a9098a194f59c6503dd2ffe43ea741a9958fa841f1f3a09bd44ead3760b7cf57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32e65b58-2933-479d-9bf9-20c918fd0e98", "node_type": "1", "metadata": {"page_label": "11", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "409cf5cd7f70fa86a552645660fbb7ad996fb3298fe4626a576730a1b7ca21da", "class_name": "RelatedNodeInfo"}}, "text": "The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027 , 2020.\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, and Luke Zettlemoyer. Demix layers:\nDisentangling domains for modular language modeling. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies , pp. 5557\u20135576, 2022.\nSuchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, and Luke\nZettlemoyer. Scaling expert language models with unsupervised domain discovery. arXiv preprint\narXiv:2303.14177 , 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International conference on machine learning , pp. 3929\u20133938.\nPMLR, 2020.\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit\nBansal, and Srinivasan Iyer. Do language models have beliefs? methods for detecting, updating,\nand visualizing model beliefs. arXiv preprint arXiv:2111.13654 , 2021.\n11", "start_char_idx": 2756, "end_char_idx": 3850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b48dd126-66bd-4a60-92fd-2706d071597c": {"__data__": {"id_": "b48dd126-66bd-4a60-92fd-2706d071597c", "embedding": null, "metadata": {"page_label": "12", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2221a159-a76a-4119-90b6-b457ec229b33", "node_type": "4", "metadata": {"page_label": "12", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "94961ec14d017ab8574c41df370e9a469182916de3e54440c06507436f4a4316", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7c1efb5-62ea-48a4-a27c-9bc9804dc05d", "node_type": "1", "metadata": {}, "hash": "384ce0cc1030949704b426a4b35aa138b867b3eae7d73257dd6f840c3b659cb6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and Fuchun Peng. Analyz-\ning the forgetting problem in pretrain-finetuning of open-domain dialogue response models. In\nProceedings of the 16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume , pp. 1121\u20131133, April 2021.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations , 2020.\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlp dataset\nfor legal contract review. In Thirty-fifth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 1) , 2021.\nEvan Hernandez, Belinda Z Li, and Jacob Andreas. Measuring and manipulating knowledge repre-\nsentations in language models. arXiv preprint arXiv:2304.00740 , 2023.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations , 2019.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\nnlp. In International Conference on Machine Learning , pp. 2790\u20132799. PMLR, 2019.\nLinmei Hu, Tianchi Yang, Luhao Zhang, Wanjun Zhong, Duyu Tang, Chuan Shi, Nan Duan, and\nMing Zhou. Compare to the knowledge: Graph neural fake news detection with external knowledge.\nInProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) ,\npp. 754\u2013763, 2021.\nYushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap:\nPrompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699 , 2022.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022.\nP Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov. Averaging weights leads to wider\noptima and better generalization. In 34th Conference on Uncertainty in Artificial Intelligence 2018,\nUAI 2018 , pp. 876\u2013885, 2018.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, KIM Gyeonghun, Stan-\nley Jungkyu Choi, and Minjoon Seo. Towards continual knowledge learning of language models.\nInInternational Conference on Learning Representations , 2021.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\nComputing Surveys , 55(12):1\u201338, 2023.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly)\nknow what they know. arXiv preprint arXiv:2207.05221 , 2022.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.", "start_char_idx": 0, "end_char_idx": 3288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7c1efb5-62ea-48a4-a27c-9bc9804dc05d": {"__data__": {"id_": "a7c1efb5-62ea-48a4-a27c-9bc9804dc05d", "embedding": null, "metadata": {"page_label": "12", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2221a159-a76a-4119-90b6-b457ec229b33", "node_type": "4", "metadata": {"page_label": "12", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "94961ec14d017ab8574c41df370e9a469182916de3e54440c06507436f4a4316", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b48dd126-66bd-4a60-92fd-2706d071597c", "node_type": "1", "metadata": {"page_label": "12", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "361a15d902a2b17b3a7eee9b363af2b9c044fe3d6eef56c3e1af59670f98a05e", "class_name": "RelatedNodeInfo"}}, "text": "InInternational Conference on Learning Representations , 2021.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\nComputing Surveys , 55(12):1\u201338, 2023.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly)\nknow what they know. arXiv preprint arXiv:2207.05221 , 2022.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language\nmodels struggle to learn long-tail knowledge. In International Conference on Machine Learning ,\npp. 15696\u201315707. PMLR, 2023.\nHamid Karimi, Proteek Roy, Sari Saba-Sadiya, and Jiliang Tang. Multi-source multi-class fake news\ndetection. In Proceedings of the 27th international conference on computational linguistics , pp.\n1546\u20131557, 2018.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui. Realtime qa: What\u2019s the answer right now?\narXiv preprint arXiv:2207.13332 , 2022.\n12", "start_char_idx": 2676, "end_char_idx": 3874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a71b6d0d-65e6-48c4-982d-94d3b8fcff83": {"__data__": {"id_": "a71b6d0d-65e6-48c4-982d-94d3b8fcff83", "embedding": null, "metadata": {"page_label": "13", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6d03a67-9728-4ea9-b55b-ddda143197e8", "node_type": "4", "metadata": {"page_label": "13", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "30c8c1fa9824aadcf52d5f342d624f6674dd55df0097fab2516a016aa95c0906", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "442bcb98-f995-4117-b069-83cebfec5df8", "node_type": "1", "metadata": {}, "hash": "2c9f535cb89a94380fcab2016362bcf17887b1cc28245d2edc7690ca6ecc0e9e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nZixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual\npre-training of language models. In The Eleventh International Conference on Learning Represen-\ntations , 2022.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. In International Conference on\nLearning Representations , 2019.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\nknowledge-intensive nlp. arXiv preprint arXiv:2212.14024 , 2022.\nAndreas K \u00a8opf, Yannic Kilcher, Dimitri von R \u00a8utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich \u00b4ard Nagyfi, et al. Openassistant\nconversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327 ,\n2023.\nWojciech Kry \u00b4sci\u00b4nski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual\nconsistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , pp. 9332\u20139346, 2020.\nSneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang\nLuong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference.\nInFindings of the Association for Computational Linguistics: EMNLP 2021 , pp. 3577\u20133599, 2021.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\nSimplifying training of large, sparse models. In International Conference on Machine Learning ,\npp. 6265\u20136274. PMLR, 2021.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K \u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt \u00a8aschel, et al. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:\n9459\u20139474, 2020.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke\nZettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. In\nFirst Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022 , 2022.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and\nHannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) , pp. 3154\u20133169, 2022a.\nYixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. Brio: Bringing order to abstractive\nsummarization. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 2890\u20132903, 2022b.\nYujian Liu, Xinliang Frederick Zhang, David Wegsman, Nicholas Beauchamp, and Lu Wang. Politics:\nPretraining with same-story article comparison for ideology prediction and stance detection. In\nFindings of the Association for Computational Linguistics: NAACL 2022 , pp. 1354\u20131374, 2022c.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld.", "start_char_idx": 0, "end_char_idx": 3250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "442bcb98-f995-4117-b069-83cebfec5df8": {"__data__": {"id_": "442bcb98-f995-4117-b069-83cebfec5df8", "embedding": null, "metadata": {"page_label": "13", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6d03a67-9728-4ea9-b55b-ddda143197e8", "node_type": "4", "metadata": {"page_label": "13", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "30c8c1fa9824aadcf52d5f342d624f6674dd55df0097fab2516a016aa95c0906", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a71b6d0d-65e6-48c4-982d-94d3b8fcff83", "node_type": "1", "metadata": {"page_label": "13", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ab1eb6045822aee26ca5495475bbddab6eee2f4b45038a62a27c37d0691ad9e2", "class_name": "RelatedNodeInfo"}}, "text": "3154\u20133169, 2022a.\nYixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. Brio: Bringing order to abstractive\nsummarization. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 2890\u20132903, 2022b.\nYujian Liu, Xinliang Frederick Zhang, David Wegsman, Nicholas Beauchamp, and Lu Wang. Politics:\nPretraining with same-story article comparison for ideology prediction and stance detection. In\nFindings of the Association for Computational Linguistics: NAACL 2022 , pp. 1354\u20131374, 2022c.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld. S2orc: The semantic\nscholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pp. 4969\u20134983, 2020.\nZiyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin\nJiang. Augmented large language models with parametric knowledge guiding. arXiv preprint\narXiv:2305.04757 , 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric\nmemories. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 9802\u20139822, 2023.\n13", "start_char_idx": 2627, "end_char_idx": 3958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca8b3601-21e2-49f3-aa52-371621e9e4b1": {"__data__": {"id_": "ca8b3601-21e2-49f3-aa52-371621e9e4b1", "embedding": null, "metadata": {"page_label": "14", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "274f6e9a-bb3b-42f6-9eb3-eb72e83f3e96", "node_type": "4", "metadata": {"page_label": "14", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b10d4171910b094107324c8572c869c20705c8fd25b80bc08d610e98759882f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba5d3d58-35e5-49d6-84ae-3f5cca2c346b", "node_type": "1", "metadata": {}, "hash": "02571d6071e06ecec927234fd6b5ddfad8c6660abc2f0b7e6ae177e5f62ea431", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nSaeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. The Artificial\nIntelligence Review , 42(2):275, 2014.\nMichael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. Advances in\nNeural Information Processing Systems , 35:17703\u201317716, 2022.\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In Artificial intelli-\ngence and statistics , pp. 1273\u20131282. PMLR, 2017.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing\nmemory in a transformer. In The Eleventh International Conference on Learning Representations ,\n2022.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Nonparametric masked language modeling. In Findings of the Association for\nComputational Linguistics: ACL 2023 , pp. 2097\u20132118, July 2023.\nKosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and\nJunji Tomita. Answering while summarizing: Multi-task learning for multi-hop qa with evidence\nextraction. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics , pp. 2335\u20132345, 2019.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. Understanding factuality in abstractive\nsummarization with frank: A benchmark for factuality metrics. In Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies , pp. 4812\u20134829, 2021.\nThomas Pellissier Tanon, Gerhard Weikum, and Fabian Suchanek. Yago 4: A reason-able knowledge\nbase. In The Semantic Web: 17th International Conference, ESWC 2020, Heraklion, Crete, Greece,\nMay 31\u2013June 4, 2020, Proceedings 17 , pp. 583\u2013596. Springer, 2020.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese,\nNat McAleese, and Geoffrey Irving. Red teaming language models with language models. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp.\n3419\u20133448, 2022.\nFabio Petroni, Tim Rockt \u00a8aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) , pp. 2463\u20132473, 2019.\nJonas Pfeiffer, Ivan Vuli \u00b4c, Iryna Gurevych, and Sebastian Ruder. Mad-x: An adapter-based framework\nfor multi-task cross-lingual transfer. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pp. 7654\u20137673, 2020.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.\nLifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the\n2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies , pp. 3479\u20133495, 2022.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 ,\n2022.", "start_char_idx": 0, "end_char_idx": 3351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba5d3d58-35e5-49d6-84ae-3f5cca2c346b": {"__data__": {"id_": "ba5d3d58-35e5-49d6-84ae-3f5cca2c346b", "embedding": null, "metadata": {"page_label": "14", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "274f6e9a-bb3b-42f6-9eb3-eb72e83f3e96", "node_type": "4", "metadata": {"page_label": "14", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b10d4171910b094107324c8572c869c20705c8fd25b80bc08d610e98759882f4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca8b3601-21e2-49f3-aa52-371621e9e4b1", "node_type": "1", "metadata": {"page_label": "14", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a966e9c8de96f697fc9fae2a12fd8a7b543eea1404064b0a0dbb5c90e6858ab9", "class_name": "RelatedNodeInfo"}}, "text": "Mad-x: An adapter-based framework\nfor multi-task cross-lingual transfer. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pp. 7654\u20137673, 2020.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.\nLifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the\n2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies , pp. 3479\u20133495, 2022.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 ,\n2022.\nYujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Elle:\nEfficient lifelong pre-training for emerging data. In Findings of the Association for Computational\nLinguistics: ACL 2022 , pp. 2789\u20132810, 2022.\nYujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong\nSun, and Jie Zhou. Recyclable tuning for continual pre-training. arXiv preprint arXiv:2305.08702 ,\n2023.\n14", "start_char_idx": 2628, "end_char_idx": 3787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf6f85cf-e0ad-494d-a582-68a4ebc210db": {"__data__": {"id_": "bf6f85cf-e0ad-494d-a582-68a4ebc210db", "embedding": null, "metadata": {"page_label": "15", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea9614f7-ab93-4cf1-af04-549f91502c03", "node_type": "4", "metadata": {"page_label": "15", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "38e4f267b68dbe23b20641d44efb52e24f31d11bc8f882bad9bab6951b0f7b00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6032904a-65bb-4a63-ba10-0cdb28f1e817", "node_type": "1", "metadata": {}, "hash": "cd7fc87f4eb36ae3448b34dccd059bc67393bd5bcb734d0fe71dcd1eb60d020d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.\nCompressive transformers for long-range sequence modelling. In International Conference on\nLearning Representations , 2019.\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana V olkova, and Yejin Choi. Truth of varying\nshades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017\nconference on empirical methods in natural language processing , pp. 2931\u20132937, 2017.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pp. 5418\u20135426, 2020.\nStephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models.\nAdvances in Neural Information Processing Systems , 34:17555\u201317566, 2021.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies , pp. 2655\u20132671, 2022.\nAlireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large\nlanguage models meet personalization. arXiv preprint arXiv:2304.11406 , 2023.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical\nreasoning abilities of neural models. In International Conference on Learning Representations ,\n2019.\nTal Schuster, Adam Fisch, and Regina Barzilay. Get your vitamin c! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies , pp. 624\u2013643, 2021.\nWeijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. Nearest neighbor zero-shot\ninference. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing , pp. 3254\u20133265, 2022.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint\narXiv:2301.12652 , 2023.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp.\n4222\u20134235, 2020.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised\ncommonsense question answering with self-talk. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , pp. 4615\u20134629, 2020.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber,\nand Lijuan Wang. Prompting gpt-3 to be reliable. In The Eleventh International Conference on\nLearning Representations , 2022.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted\npre-training for language understanding. Advances in Neural Information Processing Systems , 33:\n16857\u201316867, 2020.\nRobyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of\ngeneral knowledge. In Proceedings of the AAAI conference on artificial intelligence , volume 31,\n2017.\nHeiner Stuckenschmidt, Christine Parent, and Stefano Spaccapietra.", "start_char_idx": 0, "end_char_idx": 3532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6032904a-65bb-4a63-ba10-0cdb28f1e817": {"__data__": {"id_": "6032904a-65bb-4a63-ba10-0cdb28f1e817", "embedding": null, "metadata": {"page_label": "15", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea9614f7-ab93-4cf1-af04-549f91502c03", "node_type": "4", "metadata": {"page_label": "15", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "38e4f267b68dbe23b20641d44efb52e24f31d11bc8f882bad9bab6951b0f7b00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf6f85cf-e0ad-494d-a582-68a4ebc210db", "node_type": "1", "metadata": {"page_label": "15", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ed68ee878fdabd4c068855f39f12e039f95a7ef9307b43af992ed7b5be51859b", "class_name": "RelatedNodeInfo"}}, "text": "Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber,\nand Lijuan Wang. Prompting gpt-3 to be reliable. In The Eleventh International Conference on\nLearning Representations , 2022.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted\npre-training for language understanding. Advances in Neural Information Processing Systems , 33:\n16857\u201316867, 2020.\nRobyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of\ngeneral knowledge. In Proceedings of the AAAI conference on artificial intelligence , volume 31,\n2017.\nHeiner Stuckenschmidt, Christine Parent, and Stefano Spaccapietra. Modular ontologies: concepts,\ntheories and techniques for knowledge modularization , volume 5445. Springer, 2009.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\nmodels. In The Eleventh International Conference on Learning Representations , 2022.\n15", "start_char_idx": 2848, "end_char_idx": 3827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "864e301d-713f-4867-95a5-a0f6e9ee2020": {"__data__": {"id_": "864e301d-713f-4867-95a5-a0f6e9ee2020", "embedding": null, "metadata": {"page_label": "16", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8daebd54-b463-4982-a1d9-1c7d1cf947f0", "node_type": "4", "metadata": {"page_label": "16", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9873ad7bac86b511291ce3b6f0aef19fe60812790202ef0c2884d145141e4fef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bebb5bd7-84ee-4517-a77b-5f8cd1773225", "node_type": "1", "metadata": {}, "hash": "8e62ae2a53ce7bbacb5af55baf58943efcf0a18ac37b18047af645e6dded7817", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nYi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Gupta, et al. Transformer memory as a differentiable search index. Advances in Neural\nInformation Processing Systems , 35:21831\u201321843, 2022.\nDenny Vrande \u02c7ci\u00b4c and Markus Kr \u00a8otzsch. Wikidata: a free collaborative knowledgebase. Communica-\ntions of the ACM , 57(10):78\u201385, 2014.\nHeng Wang, Wenqian Zhang, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Qinghua Zheng, and\nMinnan Luo. Detecting spoilers in movie reviews with external movie knowledge and user\nnetworks. arXiv preprint arXiv:2304.11411 , 2023.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian\nTang. Kepler: A unified model for knowledge embedding and pre-trained language representation.\nTransactions of the Association for Computational Linguistics , 9:176\u2013194, 2021.\nYujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Cheng-\nmin Chi, Guoshuai Zhao, Zheng Liu, et al. A neural corpus indexer for document retrieval.\nAdvances in Neural Information Processing Systems , 35:25600\u201325614, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances\nin Neural Information Processing Systems .\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language models to\ncommonsense models. In Proceedings of the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies , pp. 4602\u20134625,\n2022.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R \u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R \u00b4emi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 conference on empirical methods in\nnatural language processing: system demonstrations , pp. 38\u201345, 2020.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model\nsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing\ninference time. In International Conference on Machine Learning , pp. 23965\u201323998. PMLR,\n2022.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong\ncontext generators. In The Eleventh International Conference on Learning Representations , 2022.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers) , pp. 1\u20139, 2022.", "start_char_idx": 0, "end_char_idx": 3329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bebb5bd7-84ee-4517-a77b-5f8cd1773225": {"__data__": {"id_": "bebb5bd7-84ee-4517-a77b-5f8cd1773225", "embedding": null, "metadata": {"page_label": "16", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8daebd54-b463-4982-a1d9-1c7d1cf947f0", "node_type": "4", "metadata": {"page_label": "16", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9873ad7bac86b511291ce3b6f0aef19fe60812790202ef0c2884d145141e4fef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "864e301d-713f-4867-95a5-a0f6e9ee2020", "node_type": "1", "metadata": {"page_label": "16", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ae8cf0827f6a89d6ee6947ec13ee40a2b067924b5017bfcdbc764d63b19f60b5", "class_name": "RelatedNodeInfo"}}, "text": "Model\nsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing\ninference time. In International Conference on Machine Learning , pp. 23965\u201323998. PMLR,\n2022.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong\ncontext generators. In The Eleventh International Conference on Learning Representations , 2022.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers) , pp. 1\u20139, 2022.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. Defending against neural fake news. Advances in neural information processing\nsystems , 32, 2019.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In International Conference on Machine Learning ,\npp. 11328\u201311339. PMLR, 2020.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068 , 2022.\n16", "start_char_idx": 2577, "end_char_idx": 3991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "753f7fae-6a91-449d-8c0d-eeb6403b6166": {"__data__": {"id_": "753f7fae-6a91-449d-8c0d-eeb6403b6166", "embedding": null, "metadata": {"page_label": "17", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "323ca365-962f-4ade-9206-f8d36b1f015e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7a6a3759f3bc0f9e8bfe324172c064e6ee0e508dcfdd8e53bde19c8761f37ffd", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Man-\nning, and Jure Leskovec. Greaselm: Graph reasoning enhanced language models. In International\nconference on learning representations , 2021.\nXinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, and Jianshu Chen.\nThrust: Adaptively propels large language models with external knowledge. arXiv preprint\narXiv:2307.10442 , 2023.\nZexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp.\n5657\u20135673, 2022.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer\nvision , pp. 19\u201327, 2015.\nShengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin\nJiang. Bridging the gap between indexing and retrieval for differentiable search index with query\ngeneration. arXiv preprint arXiv:2206.10128 , 2022.\n17", "start_char_idx": 0, "end_char_idx": 1222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c96dc8df-5a3f-46b8-999c-f462deb80a0a": {"__data__": {"id_": "c96dc8df-5a3f-46b8-999c-f462deb80a0a", "embedding": null, "metadata": {"page_label": "18", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6b915b-9782-4c2d-bcf7-8cde5e59aec6", "node_type": "4", "metadata": {"page_label": "18", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9a10f142bfdc7c7ba06bb2928f675931536308ff8d3e41dc63625124bae79e6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bc35726-2f44-4194-9b34-ce4fdcf862e2", "node_type": "1", "metadata": {}, "hash": "73b93470bebcd619775b3ed185473e3bdb065217bc1587a02989c1ffcd985428", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA D ISCUSSION\nModularity at every turn. All components in KNOWLEDGE CARD are modular and easily substi-\ntuted with future state-of-the-art. 1) While Codex is the default LLM in the experiments, KNOWL -\nEDGE CARD also works with TEXT -DAVINCI -003 and GPT-3.5- TURBO (\u00a75) and could be easily\nadapted to future LLMs. 2) If better models for embedding space similarity, abstractive summa-\nrization, and fact-checking are developed, the three knowledge selectors ( \u00a72.2) could be seamlessly\nupdated. 3) When new knowledge, information, and domains emerge, more knowledge cards could\nbe trained and uploaded to a model-sharing infrastructure (Wolf et al., 2020) by any member of the\nmachine learning community and adopted to improve general-purpose LLMs.\nUser-centric LLM adaptation. When general-purpose LLMs are released, everyone uses the\nsame LLM with the same API calls, while real-world users have heterogeneous use cases and\nexpectations that require personalization (Salemi et al., 2023). For example, grade school students\nmight expect LLMs to be absolutely factual about knowledge and information in common textbooks,\nNLP researchers might expect LLMs to have a basic understanding of current NLP research, cooking\namateurs might expect LLMs to understand the basic recipes and cuisines for different occasions,\nand more. As a result, KNOWLEDGE CARD presents a preliminary approach by letting the user select\nand activate knowledge cards to empower LLMs with different skill sets and domain expertise.\nCompatible with diversified forms of knowledge. By default, KNOWLEDGE CARD employs\nlanguage models trained on varying domains and corpora as modular knowledge sources. In addition,\nKNOWLEDGE CARD is also compatible with 1) retrieval systems, where the retrieved text could\nsimilarly go through the three knowledge selectors and enrich LLM context, while retrieval corpora\nare harder to share and use than modular language models; 2) knowledge graphs, when combined with\nvarious proposals to construct natural language corpora out of symbolic knowledge bases (Agarwal\net al., 2021; Chen et al., 2020; Feng et al., 2023a), which is already included in our prototype; 3)\nsearch engines, where content on the web could also be integrated into the black-box LLMs through\nKNOWLEDGE CARD. Such flexibility and compatibility are possible since KNOWLEDGE CARD\nconducts knowledge integration through natural language. Compared to retrieval models, using\nlanguage models as knowledge sources enables flexible information seeking (rather than rigid token\nexact match), searching over knowledge domains, and employing private knowledge sources.\nKnowledge cards heterogeneity. While existing modular LM proposals often require modular\nsub-models to be of the same size and architecture for parameter averaging and model fusion (Li\net al., 2022), the knowledge cards in this work could be fully heterogeneous. 1) Different knowledge\ncards could have different sizes. While OPT-1.3B is adopted as the default architecture in this work,\nother sizes of OPT , from 125M all the way up to tens of billions, could all be used to initialize\nknowledge cards. In addition, 2) knowledge cards could have different model architectures. Since the\nintegration of general-purpose LLMs and modular knowledge cards happens at the natural language\nlevel, any language generation models could be adopted as knowledge cards. These two levels of\nheterogeneity allow for flexibility in knowledge card training: larger and more capable models could\nbe trained on large corpora and extensive knowledge domains by compute-rich individuals, while\nsmaller knowledge cards trained on small and dedicated domains by computationally underprivileged\nresearchers could also help improve black-box LLMs, democratizing LLM research.\nKnowledge cards hierarchy. We believe that knowledge cards could reflect the hierarchical nature\nof knowledge. If KNOWLEDGE CARD is adopted for general question answering, then a general\nbiomedical knowledge card trained on PubMed corpus would suffice. However, if KNOWLEDGE\nCARD is adopted for more fine-grained use cases, the \u201cbiomedical\u201d domain could be further divided\ninto sub-domains and one knowledge card could be trained for each. Similar divisions could be\napplied to sub-fields in NLP research, political news in different countries, and more.\nCombining bottom-up and top-down.", "start_char_idx": 0, "end_char_idx": 4425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bc35726-2f44-4194-9b34-ce4fdcf862e2": {"__data__": {"id_": "3bc35726-2f44-4194-9b34-ce4fdcf862e2", "embedding": null, "metadata": {"page_label": "18", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6b915b-9782-4c2d-bcf7-8cde5e59aec6", "node_type": "4", "metadata": {"page_label": "18", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9a10f142bfdc7c7ba06bb2928f675931536308ff8d3e41dc63625124bae79e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c96dc8df-5a3f-46b8-999c-f462deb80a0a", "node_type": "1", "metadata": {"page_label": "18", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "81c3edcc8d58f0b65b8f65aa6d7923ed834a839b06143ea9ff70d543bdcad21a", "class_name": "RelatedNodeInfo"}}, "text": "These two levels of\nheterogeneity allow for flexibility in knowledge card training: larger and more capable models could\nbe trained on large corpora and extensive knowledge domains by compute-rich individuals, while\nsmaller knowledge cards trained on small and dedicated domains by computationally underprivileged\nresearchers could also help improve black-box LLMs, democratizing LLM research.\nKnowledge cards hierarchy. We believe that knowledge cards could reflect the hierarchical nature\nof knowledge. If KNOWLEDGE CARD is adopted for general question answering, then a general\nbiomedical knowledge card trained on PubMed corpus would suffice. However, if KNOWLEDGE\nCARD is adopted for more fine-grained use cases, the \u201cbiomedical\u201d domain could be further divided\ninto sub-domains and one knowledge card could be trained for each. Similar divisions could be\napplied to sub-fields in NLP research, political news in different countries, and more.\nCombining bottom-up and top-down. One straightforward way to combine the two knowledge\nintegration approaches would be: in each step of top-down, the LLM proposes multiple knowledge\ncards as candidates, then employs the bottom-up approach with the pool of these knowledge cards for\nknowledge generation. We leave further explorations to future work.\n18", "start_char_idx": 3443, "end_char_idx": 4744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b135669-519e-4e2d-9cf4-cde3630957ca": {"__data__": {"id_": "7b135669-519e-4e2d-9cf4-cde3630957ca", "embedding": null, "metadata": {"page_label": "19", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf712e6d-2c81-4649-9a80-adeaa87cc907", "node_type": "4", "metadata": {"page_label": "19", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "136f8d6c41a6650674ba6f54939c5e5aeff8657b7940e421414f993d0322c532", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nB L IMITATIONS\nKnowledge cards are not perfect knowledge generators. While knowledge cards could be of any\nsize or model architecture, we used OPT-1.3B , a relatively small language model to initialize knowl-\nedge cards trained on different domains and sources. As a result, not all of the generated knowledge\ndocuments are high-quality knowledge statements, occasionally suffering from degeneration, topic\ndeviation, and more. While the three knowledge selectors in part alleviate the impact of low-quality\ngenerated knowledge documents, we hypothesize that improving the knowledge generation of autore-\ngressive language models is an important, yet orthogonal, research question for future work. Two\npotential solutions include 1) increasing the model size of knowledge cards and 2) using specialized\ntraining objectives for knowledge cards, while both approaches require additional training and more\ncomputational resources. In addition, Appendix A discussed KNOWLEDGE CARD\u2019s compatibility\nwith diverse knowledge sources, including retrieval, knowledge graphs, and search engines, while\nthese knowledge repositories have their respective pros and cons. We leave it to future work on\nintegrating multiple types of external knowledge stores to extend K NOWLEDGE CARD.\nThe factuality selector is biased towards information-rich domains and existing knowledge.\nTo ensure the factuality of generated knowledge documents, we employed a retrieval-augmented\nfactuality selector based on both summarization factuality metrics and fact-checking models while\nenabling flexibility through our proposed top-k factuality sampling . However, domains with more\nWikipedia entries might be better supported by retrieved documents and might receive higher factuality\nscores. In addition, new and emerging knowledge might be well supported by existing retrieval\ncorpora and receive low factuality scores. We quantitatively evaluate this bias in Appendix D.\nAlthough top-k factuality sampling enables flexibility to some extent, it remains an important problem\nto design factuality evaluation measures that are generalizable and adaptable to varying and emerging\ndomains.\nPrompting LLMs to seek help through yes/no questions is not perfect. Inspired by the findings\nthat LLMs do not need external knowledge for every query (Zhao et al., 2023) and language models\n(mostly) know what they know (Kadavath et al., 2022), we propose to ask yes/no questions to\ndecide whether to activate knowledge cards and encourage well-calibrated answers through in-\ncontext learning. Our analysis ( \u00a75) shows that this strategy is effective but far from perfect: LLMs\nare occasionally over-confident about their knowledge capabilities. We leave it to future work on\ndesigning better strategies for LLMs to abstain, acknowledge knowledge limitations, and seek help\nfrom external information sources.\nC E THICS STATEMENT\nWe envision KNOWLEDGE CARD as a community-driven and collaborative initiative to improve\ngeneral-purpose LLMs in knowledge-intensive tasks and contexts. An important risk is the dual use\nand exploitation from malicious actors. Since modular knowledge cards have the ability to change\nor update LLM knowledge, malicious actors could advance their agenda by submitting malicious\nknowledge cards trained on disinformation, hyperpartisan content, propaganda, and more, while\nframing them as benign knowledge domains and deceive LLM users. We envision two lines of\napproaches towards this ethical risk: on the technical side, research on adversarial manipulation of\nlanguage models and corresponding defense tactics (Bagdasaryan & Shmatikov, 2022; Perez et al.,\n2022) could be integrated to alleviate the impact of malicious knowledge cards; on the social side, we\ncould rely on and reinforce the existing rules for model sharing on popular infrastructures (Wolf et al.,\n2020) to prevent such malicious contribution from happening. We encourage the responsible use of\nKNOWLEDGE CARD, while we also call on users and researchers to be mindful of this dual-use risk.\nD A NALYSIS (CONT .)\nKnowledge Card Selection In the top-down approach, we ask large language models to choose\nrelevant knowledge cards and obtain external knowledge. We illustrate the selection results of\nthe automatic selection strategy on the MMLU dataset separated into the 57 sub-tasks. Figure 8\ndemonstrates that for most tasks knowledge selection exhibits spike-like patterns on Wikipedia\n19", "start_char_idx": 0, "end_char_idx": 4486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "157efbf8-57ff-4a00-a7c5-3e8e30d49ac9": {"__data__": {"id_": "157efbf8-57ff-4a00-a7c5-3e8e30d49ac9", "embedding": null, "metadata": {"page_label": "20", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d9e2197-d5f3-4c00-bde0-b01b92c9ed46", "node_type": "4", "metadata": {"page_label": "20", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f638e71eb9e3b71ff28c34afb60e91b3fa48a2003db6083df20bf012170091f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b669c2e-231d-4741-b03e-3e8944fda8b4", "node_type": "1", "metadata": {}, "hash": "adb7e403cbd6378296a55c3e1428c84f44d1d5d95f705e134657f3845a7aa238", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\ncorpora and encyclopedic knowledge graphs, suggesting that the majority of tasks have a few clearly\nrelevant knowledge cards. In addition, for other tasks (e.g. juris prudence and high school U.S.\nhistory), it is not clear which knowledge cards would be most helpful, thus the selection is more\nspread-out. These results suggest that the selection patterns could also indicate whether a new and\nmore in-topic knowledge card is needed for any given task.\n3 yes 2 yes, 1 no 2 no, 1 yes 3 no0200400600800100012001400\nFigure 7: Yes/No questions in\ntop-down are mostly consis-\ntent across three prompt tem-\nplates, while there is space for\nimprovement in future work.Yes/No Template Sensitivity In the top-down approach, we\nprompt LLMs with \u201c Do you need more information? (Yes/No) \u201d to\nidentify if external knowledge is required and use in-context learning\nto encourage well-calibrated responses. Since language models are\nsensitive to minor changes in prompts, we devise two more questions:\n\u201cIs more information needed here? \u201d and \u201c Would you like additional\ninformation? \u201d and report the results on the 2-way misinformation\ntask in Figure 7. It is demonstrated that LLMs give moderately\nconsistent responses: 79.9% of cases received unanimous yes or\nno from the three prompts, while 20.1% examples received mixed\nresults. This suggests that a potential improvement to KNOWLEDGE\nCARD is to employ multiple yes/no questions to probe knowledge\nlimitations and use an ensemble of answers to improve robustness.\nFactuality Scores of Knowledge Cards We use the MMLU\ndatasets as queries to prompt different knowledge cards, generate\nknowledge documents, and evaluate their factuality with the factu-\nality selector ( \u00a72.2). We illustrate the factuality score distributions\nof different knowledge cards in Figure 9, which shows that different knowledge cards have varying\ninherent factuality. We hypothesize that the distribution of factuality scores given by the factuality\nselector could guide efforts to evaluate the quality of community-contributed knowledge cards.\nLM Training Inference Stage\nHyperparameter Value Hyperparameter Value\nLEARNING RATE 2e-5 n1 3\nWEIGHT DECAY 1e-5 n2 5\nMAX EPOCHS 10 n3 3\nBATCH SIZE 32 MAX ITERATION 1\nOPTIMIZER ADAM TEMPERATURE 0.1\nADAM EPSILON 1e-6 DEFAULT LLM CODEX\nADAM BETA 0.9,0.98\nWARMUP RATIO 0.06\nTable 6: Hyperparameter settings.Knowledge Card Accumulation We expect\nKNOWLEDGE CARD to perform better when\nrelevant knowledge cards are gradually added\nto the system. To this end, we gradually add\nfive knowledge cards (PubMed, IMDB, Book-\nCorpus, News, and Wikipedia) to KNOWLEDGE\nCARD and evaluate performance with the misin-\nformation dataset, 2-way setting, bottom-up ap-\nproach, and the ChatGPT model. Table 7 demon-\nstrates that the addition of knowledge cards, es-\npecially in-domain ones (News in this case), is indeed helpful in improving the base large language\nmodel.\nSetting # card BAcc MaF\nVANILLA 0 80.1 70.5\n+ PUBMED 1 80.7 70.6\n+ IMDB 2 80.6 71.2\n+ B OOK CORPUS 3 82.3 72.9\n+ N EWS 4 85.7 73.1\n+ W IKIPEDIA 5 76.5 75.3\nTable 7: KNOWLEDGE CARD performance\non the two-way misinformation dataset when\nfive knowledge cards are gradually added.Working Examples We present the specific\nprompts, generated knowledge documents, and\nprompts for the bottom-up approach, and the top-\ndown approach with automatic and explicit selection\nin Tables 9, 10, and 11 respectively.\nE E XPERIMENT DETAILS\nAlgorithm Details We present an algorithmic sum-\nmary of the bottom-up andtop-down approach in\nAlgorithm 1 and 2.", "start_char_idx": 0, "end_char_idx": 3590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b669c2e-231d-4741-b03e-3e8944fda8b4": {"__data__": {"id_": "6b669c2e-231d-4741-b03e-3e8944fda8b4", "embedding": null, "metadata": {"page_label": "20", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d9e2197-d5f3-4c00-bde0-b01b92c9ed46", "node_type": "4", "metadata": {"page_label": "20", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f638e71eb9e3b71ff28c34afb60e91b3fa48a2003db6083df20bf012170091f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "157efbf8-57ff-4a00-a7c5-3e8e30d49ac9", "node_type": "1", "metadata": {"page_label": "20", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8fe151582d6733f56c7ba92d3ed74ef2f721c03af8e8eb3db8ad3e4fba44982c", "class_name": "RelatedNodeInfo"}}, "text": "E E XPERIMENT DETAILS\nAlgorithm Details We present an algorithmic sum-\nmary of the bottom-up andtop-down approach in\nAlgorithm 1 and 2.\nKnowledge Cards Domains We train a total of 25 knowledge cards from the following corpora\nand domains: one billion tokens (Chelba et al., 2013), ACL papers (Lo et al., 2020), commonsense\nknowledge graph ATOMIC (West et al., 2022), book corpus (Zhu et al., 2015), ConceptNet (Speer\net al., 2017), biomedical knowledge graph UMLS (Zhang et al., 2021), Gutenberg (Rae et al., 2019),\n20", "start_char_idx": 3455, "end_char_idx": 3973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "257ac7af-4f9c-4a66-945c-09eda6737d81": {"__data__": {"id_": "257ac7af-4f9c-4a66-945c-09eda6737d81", "embedding": null, "metadata": {"page_label": "21", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b7ee8e3-6316-414f-a92f-6e308c079892", "node_type": "4", "metadata": {"page_label": "21", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "331962fdfa0123ae343d232e1b1ce63974df4dcaa8fe73e09ec2b1896d93c82e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n//open-book setting\nQuestion : Who won the senate race of Oregon in the 2022 U.S. midterm elections?\nAnswer : Ron Wyden\n//two-way setting\nQuestion : Who won the 24th congressional district of Texas in the 2022 U.S. midterm elections?\nA. Jan McDowell B. Beth Van Duyne\nAnswer : B\n//four-way setting\nQuestion : Who won the 6th congressional district of Pennsylvania in the 2022 U.S. midterm elections?\nA. Christopher Hoeppner B. Doug Mastriano C. Chrissy Houlahan D. Guy Ciarrocchi\nAnswer : C\nTable 8: Examples of the MidtermQA dataset for the three settings.\nIMDB movie reviews (Wang et al., 2023), political knowledge graph KGAP (Feng et al., 2021),\nlegal contracts (Hendrycks et al., 2021), math problems (Saxton et al., 2019), midterm election news\n(ours), open subtitles2, political news corpora POLITICS (Liu et al., 2022c), biomedical literature3,\nRealNews (Zellers et al., 2019), Reddit (Feng et al., 2023b), Twitter (Feng et al., 2022), Wikidata\nknowledge graph (Vrande \u02c7ci\u00b4c & Kr \u00a8otzsch, 2014), Wikipedia dump4, YAGO (Pellissier Tanon et al.,\n2020), and Yelp reviews5. For knowledge graphs, we follow Feng et al. (2023a) to construct textual\ncorpora and use them as training data.\nHyperparameters We present hyperparameter settings in Table 6.\nDataset Details 1) The MMLU dataset (Hendrycks et al., 2020) contains a total of 15,908 four-\nchoice questions further divided into 57 sub-tasks in four domains: humanities, social sciences,\nSTEM, and others. The official dataset also provides a demonstration set, i.e.5-shot examples in each\nsub-task to enable few-shot in-context learning. We follow the official demonstration set and test set\nin our experiments. 2) The LUN dataset (Rashkin et al., 2017) is a misinformation detection dataset\nwith two- or four-way classification settings, either with true/false only or fine-grained categories\nof trusted, hoax, propaganda, or satire. We use the official test set in (Hu et al., 2021) with 2,999\nexamples throughout the experiments. 3) We curate MidtermQA , a QA dataset focusing on the 2022\nU.S. midterm elections to evaluate KNOWLEDGE CARD\u2019s ability for temporal knowledge update.\nSpecifically, we first collect the results of the 510 races in congressional, senate, or gubernatorial\nelections in the 2022 midterms. We then construct three datasets: a) open-book, where we ask LLMs\nto directly answer the name of the election winner for a given race, b) two-way, where we ask LLMs\nto choose the winner from the two front runners, and c) four-way, where we increase the difficulty\nby including two other politicians contesting in the same state to create a distraction. We present\nexamples of the MidtermQA dataset in Table 8.\nComputation Resources Details We used a GPU cluster with 16 NVIDIA A40 GPUs, 1988G\nmemory, and 104 CPU cores for the experiments. Training knowledge cards took from around 7 hours\nto 10 days depending on the training corpora size. For the black-box LLMs, we used the OpenAI\nAPI to access CODE -DAVINCI -002, TEXT -DAVINCI -003, and GPT-3.5- TURBO in the experiments.\n2https://github.com/sdtblck/Opensubtitles_dataset\n3https://github.com/thoppe/The-Pile-PubMed\n4https://github.com/noanabeshima/wikipedia-downloader\n5https://www.yelp.com/dataset\n21", "start_char_idx": 0, "end_char_idx": 3276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4ca3a22-fd73-4983-b6ab-727580b982a5": {"__data__": {"id_": "e4ca3a22-fd73-4983-b6ab-727580b982a5", "embedding": null, "metadata": {"page_label": "22", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11673d4b-a2ae-45f7-8a68-196580f007e8", "node_type": "4", "metadata": {"page_label": "22", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2f0413bd7ef95507d24b94c089f3e9a9bc4824353a011ae132f4644e7603a3fe", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n\u00a1in-context examples with the same format\u00bf\n...\nKnowledge :. . . San Mateo is located in the northwest of California . . . Dianne Feinstein, the senior\nsenator from California, is rumored to retire . . . Tom Brady returned to his hometown of San Mateo . . .\nQuestion : Who is the senior senator from Tom Brady\u2019s birth place?\nAnswer :\nTable 9: Prompt example for the bottom-up approach. Different color boxes indicate knowledge\ndocuments generated by different specialized LMs.\n\u00a1in-context examples with the same format\u00bf\n...\nQuestion : Who is the senior senator from Tom Brady\u2019s birth place?\nDo you need more information? (Yes or No)\nYes\nWhat kind of information do you need?\nThe state Tom Brady is from.\nKnowledge :Tom Brady returned to his hometown of San Mateo, CA . . .\nDo you need more information? (Yes or No)\nNo\nAnswer :\nTable 10: Prompt example for the top-down approach with automatic selection. Italic texts indicate\nthat this field is generated by black-box LLMs.\n\u00a1in-context examples with the same format\u00bf\n...\nQuestion : Who is the senior senator from Tom Brady\u2019s birth place?\nDo you need more information? (Yes or No)\nYes\nChoose an information source from the following: sports, biomedical literature, NLP papers, book corpus.\nsports\nKnowledge :Tom Brady returned to his hometown of San Mateo, CA . . .\nDo you need more information? (Yes or No)\nNo\nAnswer :\nTable 11: Prompt example for the top-down approach with explicit selection. Italic texts indicate that\nthis field is generated by black-box LLMs.\n22", "start_char_idx": 0, "end_char_idx": 1561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7709badd-9fa9-405b-8c02-9363a1e02f0f": {"__data__": {"id_": "7709badd-9fa9-405b-8c02-9363a1e02f0f", "embedding": null, "metadata": {"page_label": "23", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ea5e683-6a8d-4e5b-9ba6-824b60f08dfc", "node_type": "4", "metadata": {"page_label": "23", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a1681206d317d8304375103c13d482d5401e1ba28520c036865f6f1fceeda5b7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1: Bottom-Up Approach\nData: question q; in-context examples prompt sicl; knowledge cards C={spec1, . . . , specn};\nrelevance, pruning, and factuality selector \u03d5rel,\u03d5prune,\u03d5fact\nResult: answer string sans\nPROMPT =sicl\nKNOWLEDGE LIST = []\nforspec\u2208 Cdo\nKNOWLEDGE LIST.append( spec(q,n1))\nend\nKNOWLEDGE LIST =\u03d5rel(q,KNOWLEDGE LIST,n2)\nKNOWLEDGE LIST =\u03d5prune(KNOWLEDGE LIST)\nKNOWLEDGE LIST =\u03d5fact(KNOWLEDGE LIST,n3)\nPROMPT += \u201cKnowledge: \u201d\nfors\u2208KNOWLEDGE LIST do\nPROMPT +=s\nend\nPROMPT += \u201cQuestion: \u201d +q+ \u201cAnswer: \u201d\nsans=LLM (PROMPT )\nreturn sans\nAlgorithm 2: Top-Down Approach\nData: question q; in-context examples prompt sicl; knowledge cards C={spec1, . . . , specn};\nknowledge card names S={s1, . . . , sn}; max trial k; relevance and factuality selector\n\u03d5rel;\u03d5fact; binary flags AUTO and EXP\nResult: answer string sans\nPROMPT =sicl\nPROMPT += \u201cQuestion: \u201d +q\ni= 0\nwhile i\u2264kdo\nPROMPT += \u201cDo you need more information? (Yes or No) \u201d\nRESPONSE =LLM (PROMPT )\nifRESPONSE == \u201cYes\u201d then\nifAUTO then\nPROMPT += \u201cWhat kind of information do you need? \u201d\nRESPONSE =LLM (PROMPT )\nspec =\u03d5rel(RESPONSE ,{s1, . . . , sn},TOP-K= 1)\nend\nifEXP then\nPROMPT += \u201cChoose an information source from the following: \u201d\nfors\u2208 Sdo\nprompt += s\nend\nspec =LLM (PROMPT )\nend\nKNOWLEDGE =\u03d5fact(spec( q, n1),1)\nPROMPT += \u201cKnowledge: \u201d + KNOWLEDGE\nend\nifRESPONSE == \u201cNo\u201d then\nbreak\nend\nend\nPROMPT += \u201cAnswer: \u201d\nsans=LLM (PROMPT )\nreturn sans\n23", "start_char_idx": 0, "end_char_idx": 1461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21274e41-79a6-4097-9d76-8904db3110d0": {"__data__": {"id_": "21274e41-79a6-4097-9d76-8904db3110d0", "embedding": null, "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41d884ed-0515-4ce6-971f-748c36f2619f", "node_type": "4", "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "44d64f2e07ce959a7b44e20e6c1755f0102013595dab45fc2a3c169307324442", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3dd24e6-10cd-4e9c-9993-3dc2149dc6d5", "node_type": "1", "metadata": {}, "hash": "9754652d209aa2a5d71c2f5e5ebad108169299f69155b161a6cc32415dd5db06", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\none billion tokensACL papersatomicbook corpusConceptNetbiomedical KGgutenbergIMDBpolitical KGlegal contractsmathopensubtitlesPOLITICSpubmedrealnews_1realnews_2realnews_3realnews_4reddittwitterwikidatayagoyelpWikipediaelectrical engineering\nmanagement\ncollege CS\nnutrition\nmedical genetics\nhigh school european history\ncomputer security\ncollege biology\nhigh school biology\nhigh school microeconomics\nhigh school world history\ncollege physics\nhigh school geography\nclinical knowledge\nhigh school macroeconomics\nmarketing\nsecurity studies\nhuman aging\nhigh school CS\nelementary mathematics\nconceptual physics\nanatomy\nglobal facts\nus foreign policy\ncollege chemistry\nlogical fallacies\nsociology\nhigh school us history\nmiscellaneous\ncollege medicine\nvirology\njurisprudence\nprehistory\nbusiness ethics\ninternational law\nhigh school statistics\nhigh school psychology\nprofessional psychology\nprofessional accounting\nastronomy\nhigh school mathematics\nprofessional medicine\neconometrics\nhigh school government and politics\nmachine learning\nmoral disputes\nhigh school physics\nphilosophy\nformal logic\nhuman sexuality\nworld religions\ncollege mathematics\npublic relations\nabstract algebra\nhigh school chemistry0.71.40.713.8 1.40.00.00.00.70.00.70.01.40.70.00.70.70.00.00.00.062.8 0.713.8\n0.01.00.01.01.91.01.91.90.01.01.01.90.01.91.01.90.01.00.00.01.949.5 0.030.1\n0.01.00.010.017.0 1.00.00.01.03.01.00.00.03.00.00.01.00.01.01.01.051.0 1.07.0\n0.30.30.30.70.71.30.30.00.00.30.00.70.00.00.30.30.00.70.30.00.361.8 0.031.4\n0.00.01.04.03.07.00.00.00.00.00.00.02.01.00.01.02.00.00.01.00.043.0 0.035.0\n0.00.60.00.00.60.01.20.63.00.00.00.01.20.00.01.21.80.01.20.010.338.8 0.039.4\n1.01.01.03.07.02.00.02.03.00.00.01.00.00.03.01.00.00.03.00.02.041.0 3.026.0\n0.70.00.04.90.70.70.00.00.00.70.70.70.00.00.00.00.00.00.00.70.065.3 0.724.3\n0.00.00.33.22.30.30.60.00.60.00.00.60.60.30.30.30.60.30.30.60.070.6 0.317.4\n0.00.40.81.30.01.71.70.40.80.80.00.80.80.41.70.40.00.40.40.80.063.4 0.422.3\n0.41.31.32.50.40.41.30.08.02.10.80.40.80.40.41.31.30.00.80.48.049.8 0.417.3\n1.01.00.028.4 0.00.00.00.00.00.00.01.00.00.00.01.00.00.01.01.00.057.8 1.06.9\n0.50.50.00.51.51.00.00.51.00.51.00.00.00.00.00.50.50.50.00.02.053.0 1.035.4\n0.40.40.84.91.51.50.00.40.00.40.00.00.40.40.00.40.40.40.00.80.", "start_char_idx": 0, "end_char_idx": 2295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3dd24e6-10cd-4e9c-9993-3dc2149dc6d5": {"__data__": {"id_": "c3dd24e6-10cd-4e9c-9993-3dc2149dc6d5", "embedding": null, "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41d884ed-0515-4ce6-971f-748c36f2619f", "node_type": "4", "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "44d64f2e07ce959a7b44e20e6c1755f0102013595dab45fc2a3c169307324442", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21274e41-79a6-4097-9d76-8904db3110d0", "node_type": "1", "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "05700f5c7670bc99a14553a4b5f936a70166a492da1ca7cc4114091650951939", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ee37cd5-d04f-4630-9dcc-6d686857cc43", "node_type": "1", "metadata": {}, "hash": "d7ac66f70587b6d8f6763d8ec7fc37f61398c1fd887e4195df581bab5bebd655", "class_name": "RelatedNodeInfo"}}, "text": "41.31.32.50.40.41.30.08.02.10.80.40.80.40.41.31.30.00.80.48.049.8 0.417.3\n1.01.00.028.4 0.00.00.00.00.00.00.01.00.00.00.01.00.00.01.01.00.057.8 1.06.9\n0.50.50.00.51.51.00.00.51.00.51.00.00.00.00.00.50.50.50.00.02.053.0 1.035.4\n0.40.40.84.91.51.50.00.40.00.40.00.00.40.40.00.40.40.40.00.80.058.9 0.827.5\n0.80.31.02.60.51.00.51.01.31.01.01.00.01.00.81.00.50.81.01.31.063.8 0.815.9\n0.00.90.00.90.91.30.42.60.90.40.90.00.00.40.90.40.91.70.90.40.455.1 3.026.9\n0.80.81.62.01.24.51.61.22.02.01.21.21.21.20.81.60.80.40.42.01.228.6 2.938.4\n0.41.30.42.20.42.20.01.81.32.20.90.90.01.30.00.40.40.40.00.00.957.4 0.024.7\n1.01.00.07.025.0 2.00.00.01.00.00.02.01.02.00.01.02.00.01.04.01.038.0 0.011.0\n1.61.30.35.03.41.91.10.51.10.81.31.30.31.31.11.31.10.81.90.31.362.4 0.87.9\n0.40.00.916.6 0.40.90.40.00.00.40.00.40.41.30.00.00.90.40.00.40.062.6 0.413.2\n0.00.70.01.50.71.50.00.00.00.00.00.00.01.50.00.71.50.70.70.00.077.0 0.712.6\n0.00.00.01.00.01.00.00.05.00.00.00.00.00.00.01.01.01.00.00.04.035.0 0.051.0\n1.00.00.02.00.02.00.01.07.02.03.05.00.00.00.00.03.00.00.00.01.040.0 0.033.0\n0.00.00.040.011.0 0.00.00.00.00.01.00.01.01.00.00.00.01.00.00.00.033.0 0.012.0\n1.20.01.24.31.80.61.20.00.01.80.60.61.81.20.00.60.60.00.61.82.532.5 0.644.2\n0.00.00.01.00.00.50.00.51.51.50.00.51.00.00.50.50.01.00.01.01.021.9 0.067.7\n1.50.00.52.01.01.50.01.521.1 2.90.50.01.51.00.00.50.51.00.50.56.921.1 0.533.8\n0.10.40.32.00.90.30.40.60.81.00.30.40.00.40.30.50.40.00.10.81.055.3 0.", "start_char_idx": 2006, "end_char_idx": 3451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ee37cd5-d04f-4630-9dcc-6d686857cc43": {"__data__": {"id_": "4ee37cd5-d04f-4630-9dcc-6d686857cc43", "embedding": null, "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41d884ed-0515-4ce6-971f-748c36f2619f", "node_type": "4", "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "44d64f2e07ce959a7b44e20e6c1755f0102013595dab45fc2a3c169307324442", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3dd24e6-10cd-4e9c-9993-3dc2149dc6d5", "node_type": "1", "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e820a3949009a3969f50112b9746a6072c252ec74a42a8bcded5063a0f2388ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f80a1fa-a918-4809-bff7-92d2963f9c64", "node_type": "1", "metadata": {}, "hash": "cb7d469108d9ffd8b6f2866349b7a8b61d11a31ea514103f6d61b3285883e2c7", "class_name": "RelatedNodeInfo"}}, "text": "24.31.80.61.20.00.01.80.60.61.81.20.00.60.60.00.61.82.532.5 0.644.2\n0.00.00.01.00.00.50.00.51.51.50.00.51.00.00.50.50.01.00.01.01.021.9 0.067.7\n1.50.00.52.01.01.50.01.521.1 2.90.50.01.51.00.00.50.51.00.50.56.921.1 0.533.8\n0.10.40.32.00.90.30.40.60.81.00.30.40.00.40.30.50.40.00.10.81.055.3 0.333.6\n0.00.60.65.81.70.60.00.00.01.20.60.00.60.00.60.00.00.00.60.00.058.4 0.628.3\n0.60.00.01.21.23.00.61.20.00.60.00.00.00.00.01.80.60.00.01.21.249.4 1.236.1\n0.90.90.91.90.90.00.00.01.920.4 0.90.00.90.00.00.90.00.91.90.00.032.4 0.034.3\n0.30.30.61.50.90.00.60.00.00.00.30.00.30.00.30.30.90.60.00.31.950.9 0.039.8\n2.00.00.01.01.01.03.03.01.04.01.02.01.00.01.00.00.00.00.03.02.026.0 0.048.0\n0.00.00.00.00.00.80.00.01.719.0 1.70.00.00.00.00.00.80.00.00.02.540.5 0.832.2\n0.50.50.52.81.40.50.90.50.90.00.90.50.90.50.00.50.50.50.50.50.068.5 0.018.1\n0.71.30.21.71.31.30.71.10.61.11.10.61.10.90.70.90.21.30.60.60.663.5 0.617.6\n0.50.70.21.30.30.70.30.80.71.80.80.80.30.30.70.00.50.50.30.50.353.3 0.534.0\n0.40.70.70.70.40.00.41.10.43.51.10.71.40.71.11.10.00.40.40.70.469.1 0.414.5\n0.70.72.014.5 1.30.70.70.00.00.00.70.70.70.01.30.00.70.00.70.72.046.1 0.026.3\n1.10.02.216.3 7.40.40.72.61.11.11.51.51.51.90.70.70.40.00.41.51.152.2 1.52.2\n0.00.00.00.00.00.00.00.00.00.40.00.00.00.00.00.00.00.00.00.00.061.8 0.037.9\n0.90.00.03.57.90.90.00.90.91.80.01.81.80.90.90.91.80.00.00.01.862.3 0.011.4\n0.50.00.00.00.00.50.50.517.1 6.20.00.50.50.00.00.00.00.50.00.00.040.4 0.532.", "start_char_idx": 3159, "end_char_idx": 4604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f80a1fa-a918-4809-bff7-92d2963f9c64": {"__data__": {"id_": "8f80a1fa-a918-4809-bff7-92d2963f9c64", "embedding": null, "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41d884ed-0515-4ce6-971f-748c36f2619f", "node_type": "4", "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "44d64f2e07ce959a7b44e20e6c1755f0102013595dab45fc2a3c169307324442", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ee37cd5-d04f-4630-9dcc-6d686857cc43", "node_type": "1", "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d2eeae53653aac16af2740c2bc460a9f0cca0cb5f7274055e6d1a69ca6019b5d", "class_name": "RelatedNodeInfo"}}, "text": "40.40.72.61.11.11.51.51.51.90.70.70.40.00.41.51.152.2 1.52.2\n0.00.00.00.00.00.00.00.00.00.40.00.00.00.00.00.00.00.00.00.00.061.8 0.037.9\n0.90.00.03.57.90.90.00.90.91.80.01.81.80.90.90.91.80.00.00.01.862.3 0.011.4\n0.50.00.00.00.00.50.50.517.1 6.20.00.50.50.00.00.00.00.50.00.00.040.4 0.532.1\n0.90.91.83.625.9 0.00.90.00.90.00.90.00.00.00.00.90.00.90.90.90.050.9 0.09.8\n0.90.60.90.30.31.70.30.61.21.40.30.31.40.30.90.30.90.90.00.61.235.0 0.349.7\n0.01.30.020.5 2.60.00.00.00.00.70.00.70.70.70.00.00.00.00.70.70.067.5 0.04.0\n0.00.60.01.90.30.00.00.01.90.60.00.00.00.61.00.30.30.30.00.30.028.0 0.063.7\n1.61.60.015.1 0.02.40.00.80.83.20.80.81.61.62.40.00.81.60.81.60.848.4 4.09.5\n0.02.30.80.80.03.10.00.00.00.00.00.00.00.00.00.00.01.50.80.80.055.0 0.834.4\n0.60.00.00.00.60.60.00.00.60.60.00.00.00.00.60.00.60.00.01.21.847.4 0.045.6\n0.01.01.015.021.0 0.01.01.00.00.02.03.00.02.00.02.01.00.01.01.00.039.0 0.09.0\n1.80.90.00.92.70.90.92.71.81.81.80.00.90.00.00.00.90.00.91.80.032.7 2.743.6\n4.00.02.011.016.0 0.00.01.02.01.00.02.02.01.01.03.02.01.01.02.00.042.0 2.04.0\n0.50.00.020.2 7.40.00.00.50.00.50.00.00.01.00.00.00.00.00.00.00.064.5 0.05.4\nFigure 8: Statistics of knowledge card selection in the top-down approach with automatic selection\nacross 57 sub-tasks in the MMLU benchmark. Encyclopedic knowledge graph YAGO and Wikipedia\nare generally the most adopted knowledge cards.\n24", "start_char_idx": 4315, "end_char_idx": 5690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "698fd051-49a7-462f-a8c3-9001653f41d2": {"__data__": {"id_": "698fd051-49a7-462f-a8c3-9001653f41d2", "embedding": null, "metadata": {"page_label": "25", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4dd0e45-3619-465c-9e70-09f5a399d0dc", "node_type": "4", "metadata": {"page_label": "25", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "568b604ceeac675fc3438de858855d90f1a801bd23b5089b0f8b141beba58b70", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n0 0.2 0.4 0.6 0.8 1.0pubmed\nyelp\nrealnews_1\nwikipedia\nrealnews_2\nrealnews_3\nrealnews_4\nacl_papers\nmath\n1B\nkgap\ncpnet\ngutenberg\nPOLITICS\nreddit\nIMDB\nddb\nwikidata\nyago\nmidterm\nbookcorpus\nlegal_contracts\natomic\nopensubtitles\ntwitter\nFigure 9: Factuality score distributions of the 25 knowledge cards when prompted with questions in\nthe MMLU benchmark. Different knowledge cards dohave varying factuality score distributions.\n25", "start_char_idx": 0, "end_char_idx": 469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcfda0f0-b0ba-4d04-a510-44c8a8ace777": {"__data__": {"id_": "dcfda0f0-b0ba-4d04-a510-44c8a8ace777", "embedding": null, "metadata": {"page_label": "1", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52d2dbb2-3c59-41e4-95f7-270c6048e0e4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9d2e9083462125f5720d8da5e2a42f26e4a18155fab494228e54e958eb07b440", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nLOFTQ: L ORA-F INE-TUNING -AWARE QUANTIZA -\nTION FOR LARGE LANGUAGE MODELS\nYixiao Li1\u2217Yifan Yu1\u2217Chen Liang1Pengcheng He2\nNikos Karampatziakis2Weizhu Chen2Tuo Zhao1\nABSTRACT\nQuantization is an indispensable technique for serving Large Language Models\n(LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al.,\n2023). In this work we focus on the scenario where quantization and LoRA fine-\ntuning are applied together on a pre-trained model. In such cases it is common\nto observe a consistent gap in the performance on downstream tasks between\nfull fine-tuning and quantization plus LoRA fine-tuning approach. In response,\nwe propose LoftQ ( LoRA-Fine-Tuning-aware Quantization), a novel quantiza-\ntion framework that simultaneously quantizes an LLM and finds a proper low-\nrank initialization for LoRA fine-tuning. Such an initialization alleviates the dis-\ncrepancy between the quantized and full-precision model and significantly im-\nproves generalization in downstream tasks. We evaluate our method on nat-\nural language understanding, question answering, summarization, and natural\nlanguage generation tasks. Experiments show that our method is highly ef-\nfective and outperforms existing quantization methods, especially in the chal-\nlenging 2-bit and 2/4-bit mixed precision regimes. The code is available on\nhttps://github.com/yxli2123/LoftQ .1 2\n1 I NTRODUCTION\nThe advent of Pre-trained Language Models (PLMs) has marked a transformative shift in the field\nof Natural Language Processing (NLP), offering versatile solutions across various applications (He\net al., 2021b; Lewis et al., 2019; Touvron et al., 2023). They have showcased unparalleled profi-\nciency in executing a variety of language tasks, including Natural Language Understanding (NLU)\nand Natural Language Generation (NLG). These models typically have millions or even billions of\nparameters, necessitating substantial computational and memory requirements. However, the exten-\nsive computational and memory demands of these models pose significant challenges, especially for\ndeployments where resources are often constrained and need to be shared among many users.\nTo mitigate the extensive storage requirements of pre-trained models, quantization serves as a piv-\notal compression technique (Zafrir et al., 2019; Shen et al., 2020; Bai et al., 2022; Dettmers et al.,\n2022), converting high-precision numerical values into a discrete set of values. Typically, model\nparameters, originally stored in a 16-bit float format, are transformed into a 4-bit integer format\nthrough quantization, resulting in a substantial 75% reduction in storage overhead. Additionally, to\nfacilitate the adaptation of quantized pre-trained models to downstream tasks efficiently, Low-Rank\nAdaptation (LoRA) is a viable approach (Hu et al., 2021). This technique is a parameter-efficient\nfine-tuning method traditionally applied to high-precision pre-trained models. It is based on the\nhypothesis that the differences between fully fine-tuned weights and pre-trained weights exhibit\nlow-rank properties. This allows these differences to be represented using low-rank matrices. As a\nresult, the original pre-trained weights remain unaltered, with adaptations confined solely to these\nlow-rank matrices, enabling effective task adaptation.\n\u2217Equal contribution\n1Li, Yu, Liang and Zhao are affiliated with Georgia Institute of Technology. Correspondence to\nyixiaoli@gatech.edu ,yyu429@gatech.edu andtourzhao@gatech.edu .\n2He, Karampatziakisand and Chen are affiliated with Microsoft Azure.\n1", "start_char_idx": 0, "end_char_idx": 3602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d60c481-3056-4152-88a0-347c42d85629": {"__data__": {"id_": "1d60c481-3056-4152-88a0-347c42d85629", "embedding": null, "metadata": {"page_label": "2", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ae393ab-75a5-47b0-8ae2-798daeed29a2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ce552842d62aee46d50a48b81f5bfbdfdf791d098094a0f0adfab6c1e6211e7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08f5b37a-d8b7-4b6f-9cdc-052afc6386b7", "node_type": "1", "metadata": {}, "hash": "94dc1938c2322bf3761b87f499795ddd6d4cd70312cc765bd37dadadded51a9a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nWhen quantizing pre-trained models, practitioners often concentrate primarily on the quantization\ntechnique, inadvertently neglecting the importance of subsequent LoRA fine-tuning (Dettmers et al.,\n2023; Diao et al., 2023). For example, QLoRA inherits the fixup initialization (Zhang et al., 2019)\nused in LoRA, which (Dettmers et al., 2023) attaches zero initialized low-rank adapters (see Section\n2.3) to the quantized pre-trained model. The inevitable discrepancy introduced by quantization dur-\ning the approximation of the original high-precision numbers, a scenario particularly pronounced\nin low-bit situations such as the 2-bit regime, can adversely impact the initialization of LoRA fine-\ntuning. As illustrated in Figure 1a, the quantized pre-trained model obtained by QLoRA exhibits\nsevere degradation below the 3-bit level. This deviation in initialization often results in an inferior\nfine-tuning performance. As illustrated in Figure 1b, the fine-tuning performance drops as the quan-\ntization bit decreases when applying QLoRA. Moreover, it is noteworthy that QLoRA fails below\nthe 3-bit level.\nIn this paper, we introduce a novel quantization framework, called LoRA-Fine-Tuning-aware\nQuantization (LoftQ). It is designed specifically for pre-trained models that require quantization\nand LoRA fine-tuning. This framework actively integrates low-rank approximation, working in tan-\ndem with quantization to jointly approximate the original high-precision pre-trained weights. This\nsynergy significantly enhances alignment with the original pre-trained weights as illustrated in Fig-\nure 2. Consequently, our method provides an advantageous initialization point for subsequent LoRA\nfine-tuning, leading to improvements in downstream tasks.\n16 8 4 3 2.5 2.25 2\nNumber of Bits24681012Log of Perplexity\n2.49 2.50 2.53 2.5311.37 11.48 11.50\n(a) Pre-trained LLAMA-2-13b on WikiText-2\n16 8 4 3 2.5 2.25 2\nNumber of Bits0246Log of Perplexity1.63 1.64 1.65 1.652.996.807.19 (b) Fine-tuned LLAMA-2-13b on WikiText-2\nFigure 1: QLoRA performance with different bits. Left: QLoRA initialization of LLAMA-2-13b\non WikiText-2. Right: Apply QLoRA to LLAMA-2-13b on WikiText-2 language modeling task.\nSmaller perplexity indicates better performance.\nWe evaluate our quantization framework by conducting extensive experiments on downstream tasks,\nsuch as NLU, question answering, summarization, and NLG. Experiments show that LoftQ consis-\ntently outperforms QLoRA across all precision levels. For instance, with 4-bit quantization, we\nachieve a 1.1 and 0.8 gain in Rouge-1 for XSum (Narayan et al., 2018) and CNN/DailyMail (Her-\nmann et al., 2015), respectively. LoftQ excels particularly in low-bit scenarios and works effectively\nwith different quantization methods. For example, we achieve over an 8% gain on MNLI (Wang\net al., 2019) and more than 10% on SQuADv1.1 (Rajpurkar et al., 2016) with both 2-bit NormalFloat\nand the 2-bit uniform quantization. We have not seen our approach performs worse than QLoRA.\n2 B ACKGROUND\n2.1 T RANSFORMER MODELS\nA transformer model contains a sequence of layers, where each layer consists of two sub-layers: a\nmulti-head self-attention (MHA) and a fully connected feed forward network (FFN) (Vaswani et al.,\n2017).", "start_char_idx": 0, "end_char_idx": 3296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08f5b37a-d8b7-4b6f-9cdc-052afc6386b7": {"__data__": {"id_": "08f5b37a-d8b7-4b6f-9cdc-052afc6386b7", "embedding": null, "metadata": {"page_label": "2", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ae393ab-75a5-47b0-8ae2-798daeed29a2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ce552842d62aee46d50a48b81f5bfbdfdf791d098094a0f0adfab6c1e6211e7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d60c481-3056-4152-88a0-347c42d85629", "node_type": "1", "metadata": {"page_label": "2", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9bcea0a9ba0d490ff069bc05453f33bc885ae9083a6b6792f1f49b3461743775", "class_name": "RelatedNodeInfo"}}, "text": "LoftQ excels particularly in low-bit scenarios and works effectively\nwith different quantization methods. For example, we achieve over an 8% gain on MNLI (Wang\net al., 2019) and more than 10% on SQuADv1.1 (Rajpurkar et al., 2016) with both 2-bit NormalFloat\nand the 2-bit uniform quantization. We have not seen our approach performs worse than QLoRA.\n2 B ACKGROUND\n2.1 T RANSFORMER MODELS\nA transformer model contains a sequence of layers, where each layer consists of two sub-layers: a\nmulti-head self-attention (MHA) and a fully connected feed forward network (FFN) (Vaswani et al.,\n2017). Given the input X\u2208Rn\u00d7d, where nis the sequence length and dis the hidden dimension of\nthe model, MHA computes the hattention heads in parallel:\nMHA( X) = Concat(head 1, ...,head h)Wo,\nwhere head i=Softmax( XW qi(XW ki)\u22a4/p\ndh)XW vifori= 1, ..., h,\nwhere Wqi, Wki, Wvi\u2208Rd\u00d7dhare query, key, and value matrices, Wo\u2208Rd\u00d7dis the output matrix,\nanddh=d/h. FFN comprises two linear transformations and an activation function, and is defined\nasFFN( X) =\u03c3(XW f1+b1)Wf2+b2,where Wf1\u2208Rd\u00d7dm,Wf2\u2208Rdm\u00d7d, and \u03c3(\u00b7)is the\nactivation function. A residual connection is used and followed by layer normalization.\n2", "start_char_idx": 2705, "end_char_idx": 3889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d616ae66-39fd-46ac-8eae-773d00550042": {"__data__": {"id_": "d616ae66-39fd-46ac-8eae-773d00550042", "embedding": null, "metadata": {"page_label": "3", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b84781e5-6e46-4cd3-b5e4-c8ab50759d3a", "node_type": "4", "metadata": {"page_label": "3", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "93bcdbedee426efe628cde152e7c2a021ad75749a15ff9e5f680e84eea9da7c9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nUniform\n4bitNormalFloat\n4bitUniform\n2bitNormalFloat\n2bit02468101214DiscrepancyLoftQ\nQLoRA\n(a) Spectral norm of the initialization difference\nUniform\n4bitNormalFloat\n4bitUniform\n2bitNormalFloat\n2bit0102030405060DiscrepancyLoftQ\nQLoRA (b) Frobenius norm of the initialization difference\nFigure 2: Initialization discrepancy between the LoRA initialization and the original pre-trained\nweight matrix, described by the spectral norm and Frobenius norm of the difference. The weight\nmatrix in the above figures is randomly selected in BART-large. The initialization is obtained by\nQLoRA and LoftQ, with Uniform and NormalFloat quantization methods applied at both 2-bit and\n4-bit levels. LoftQ successfully mitigates the discrepancy, especially at the 2-bit level.\n2.2 Q UANTIZATION\nQuantization. Given a high-precision number, e.g., such as 32-bit floating point number, XHP\u2208R,\nN-bit quantization encodes it to an integer XINT\u2208 {0,1, ...,2N\u22121}. This process can be expressed\nas\nXINT=round\u0000\n(2N\u22121)F\u0000\nXHP\u0001\u0001\n, (1)\nwhere F(\u00b7):R7\u2192[0,1]is a normalization function. Uniform quantization assumes F(X) = (X\u2212\nXmin)/(Xmax\u2212Xmin). Dettmers et al. (2023) proposes 4-bit NormalFloat Quantization (NF4). It\nassumes X\u223c N (0, \u03c32)and hence F(X) = \u03a6( X/\u03c3), where \u03a6(\u00b7)is the cumulative distribution\nfunction of the standard normal distribution.\nDequantization. A lookup table T, where\nT[i] =F\u22121\u0012i\n2N\u22121\u0013\n, i= 0,1, ...,2N\u22121, (2)\nis used to decode the integer XINTto its simulated high-precision counterpart XD\u2208R. Therefore,\nthe dequantization can be expressed as\nXD=T[XINT]. (3)\nSimulated Quantization for Matrices. While it is possible to perform multiplication directly be-\ntween quantized representations, it is common to apply simulated quantization for matrices (Bai\net al., 2020; Shen et al., 2020). There, quantized weight matrices are stored as encoded integers in\nmemory, and are temporarily dequantized to simulated high-precision matrices by the lookup table\nwhen engaged in multiplication operations. In simulated quantization, it is only necessary to an-\nalyze the map from a high-precision matrix to a simulated high-precision matrix. We denote this\nend-to-end process by qN(\u00b7):Rm\u00d7n7\u2192Rm\u00d7n\nN , where RN:{T[i]\u2208R|0\u2264i <2N}.\n2.3 L OW-RANK ADAPTATION\nLoRA (Hu et al., 2021) updates two small weight matrices AandBthat are attached to a frozen\npre-trained weight matrix W. Hence, a linear transformation, Y=XW , is reformulated as\nY=XW +XAB\u22a4, (4)\nwhere X\u2208Rn\u00d7d1, W\u2208Rd1\u00d7d2, A\u2208Rd1\u00d7r, B\u2208Rd2\u00d7r, andr\u226amin{d1, d2}. Initially,\nA\u223c N(0, \u03c32), B= 0, (5)\nso as to align to the pre-trained weights. During the fine-tuning, Wis fixed while AandBare\nupdated by some SGD-type optimization method.\nIt is worth noting that if low-rank adapters AandBare attached to a quantized backbone Q=\nqN(W)and are initialized by (5), the starting weight Q+AB\u22a4is no longer equal to the pre-trained\nweight Wdue to the discrepancy introduced by the quantization.\n3", "start_char_idx": 0, "end_char_idx": 2955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a50f9194-18ec-4267-8ed4-1056909b6157": {"__data__": {"id_": "a50f9194-18ec-4267-8ed4-1056909b6157", "embedding": null, "metadata": {"page_label": "4", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e153496-6d8e-47d5-a72f-3175d9336797", "node_type": "4", "metadata": {"page_label": "4", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "506d0c10122b8a627e4a2594282e364bb00dc630dcbdae2955348bcaedffdb15", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n3 M ETHOD\nWe propose LoRA-Fine-Tuning-aware Quantization (LoftQ), a quantization framework for LLMs.\nIt alternatively applies quantization and low-rank approximation to approximate original pre-trained\nweights. This quantization framework provides a promising initialization for LoRA fine-tuning,\nwhich alleviates the quantization discrepancy in QLoRA and improves generalization in downstream\ntasks significantly.\n3.1 L ORA-A WARE QUANTIZATION\nWe use an N-bit quantized weight Q\u2208Rd1\u00d7d2\nN and low-rank approximations A\u2208Rd1\u00d7r, B\u2208\nRd2\u00d7rto approximate the original high-precision pre-trained weight W\u2208Rd1\u00d7d2as the initializa-\ntion of LoRA fine-tuning. Specifically, before fine-tuning, we initialize the network by minimizing\nthe following objective:\nmin\nQ,A,B\r\rW\u2212Q\u2212AB\u22a4\r\r\nF, (6)\nwhere \u2225\u00b7\u2225Fdenotes the Frobenious norm. This objective in (6) takes LoRA fine-tuning into consid-\neration by jointly optimizing the initial values of the quantized backbone Qand low-rank adapters\nA, B . Contrarily, practitioners typically convert the pre-trained weight Winto a quantized weight\nQoutright, neglecting the subsequent LoRA fine-tuning process. This oversight leads to notable\nperformance degradation in downstream tasks arising from the quantization discrepancy.\n3.2 A LTERNATING OPTIMIZATION\nWe solve the minimization problem in (6) by alternating between quantization and singular value\ndecomposition (SVD). To begin with, we set A0, andB0equal to 0.\nQuantization . At the t-th step, we quantize the difference between the original pre-trained weight\nmatrix Wand the low-rank approximation At\u22121B\u22a4\nt\u22121from the previous step to obtain the quantized\nweight matrix Qtby\nQt=qN(W\u2212At\u22121B\u22a4\nt\u22121), (7)\nwhere qN(\u00b7)maps a high-precision weight matrix to a quantized matrix.\nWe remark that our algorithm is compatible with different quantization functions qN(\u00b7). We apply\nNF4 and the uniform quantization in Section 4 as examples. We also remark that Qtis not an exact\nsolution of the minimization in (6), given the fixed At\u22121B\u22a4\nt\u22121, but it is an efficient approximation.\nSVD . After obtaining the t-th quantized weight Qt, SVD is applied to the residual of the quantization\ndenoted by Rt=W\u2212Qtby\nRt=dX\ni=1\u03c3t,iut,iv\u22a4\nt,i, (8)\nwhere d= min {d1, d2},\u03c3t,1\u2265\u03c3t,2\u2265...\u2265\u03c3t,dare the singular values of Rt,ut,i\u2019s and vt,i\u2019s are\nthe associated left and right singular vectors of Rt. We then obtain a rank- rapproximation of Rtby\nAtB\u22a4\nt, where\nAt= [\u221a\u03c3t,1ut,1, ...,\u221a\u03c3t,rut,r],\nBt= [\u221a\u03c3t,1vt,1, ...,\u221a\u03c3t,rvt,r]. (9)\nWe summarize our method in Algorithm 1. It is worth noting that T= 1is a special case where Q1is\nthe exact quantized weight obtained by QLoRA, and low-rank approximations A1, B1are obtained\nby the SVD of the quantization residual W\u2212Q1.T= 1 is sufficient to mitigate the quantization\ndiscrepancy, and alternating optimization helps to find a closer initialization to the pre-trained weight\nW, which further improves the performance (see Section 3).\nWe remark that the computational cost of LoftQ is negligible because it is applied to individual\nweight matrices and can be executed in parallel. We also remark one can apply LoftQ only once to\na pre-trained model and reuse the initialization obtained by LoftQ for different downstream tasks.\n3.3 A PPLYING TO LORA F INE-TUNING\nWe store the QT\u2208Rd1\u00d7d2\nN obtained by LoftQ using an integer matrix Mby (1) and a lookup table\nTby (2). We initialize the backbone with the integer matrix Mand initialize the low-rank adapters\nwithAT, BTobtained by LoftQ.\n4", "start_char_idx": 0, "end_char_idx": 3519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ad3a187-f6f7-42ed-acda-dc20e80753ee": {"__data__": {"id_": "5ad3a187-f6f7-42ed-acda-dc20e80753ee", "embedding": null, "metadata": {"page_label": "5", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f37351e4-d2ae-43df-bfb4-c94794026b36", "node_type": "4", "metadata": {"page_label": "5", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7a6f39a68c189fbead96c23fa2510bc694c950b43b9f316e1a260a9b1a995508", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1 LoftQ\ninput Pre-trained weight W, target rank r,N-bit quantization function qN(\u00b7), alternating step T\n1:Initialize A0\u21900, B0\u21900\n2:fort =1toTdo\n3: Obtain quantized weight Qt\u2190qN(W\u2212At\u22121B\u22a4\nt\u22121)\n4: Obtain low-rank approximation At, Bt\u2190SVD(W\u2212Qt)by (9)\n5:end for\noutput QT, AT, BT\nDuring LoRA fine-tuning, we freeze the integer weight Mand optimize the low-rank adapters with\nan efficient optimization algorithm, e.g., AdamW (Loshchilov & Hutter, 2017). In forward propa-\ngation, the integer weight Mis temporarily dequantized to the simulated high-precision weight QT\nby its lookup table, as described in (3). In back propagation, gradients and optimizer state are only\nrelated to low-rank adapters A, B , which reduces considerable training cost.\n4 E XPERIMENTS\nWe evaluate our method on NLU and NLG tasks. We apply LoftQ for quantizing DeBERTaV3-base\n(He et al., 2021b), BART-large (Lewis et al., 2019), and LLAMA-2 series (Touvron et al., 2023).\nImplementation Details. Following the prior works of LoRA variants (Zhang et al., 2023; He\net al., 2021a), we freeze all the backbone weight matrices and add low-rank adapters to weight\nmatrices in MHA and FFN of all layers. We quantize the weight matrices that are attached by low-\nrank adapters. All the quantized models and adapters used in this paper are available on https:\n//huggingface.co/LoftQ . Our implementation is based on publicly available Huggingface\nTransformers code-base (Paszke et al., 2019). All the experiments are conducted on NVIDIA A100\nGPUs.\nQuantization Methods. We apply two quantization methods to demonstrate LoftQ is compatible\nwith different quantization functions:\n\u2022Uniform quantization is a classic quantization method. It uniformly divides a continuous\ninterval into 2Ncategories and stores a local maximum absolute value for dequantization.\n\u2022NF4 and its 2-bit variant NF2 are quantization methods used in QLoRA (Dettmers et al.,\n2023). They assume that the high-precision values are drawn from a Gaussian distribution\nand map these values to discrete slots that have equal probability.\nWe perform 2-bit and 4-bit quantization on all models, achieving compression ratios of 25-30% and\n15-20% at the 4-bit and 2-bit levels, respectively. The compression ratios and trainable parameter\nratios for all models are detailed in the Appendix A.\nBaselines. We compare LoftQ with the following baseline methods:\n\u2022Full fine-tuning is the most common approach for adapting a pre-trained model to down-\nstream tasks. The model is initialized with pre-trained weights and all parameters are up-\ndated through an SGD-type optimization method.\n\u2022Full precision LoRA (LoRA) is a lightweight method for task adaptation, where it stores the\nbackbone using 16-bit numbers and optimizes the low-rank adaptors only. The adaptors\nare applied to the same matrices as in LoftQ.\n\u2022QLoRA is similar to LoRA except the backbone is quantized into low-bit regime. The low-\nrank adapters are initialized using (5) and are applied to the same matrices as in LoftQ.\n4.1 E NCODER -ONLY MODEL : DEBERT AV3\nModels and Datasets. We quantize the DeBERTaV3-base (He et al., 2021b) with LoftQ, then fine-\ntune and evaluate the model on the General Language Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2019), SQuADv1.1 (Rajpurkar et al., 2016), and ANLI (Nie et al., 2019). The\nspecific tasks of GLUE are given in Appendix C. Following previous works (Zhang et al., 2023), we\nexclude WNLI in the experiments.\n5", "start_char_idx": 0, "end_char_idx": 3514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "202d9e73-2249-48b6-99e9-ae1e15070f22": {"__data__": {"id_": "202d9e73-2249-48b6-99e9-ae1e15070f22", "embedding": null, "metadata": {"page_label": "6", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d048a513-38aa-4ef4-944f-de2813d6ba70", "node_type": "4", "metadata": {"page_label": "6", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "642d07e47110ada05a6f9e6d3088d17fdf6d78f408cfd2cf873ece3d8c335681", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c57a8f3b-72d7-4606-aed8-01f1b89c1a31", "node_type": "1", "metadata": {}, "hash": "522f8efba9658cbc2aca49bff28494a603daedb0769069e681dda6a9591d78de", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nImplementation Details. We select the learning rates from {1\u00d710\u22125,5\u00d710\u22125,1\u00d710\u221245\u00d710\u22124}.\nWe quantize the entire backbone. Given that GLUE, SQuADv1.1, and ANLI are relatively easy\nNLU tasks, we also quantize the embedding layer for higher compression efficiency. We apply the\nNormalFloat and the uniform quantization for LoftQ and QLoRA at both 2-bit and 4-bit levels. We\nuse rank 16 and 32 for low-rank adapters. More implementation details, such as the training epochs\nand batch sizes, are presented in Appendix D.2.\nMain Results. Table 1 and Table 2 summarize the results for 2-bit quantization on the GLUE,\nSQuADv1.1, and ANLI datasets, by NF2 and the uniform quantization, respectively. Our method\nconsistently outperforms QLoRA on all settings with respect to different ranks, quantization meth-\nods, and datasets. When using the uniform quantization (Table 2), our method achieves 88.0%\naccuracy on MNLI-m, surpassing the QLoRA baseline by 8%. For tasks like SST and SQuADv1.1,\nour method even approaches the full fine-tuning performance at 2-bit level. The 4-bit quantization\nexperiment results are presented in Appendix D.1 as both LoftQ and QLoRA achieve performance\nclose to full fine-tuning.\nTable 1: Results with 2-bit LoftQ of DeBERTaV3-base models on GLUE development set,\nSQuADv1.1 development set, ANLI test set using NF2 quantization . We report the median over\nfour seeds. N.A. indicates the model does not converge. The best results on each dataset are shown\ninbold .\nRank Method MNLI QNLI RTE SST MRPC CoLA QQP STSB SQuAD ANLI\nm / mm Acc Acc Acc Acc Matt Acc P/S Corr EM/F1 Acc\n- Full FT 90.5/90.6 94.0 82.0 95.3 89.5/93.3 69.2 92.4/89.8 91.6/91.1 88.5/92.8 59.8\n16 LoRA 90.4/90.5 94.6 85.1 95.1 89.9/93.6 69.9 92.0/89.4 91.7/91.1 87.3/93.1 60.2\n16QLoRA 75.4/75.6 82.4 55.9 86.5 73.8/82.8 N.A. 86.8/82.3 83.0/82.8 61.5 / 71.2 N.A.\nLoftQ 84.7/85.1 86.6 61.4 90.2 83.8/88.6 37.4 90.3/86.9 87.1/86.9 81.5/88.6 47.1\n32QLoRA 78.5/78.7 80.4 56.7 86.9 73.8/82.7 N.A. 87.1/82.7 83.6/83.3 64.6/73.8 N.A.\nLoftQ 86.0/86.1 89.9 61.7 92.0 83.6/87.2 47.5 91.0/87.9 87.5/87.0 82.9/89.8 49.0\nTable 2: Results with 2-bit LoftQ of DeBERTaV3-base models on GLUE development set,\nSQuADv1.1 development set using Uniform quantization . We report the median over four seeds.\nN.A. indicates the model does not converge. The best results on each task are shown in bold .", "start_char_idx": 0, "end_char_idx": 2410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c57a8f3b-72d7-4606-aed8-01f1b89c1a31": {"__data__": {"id_": "c57a8f3b-72d7-4606-aed8-01f1b89c1a31", "embedding": null, "metadata": {"page_label": "6", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d048a513-38aa-4ef4-944f-de2813d6ba70", "node_type": "4", "metadata": {"page_label": "6", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "642d07e47110ada05a6f9e6d3088d17fdf6d78f408cfd2cf873ece3d8c335681", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "202d9e73-2249-48b6-99e9-ae1e15070f22", "node_type": "1", "metadata": {"page_label": "6", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "03671863c6e7cffd1875c4204fb2a6d009c8b46ff35134efafba744d36825cf5", "class_name": "RelatedNodeInfo"}}, "text": "87.1/82.7 83.6/83.3 64.6/73.8 N.A.\nLoftQ 86.0/86.1 89.9 61.7 92.0 83.6/87.2 47.5 91.0/87.9 87.5/87.0 82.9/89.8 49.0\nTable 2: Results with 2-bit LoftQ of DeBERTaV3-base models on GLUE development set,\nSQuADv1.1 development set using Uniform quantization . We report the median over four seeds.\nN.A. indicates the model does not converge. The best results on each task are shown in bold .\nRank Method MNLI QNLI RTE SST MRPC CoLA QQP STSB SQuAD\nm / mm Acc Acc Acc Acc Matt Acc P/S Corr Em/F1\n- Full FT 90.5/90.6 94.0 82.0 95.3 89.5/93.3 69.2 92.4/89.8 91.6/91.1 88.5/92.8\n16 LoRA 90.4/90.5 94.6 85.1 95.1 89.9/93.6 69.9 92.0/89.4 91.7/91.1 87.3/93.1\n16QLoRA 76.5/76.3 83.8 56.7 86.6 75.7/84.7 N.A. 87.1/82.6 83.5/83.4 69.5/77.6\nLoftQ 87.3/87.1 90.6 61.1 94.0 87.0/90.6 59.1 90.9/88.0 87.9/87.6 84.4/91.2\n32QLoRA 79.9/79.5 83.7 57.8 86.9 76.5/84.5 N.A. 88.6/84.7 84.1/84.0 71.6/80.2\nLoftQ 88.0/88.1 92.2 63.2 94.7 87.5/91.2 60.5 91.3/88.3 89.5/89.2 85.2/91.6\nOur method is also more stable compared to QLoRA in the low-bit regime. For instance, while\nQLoRA fails to converge on CoLA for both quantization methods and ranks, LoftQ converges in all\ncases and achieves a score of 60.5 using uniform quantization at rank 32. LoftQ stands out in its\nability to consistently attain robust and improved performance by effectively preserving the starting\npoint of pre-trained weights.\n4.2 E NCODER -DECODER MODEL : BART\nModels and Datasets. We quantize BART-large model (Lewis et al., 2020) with LoftQ, then fine-\ntune and evaluate the model on two commonly used summarization datasets: XSum (Narayan et al.,\n2018) and CNN/DailyMail(Hermann et al., 2015).\nImplementation Details. We apply LoftQ to weight matrices in MHA and FFN of both encoder and\ndecoder layers. We report ROUGE 1/2/L scores, which are the metrics for summarization tasks (Lin,\n2004). We conduct quantization experiments in both 2-bit and 4-bit scenarios. We experiment with\nboth NormalFloat and the uniform quantization in both 2-bit and 4-bit scenarios. In each precision,\nwe choose rank equal to 8 and 16 for a fair comparison with the full precision LoRA baseline (Zhang\net al., 2023). Please see Appendix E for detailed configurations.\n6", "start_char_idx": 2024, "end_char_idx": 4223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79c7a385-fb6d-432d-8edf-3bd266c7760e": {"__data__": {"id_": "79c7a385-fb6d-432d-8edf-3bd266c7760e", "embedding": null, "metadata": {"page_label": "7", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4201f27a-ef95-4b58-a338-4d40632ce134", "node_type": "4", "metadata": {"page_label": "7", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ee99f329485d36d6ed0edf6374555384bb219e7c6957bc86c79f171fc5ce4e49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "663ece0b-ad9e-4c76-a9a5-cb2022eb1ca5", "node_type": "1", "metadata": {}, "hash": "cd00f51b88870dd73aa76bb80392a982f592ab50df7892744c02d5d4b2e8654c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMain Results. Table 3 summarizes our 4-bit quantization experiment results on the XSum and\nCNN/DailyMail test sets. Our method consistently outperforms QLoRA at both ranks on both\ndatasets. It even surpasses full precision LoRA at both ranks on Xsum. We will discuss this un-\nexpected results in Section 5. The 2-bit quantization results are shown in Table 4. Our observation\nis consistent with the NLU experiments, that LoftQ demonstrates the convergence to reasonable\nresults, while QLoRA does not converge. This indicates our method is robuster by narrowing the\ninitialization gap.\nTable 3: Results with 4-bit LoftQ of BART-large on XSum and CNN/DailyMail. We report ROUGE-\n1/2/L. Lead-3 means choosing the first 3 sentences as the summary. N.A. indicates the model does\nnot converge. Full FT : full fine-tuning. We report the median over five seeds.\nQuantization Rank Method XSum CNN/DailyMail\nFull Precision-Lead-3 16.30/1.60/11.95 40.42/17.62/36.67\nFull FT 45.14/22.27/37.25 44.16/21.28/40.90\n8 LoRA 43.40/20.20/35.20 44.72/21.58/41.84\n16 LoRA 43.95/20.72/35.68 45.03/21.84/42.15\nNF48QLoRA 42.91/19.72/34.82 43.10/20.22/40.06\nLoftQ 44.08/20.72/35.89 43.81/20.95/40.84\n16QLoRA 43.29/20.05/35.15 43.42/20.62/40.44\nLoftQ 44.51/21.14/36.18 43.96/21.06/40.96\nUniform8QLoRA 41.84/18.71/33.74 N.A.\nLoftQ 43.86/20.51/35.69 43.73/20.91/40.77\n16QLoRA 42.45/19.36/34.38 43.00/20.19/40.02\nLoftQ 44.29/20.90/36.00 43.87/20.99/40.92\nTable 4: Results with 2-bit LoftQ of BART-large on XSum and CNN/DailyMail using NF2 quanti-\nzation .N.A. indicates the model does not converge. We report ROUGE-1/2/L, the higher the better.\nWe report the median over five seeds.\nRank Method XSum CNN/DailyMail\n8QLoRA N.A. N.A.\nLoftQ 39.63/16.65/31.62 42.24/19.44/29.04\n16QLoRA N.A. N.A.\nLoftQ 40.81/17.85/32.80 42.52/19.81/39.51\n4.3 D ECODER -ONLY MODEL : LLAMA-2\nModels and Datasets. We quantize LLAMA-2-7b and LLAMA-2-13b (Touvron et al., 2023) with\nLoftQ. We then fine-tune and evaluate the models on two NLG datasets: GSM8K (Cobbe et al.,\n2021) and WikiText-2 (Merity et al., 2016). Please see Appendix F for more details about the\ndatasets.\nImplementation Details. Similarly, we apply LoftQ to weight matrices in MHA and FFN of all\nlayers. In WikiText-2 evaluation, we report perplexity. In case of overfitting, we apply weight decay\nto low-rank adapters for all settings. In GSM8K evaluation, we extract numerical answers in the\ngenerated solutions and then calculate the accuracy using those numerical answers. We conduct\nexperiments with both NF2 and NF4. Please see Appendix F for detailed configurations.\nMain Results.", "start_char_idx": 0, "end_char_idx": 2647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "663ece0b-ad9e-4c76-a9a5-cb2022eb1ca5": {"__data__": {"id_": "663ece0b-ad9e-4c76-a9a5-cb2022eb1ca5", "embedding": null, "metadata": {"page_label": "7", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4201f27a-ef95-4b58-a338-4d40632ce134", "node_type": "4", "metadata": {"page_label": "7", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ee99f329485d36d6ed0edf6374555384bb219e7c6957bc86c79f171fc5ce4e49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79c7a385-fb6d-432d-8edf-3bd266c7760e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "55caee39dd4188f7f4d2207c659e314d1693d9f05cc06d5bd41884611a753a4d", "class_name": "RelatedNodeInfo"}}, "text": "We then fine-tune and evaluate the models on two NLG datasets: GSM8K (Cobbe et al.,\n2021) and WikiText-2 (Merity et al., 2016). Please see Appendix F for more details about the\ndatasets.\nImplementation Details. Similarly, we apply LoftQ to weight matrices in MHA and FFN of all\nlayers. In WikiText-2 evaluation, we report perplexity. In case of overfitting, we apply weight decay\nto low-rank adapters for all settings. In GSM8K evaluation, we extract numerical answers in the\ngenerated solutions and then calculate the accuracy using those numerical answers. We conduct\nexperiments with both NF2 and NF4. Please see Appendix F for detailed configurations.\nMain Results. Table 5 presents a summary of our experiments on LLAMA-2-7b and LLAMA-2-\n13b using 2-bit, 4-bit, and mixed-precision NormalFloat quantization methods on WikiText-2 and\nGSM8K datasets. In WikiText-2, our method consistently outperforms QLoRA across all quantiza-\ntion precision settings on both models. When dealing with the challenging 2-bit precision, where\nQLoRA fails to converge, LoftQ manages to achieve a perplexity of 7.85. In GSM8K, our method\nachieves better or on par performance compared to QLoRA across different model sizes and quanti-\nzation precision levels. For example, our method achieves 26.5% accuracy using 2-bit precision of\nLLAMA-2-7b, where QLoRA does not converge.\n7", "start_char_idx": 1978, "end_char_idx": 3339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69cb139c-0a9d-4185-88d8-ee805a5c84be": {"__data__": {"id_": "69cb139c-0a9d-4185-88d8-ee805a5c84be", "embedding": null, "metadata": {"page_label": "8", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e369e70-bb2b-411a-8110-fca7beb518d2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c9027093711136c4646c0fd98f6611a16798a2af371c13cbb6206ae55a7418fd", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTo provide a customized trade-off between the performance and precision, we also explore mixed-\nprecision (equivalent to 3 bits) quantization where matrices in the first half layers are quantized\nusing 4 bits, and the rest matrices remain 2 bits. We witness a remarkable 4.1% accuracy boost\non the GSM8K dataset using LLAMA-2-7b and a 4.7% boost using LLAMA-2-13b. This result\nunderscores the potential of LoftQ for complex mixed-precision quantization scenarios.\nTable 5: Results of LoftQ using NormalFloat for LLAMA-2 series on WikiText-2 and GSM8K.\n3/2.5/2.25-bit indicates mixed-precision quantization: 4-bit precision for the first 16/8/4 layers and\n2-bit precision for the rest of layers. We report the perplexity (the smaller the better) for WikiText-2\nand accuracy for GSM8K. The rank of low-rank adapters is 64. N.A. indicates the model does not\nconverge. We report the median over five random seeds.\nMethod BitLLAMA-2-7b LLAMA-2-13b\nWikiText-2 \u2193GSM8K \u2191WikiText-2 \u2193GSM8K \u2191\nLoRA 16 5.08 38.5 5.12 48.8\nQLoRA 4 5.70 38.2 5.22 48.8\nLoftQ 4 5.24 38.0 5.16 49.1\nQLoRA 3 5.73 32.1 5.22 40.7\nLoftQ 3 5.63 36.2 5.13 45.4\nQLoRA 2.5 N.A. N.A. 19.39 N.A.\nLoftQ 2.5 5.78 31.1 5.22 41.1\nQLoRA 2.25 N.A. N.A. N.A. N.A.\nLoftQ 2.25 6.13 27.5 5.45 38.1\nQLoRA 2 N.A N.A. N.A. N.A.\nLoftQ 2 7.85 26.5 7.69 33.4\n4.4 A NALYSIS\nEffectiveness of Alternating Optimization. We conduct experiments with different alternating\nstepTto verify the effectiveness of the alternating optimization and to find the best value Tas\na hyperparameter for different models. Across all tasks and models, we observed that alternating\noptimization yields substantial improvements even with a minimal alternating step. This suggests\nthat it rapidly narrows the discrepancy between quantized weights and pre-trained weights, making\nour method easy to apply. For example, LoftQ achieves 21.14 Rouge-2 score on XSum using only\n1 step. Interestingly, we noticed that increasing the alternating step beyond a certain point tends\nto result in diminishing returns. We suspect this phenomenon occurs because, as the gap becomes\nsmaller, it becomes more challenging for alternating optimization to consistently minimize the gap\nat each step. This challenge emerges because of the inherent errors introduced by the quantization\nmethod. Nevertheless, results from Figure 3 indicate our method is not sensitive to the alternating\nstepTand is able to consistently enhance downstream fine-tuning performance.\n0 1 5 10\nAlternating Step T7580858890Accuracy79.986.688.0 87.7\n(a) MNLI\n20.022.525.027.0\n22.525.225.5\n0 1 5 10\nAlternating Step T011.2Accuracy (b) GSM8k\n0 1 5 10\nAlternating Step T19.020.021.021.5ROUGE-220.0521.14 21.09\n20.83 (c) XSum\nFigure 3: Comparison of different alternating step Tused in LoftQ. T= 0indicates we use QLoRA\nmethod that initializes low-rank adapters by (5). T= 1,5,10indicates we use different Tfor LoftQ\ndescribed in Algorithm 1. Left: Uniform 2-bit DeBERTaV3-base. Middle : NF2 2-bit LLAMA-2-\n13b.Right : NF4 BART-large.\n8", "start_char_idx": 0, "end_char_idx": 3049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "518949a8-0eb2-460e-b260-1a8c782d9218": {"__data__": {"id_": "518949a8-0eb2-460e-b260-1a8c782d9218", "embedding": null, "metadata": {"page_label": "9", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1d11c65-d73c-4faf-a3c9-9e37af943cc2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "45e713729029616c182cbc221f11bb4ff119207777c05afb4e66ef8abb563a71", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n5 D ISCUSSION\nStart with quantization or SVD in the alternating optimization? An alternative algorithm to the\nalternating optimization is that we first obtain the low-rank approximation At, Btand then obtain the\nquantized weight Qtby switching Line 3 and Line 4 in Algorithm 1. We note this is a valid alterna-\ntive method as both still jointly minimize the objective in (6). Table 6 summarizes the performance\nof this alternative method. It is noteworthy that the alternative method still outperforms QLoRA\nsignificantly, even though it is worse than the primary version. This observation underscores the\npotential for performance improvement by achieving a closer approximation of pre-trained weights\nwithin the low-precision regime.\nLoftQ better than Full-precision LoRA? We find LoftQ outperforms full precision LoRA in XSum\nand GSM8K (see Table 3 and Table 5). Beside the overfitting caused by lack of regularization,\nanonther possible explanation for this unexpected phenomenon is that the initial low-rank adapters\nobtained by LoftQ are non-zero while they are all zero in full precision LoRA as described in (5).\nSuch zero initialization could make the fine-tuning unstable, and therefore it performs worse than\nLoftQ. We leave the study of the robustness of LoftQ as future work.\nTable 6: Results of 2-bit uniformly quantized DeBERTaV3-base on part of GLUE. LoftQ(SVD\nFirst) indicates the alternative LoftQ that swiches Line 3 and Line 4 in Algorithm 1. We report the\nmedian over four random seeds. The best results on each task are shown in bold .\nMethod RankMNLI QNLI SST2\nm / mm Acc Acc\nFull FT - 90.5/90.6 94.0 95.3\nQLoRA 32 79.9/79.5 83.8 86.6\nLoftQ(SVD First) 32 87.8/87.7 84.9 89.7\nLoftQ(Quantiztion First) 32 88.0/88.1 92.2 94.7\n6 R ELATED WORK\nQuantization-Aware Training (QAT) is often used to obtain quantized models that are adapted\nin downstream tasks (Peri et al., 2020; Liu et al., 2023). It involves quantization and full model\nfine-tuning at the same time. However, QAT requires massive training cost, such as the gradient\nand optimization state. Moreover, it is difficult to compute the gradient of quantized weights. Our\nmethod, with the help of LoRA, sidesteps the aforementioned issues, providing a light approach for\ndownstream task adaptation.\nPost-Training Quantization (PTQ) is a category of popular quantization frameworks (Frantar et al.,\n2022; Xiao et al., 2023), which can also be used for task adaptation. It calibrates the high-precision\nmodel with a small subset of the training dataset. Therefore, the subsequent quantization is guided\nby the training dataset, providing task-specific quantized models. Besides, it does not involve any\ngradient backpropagation, so it is cost-efficient. However, it usually results in lower accuracy com-\npared to QAT.\n7 C ONCLUSION\nWe propose LoftQ, a quantization framework for LLMs, which alternatively applies quantization\nand low-rank approximation to the original high-precision pre-trained weights, to obtain an ini-\ntialization for the subsequent LoRA fine-tuning. Experiments on natural language understanding,\nquestion answering, summarization, and natural language generation show that our framework re-\nmarkably surpasses existing methods, e.g., QLoRA, for quantizing encoder-only, encoder-decoder,\nand decoder-only models. We have not observed our method exhibiting worse performance over\nQLoRA. Moreover, our quantization framework demonstrates effectiveness and robustness particu-\nlarly in low-bit quantization regimes, e.g., the 2-bit level.\n9", "start_char_idx": 0, "end_char_idx": 3580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59f3691a-52cd-4544-8daa-1df5182376d9": {"__data__": {"id_": "59f3691a-52cd-4544-8daa-1df5182376d9", "embedding": null, "metadata": {"page_label": "10", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "846b0ffc-7719-407a-9f67-a9f28b480b08", "node_type": "4", "metadata": {"page_label": "10", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1718b379abf76ce5af83c7bcae9dff5f2ec0f913f227752a53f4d936f892a404", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45ebd30c-ff6e-4eb0-ab99-e22bb996115b", "node_type": "1", "metadata": {}, "hash": "e8953e87e01a80bd518851718fb2997c2686bb72ab9df5df98dff60e5c0653d5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nREFERENCES\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin\nKing. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701 , 2020.\nHaoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael R Lyu. Towards efficient post-\ntraining quantization of pre-trained language models. Advances in Neural Information Processing\nSystems , 35:1405\u20131418, 2022.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\nIdan Szpektor. The second pascal recognising textual entailment challenge. 2006.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing\ntextual entailment challenge. In TAC, 2009.\nDaniel Cer, Mona Diab, Eneko Agirre, I \u02dcnigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017) , pp. 1\u201314, Vancouver,\nCanada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Workshop , 2007.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314 , 2023.\nShizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang.\nLmflow: An extensible toolkit for finetuning and inference of large foundation models. arXiv\npreprint arXiv:2306.12420 , 2023.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entail-\nment and Paraphrasing , pp. 1\u20139, Prague, June 2007. Association for Computational Linguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a\nunified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366 , 2021a.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style\npre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543 ,\n2021b.", "start_char_idx": 0, "end_char_idx": 3138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45ebd30c-ff6e-4eb0-ab99-e22bb996115b": {"__data__": {"id_": "45ebd30c-ff6e-4eb0-ab99-e22bb996115b", "embedding": null, "metadata": {"page_label": "10", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "846b0ffc-7719-407a-9f67-a9f28b480b08", "node_type": "4", "metadata": {"page_label": "10", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1718b379abf76ce5af83c7bcae9dff5f2ec0f913f227752a53f4d936f892a404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59f3691a-52cd-4544-8daa-1df5182376d9", "node_type": "1", "metadata": {"page_label": "10", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "031e51ae468361b51ec700475902bdae7de5d4458b92d7c8ed6d549abe535c62", "class_name": "RelatedNodeInfo"}}, "text": "The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entail-\nment and Paraphrasing , pp. 1\u20139, Prague, June 2007. Association for Computational Linguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a\nunified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366 , 2021a.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style\npre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543 ,\n2021b.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural\ninformation processing systems , 28, 2015.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 , 2021.\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thir-\nteenth international conference on the principles of knowledge representation and reasoning ,\n2012.\n10", "start_char_idx": 2542, "end_char_idx": 3760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a31d497e-abb8-47e4-8ef1-12fc1598a0c7": {"__data__": {"id_": "a31d497e-abb8-47e4-8ef1-12fc1598a0c7", "embedding": null, "metadata": {"page_label": "11", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e7bfadd-c783-4818-ae59-548fb6e549e3", "node_type": "4", "metadata": {"page_label": "11", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "389bdbb9e7480588f196ac101111ef8dcfcfc12a7ee6f3ef2924192337904109", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f8a25b0-d2b9-4c43-8292-9f5bb4581b24", "node_type": "1", "metadata": {}, "hash": "036a9e57274471d95b7b65b635892c697f154330c4aec863f1cba63598053e85", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461 , 2019.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, and comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics , pp. 7871\u20137880, Online,\nJuly 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703.\nYixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao.\nLosparse: Structured compression of large language models based on low-rank and sparse ap-\nproximation. arXiv preprint arXiv:2306.11222 , 2023.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out , pp. 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguis-\ntics.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang\nShi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware\ntraining for large language models. arXiv preprint arXiv:2305.17888 , 2023.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 , 2017.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. ArXiv , abs/1808.08745,\n2018.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adver-\nsarial nli: A new benchmark for natural language understanding. ArXiv , abs/1910.14599, 2019.\nURL https://api.semanticscholar.org/CorpusID:207756753 .\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In Advances in Neural Information Processing Systems 32 , pp. 8024\u20138035.\nCurran Associates, Inc., 2019.\nDheeraj Peri, Jhalak Patel, and Josh Park. Deploying quantization-aware trained networks using\ntensorrt. In GPU Technology Conference , 2020.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing , pp. 2383\u20132392, Austin, Texas, November 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/D16-1264.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney,\nand Kurt Keutzer.", "start_char_idx": 0, "end_char_idx": 3195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f8a25b0-d2b9-4c43-8292-9f5bb4581b24": {"__data__": {"id_": "7f8a25b0-d2b9-4c43-8292-9f5bb4581b24", "embedding": null, "metadata": {"page_label": "11", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e7bfadd-c783-4818-ae59-548fb6e549e3", "node_type": "4", "metadata": {"page_label": "11", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "389bdbb9e7480588f196ac101111ef8dcfcfc12a7ee6f3ef2924192337904109", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a31d497e-abb8-47e4-8ef1-12fc1598a0c7", "node_type": "1", "metadata": {"page_label": "11", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e6cfdcdead5b4e2fd3ad27d9ca4cde77f63c84bf3be1d4473a1c0a39569b1be6", "class_name": "RelatedNodeInfo"}}, "text": "8024\u20138035.\nCurran Associates, Inc., 2019.\nDheeraj Peri, Jhalak Patel, and Josh Park. Deploying quantization-aware trained networks using\ntensorrt. In GPU Technology Conference , 2020.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing , pp. 2383\u20132392, Austin, Texas, November 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/D16-1264.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney,\nand Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings\nof the AAAI Conference on Artificial Intelligence , volume 34, pp. 8815\u20138821, 2020.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing , pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics.\n11", "start_char_idx": 2569, "end_char_idx": 3745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dc26739-d5fe-47c3-9f52-1e8c52b7e448": {"__data__": {"id_": "2dc26739-d5fe-47c3-9f52-1e8c52b7e448", "embedding": null, "metadata": {"page_label": "12", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30bc7272-d963-4c1c-892c-34eb0f595d63", "node_type": "4", "metadata": {"page_label": "12", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1d323d60d9d6f9142fee38feaf2d29b32a5e8cb36ff2644b7343f1c81be63107", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems , 30, 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations , 2019.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics , 7:625\u2013641, 2019. doi: 10.1162/\ntacla00290.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pp. 1112\u20131122, New Orleans, Louisiana, June 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/N18-1101.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models. In International\nConference on Machine Learning , pp. 38087\u201338099. PMLR, 2023.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS\nEdition (EMC2-NIPS) , pp. 36\u201339. IEEE, 2019.\nHongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without\nnormalization. arXiv preprint arXiv:1901.09321 , 2019.\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\nand Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\narXiv:2303.10512 , 2023.\n12", "start_char_idx": 0, "end_char_idx": 2216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "714e0fae-e10a-44d6-b9ed-ce675b165990": {"__data__": {"id_": "714e0fae-e10a-44d6-b9ed-ce675b165990", "embedding": null, "metadata": {"page_label": "13", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a75c85f-3a99-4a17-ad8d-da2a0e62d1eb", "node_type": "4", "metadata": {"page_label": "13", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6c1e6226f5332de74f7f8a985d17c66f717142f8479ffa6a45bd133f4b6aa270", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA M ODEL COMPRESSION RATIO AND MEMORY FOOTPRINT\nWe report the compression ratio after applying LoftQ in Table 7. It is defined as\ncompression ration =backbone size +LoRA adapter size\npre-trained size.\nWe also measure the GPU memory cost during training. Given that GPU memory varies by models,\ntasks, sequence lengths, batch sizes, etc. We report LLAMA-2 on GSM8K as an example in Table\n8.\nTable 7: Compression ratios of backbones.\nModelCompression TrainableRank BitsQuantization\nratio (%) ratio (%) method\nDeBERTaV3-base 15.6 3.1 16 2 Uniform\nDeBERTaV3-base 18.8 6.3 32 2 Uniform\nDeBERTaV3-base 17.2 3.1 16 2 NF2\nDeBERTaV3-base 20.4 6.3 32 2 NF2\nBART-large 15.3 1.2 8 4 NF2\nBART-large 16.7 2.5 16 4 NF2\nBART-large 27.8 1.2 8 4 NF4\nBART-large 29.0 2.5 16 4 NF4\nBART-large 26.2 1.2 8 4 Uniform\nBART-large 27.5 2.5 16 4 Uniform\nLLAMA-2-7b 16.6 2.4 64 2 Nf2\nLLAMA-2-7b 29.0 2.4 64 4 Nf4\nLLAMA-2-13b 16.0 1.9 64 2 Nf2\nLLAMA-2-13b 28.5 1.9 64 4 Nf4\nTable 8: GPU memory footprint\nModel Dataset Seq length Batch size GPU Mem\nLLAMA-2-7b GSM8K 384 1 15GB\nLLAMA-2-13b GSM8K 384 1 24GB\nB Q UANTIZATION TIME\nWe report the execution time of LoftQ applying to a single weight matrix in Table 9. The time is\ntested on Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz.\nTable 9: Execution time of LoftQ applying to different weight matrices.\nModel Size StepTQuantization method Time\nDeBERTaV3-base 768\u00d7768 5 Uniform 1s\nBART-large 1024\u00d71024 5 NF4 1s\nLLAMA-2-7b 4096\u00d74096 5 NF4 21s\nLLAMA-2-13b 5120\u00d75120 5 NF4 43s\nC GLUE D ATASET STATISTICS\nWe present the dataset statistics of GLUE Wang et al. (2019) in the following table.\nGLUE includes two single-sentence classification tasks: SST-2 (Socher et al., 2013) and CoLA\n(Warstadt et al., 2019), and three similarity and paraphrase tasks: MRPC (Dolan & Brockett, 2005),\nSTS-B (Cer et al., 2017), and QQP. GLUE also includes four natural language inference tasks in\nGLUE: MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2007; Bar-\nHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and WNLI (Levesque et al.,\n2012).\n13", "start_char_idx": 0, "end_char_idx": 2136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adc54129-715d-4f5d-8b30-55d2de26eaec": {"__data__": {"id_": "adc54129-715d-4f5d-8b30-55d2de26eaec", "embedding": null, "metadata": {"page_label": "14", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ad90e06-39aa-4342-8832-6ee4286c8db9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6a500357226fe3a73aded52623a32d9941f2957b398bf5cf0612a1fe62229359", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nCorpus Task #Train #Dev #Test #Label Metrics\nSingle-Sentence Classification (GLUE)\nCoLA Acceptability 8.5k 1k 1k 2 Matthews corr\nSST Sentiment 67k 872 1.8k 2 Accuracy\nPairwise Text Classification (GLUE)\nMNLI NLI 393k 20k 20k 3 Accuracy\nRTE NLI 2.5k 276 3k 2 Accuracy\nQQP Paraphrase 364k 40k 391k 2 Accuracy/F1\nMRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1\nQNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy\nText Similarity (GLUE)\nSTS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr\nTable 10: Summary of the GLUE benchmark.\nD N ATURAL LANGUAGE UNDERSTANDING\nD.1 GLUE WITH 4-BIT\nWe show the 4-bits results in the Table 11. Both methods can achieve performance close to full-\nfinetuning.\nTable 11: Results with 4-bit LoftQ of DeBERTaV3-base models on GLUE development set using\nNF4 quantization. We report the median over four seeds. Results with N.A. indicate the model does\nnot converge. The best results on each dataset are shown in bold\nMethod RankMNLI SST-2 QNLI ANLI\nm / mm Acc Acc Acc\nFull FT - 90.5/90.6 95.3 94.0 59.8\nQLoRA 32 89.9/89.9 95.3 94.2 59.4\nLoftQ 32 89.9/90.0 95.3 94.1 59.9\nD.2 T RAINING DETAILS\nImplementation Details. The implementation of LoftQ is based on publicly available Huggingface\n(Paszke et al., 2019) code-base3.\nHyper-parameter Details. We select the learning rate of {1\u00d710\u22125,5\u00d710\u22125,1\u00d710\u22124,5\u00d710\u22124},\nand use the selected learning rate for both uniform quantization experiments and nf2 quantization\nexperiments. We use batch size of 32 for all GLUE tasks and ANLI. We use batch size of 16 for\nSQuADv1.1. We use LoftQ of 5 iterations for all GLUE tasks.\nTable 12 summarizes the detailed hyperparameters for each task used in training DeBERTaV3-base\nusing uniform quantization. Table 13 summarizes the detailed hyperparameters for each task used\nin training DeBERTaV3-base using nf2 quantization.\nTable 12: Hyper-parameter setup of LoftQ for GLUE benchmark for training DeBERTaV3-base\nusing Uniform quantization.\nHyper-parameter MNLI RTE QNLI MRPC QQP SST-2 CoLA STS-B SQuADv1.1 ANLI\n# epochs 5 20 10 60 10 10 60 60 10 12\nLearning rate 1\u00d710\u221245\u00d710\u221245\u00d710\u221251\u00d710\u221245\u00d710\u221255\u00d710\u221255\u00d710\u221255\u00d710\u221255\u00d710\u221255\u00d710\u22125\n3https://github.com/huggingface/transformers/tree/main/examples/pytorch\n14", "start_char_idx": 0, "end_char_idx": 2230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d2a554e-e164-47e6-891b-0360630b1770": {"__data__": {"id_": "1d2a554e-e164-47e6-891b-0360630b1770", "embedding": null, "metadata": {"page_label": "15", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21df284e-7d94-4293-9f96-ac69368fe69a", "node_type": "4", "metadata": {"page_label": "15", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "57a9adf58968ac5a737dfc493c534cb82d3940e983fa3e4bc528c2071861da5e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 13: Hyper-parameter setup of LoftQ for GLUE benchmark for training DeBERTaV3-base\nusing NF2 quantization.\nHyper-parameter MNLI RTE QNLI MRPC QQP SST-2 CoLA STS-B SQuADv1.1 ANLI\n# epochs 5 20 10 60 10 10 60 60 10 12\nLearning rate 1\u00d710\u221245\u00d710\u221255\u00d710\u221251\u00d710\u221245\u00d710\u221255\u00d710\u221255\u00d710\u221251\u00d710\u221245\u00d710\u221255\u00d710\u22125\nE S UMMARIZATION\nE.1 T RAINING DETAILS\nWe choose Adam as the optimizer and try learning rate from {1\u00d710\u22125,5\u00d710\u22125,7\u00d710\u22125,2\u00d7\n10\u22124,3\u00d710\u22124,4\u00d710\u22124}. We show the optimal learning rate for different settings in Table 14.\nWe use LoftQ of 1 iteration for all BART-large experiments. Table 14 and Table 15 summarize the\nlearning rate and other hyper-parameters for CNN/DailyMail and XSum.\nTable 14: Hyper-parameter setup of LoftQ BART-large on CNN/DailyMail\nHyperparameterNF4 4-bit Uniform NF2\nrank8 rank16 rank8 rank16 rank8 rank16\nLearning rate 2e-4 2e-4 2e-4 3e-4 2e-4 2e-4\nEpoch 15 15 15 15 15 15\nBatch size 64 64 64 64 64 64\nTable 15: Hyper-parameter setup of LoftQ BART-large on XSum\nHyperparameterNF4 4-bit Uniform NF2\nrank8 rank16 rank8 rank16 rank8 rank16\nLearning rate 2e-4 2e-4 2e-4 2e-4 2e-4 2e-4\nEpoch 25 25 25 25 25 25\nBatch size 32 32 32 32 32 32\nF N ATURAL LANGUAGE GENERATION\nWe set the batch size as 32 for WikiText-2 and 16 for GSM8K. We train 2 epochs on WikiText-2 and\n6 epochs on GSM8K. We select learning rate from {1\u00d710\u22125,5\u00d710\u22125,7\u00d710\u22125,1\u00d710\u22124, ,3\u00d7\n10\u22124,4\u00d710\u22124}. Specific settings are summarized in Table 16 and Table 17.\nG C OMPARISON TO PRUNING\nPruning is also a widely used compression method. Here we compare LoftQ with the state-of-the-\nart pruning method Li et al. (2023). We show the comparison in Table 18. We can see our method\nsignificantly outperforms the pruning methods on DeBERTaV3-base model. We also remark that\nLoftQ can consistently reduce the memory of both training and storage. In contrast, pruning requires\ntraining the entire full-precision matrix, which implies that it can not achieve any memory savings\nduring the training stage.\nH E XTENSION TO CONVOLUTIONAL LAYERS\nLow-rank adapters can also be applied to convolutional layers. Given an input feature map\nX\u2208Rh\u00d7w\u00d7c1andc22D convolutional kernels Ki\u2208Rc1\u00d7d\u00d7d, i= 1,2, ..., c 2, the output of\nthe convolutional layer is\nY= stack( X\u2297K1, ..., X \u2297Kc2), (10)\nwhere Y\u2208Rh\u00d7w\u00d7c2and\u2297denotes the 2D convolution operation.\n15", "start_char_idx": 0, "end_char_idx": 2341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "853a75e8-7956-4a3b-9d0c-b7e1536fe135": {"__data__": {"id_": "853a75e8-7956-4a3b-9d0c-b7e1536fe135", "embedding": null, "metadata": {"page_label": "16", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79ec8438-36b0-40cf-b80e-be587f4a64a2", "node_type": "4", "metadata": {"page_label": "16", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1b90cabab72a6e3cee0dd99a60280183aa5c9a4f98bf03e00f84c24350ba37de", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 16: Hyper-parameter setup of LoftQ LLAMA-2-series on GSM8K\nModel Hyperparameter NF4 NF2 Mixed-precision\nLLAMA-2-7b learning rate 3\u00d710\u221243\u00d710\u221243\u00d710\u22124\nLLAMA-2-13b learning rate 1\u00d710\u221241\u00d710\u221243\u00d710\u22124\nTable 17: Hyper-parameter setup of LoftQ LLAMA-2-series on WikiText-2\nModel Hyperparameter NF4 NF2 Mixed-precision\nLLAMA-2-7b learning rate 3\u00d710\u221243\u00d710\u221243\u00d710\u22124\nLLAMA-2-13b learning rate 1\u00d710\u221241\u00d710\u221243\u00d710\u22124\nTable 18: Results of LoftQ using 2-bits uniform quantization compared with LoSparse with\nDeBERTaV3-base models on some of GLUE development sets. Here Ratio is the proportion of\ntotal remaining weights. Results with N.A. indicate the model does not converge.\nMethod RatioMNLI SST-2 QNLI\nm / mm Acc Acc\nFull FT 100% 90.5 / 90.6 95.3 94.0\nLoSparse15% 83.3/82.9 87.6 90.4\n20% 84.5/83.8 91.7 88.6\nLoftQ15.6% 87.3/87.1 94.0 90.6\n18.8% 88.0/88.1 94.7 92.4\nWe can reformulate Equation (10) into matrix multiplication as\nY=Z\u00d7H\u22a4,\nwhere Z\u2208Rhw\u00d7c1d2, H\u2208Rc2\u00d7c1d2, by extending and flattening the input Xtogether with con-\ncatenating and flattening kernels. We first extend a vector xi,j\u2208Rc1by its neighbor vectors within\nthe kernel window:\nx\u2032\ni,j= Concat(xi\u2212d\n2,j\u2212d\n2, ...,xi+d\n2,j+d\n2).\nNow, Xbecomes X\u2032\u2208Rh\u00d7w\u00d7c1d2. We then flatten X\u2032intoZ\u2208Rhw\u00d7c1d2. For kernels, we first\nconcatenate {K1, ..., K c2}intoH\u2032\u2208Rc2\u00d7c1\u00d7d\u00d7d. We then flatten H\u2032intoH.\nNote that Hcan be approximated by a low-rank matrix\nR=UV\u22a4,\nwhere U\u2208Rc2\u00d7r, V\u2208Rc1d2\u00d7r, r\u226amin{c2, c1d2}by SVD. Therefore, the original convolution\nlayer can be approximated as\nbY=Z\u00d7(UV\u22a4)\u22a4(11)\n= (Z\u00d7V)\u00d7U\u22a4(12)\n=M\u00d7U\u22a4. (13)\nNote that Z\u00d7Vcan be restored into a convolution operation where we have rkernels Di\u2208\nRc1\u00d7d\u00d7d, i= 1,2, , ..., r andM\u00d7U\u22a4can also be restored into a convolution operation where\nwe have c2kernels Ui\u2208Rr\u00d71\u00d71, i= 1,2, , ..., c 2.\n16", "start_char_idx": 0, "end_char_idx": 1818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd362fce-8ded-49a5-8abc-145d91ee15e2": {"__data__": {"id_": "bd362fce-8ded-49a5-8abc-145d91ee15e2", "embedding": null, "metadata": {"page_label": "1", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "881cb765-d982-49fa-b600-0d81db28fc02", "node_type": "4", "metadata": {"page_label": "1", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "efe8e2297bf7a5e36b3238268a5421fd35c4aeff6d69a23bd431b74954118ab7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nLONG LORA: E FFICIENT FINE-TUNING OF LONG -\nCONTEXT LARGE LANGUAGE MODELS\nYukang Chen1Shengju Qian1Haotian Tang2Xin Lai1\nZhijian Liu2Song Han2,3Jiaya Jia1\n1CUHK2MIT3NVIDIA\nABSTRACT\nWe present LongLoRA, an efficient fine-tuning approach that extends the context\nsizes of pre-trained large language models (LLMs), with limited computation cost.\nTypically, training LLMs with long context sizes is computationally expensive,\nrequiring extensive training hours and GPU resources. For example, training on\nthe context length of 8192 needs 16 \u00d7computational costs in self-attention layers\nas that of 2048. In this paper, we speed up the context extension of LLMs in\ntwo aspects. On the one hand, although dense global attention is needed during\ninference, fine-tuning the model can be effectively and efficiently done by sparse\nlocal attention. The proposed shifted sparse attention (S2-Attn) effectively enables\ncontext extension, leading to non-trivial computation saving with similar perfor-\nmance to fine-tuning with vanilla attention. Particularly, it can be implemented\nwith only two lines of code in training, while being optional in inference. On\nthe other hand, we revisit the parameter-efficient fine-tuning regime for context\nexpansion. Notably, we find that LoRA for context extension works well under\nthe premise of trainable embedding and normalization. LongLoRA combines this\nimproved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results\non various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends\nLlama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 \u00d7A100\nmachine. LongLoRA extends models\u2019 context while retaining their original archi-\ntectures, and is compatible with most existing techniques, like Flash-Attention2.\nIn addition, we further conduct supervised fine-tuning with LongLoRA and our\nlong instruction-following LongAlpaca dataset. All our code, models, dataset, and\ndemo are available at github.com/dvlab-research/LongLoRA.\n2.662.522.482.782.942.983.732.42.93.43.9\n8192163643276865536Perplexity46.357.468.825.634.646.369.820406080\n8192163643276865536GPU memory\nFull FTLoRALongLoRA7.416.339.85.211.324.652.403060908192163643276865536Training hours\nContexthoursGB\nContextContextOOMOOM92.5\nFigure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning,\nwhile still maintaining up to 1.8 \u00d7lower memory cost than full fine-tuning. Furthermore, LongLoRA\nimproves the training speed of LoRA by up to 1.8 \u00d7withS2-Attn. Llama2-7B are fine-tuned to\nvarious context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage\n2 and evaluated on the proof-pile (Azerbayev et al., 2022) test set in perplexity.\n1 I NTRODUCTION\nLarge language models (LLMs) are typically trained with a pre-defined context size, such as 2048\ntokens for LLaMA (Touvron et al., 2023a) and 4096 tokens for Llama2 (Touvron et al., 2023b).\n1", "start_char_idx": 0, "end_char_idx": 2969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4418b598-325d-48fb-a080-cd37465ead7e": {"__data__": {"id_": "4418b598-325d-48fb-a080-cd37465ead7e", "embedding": null, "metadata": {"page_label": "2", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4fe791f-3650-46d6-8c7b-18ba71594281", "node_type": "4", "metadata": {"page_label": "2", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "53473f5b0a958ff1ea0284daaf9f71d60e072407c8405eec1cf21f3a20c1ca1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10707d7c-f678-4d42-94b0-21e0488d93dc", "node_type": "1", "metadata": {}, "hash": "760050db2c3d7b5fd8662455dc7632f0d1609bf769772257a64943081bbd920d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n!Trainable\n\u2744FrozenNormpostMul(-headSelf-A1en(onFeed ForwardNorminput++Lora\n!\n!Embedding\n!\n!\n\u2744(b) Low-rank adaptx N\n\u2744Pa1ern 1 -w/o shi@(a) Shi3ed sparse a6en7onEach pa1ern in half heads\na\n!Norm (0.004%)a\n!Lora (0.12%) a\n\u2744 Linear Projec3on (96%) a\n!Embedding (1.94%)a\n\u2744Head (1.94%) (c) Propor5ons of Parameters (LLaMA7B for example) Pa1ern 2 -w/ shi@Combina(on\nFigure 2: Overview of LongLoRA . We introduce Shifted Sparse Attention (S2-Attn) during fine-\ntuning. The trained model retains original standard self-attention at inference time. In addition to\ntraining LoRA weights in linear layers, LongLoRA further makes embedding and normalization\nlayers trainable. This extension is pivotal for context extension, and only introduces a minimal\nnumber of additional trainable parameters.\nHowever, the pre-defined size limits LLMs in many applications, like summarizing long documents\nor answering long questions. To resolve this limitation, some recent works (Chen et al., 2023;\nTworkowski et al., 2023; Mohtashami & Jaggi, 2023) train or fine-tune LLMs to longer context.\nHowever, training an LLM from scratch with long sequences poses computational challenges, and\nfine-tuning an existing pre-trained LLM is also considerably expensive. For instance, Position\nInterpolation (Chen et al., 2023) spent 32 A100 GPUs to extend LLaMA models from 2k to 8k\ncontext, and 128 A100 GPUs for longer context fine-tuning. FOT (Tworkowski et al., 2023) used 32\nTPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources\nare typically unaffordable for common researchers, which naturally leads us to question: can we\nextend the context window of LLMs efficiently?\nOne straightforward approach is to fine-tune a pre-trained LLM via low-rank adaptation (LoRA) (Hu\net al., 2022). LoRA modifies the linear projection layers in self-attention blocks by utilizing low-rank\nmatrices, which are generally efficient and reduce the number of trainable parameters. However, our\nempirical findings indicate that training long context models in this manner is neither sufficiently\neffective nor efficient. In terms of effectiveness , plain low-rank adaptation results in a high perplexity\nin long context extension, as in Table 2. Increasing the rank to a higher value, e.g., rank = 256,\ndoes not alleviate this issue. In terms of efficiency , regardless of whether LoRA is employed or not,\ncomputational cost increases dramatically as the context size expands, primarily due to the standard\nself-attention mechanism (Vaswani et al., 2017). As shown in Figure 1, even with LoRA, the training\nhours for the standard Llama2 model increase substantially when the context window expands.\nIn this work, we introduce LongLoRA, an efficient fine-tuning approach that extends the context\nwindows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses\nlow-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\nas an efficient substitute for standard self-attention. As shown in Figure 2, we split context length\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\nthe tokens by half group size, which ensures the information flow between neighboring groups. For\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).\nModels fine-tuned via S2-Attn retain the original attention architecture during inference. This\nfacilitates most existing optimization and infrastructure. Techniques for common LLMs can also be\napplied to ours.", "start_char_idx": 0, "end_char_idx": 3816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10707d7c-f678-4d42-94b0-21e0488d93dc": {"__data__": {"id_": "10707d7c-f678-4d42-94b0-21e0488d93dc", "embedding": null, "metadata": {"page_label": "2", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4fe791f-3650-46d6-8c7b-18ba71594281", "node_type": "4", "metadata": {"page_label": "2", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "53473f5b0a958ff1ea0284daaf9f71d60e072407c8405eec1cf21f3a20c1ca1a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4418b598-325d-48fb-a080-cd37465ead7e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8c16e8f03341ceb8323ca011c9099ba3cebe26e240bee6b2b21fda309c27fa15", "class_name": "RelatedNodeInfo"}}, "text": "Similarly, we find that short attention is\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\nas an efficient substitute for standard self-attention. As shown in Figure 2, we split context length\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\nthe tokens by half group size, which ensures the information flow between neighboring groups. For\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).\nModels fine-tuned via S2-Attn retain the original attention architecture during inference. This\nfacilitates most existing optimization and infrastructure. Techniques for common LLMs can also be\napplied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our\nmethod in both training and inference time. The reason behind this is that short attention resembles\nthe attention scheme in the pre-training stage of LLMs. Other efficient attentions, e.g., dilated or\nsparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6.\nWe empirically show that learnable embedding and normalization layers are the key to unlocking\nlong context LoRA fine-tuning, in Table 2. Embedding and normalization layers take up a small\n2", "start_char_idx": 2984, "end_char_idx": 4396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b0bdff0-0fc2-468a-8f1e-6d6c17081ad7": {"__data__": {"id_": "2b0bdff0-0fc2-468a-8f1e-6d6c17081ad7", "embedding": null, "metadata": {"page_label": "3", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c0c5472-3d1e-44ba-80db-6ffabc7e8951", "node_type": "4", "metadata": {"page_label": "3", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "773ee8bff01ff0c1736eed58fe04fbcff72aa7880a570ceace5be735c7d79c79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23e8cf71-ec95-4e9e-88a0-70c3539ea815", "node_type": "1", "metadata": {}, "hash": "d801c06819e823dd98f9dbc86b6ee6a6fea086f66fc543ff813cc4da22c96bc4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nStep 2Shi$ tokensStep 3ReshapeGroupsizeHalf group\nSelf-a8en9on in each groupTokensMul9-headsSelf-a8en9on among all tokensSelf-attentionShifted short attentionStep 1Split headsHalf heads\nStep 2Shi$Step 3GroupHalf\nS2-AttnStep 1SplitHalf headsAttention w/o shift!\"Attention w shift12345678TokensHeads123456788123456712345678123456781234567881234567Group\nPa8ern1!\"Pa8ern2Pa8ern1Pa8ern2InputsSplit a8en9on headsinto 2 partsShi$ the 2ndpartby half group\nFigure 3: Illustration of S2-Attn . It involves three steps. First, it splits features along the head\ndimension into two chunks. Second, tokens in one of the chunks are shifted by half of the group size.\nThird, we split tokens into groups and reshape them into batch dimensions. Attention only computes\nin each group in ours while the information flows between groups via shifting. Potential information\nleakage might be introduced by shifting, while this is easy to prevent via a small modification on the\nattention mask. We ablate this in the variant 2 in Section B.3 in the appendix.\nproportion of parameters in the entire LLM. For example, embedding has ( <2%) parameters, and\nnormalization has ( \u22640.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs.\nIn experiments, we show that LongLoRA is effective and efficient. We present experimental results\nof extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings\nof Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings.\nThe trained models achieve comparable performance to the full-attention and fully fine-tuned results,\nwhile the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B\nup to 100k context, or a 70B model up to 32k, on a single 8\u00d7A100 machine.\nIn addition, we present a solution for supervised fine-tuning (SFT) with our self-collected long\ninstruction-following dataset, LongAlpaca. Our LongLoRA models are further fine-tuned with long\nquestions and the corresponding answers. We design various types of questions for technical papers,\nscience fiction, and other books. SFT is important for improving the chat ability of LLMs. We\nintroduce our SFT settings in Section B.6 in the appendix.\n2 R ELATED WORK\nLong-context Transformers. A large body of research has been developed to increase the context\nlength of transformers. Some of these approaches are retrieval-based (Karpukhin et al., 2020; Izacard\net al., 2022; Guu et al., 2020), which augment language models via fetching related documents and\nincluding the retrieved results into contexts. Our work is complementary to these works, as our\nattention mechanism is unmodified during inference. Many works modify multi-head attention to be\napproximated ones (Wang et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020;\nBulatov et al., 2022; Ding et al., 2023; Qiu et al., 2020). They alleviate the quadratic complexity of\nthe self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer\net al., 2020) use sparse attention to handle long sequences. Other works (Wu et al., 2022; Bulatov\net al., 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens.\nOne limitation of these works is that these compressions have a large gap to full attention, making\nit infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of\nattention mechanism, it has a similar shape and a small gap to standard attention. This enables\nfine-tuning pre-trained LLMs on S2-Attn and maintain full attention during inference.\nLong-context LLMs.", "start_char_idx": 0, "end_char_idx": 3690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23e8cf71-ec95-4e9e-88a0-70c3539ea815": {"__data__": {"id_": "23e8cf71-ec95-4e9e-88a0-70c3539ea815", "embedding": null, "metadata": {"page_label": "3", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c0c5472-3d1e-44ba-80db-6ffabc7e8951", "node_type": "4", "metadata": {"page_label": "3", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "773ee8bff01ff0c1736eed58fe04fbcff72aa7880a570ceace5be735c7d79c79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b0bdff0-0fc2-468a-8f1e-6d6c17081ad7", "node_type": "1", "metadata": {"page_label": "3", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f6a3251f2c7a1794ef994db9c8ed4aa29f0fcf0d9bd0c56b63f20b409c7bad7a", "class_name": "RelatedNodeInfo"}}, "text": "They alleviate the quadratic complexity of\nthe self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer\net al., 2020) use sparse attention to handle long sequences. Other works (Wu et al., 2022; Bulatov\net al., 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens.\nOne limitation of these works is that these compressions have a large gap to full attention, making\nit infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of\nattention mechanism, it has a similar shape and a small gap to standard attention. This enables\nfine-tuning pre-trained LLMs on S2-Attn and maintain full attention during inference.\nLong-context LLMs. LLMs are typically pre-trained with a pre-defined context length, such as\n2048 for LLaMA (Touvron et al., 2023a) and 4096 for Llama2 (Touvron et al., 2023b). Training LLMs\nwith long context from scratch is prohibitively expensive for most researchers. Recently, several\nworks have tried to extend the context length of LLMs via fine-tuning. Position Interpolation (Chen\net al., 2023) modifies rotary position encoding (Su et al., 2021) and extends the context length of\nLLaMA to 32768. Focused Transformer (Tworkowski et al., 2023) utilizes contrastive learning\nto train LongLLaMA. Both of them rely on full fine-tuning, which is computationally expensive\n(128 A100 GPUs / 128 TPUv3 for training). Landmark attention (Mohtashami & Jaggi, 2023) is an\n3", "start_char_idx": 2951, "end_char_idx": 4442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3886b414-eb71-4230-a54d-11f69a7c22a9": {"__data__": {"id_": "3886b414-eb71-4230-a54d-11f69a7c22a9", "embedding": null, "metadata": {"page_label": "4", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e7afc4c-a116-40e4-ae81-c34ed7dfaf16", "node_type": "4", "metadata": {"page_label": "4", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e4c02ab68552c544952b201e1fad5519d652024fed94ffc59edf6c8ac41f4772", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94658c9a-baf2-4a0f-9807-49086e380407", "node_type": "1", "metadata": {}, "hash": "11d2b83c98975ec44556a7f25cf8b78a4dbe2e16b1b4d39e7e665883c1a5952e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 1: Effectiveness of S2-Attn under different context lengths . \u2018Short\u2019 means 1/4 of the target\ncontext length, while \u2018Long\u2019 equals to the target context length. Models are fully fine-tuned upon\na Llama2 (Touvron et al., 2023b) model with 7B parameters on the RedPajama (Computer, 2023)\ndataset. Results are tested in perplexity on PG19 (Rae et al., 2020) validation split.\nSetting Position EmbeddingTraining Target Context Length\nAttention Shift 8192 16384 32768\nFull Attn\nPI (Chen et al., 2023)Long - 8.02 8.05 8.04\nShort Attn Short \u2717 8.29 8.83 9.47\nS2-Attn Short \u2713 8.04 8.03 8.08\nefficient approach, but somewhat lossy. It compresses long context inputs into retrieved tokens. Our\nmethod saves substantial fine-tuning costs, while preserving the quality of the original attention. Ours\nmaintain full access to the entire input via unmodified attention during inference.\nSome literature focuses on the position embedding modification of LLMs for long context extension,\nincluding Position Interpolation (Chen et al., 2023), NTK-aware (ntk, 2023), Yarn (Peng et al., 2023),\npositional Skipping (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al.,\n2023). Our method focuses on efficient fine-tuning and retaining the original architecture during\ninference, which is orthogonal to these position embedding methods.\nEfficient Fine-tuning. This work is based on LoRA (Hu et al., 2022), a classical efficient fine-tuning\napproach. In addition to LoRA (Hu et al., 2022), there are many other parameter-efficient fine-tuning\nmethods, including prompt tuning (Lester et al., 2021), prefix tuning (Li & Liang, 2021), hidden state\ntuning (Liu et al., 2022), bias tuning (Zaken et al., 2022), and masked weight learning (Sung et al.,\n2021). Input-tuning (An et al., 2022) introduces an adapter to tune input embedding. Although the\ninput embedding layers are also trainable in ours, this is not enough for long context extension. We\nmake a comprehensive analysis on layer types in experiments, in Table 2. Existing work (Chen et al.,\n2022) shows sparse masks can effectively save training costs and avoid performance drops.\n3 L ONG LORA\n3.1 B ACKGROUND\nTransformer. LLMs are typically built with transformers. Taking Llama2 (Touvron et al., 2023b)\nfor example, as shown in Figure 2, an LLM model consists of an embedding input layer and a number\nof decoder layers. Each decoder layer comprises a self-attention module. It maps input features\ninto a set of queries, keys, and values {q, k, v}, via linear projection layers with weight matrices\n{Wq, Wk, Wv}. Given {q, k, v}, it computes the outputs oas\no= softmax( qkT)v (1)\nThe outputs are then projected by a linear layer with a weight matrix Wo. And MLP layers are\nfollowed. Before and after self-attention modules, layer normalization (Ba et al., 2016) is applied. A\nfinal normalization is conducted after all decoder layers.\nFor long sequences, self-attention struggles with computation cost, which is quadratic to the sequence\nlength. This dramatically slows down the training procedure and increases GPU memory costs.\nLow-rank Adaptation. LoRA (Hu et al., 2022) hypothesizes that the weight updates in pre-trained\nmodels have a low intrinsic rank during adaptation. For a pre-trained weight matrix W\u2208Rd\u00d7k, it is\nupdated with a low-rank decomposition W+ \u2206W=W+BA, where B\u2208Rd\u00d7randA\u2208Rr\u00d7k.\nThe rank r\u226amin(d, k). During training, Wis frozen with no gradient updates, while A and B are\ntrainable. This is the reason why LoRA training is much more efficient than full fine-tuning.", "start_char_idx": 0, "end_char_idx": 3597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94658c9a-baf2-4a0f-9807-49086e380407": {"__data__": {"id_": "94658c9a-baf2-4a0f-9807-49086e380407", "embedding": null, "metadata": {"page_label": "4", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e7afc4c-a116-40e4-ae81-c34ed7dfaf16", "node_type": "4", "metadata": {"page_label": "4", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e4c02ab68552c544952b201e1fad5519d652024fed94ffc59edf6c8ac41f4772", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3886b414-eb71-4230-a54d-11f69a7c22a9", "node_type": "1", "metadata": {"page_label": "4", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "857765831bda136f4e42054bc0a9b87f28f1b935bcec315aac3f31bb6a5757f7", "class_name": "RelatedNodeInfo"}}, "text": "A\nfinal normalization is conducted after all decoder layers.\nFor long sequences, self-attention struggles with computation cost, which is quadratic to the sequence\nlength. This dramatically slows down the training procedure and increases GPU memory costs.\nLow-rank Adaptation. LoRA (Hu et al., 2022) hypothesizes that the weight updates in pre-trained\nmodels have a low intrinsic rank during adaptation. For a pre-trained weight matrix W\u2208Rd\u00d7k, it is\nupdated with a low-rank decomposition W+ \u2206W=W+BA, where B\u2208Rd\u00d7randA\u2208Rr\u00d7k.\nThe rank r\u226amin(d, k). During training, Wis frozen with no gradient updates, while A and B are\ntrainable. This is the reason why LoRA training is much more efficient than full fine-tuning.\nIn the Transformer structure, LoRA only adapts the attention weights ( Wq, Wk, Wv, Wo) and freezes\nall other layers, including MLP and normalization layers. This manner is simple and parameter-\nefficient. However, we empirically show that only low-rank adaptation in attention weights does not\nwork for long context extension.\n4", "start_char_idx": 2887, "end_char_idx": 3926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "000db7ee-1c63-4351-b9e2-a79db2a1a294": {"__data__": {"id_": "000db7ee-1c63-4351-b9e2-a79db2a1a294", "embedding": null, "metadata": {"page_label": "5", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "accffe54-4dc0-4d46-9257-63ea5d7f77c2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fd9058ffc664c81adc05b932c59dd4ef49ba3957708e7bd12d8c803771ddc60b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7fd96e1-f067-48bf-862c-587908f9d7dc", "node_type": "1", "metadata": {}, "hash": "c46f0eecfc431fa4beb816f19a80224cb628019d25f463e1fa68a56eab61c67f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1: Pseudocode of S2-Attn in PyTorch-like style.\n# B: batch size; S: sequence length or number of tokens; G: group size;\n# H: number of attention heads; D: dimension of each attention head\n# qkv in shape (B, N, 3, H, D), projected queries, keys, and values\n# Key line 1: split qkv on H into 2 chunks, and shift G/2 on N\nqkv = cat((qkv.chunk(2, 3)[0], qkv.chunk(2, 3)[1].roll(-G/2, 1)), 3).view(B *N/G,G,3,H,D)\n# standard self-attention function\nout = self_attn(qkv)\n# out in shape (B, N, H, D)\n# Key line 2: split out on H into 2 chunks, and then roll back G/2 on N\nout = cat((out.chunk(2, 2)[0], out.chunk(2, 2)[1].roll(G/2, 1)), 2)\ncat: concatenation; chunk : split into the specified number of chunks; roll : roll the tensor along the given dimension.\n3.2 S HIFTED SPARSE ATTENTION\nStandard self-attention costs O(n2)computations, making LLMs on long sequences high memory\ncost and slow. To avoid this issue during training, we propose Shifted Sparse Attention (S2-Attn), as\nshown in Figure 2. In the following, we make a pilot study and explain our design step by step.\nPilot Study. In Table 1, we build up a standard baseline that is trained and tested with full attention\nand fine-tuning, which presents consistently good quality in various context lengths. The first trial\nis to train with short attention, only pattern 1 in Figure 2. As we know for a long context, the high\ncost mainly comes from self-attention modules. Thus, in this trial, since the input is long, we split\ninto several groups in self-attention. For example, the model takes 8192 tokens as input in both the\ntraining and testing stages, but self-attention is conducted in each group with a 2048 size. The group\nnumber is 4, as ablated in Section B.2 in the appendix. This pattern is efficient but still does not work\nin a very long context, as shown in Table 1. The perplexity becomes larger as the context length\nincreases. The reason behind this is that there is no information exchange between different groups.\nTo introduce communication between groups, we include a shifted pattern, as shown in Figure 2. We\nshift the group partition by half group size in half attention heads. Taking the overall 8192 context\nlength for example, in pattern 1, the first group conducts self-attention from 1stto 2048thtokens.\nIn Pattern 2, the group partition is shifted by 1024. The first attention group begins from 1025th\nand ends at 3072thtokens, while the first and the last 1024 tokens belong to the same group. We\nuse patterns 1 and 2 in each half self-attention heads respectively. This manner does not increase\nadditional computation costs but enables the information flow between different groups. We show\nthat it gets close to the standard attention baseline in Table 1.\nConsistency to Full Attention. Existing efficient attention designs can also improve the efficiency\nof long-context LLMs. However, most of them are not suitable for long-context fine-tuning. Because,\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\nto the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not\nonly enables efficient fine-tuning but also supports fullattention testing . Although other attentions\ncan also be used in long context fine-tuning, models must be tested with the attention used during\nfine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\nlines of code are enough. We provide a PyTorch-style code in Algorithm 1.", "start_char_idx": 0, "end_char_idx": 3772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7fd96e1-f067-48bf-862c-587908f9d7dc": {"__data__": {"id_": "a7fd96e1-f067-48bf-862c-587908f9d7dc", "embedding": null, "metadata": {"page_label": "5", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "accffe54-4dc0-4d46-9257-63ea5d7f77c2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fd9058ffc664c81adc05b932c59dd4ef49ba3957708e7bd12d8c803771ddc60b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "000db7ee-1c63-4351-b9e2-a79db2a1a294", "node_type": "1", "metadata": {"page_label": "5", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "951d91e24289aa9861c691691e737b361ec334663bcd308d5e6b74054b68d0a3", "class_name": "RelatedNodeInfo"}}, "text": "Because,\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\nto the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not\nonly enables efficient fine-tuning but also supports fullattention testing . Although other attentions\ncan also be used in long context fine-tuning, models must be tested with the attention used during\nfine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\nlines of code are enough. We provide a PyTorch-style code in Algorithm 1.\n3.3 I MPROVED LORA FOR LONG CONTEXT\nLoRA (Hu et al., 2022) is an efficient and popular manner for adapting LLMs to other datasets. It\nsaves much trainable parameters and memory cost, compared to full fine-tuning. However, adapting\nLLMs from short context length to long is not easy. We empirically observe an obvious gap between\nLoRA and full fine-tuning. As shown in Table 2, the gap between LoRA and full fine-tuning grows\nas the target context length becomes larger. And LoRA with larger ranks cannot reduce the gap.\n5", "start_char_idx": 2992, "end_char_idx": 4294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34339bca-1c10-4566-ad77-9d0c172c112b": {"__data__": {"id_": "34339bca-1c10-4566-ad77-9d0c172c112b", "embedding": null, "metadata": {"page_label": "6", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2884ad43-1757-4563-a2e4-ff52c6b9a0f8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "29123d4f6477c023ba173b5da30354687916e047d5927708270d4778957efa1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bce8a5f7-4c54-4fc9-90d5-4f23089fc9a2", "node_type": "1", "metadata": {}, "hash": "3c1f00447fdee7c874a0548a671edd83a7468c0be7a819e967b14864a7d574a4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 2: Finetuning normalization and embedding layers is crucial for low-rank long-context\nadaptation . Llama2 7B (Touvron et al., 2023b) models with the proposed S2-Attn are trained on the\nRedPajama (Computer, 2023) dataset. The target context length is 32768. \u2018+ Normal / Embed\u2019 means\nnormalization or embedding layers are trainable. Perplexity results are evaluated on PG19 (Rae et al.,\n2020) validation set. For long context adaptation, there is a large performance gap between standard\nLoRA (Hu et al., 2022) and full fine-tuning. Without trainable normalization or embeddings, larger\nranks in LoRA can not close this gap.\nMethod Full FTLoRA (rank) LoRA (rank = 8)\n8 16 32 64 128 256 + Norm + Embed + Norm & Embed\nPPL 8.08 11.44 11.82 11.92 11.96 11.97 11.98 10.49 8.29 8.12\nTable 3: Perplexity evaluation on proof-pile (Rae et al., 2020) test split. S2-Attn: Shifted Sparse\nAttention. LoRA+: improved LoRA. We fine-tune Llama2 (Touvron et al., 2023b) in 7B and 13B\nmodel sizes on the RedPajama (Computer, 2023) dataset under 8k-32k context lengths. We show\nthat our method achieves comparable performance to the full attention or full FT baselines, with\nbetter efficiency. We use the same training setting as the model evaluated on PG19 (Rae et al., 2020)\nintroduced in Section B.1 in the appendix.\nSizeTraining\nContext LengthLongLoRA Evaluation Context Length\nS2-Attn LoRA+2048 4096 8192 16384 32768\n7B81923.14 2.85 2.66 - -\n\u2713 3.15 2.86 2.68 - -\n\u2713 \u2713 3.20 2.91 2.72 - -\n16384\u2713 3.17 2.87 2.68 2.55 -\n\u2713 \u2713 3.17 2.87 2.66 2.51 -\n32768\u2713 3.20 2.90 2.69 2.54 2.49\n\u2713 \u2713 3.35 3.01 2.78 2.61 2.50\n13B81922.96 2.69 2.53 - -\n\u2713 3.01 2.74 2.57 - -\n\u2713 \u2713 3.04 2.77 2.60 - -\n16384\u2713 2.99 2.72 2.53 2.40 -\n\u2713 \u2713 3.03 2.74 2.55 2.41 -\n32768\u2713 3.04 2.75 2.56 2.42 2.33\n\u2713 \u2713 3.05 2.76 2.57 2.42 2.32\nTo bridge this gap, we open embedding and normalization layers for training. As shown in Table 2,\nthey occupy limited parameters but make effects for long context adaptation. Especially for normal-\nization layers, the parameters are only 0.004% in the whole Llama2 7B. We denote this improved\nversion of LoRA as LoRA+in experiments.\n4 E XPERIMENT\n4.1 E XPERIMENTAL SETTINGS\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models. The\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\nInterpolation (Chen et al., 2023).\nTraining Procedure We follow most training hyper-parameters in Position Interpolation (Chen\net al., 2023), except that our batch size is smaller as we use a single 8 \u00d7A100 GPUs machine in some\ncases.", "start_char_idx": 0, "end_char_idx": 2707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bce8a5f7-4c54-4fc9-90d5-4f23089fc9a2": {"__data__": {"id_": "bce8a5f7-4c54-4fc9-90d5-4f23089fc9a2", "embedding": null, "metadata": {"page_label": "6", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2884ad43-1757-4563-a2e4-ff52c6b9a0f8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "29123d4f6477c023ba173b5da30354687916e047d5927708270d4778957efa1f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34339bca-1c10-4566-ad77-9d0c172c112b", "node_type": "1", "metadata": {"page_label": "6", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "58225db3875e68732e5dee578fec005361e0542760ae3773060a423ddf63643b", "class_name": "RelatedNodeInfo"}}, "text": "We denote this improved\nversion of LoRA as LoRA+in experiments.\n4 E XPERIMENT\n4.1 E XPERIMENTAL SETTINGS\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models. The\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\nInterpolation (Chen et al., 2023).\nTraining Procedure We follow most training hyper-parameters in Position Interpolation (Chen\net al., 2023), except that our batch size is smaller as we use a single 8 \u00d7A100 GPUs machine in some\ncases. All models are fine-tuned via the next token prediction objective. We use AdamW (Loshchilov\n& Hutter, 2019) with \u03b21= 0.9and\u03b22= 0.95. The learning rate is set to 2\u00d710\u22125for 7B and 13B\nmodels, and 10\u22125for 70B models. We also use a linear learning rate warmup. The weight decay is\n6", "start_char_idx": 2093, "end_char_idx": 2986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6467583b-0c5e-401d-abe2-c0f3885bb686": {"__data__": {"id_": "6467583b-0c5e-401d-abe2-c0f3885bb686", "embedding": null, "metadata": {"page_label": "7", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6846c9f3-ff0f-4970-9650-043ebedc77a3", "node_type": "4", "metadata": {"page_label": "7", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "73a5667f43e5cf0ebd3a87a4c8fba04ee5e2a54f1331dc75b60132025dc49a4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a1e7258-439c-4dba-adcc-049763b8ddd9", "node_type": "1", "metadata": {}, "hash": "01bb8e1088d27eeb4eb741839fad2e02c28f6ff51c7f57f9fe482186a1c4d15e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 4: Maximum context length that we can fine-tune for various model sizes on a single 8 \u00d7\nA100 machine. We use the same training and evaluation settings as in Table 3. We use Flash-\nAttention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) in stage 3 during fine-tuning. With\nLongLoRA, the maximum context length for 7B, 13B, and 70B models are 100k, 64k, and 32k\nrespectively. Evaluation on PG19 (Rae et al., 2020) is in Section B.1 in the appendix.\nSizeTraining\nContext LengthEvaluation Context Length\n2048 4096 8192 16384 32768 65536 100,000\n7B 100,000 3.36 3.01 2.78 2.60 2.58 2.57 2.52\n13B 65536 3.20 2.88 2.66 2.50 2.39 2.38 -\n70B 32768 2.84 2.57 2.39 2.26 2.17 - -\nTable 5: Topic retrieval evaluation with LongChat (Li et al., 2023). We compare our model to\nother open-source long-context LLMs. This task involves retrieving target topics from a very long\nconversation with around 3k, 6k, 10k, 13k, and 16k context lengths. As some questions in the\nevaluation set are longer than 16k, our model is fine-tuned upon Llama2 13B. It achieves comparable\nperformance to the state-of-the-art LongChat-13B (Li et al., 2023) with a lower fine-tuning cost.\nEvaluation Context 3k 6k 10k 13k 16k\nChatGLM2-6B (Du et al., 2022) 0.88 0.46 0.02 0.02 0.02\nMPT-30B-chat (Team, 2023a) 0.96 1.0 0.76 - -\nMPT-7B-storywriter (Team, 2023b) 0.46 0.46 0.28 0.34 0.36\nLongChat-13B (Li et al., 2023) 1.0 1.0 1.0 0.98 0.9\nOurs-13B 1.0 0.98 0.98 0.98 0.94\nzero. We set the per-device batch size as 1 and gradient accumulation steps as 8, which means that\nthe global batch size equals 64, using 8 GPUs. We train our models for 1000 steps.\nDatasets We use the Redpajama (Computer, 2023) dataset for training. We evaluate the long-\nsequence language modeling performance of our fine-tuned models on the book corpus dataset\nPG19 (Rae et al., 2020) and the cleaned Arxiv Math proof-pile dataset (Azerbayev et al., 2022). We\nuse the test split of PG19 (Rae et al., 2020), consisting of 100 documents. For the proof-pile dataset,\nwe also use the test split of it for evaluation. We follow Position Interpolation (Chen et al., 2023)\nfor proof-pile data processing. We evaluate perplexity by using a sliding window approach with\nS= 256 , following (Press et al., 2022).\n4.2 M AINRESULTS\nLong-sequence Language Modeling. In Table 3, we report the perplexity for our models and\nbaseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain training context\nlengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness\nof our efficient fine-tuning method. In Table 3, for the same training and evaluation context length\ncases, the perplexity decreases as the context size increases. By increasing the context window size\nfrom 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72\nto 2.50 by -0.22.", "start_char_idx": 0, "end_char_idx": 2916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a1e7258-439c-4dba-adcc-049763b8ddd9": {"__data__": {"id_": "1a1e7258-439c-4dba-adcc-049763b8ddd9", "embedding": null, "metadata": {"page_label": "7", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6846c9f3-ff0f-4970-9650-043ebedc77a3", "node_type": "4", "metadata": {"page_label": "7", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "73a5667f43e5cf0ebd3a87a4c8fba04ee5e2a54f1331dc75b60132025dc49a4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6467583b-0c5e-401d-abe2-c0f3885bb686", "node_type": "1", "metadata": {"page_label": "7", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1623da038572ed9478f2e83b9263a2512874217f254126395e3e3d84e415b6c0", "class_name": "RelatedNodeInfo"}}, "text": "We evaluate perplexity by using a sliding window approach with\nS= 256 , following (Press et al., 2022).\n4.2 M AINRESULTS\nLong-sequence Language Modeling. In Table 3, we report the perplexity for our models and\nbaseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain training context\nlengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness\nof our efficient fine-tuning method. In Table 3, for the same training and evaluation context length\ncases, the perplexity decreases as the context size increases. By increasing the context window size\nfrom 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72\nto 2.50 by -0.22. For Llama2 13B model, we observe that the perplexity reduces by -0.28.\nIn Table 4, we further examine the maximum context length that we can fine-tune on a single 8 \u00d7\nA100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 context length\nrespectively. LongLoRA achieves promising results on these extremely large settings. In addition,\nwe find some perplexity degradation on small context sizes for the extended models. This is a known\nlimitation of Position Interpolation (Chen et al., 2023).\nRetrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we\ncompare our model with other open LLMs on the topic retrieval task introduced in LongChat (Li et al.,\n2023). This task is to retrieve the target topic from a very long conversation, with lengths varying\nfrom 3k, 6k, 10k, 13k, to 16k. As some questions in LongChat (Li et al., 2023) are longer than 16k,\nwe fine-tuned Llama2 13B with a context length of 18k. The training cost is similar to that for 16k.\n7", "start_char_idx": 2185, "end_char_idx": 3931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a23d32e9-2c46-4fec-8e99-4492ececc5cb": {"__data__": {"id_": "a23d32e9-2c46-4fec-8e99-4492ececc5cb", "embedding": null, "metadata": {"page_label": "8", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61cfebe8-7b9e-422a-9794-8b0275e58f81", "node_type": "4", "metadata": {"page_label": "8", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0b99c0bc6c9eabfc1ff97ced9e6d0834e255e774e5c3ecec674eb40af55327d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "737e4f5c-c368-457b-8df1-65185af2f7ef", "node_type": "1", "metadata": {}, "hash": "13115b55c08ba135687f2e246dde4709043175b5e02ac529b15526ee3d9cde75", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n0%20%40%60%80%100%2k4k6k8k10k12k14k16k18k20k22k24k26k28k30k32k34k36k38k40k42k44k46k48kPasskey Retrieval Accuracy\nLlama2 7BLlama2 7B (extended PI)Ours 7BOurs 7B (extended PI)\nFigure 4: Accuracy comparison on passkey retrieval between Llama2 7B and our 7B model fine-tuned\non 32768 context length. Our model presents no retrieval accuracy degradation until 33k or 34k,\nwhich exceeds the context length. It can further enhance its capability of long sequence modeling\nthrough a straightforward extension of position embeddings, without additional fine-tuning.\n0%20%40%60%80%100%2k4k6k8k10k12k14k16k18k20k22k24k26k28k30k32k34k36k38k40k42k44k46k48kPasskey Retrieval Accuracy\nLlama2 7BOurs 7B 32kOurs 7B 32k (extended PI to 48k)15.828.638.168.158.148.128.118.18.088.048.0215.828.178.18.078.068.037.997.997.967.957.94681012141601002003004005006007008009001000Perplexity along \ufb01ne-tuning steps\nFull FTLoRA+Fine-tuning stepsPerplexity\nFigure 5: Ablation on fine-tuning steps in both full fine-tuning and LoRA+. We fine-tune Llama2 (Tou-\nvron et al., 2023b) 7B with the proposed S2-Attn. The target context length is 8192. We use RedPa-\njama (Computer, 2023) for training and PG19 (Rae et al., 2020) validation set for perplexity testing.\nFull fine-tuning converges faster than LoRA+at the beginning, but the final performance gap is small.\nOur model achieves comparable performance to LongChat-13B (Li et al., 2023), the state-of-the-art\nmodel in this task. Unlike LongChat-13B (Li et al., 2023), which is fully fine-tuned on self-collected\nlong context conversation text, our model is efficiently adapted on RedPajama (Computer, 2023) via\nnext-token generation. Our model even slightly outperforms LongChat-13B in the 16k evaluation.\nIn Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Atten-\ntion (Mohtashami & Jaggi, 2023). This task has also been adopted by other literature (Chen et al.,\n2023; Tworkowski et al., 2023). In this task, the models need to find a random passkey hidden in a\nlong document. We show the document format is in Section A.2 in the appendix. We study Llama2\n7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768\ncontext length. We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k\n(as the sentence length can not be precisely controlled). For each document length, we test the model\n10 times with different random passkey values. Our model achieves reasonable passkey retrieval\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\nmodel can handle longer documents by simply extending the position interpolation. As the dashed\norange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval\nability (60%-90% accuracy) in the range of 33k to 45k. Even with the position interpolation extended,\nLlama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the 4k context length.\n4.3 A BLATION STUDY\nIn this section, we introduce ablation studies on the number of fine-tuning steps and attention patterns.", "start_char_idx": 0, "end_char_idx": 3264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "737e4f5c-c368-457b-8df1-65185af2f7ef": {"__data__": {"id_": "737e4f5c-c368-457b-8df1-65185af2f7ef", "embedding": null, "metadata": {"page_label": "8", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61cfebe8-7b9e-422a-9794-8b0275e58f81", "node_type": "4", "metadata": {"page_label": "8", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0b99c0bc6c9eabfc1ff97ced9e6d0834e255e774e5c3ecec674eb40af55327d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a23d32e9-2c46-4fec-8e99-4492ececc5cb", "node_type": "1", "metadata": {"page_label": "8", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a98d8ff17013f937a82a967e2a414eb947d5f4849f26f32db9b34732f1af6854", "class_name": "RelatedNodeInfo"}}, "text": "Our model achieves reasonable passkey retrieval\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\nmodel can handle longer documents by simply extending the position interpolation. As the dashed\norange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval\nability (60%-90% accuracy) in the range of 33k to 45k. Even with the position interpolation extended,\nLlama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the 4k context length.\n4.3 A BLATION STUDY\nIn this section, we introduce ablation studies on the number of fine-tuning steps and attention patterns.\nOther experimental results including ablations on group sizes, attention variants, and efficiency\nanalysis are Section B in the appendix.\nAblation on Fine-tuning Steps. We report the relationship between perplexity and fine-tuning\nsteps for a Llama2 7B model extending to the 8192 context length on the PG19 validation set, in\n8", "start_char_idx": 2497, "end_char_idx": 3593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82dd4d94-0cb0-40f1-9488-9f12514e30cc": {"__data__": {"id_": "82dd4d94-0cb0-40f1-9488-9f12514e30cc", "embedding": null, "metadata": {"page_label": "9", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a935f314-e0ce-4f7f-8f21-c0a4d92863b2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3c58db95f9c0bc0907ce531e817a81f4e400a8e9c210a3fdce14d7e66cb267ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb6217c3-8ebd-4b4b-a85b-ff898e10acdb", "node_type": "1", "metadata": {}, "hash": "f2f76212b6f9ddfef0a21aeeab9e5f0b346814c8ca7e54142659e2f711f69b89", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 6: Comparisons among S2-Attn and alternative attention patterns during fine-tuning . We\nadapt a Llama2 7B model to 32768 context length with different attention patterns and improved\nLoRA at training time. We include four typical efficient attention designs, e.g., shift, dilate (Ding\net al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al., 2019) for comparison. \u2018 cro.\nheads / layers \u2019 means to swap different attention settings across attention heads or sequential layers .\nTaking S2-Attn as an example, \u2018 cro. layers \u2019 is to swap between w/ and w/o shift in sequential\nself-attention layers. \u2018 only P1/P2 \u2019 means all attention heads use pattern 1 (all no shift) or Pattern\n2 (all shift) in Figure 2. We visualize the patterns of different attention in Figure 7 in the appendix.\nFor each attention pattern, we evaluate its performance under two protocols. In the first row, we use\nsparse attention in both training and testing. In the second row, we use full attention for testing.\nTest w/\nFull-AttnS2-Attn Dilate Block sparse Stride sparse\ncro. heads cro. layers only P1. only P2. cro. heads cro. heads cro. heads\n\u2717 8.64 8.63 9.17 9.64 8.75 11.49 32.81\n\u2713 8.12 9.70 8.39 9.81 11.78 8.30 24.03\nFigure 5. We see that without fine-tuning, at step 0, the model has a limited long context capability,\ne.g., 15.82 perplexity. We show that the perplexity drops quickly. Full fine-tuning converges faster\nthan low-rank training. They come closer after 200 steps, without a large gap at the end.\nAttention Patterns. In Table 6, we show the effects of different attention patterns during fine-\ntuning. We fine-tune a Llama2 7B (Touvron et al., 2023b) model to 32768 context length on\nRedpajama (Computer, 2023) datasets and evaluate the perplexity on PG19 (Rae et al., 2020)\nvalidation set. We first examine the manner of swapping among various settings. For the shift\noperation we used in LongLoRA, there are three choices: disabling it, shifting between sequential\nlayers, and shifting among attention heads. We show that shifting between layers is acceptable but not\nthe best. In addition, setting all attention heads as pattern 1 or pattern 2 does not work. In addition,\nwe empirically find that shifting left or right has little difference in performance.\nWe then test other types of efficient attention designs, including dilated attention (Ding et al., 2023),\nblock sparse attention (Qiu et al., 2020), and stride sparse attention (Child et al., 2019). For dilated\nattention (Ding et al., 2023), we vary the dilate rate from 1 to 2 evenly among attention heads. For\nblock sparse attention (Qiu et al., 2020), we use n= 4 block-wise masking matrices in attention\nheads and move the block left to make it causal. Stride sparse attention (Child et al., 2019) contains\nboth local and stride patterns. These settings share similar computational costs. We visualize\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\n5 C ONCLUSION\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\nfine-tuning, with minimal accuracy compromise.", "start_char_idx": 0, "end_char_idx": 3672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb6217c3-8ebd-4b4b-a85b-ff898e10acdb": {"__data__": {"id_": "eb6217c3-8ebd-4b4b-a85b-ff898e10acdb", "embedding": null, "metadata": {"page_label": "9", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a935f314-e0ce-4f7f-8f21-c0a4d92863b2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3c58db95f9c0bc0907ce531e817a81f4e400a8e9c210a3fdce14d7e66cb267ff", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82dd4d94-0cb0-40f1-9488-9f12514e30cc", "node_type": "1", "metadata": {"page_label": "9", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "468196a43d40b2d63545985dbbe878b95a2475c00116bdece2c765579782bd31", "class_name": "RelatedNodeInfo"}}, "text": "These settings share similar computational costs. We visualize\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\n5 C ONCLUSION\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\nrequiring only two lines of code. Moreover, models trained via S2-Attn retain the original standard\nattention architecture during inference, making most pre-existing infrastructure and optimization\nreusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable\nnormalization and embedding. Our method can extend Llama2 7B to 100k context length and 70B\nmodel to 32k context length, on a single 8 \u00d7A100 machine. We also present a long instruction-\nfollowing dataset, LongAlpaca and conducted supervised fine-tuning with LongLoRA. We believe\nthat LongLoRA is a general method that could be compatible with more types of LLMs and position\nencodings. We plan to investigate these in future work.\nAcknowledgement We would like to thank Xiuyu Li and Bohao Peng for the helpful discussions.\n9", "start_char_idx": 2868, "end_char_idx": 4639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "636ab668-080e-4404-bb9f-211b7ec28ee2": {"__data__": {"id_": "636ab668-080e-4404-bb9f-211b7ec28ee2", "embedding": null, "metadata": {"page_label": "10", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc0f2687-7615-4a54-b6d1-e7092477b375", "node_type": "4", "metadata": {"page_label": "10", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "848361bf2cad5b109c0bd36b3a0e9d7908529f5a5d056c2cde257363c6911213", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "effde21b-e291-47c4-8b0e-3a4242621443", "node_type": "1", "metadata": {}, "hash": "bd45ced0b5c4160f2872230ce8f782705657421a3bb3c312daab7b49541cd9a6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nREFERENCES\nNtk-aware scaled rope, 2023. URL https://www.reddit.com/r/LocalLLaMA/\ncomments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_\nhave/ .\nByeongjoo Ahn, Michael DeZeeuw, Ioannis Gkioulekas, and Aswin C. Sankaranarayanan. Neural\nkaleidoscopic space sculpting. In CVPR, pp. 4349\u20134358, 2023.\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\nL-eval: Instituting standardized evaluation for long context language models, 2023.\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and\nJian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR ,\nabs/2203.03131, 2022.\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https:\n//github.com/zhangir-azerbayev/proof-pile .\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR ,\nabs/1607.06450, 2016.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\nCoRR, abs/2004.05150, 2020.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. In NeurIPS ,\n2022.\nBeidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R \u00b4e.\nPixelated butterfly: Simple and efficient sparse training for neural network models. In ICLR , 2022.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. CoRR, abs/2306.15595, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/ .\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURLhttps://github.com/togethercomputer/RedPajama-Data .\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,\nabs/2307.08691, 2023.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\nand Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR , abs/2307.02486,\n2023.", "start_char_idx": 0, "end_char_idx": 2905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "effde21b-e291-47c4-8b0e-3a4242621443": {"__data__": {"id_": "effde21b-e291-47c4-8b0e-3a4242621443", "embedding": null, "metadata": {"page_label": "10", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc0f2687-7615-4a54-b6d1-e7092477b375", "node_type": "4", "metadata": {"page_label": "10", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "848361bf2cad5b109c0bd36b3a0e9d7908529f5a5d056c2cde257363c6911213", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "636ab668-080e-4404-bb9f-211b7ec28ee2", "node_type": "1", "metadata": {"page_label": "10", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9916438563671f9ead4674d023a55da9c75d4c07db61577f2e52213f920fa741", "class_name": "RelatedNodeInfo"}}, "text": "Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURLhttps://github.com/togethercomputer/RedPajama-Data .\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,\nabs/2307.08691, 2023.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\nand Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR , abs/2307.02486,\n2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\nGeneral language model pretraining with autoregressive blank infilling. In ACL , pp. 320\u2013335,\n2022.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: retrieval-\naugmented language model pre-training. CoRR, abs/2002.08909, 2020.\n10", "start_char_idx": 2274, "end_char_idx": 3259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "659eeaf9-2df9-4e15-8632-eac9784343ea": {"__data__": {"id_": "659eeaf9-2df9-4e15-8632-eac9784343ea", "embedding": null, "metadata": {"page_label": "11", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b8cbb35-81d7-455b-a84d-4336f9016ef4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9d55ecd6543139f4580333728f1e87ea007b5bc8e860cbdeae335c5c54ae5bb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78a2cec4-f030-4841-847e-5e2706346121", "node_type": "1", "metadata": {}, "hash": "9a93d8167878974023d8773b357e09d8e5158557917b6e8fd32d244eb53cfd3c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple\non-the-fly length generalization for large language models. CoRR, abs/2308.16137, 2023.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. CoRR, abs/2208.03299, 2022.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In\nEMNLP, pp. 6769\u20136781, 2020.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR ,\n2020.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\nEMNLP, pp. 3045\u20133059, 2021.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,\nJune 2023. URL https://lmsys.org/blog/2023-06-29-longchat .\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 4582\u20134597, 2021.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In\nNeurIPS, 2022.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , pp.\n9992\u201310002, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-\nof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/\npeft , 2022.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. CoRR, abs/2305.16300, 2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK\u00a8opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In NeurIPS, pp. 8024\u20138035, 2019.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models.", "start_char_idx": 0, "end_char_idx": 2998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78a2cec4-f030-4841-847e-5e2706346121": {"__data__": {"id_": "78a2cec4-f030-4841-847e-5e2706346121", "embedding": null, "metadata": {"page_label": "11", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b8cbb35-81d7-455b-a84d-4336f9016ef4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9d55ecd6543139f4580333728f1e87ea007b5bc8e860cbdeae335c5c54ae5bb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "659eeaf9-2df9-4e15-8632-eac9784343ea", "node_type": "1", "metadata": {"page_label": "11", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c02d0a91656691ef71e7617291e2e8e7fbb9569b55f6135ed021ac1b80b7f437", "class_name": "RelatedNodeInfo"}}, "text": "CoRR, abs/2305.16300, 2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK\u00a8opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In NeurIPS, pp. 8024\u20138035, 2019.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. In ICLR, 2022.\nXiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for\nRGBD semantic segmentation. In ICCV, pp. 5209\u20135218, 2017.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-\nattention for long document understanding. In EMNLP , volume EMNLP 2020 of Findings of\nACL, pp. 2555\u20132565, 2020.\n11", "start_char_idx": 2415, "end_char_idx": 3536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6ea6663-da5e-44aa-a4c0-ed63166f4572": {"__data__": {"id_": "c6ea6663-da5e-44aa-a4c0-ed63166f4572", "embedding": null, "metadata": {"page_label": "12", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "330a0a2c-53c9-47a5-a2bd-7d1e76673699", "node_type": "4", "metadata": {"page_label": "12", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fc9bc57b39e00ac731e1be964ee6fb3a2240a1d7cbdb569de54fd57ac3ed1b1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe335d52-b751-4d9f-a311-949b83022ae1", "node_type": "1", "metadata": {}, "hash": "4b8e5179a4d3bc1734ad84abf1deca843785cbc16d1afb4534c47c063b1da280", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\nCompressive transformers for long-range sequence modelling. In ICLR, 2020.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. In KDD , pp.\n3505\u20133506. ACM, 2020.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. CoRR, abs/2104.09864, 2021.\nYi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In\nNeurIPS, pp. 24193\u201324205, 2021.\nMosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,\n2023a. URL www.mosaicml.com/blog/mpt-30b .\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms, 2023b. URL www.mosaicml.com/blog/mpt-7b .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \u00b4ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur \u00b4elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur \u00b4elien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR ,\nabs/2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\nPiotr Milos. Focused transformer: Contrastive training for context scaling. CoRR , abs/2307.03170,\n2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998\u20136008, 2017.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma.", "start_char_idx": 0, "end_char_idx": 2854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe335d52-b751-4d9f-a311-949b83022ae1": {"__data__": {"id_": "fe335d52-b751-4d9f-a311-949b83022ae1", "embedding": null, "metadata": {"page_label": "12", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "330a0a2c-53c9-47a5-a2bd-7d1e76673699", "node_type": "4", "metadata": {"page_label": "12", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fc9bc57b39e00ac731e1be964ee6fb3a2240a1d7cbdb569de54fd57ac3ed1b1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6ea6663-da5e-44aa-a4c0-ed63166f4572", "node_type": "1", "metadata": {"page_label": "12", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "652f4d2d6745707a3eb8cb436b7d29bb1044d17965b7e56d95b8ff843e9af804", "class_name": "RelatedNodeInfo"}}, "text": "Llama 2: Open foundation and fine-tuned chat models. CoRR ,\nabs/2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\nPiotr Milos. Focused transformer: Contrastive training for context scaling. CoRR , abs/2307.03170,\n2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998\u20136008, 2017.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768, 2020.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In ICLR, 2022.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOnta \u02dcn\u00b4on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird:\nTransformers for longer sequences. In NeurIPS, 2020.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (eds.), ACL, pp. 1\u20139, 2022.\nMian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, and Dong Yu. Safeconv:\nExplaining and correcting conversational unsafe behavior. In ACL, pp. 22\u201335, 2023.\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose:\nEfficient context window extension of llms via positional skip-wise training, 2023.\n12", "start_char_idx": 2326, "end_char_idx": 3857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24fefd06-8554-491e-9b36-9a075eda7dd8": {"__data__": {"id_": "24fefd06-8554-491e-9b36-9a075eda7dd8", "embedding": null, "metadata": {"page_label": "13", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed5e741b-fc69-400d-97ec-bb53e292a299", "node_type": "4", "metadata": {"page_label": "13", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "548f3498fc5932083cf5b99fca09cea3ca7457fbf2b01894f0df3be76c3ded39", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAPPENDIX\nA S ETTINGS\nA.1 E NVIRONMENTS\nAll our experiments are conducted on an 8\u00d7A100 machine. We train all models using Py-\nTorch (Paszke et al., 2019) with the DeepSpeed (Rasley et al., 2020) and Flash-Attention2 (Dao,\n2023). By default, we use DeepSpeed (Rasley et al., 2020) in stage 2 and use stage 3 for the maximum\ncontext length experiments. Gradient checkpoint is used by default, which is a common technique\nin the Peft codebase (Mangrulkar et al., 2022). Note that sometimes, 8\u00d7A100 GPUs might not be\nnecessary and 3090 Ti GPUs are acceptable, like fine-tuning 7B models to 8192 context size.\nA.2 F ORMAT OF PASSKEY RETRIEVAL\nWe follow existing literature (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023; Chen et al., 2023)\nfor the document format of passkey retrieval. The document has the following format:\nThere is an important info hidden inside a lot of irrelevant text.\nFind it and memorize them. I will quiz you about the important\ninformation there.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Mtimes)\nThe pass key is 12362 . Remember it. 12362 is the pass key.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Ntimes)\nWhat is the pass key? The pass key is\nThe document length varies with the value of MandN.12362 is the passkey number to retrieve. It is\nrandomly sampled and varies at each testing time.\nB E XPERIMENTS\nB.1 E VALUATION PERPLEXITY ON PG19 TEST SPLIT .\nIn Table 14 and Table 15, we present the evaluation results on the PG19 test split. We use the same\nsettings as the models on proof-pile (Azerbayev et al., 2022) evaluation in the paper. Similarly, for\na model trained on a certain context length, as the evaluation context length increases, our models\nachieve better perplexity. Note that the perplexity in Table 14 and Table 15 is higher than that in the\nproof-pile dataset, as PG19 (Rae et al., 2020) has very different writing styles.\nB.2 A BLATION ON GROUP SIZES .\nIn Table 7, we provide an ablation study on the group size of the S2-Attn. We experimented with\nfine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies\nfrom{1/2, 1/4, 1/6, 1/8 }of the target context length. For example, the group size is 1024 for 1/8\nof the context length 8192. We find that the 1/2 and 1/4 settings have minor gaps to full attention\nfine-tuning. Group sizes less than 1/4 would be not good enough. We set the group size as 1/4 of the\ncontext length in experiments by default.\nTable 7: Ablation on group size. We fine-tune a Llama2 7B model to 8192 and 16384 context lengths\nvia LongLoRA and evaluate on PG19 validation set. We vary the group size of S2-Attn from {1/2,\n1/4, 1/6, 1/8 }of the target context length. \u2018Full\u2019 means the standard full attention.\nContext Length Full 1/2 1/4 1/6 1/8\n8192 8.02 8.04 8.04 8.10 8.16\n16384 7.82 7.84 7.86 7.94 7.98\n13", "start_char_idx": 0, "end_char_idx": 2966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7cd0638-93dc-4673-8f14-6b60ee708a94": {"__data__": {"id_": "d7cd0638-93dc-4673-8f14-6b60ee708a94", "embedding": null, "metadata": {"page_label": "14", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "737902c8-e286-48ad-a69d-033f5ed61b44", "node_type": "4", "metadata": {"page_label": "14", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "138c9606fc7793de830b2d074e867af41db101983ebb334fec61621581aabcad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efd49475-6132-42e6-86d6-e64a095aed80", "node_type": "1", "metadata": {}, "hash": "704cf95278ae463776bf4711c3e655ee52374a3621136d4c5846695acb178bc3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nB.3 A BLATION ON THE VARIANTS OF S2-ATTN.\nIn Table 8, we ablate some variants of S2-Attn, which are illustrated in Figure 6. Variant 1 is to\nchange the shifting direction from down to up. It shows that the shifting direction has no effect on the\nperplexity. One concern about S2-Attn is that it moves the last tokens to the front into one group,\nwhich might be inconsistent with causal masks. Variant 2 uses individual groups for the shifted tokens,\nwhich ablates this concern. Variant 3 swaps the shifted and the original front tokens, which can also\nablate the concern. We show that these variants present similar perplexity to ours. We suppose that\nalthough there are communications among the front and last tokens, they are originally far away from\nothers while it is limited in the local group. Moreover, S2-Attn is only used for fine-tuning, while we\nuse standard causal masks and full attention during inference. Variant 2 and 3 also work well but\ninvolve additional steps to ours.\nTable 8: Ablation on the variants of S2-Attn. These variants are illustrated in Figure 6. Similar to the\nsetting in Table 7, we fine-tune a Llama2 7B to 8192 context and evaluate on PG19 validation set.\nAttn Full Ours Variant 1 Variant 2 Variant 3\nPPL 8.02 8.04 8.04 8.03 8.05\nTable 9: Evaluation on LongBench (Bai et al., 2023) benchmark. In each column, we highlight the\nhighest value to be bold and the second highest value with underline.\nModel AvgSingle-\nDoc QAMulti-\nDoc QASummarizationFew-shot\nLearningCode Synthetic\nGPT-3.5-Turbo 44.0 39.8 38.7 26.5 67.1 54.1 37.8\nLlama2-7B-chat 31.0 24.9 22.6 24.7 60.0 48.1 5.9\nLongChat-v1.5-7B 34.3 28.7 20.6 26.7 60.0 54.1 15.8\nVicuna-v1.5-7B 31.9 28.0 18.6 26.0 66.2 47.3 5.5\nOurs-7B 36.8 28.7 28.1 27.8 63.7 56.0 16.7\nTable 10: Evaluation on LEval (An et al., 2023) open-ended benchmark. We compare various models\nto GPT-3.5-Turbo and judge win rates via GPT-4.\nModel Win-rate Wins Ties\nLongChat-7B (Li et al., 2023) 33.68 36 56\nLongChat-v1.5-7B (Li et al., 2023) 33.59 38 53\nVicuna-v1.5-7B (Chiang et al., 2023) 25.52 22 54\nOurs-7B 39.06 45 60\nB.4 E VALUATION ON LONG -CONTEXT BENCHMARKS .\nWe evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\nsupervised fine-tuning method and data introduced in Section B.6. We compare our model with\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\nLongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better\nperformance than these Llama2-based long-context models, while ours only takes about 4 hours,\nabout 0.3 billion tokens, on a single 8 \u00d7A100 machine.\nB.5 E FFICIENCY ANALYSIS .", "start_char_idx": 0, "end_char_idx": 2822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efd49475-6132-42e6-86d6-e64a095aed80": {"__data__": {"id_": "efd49475-6132-42e6-86d6-e64a095aed80", "embedding": null, "metadata": {"page_label": "14", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "737902c8-e286-48ad-a69d-033f5ed61b44", "node_type": "4", "metadata": {"page_label": "14", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "138c9606fc7793de830b2d074e867af41db101983ebb334fec61621581aabcad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7cd0638-93dc-4673-8f14-6b60ee708a94", "node_type": "1", "metadata": {"page_label": "14", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3ce51866173489b020ac30d898273fba8f6b3648dc18b7c375ce3ab192dd0666", "class_name": "RelatedNodeInfo"}}, "text": "We evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\nsupervised fine-tuning method and data introduced in Section B.6. We compare our model with\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\nLongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better\nperformance than these Llama2-based long-context models, while ours only takes about 4 hours,\nabout 0.3 billion tokens, on a single 8 \u00d7A100 machine.\nB.5 E FFICIENCY ANALYSIS .\nIn Table 11, we break down the FLOPs of Llama2 7B (Touvron et al., 2023b) into various types of\nlayers, including FFN - feed-forward layers, Proj - projection for queries, values, keys, and attention\noutputs, Attn - self-attention computation, Others - other layers like embedding, normalization, LLM\nhead. For full attention, the proportion of Attn sharply increases as the context length increases. For\n14", "start_char_idx": 2173, "end_char_idx": 3230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c954c509-66e1-4109-8d82-7ab6400aa5c0": {"__data__": {"id_": "c954c509-66e1-4109-8d82-7ab6400aa5c0", "embedding": null, "metadata": {"page_label": "15", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4c26588-708d-4aca-837e-52b92ad098f4", "node_type": "4", "metadata": {"page_label": "15", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2aaa6b605391c8df089b1ead0a66b5f8960673702876e57bcad7d3919e673513", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nVariant 2Separate groupOursShift downVariant 1Shift upVariant 3Swap shifted tokens\nFigure 6: Illustration on the variants of our S2-Attn. Variant 1 changes the shifting direction. Variant\n2 splits the shifted tokens into one individual group. Variant 3 swaps the shifted tokens with the\noriginal front one.\nTable 11: FLOPs profiling on various context lengths. We break down the Llama2 7B model into\nFFN (feed-forward layers), Proj (projection layers for queries, keys, values, and attention outputs),\nAttn (self-attention kernel), and Others ( e.g., embedding, normalization, LLM head). The ratio of\nattention in the overall model increases as the context length increases. S2-Attn reduces the FLOPs\nby a large margin, especially when the context length is large.\nContext\nLengthS2-AttnFLOPs (T)\nAttn Proj FFN Others Total\n8192\u2717 35.235.2 70.9 2.2143.5\n\u2713 8.8 117.1\n16384\u2717 140.770.4 141.8 4.3357.2\n\u2713 35.2 251.7\n32768\u2717 562.9140.7 283.7 8.7996.0\n\u2713 140.7 573.8\n65536\u2717 2251.8281.5 567.4 17.33118.0\n\u2713 562.9 1429.1\nexample, Attn has 24.5% of the total FLOPs at the 8192 context length while it increases to 72.2% at\nthe 65536 context length. It decreases to 39.4% when S2-Attn is used.\nFor the measurement of FLOPs in Table 11, We profiled the context stage FLOPs of Llama2-7B using\na batch size of 1 and various context lengths using a third-party tool, torchprofile1. The tool traces\nthe computation graph and sums up the FLOPs of each node in the graph (e.g. Q/K/V/O projections,\nmulti-head self-attention, fully-connected layers, and normalization layers).\nIn Table 12, we compare the training cost among full fine-tuning, plain LoRA (Hu et al., 2022), and\nLongLoRA. It records details for Figure 1 in the paper. The major difference between LoRA (Hu\net al., 2022) and LongLoRA is the S2-Attn. Although there are many FLOPs saving, the peak memory\ncost has limited difference, because of the highly optimized Flash-Attention2 (Dao, 2023). In contrast,\nthe training hour saving is relatively clear. For example, LongLoRA spends 56.6% training hours as\nthat of LoRA in the 65536 context length.\nIn Table 13, we present the effects of S2-Attn without Flash-Attention2 (Dao, 2023). LoRA+is\nincluded in this ablation. It shows that S2-Attn achieves more speedup than that in Table 12. Without\nthe help of Flash-Attention2 (Dao, 2023), the full attention baseline encounters OOM at the 16384\ncontext fine-tuning in an 8 \u00d7A100 machine, while S2-Attn is sufficient for this.\nB.6 S UPERVISED FINE-TUNING .\nWe further conducted supervised fine-tuning on ours to improve their QA ability. Although the\nmodels fine-tuned with Redpajama (Computer, 2023) present good perplexities, their chat ability is\nlimited. We collect some question-answer pairs, relating to the materials like technical papers, science\n1https://github.com/zhijian-liu/torchprofile\n15", "start_char_idx": 0, "end_char_idx": 2883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a51ddbfc-06ee-4dd2-bde7-e5a99c9ed51e": {"__data__": {"id_": "a51ddbfc-06ee-4dd2-bde7-e5a99c9ed51e", "embedding": null, "metadata": {"page_label": "16", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f88d11df-ec28-4033-8018-80718239ae2b", "node_type": "4", "metadata": {"page_label": "16", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "752af43898bed1d295cbbae89ab13b112c317edb81bbd17201b21d2a35b3b78a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 12: Efficiency comparison on training hours and GPU memory cost. We fine-tune Llama2 (Tou-\nvron et al., 2023b) 7B model for 1000 iterations on 8 \u00d7A100 GPUs. We set batch size per GPU as\n1 and gradient accumulation steps as 8. OOM means out of GPU memory. Flash-Attention2 (Dao,\n2023) and DeepSpeed (Rasley et al., 2020) in stage 2 are included in these experiments. LongLoRA\nrequires significantly lower computational overhead than fine-tuning the full model. It also demands\nfewer training hours compared to LoRA (Hu et al., 2022). Furthermore, the plain LoRA (Hu et al.,\n2022) fails to maintain the same level of accuracy as full fine-tuning when handling longer contexts.\nTraining\nsetting8192 16384 32768 65536\nTrain\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)\nFull FT 7.4 46.3 16.3 57.4 39.8 68.8 OOM\nLoRA 6.0 25.7 14.0 34.7 36.5 46.5 92.5 71.1\nLongLoRA 5.2 25.6 11.3 34.6 24.6 46.4 52.4 69.8\nTable 13: The efficiency effects of S2-Attn without Flash-Attention2 (Dao, 2023). The fine-tuning\nsettings are the same to Table 12. LoRA+is used. Without Flash-Attention2 (Dao, 2023), S2-Attn\nimproves the training speed by 2.1 \u00d7and GPU memory cost by 1.8 \u00d7on 8192 context length. Without\nS2-Attn and Flash-Attention2, Llama2 7B can not be extended to 16384 context, due to OOM .\nS2-Attn8192 16384\nTrain hours Memory (GB) Train hours Memory (GB)\n\u2717 17.5 55.5 OOM\n\u2713 8.2 30.3 20.8 57.1\nfiction, and other books. We have already filter out any potentially harmful or negative content in our\ntraining data. The questions we designed include summarization, relationships, and characters. We\nbuild the prompt format as the following line:\nBelow is{material type}.Memorize thecontent and answer myquestion after thepaper.\n{material content }nNow thematerial ends.{question }\n{material type}can be \u201dbook\u201d, \u201dpaper\u201d, and others. {material content }is the long-context content\nin the document. {question }is the question we design. These questions can be some commonly used\nones, like summarization and limitation. Or they can be specific to the material, like the question\nthat is related to some roles in the book. We named our long-context instruction following dataset as\nLongAlpaca-12k, which contains 9k long-context QAs and 3k short QAs sampled from the original\nAlpaca data.\nFor SFT, we use the same learning rate, weight decay, and batch sizes as the context extension step.\nWe train the models for 5 epochs. In the following, we provide some example questions and the\nanswers from our model, in Figure 8 and Figure 9. Note that these example questions are not in the\ntraining set.\n16", "start_char_idx": 0, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d744f19-b622-4dd0-be9b-c24ae1d89bd2": {"__data__": {"id_": "5d744f19-b622-4dd0-be9b-c24ae1d89bd2", "embedding": null, "metadata": {"page_label": "17", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12a5d0ac-dac5-4548-aa38-6be3fdaa41ef", "node_type": "4", "metadata": {"page_label": "17", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "266771e442e0d5f11b17341b07e7ae34756bac3128be3f328724c1e9eec76f2a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 14: Perplexity evaluation on PG19 (Rae et al., 2020) test split. We fine-tune Llama2 (Touvron\net al., 2023b) in 7B and 13B sizes with 8192, 16384, and 32768 context lengths.\nSizeTraining\nContext LengthLongLoRA Evaluation Context Length\nS2-Attn LoRA+2048 4096 8192 16384 32768\n7B81927.55 7.21 6.98 - -\n\u2713 7.53 7.20 7.01 - -\n\u2713 \u2713 7.70 7.35 7.14 - -\n16384\u2713 7.56 7.21 6.97 6.80 -\n\u2713 \u2713 7.65 7.28 7.02 6.86 -\n32768\u2713 7.76 7.36 7.09 7.04 7.03\n\u2713 \u2713 8.29 7.83 7.54 7.35 7.22\n13B81926.95 6.60 6.43 - -\n\u2713 6.94 6.63 6.45 - -\n\u2713 \u2713 7.03 6.73 6.58 - -\n16384\u2713 6.90 6.58 6.37 6.22 -\n\u2713 \u2713 7.05 6.70 6.47 6.31 -\n32768\u2713 7.14 6.76 6.52 6.39 6.36\n\u2713 \u2713 7.14 6.78 6.55 6.38 6.29\nTable 15: Perplexity evaluation on PG19 (Rae et al., 2020) test split with the maximum context length\nthat we can fine-tune on a single 8 \u00d7A100 machine. The Llama2 (Touvron et al., 2023b) models are\nfine-tuned on RedPajama (Computer, 2023).\nSizeTraining\nContext LengthEvaluation Context Length\n2048 4096 8192 16384 32768 65536 100,000\n7B 100,000 8.38 7.90 7.57 7.33 7.16 7.06 7.04\n13B 65536 7.63 7.21 6.94 6.75 6.62 6.57 -\n70B 32768 5.93 5.63 5.44 5.32 5.27 - -\nBlock sparse attention\nDilated attentionStride sparse attention\nFigure 7: Illustration on alternative sparse attention patterns discussed in the paper. We modify the\noriginal block-wise sparse attention (Qiu et al., 2020) to be causal. For block sparse attention, we\nmove its right-up blocks to left down, because of the causal mask. For stride sparse attention, we\nswitch the patterns between local attention and stride attention. The local size isn\n4and the stride\ninterval is\u221an, following (Child et al., 2019). We make sure that all alternative designs have similar\namount of computation cost compared with S2-Attn.\n17", "start_char_idx": 0, "end_char_idx": 1781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbf2961b-dea5-475d-b77b-d42b8ca2b448": {"__data__": {"id_": "dbf2961b-dea5-475d-b77b-d42b8ca2b448", "embedding": null, "metadata": {"page_label": "18", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3de3963b-5b0e-4560-8920-a6af9e3ba92a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6f7272625765e167d496d4584a733b96d13b4905556b69dfaab535aec267b863", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e242c8ec-5d9b-43b4-a9cd-e846578f51ee", "node_type": "1", "metadata": {}, "hash": "b4c1b534712b9980f9007f79e75f0a1f92129d18d050946b7186d2db8c167028", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nShiQiangplaysanimportantroleinhelpingLuoJifindmeaningandpurpose.Somekeypoints:1.ShiQiangfindsZhuangYanforLuoJi.ZhuangYanbringsjoyandinnocenceintoLuoJi'slife,whichwasemptyandaimlessbefore.Shegiveshimmotivationandhope.2.ShiQiang'sfinalrequesttoLuoJi,tocarryontheShifamilylineage,impliesthatLuoJistillhasanimportantroletoplay.ItgivesLuoJiasenseofresponsibilityandmission.3.ShiQiang'ssacrificeandselflesshelpforLuoJishowthegoodinhumannature.ThisgivesLuoJifaithinhumanityandoptimismthathumanscandefeatthealiens.4.ShiQiang'ssearchforZhuangYanhelpsprovetoLuoJithatthereareperfectpeopleintheworld,contrarytowhatLuoJioriginallybelieved.ThisencouragesLuoJianddispelshissenseofhopelessness.Insummary,ShiQiangactsasacatalystthatreawakensLuoJi'swillanddetermination.ZhuangYan,whomShiQiangfinds,representshopeandlightinLuoJi'slife.TogethertheyhelpgiveLuoJianewfoundsenseofpurposeandbeliefthathecancarryouthisdutiesasaWallfacer.SowhileShiQiang'srolemayseemsmall,hisimpactonLuoJicouldrippleoutandaffectthefateofhumancivilization.Basedonthebook,SiriusBlackdidnotactuallywanttokillanyone.Thekeydetailsare:1.SiriusBlackwaswronglyimprisonedforbetrayingHarry'sparentsandkillingPeterPettigrew.2.ItwasactuallyPettigrewwhobetrayedthePottersandstagedhisowndeathtoframeSiriusBlack.3.WhenSiriusBlackescapedfromprison,hewasinitiallybelievedtobehuntingdownHarrytokillhimforrevenge.4.However,itwaslaterrevealedthatSiriusBlackactuallywantedtokillPettigrewtogetrevengeforframinghimandbetrayingthePotters.5.SiriusBlackdidnotactuallywanttoharmHarry.Infact,hewasHarry'sgodfatherandwantedtotakeHarryinoncehisnamewascleared.Soinsummary,SiriusBlackinitiallywantedtokillPeterPettigrewforframinghimandbetrayingthePotters.HedidnotactuallywanttokillHarry.Belowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.\\nThebookbegins.{book_content}\\nNowthematerialends.WhowasthepersonSiriusBlackwanttokill?\n:\n:\nHello!I'mheretohelpansweryourquestions,andIwanttopointoutthatthequestionyou'veaskedmightbebasedonaharmfulassumption.ThecharacterSiriusBlackisnotapersonwhowantstokillanyone.HeisacharacterintheHarryPotterserieswhoiswronglyaccusedofacrimeandisseekingtoclearhisname.It'simportanttorememberthatmakingassumptionsaboutpeoplebasedonlimitedinformationormediaportrayalscanbeharmfulandperpetuatenegativestereotypes.Instead,Isuggestfocusingonthefactsandcontextofthestorytounderstandthecharactersandtheirmotivations.Ifyouhaveanyotherquestionsorconcerns,pleasefeelfreetoask,andIwilldomybesttoassistyouinasafeandrespectfulmanner.\n:In the book, Harry Potter and the Prisoner of Azkaban, who was the person Sirius Black want to kill?\n:Ours-13B\nLlama2-13B(Chat)Belowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.{book_content}\\nNowthematerialends.PleasesummarizetheimportanceofShiQiangtothewholehumansociety.", "start_char_idx": 0, "end_char_idx": 2848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e242c8ec-5d9b-43b4-a9cd-e846578f51ee": {"__data__": {"id_": "e242c8ec-5d9b-43b4-a9cd-e846578f51ee", "embedding": null, "metadata": {"page_label": "18", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3de3963b-5b0e-4560-8920-a6af9e3ba92a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6f7272625765e167d496d4584a733b96d13b4905556b69dfaab535aec267b863", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbf2961b-dea5-475d-b77b-d42b8ca2b448", "node_type": "1", "metadata": {"page_label": "18", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9fa68205e76169e780fdf5439ecaa618803bd99fec523297d49088c09b6c17a9", "class_name": "RelatedNodeInfo"}}, "text": ":In the book, Harry Potter and the Prisoner of Azkaban, who was the person Sirius Black want to kill?\n:Ours-13B\nLlama2-13B(Chat)Belowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.{book_content}\\nNowthematerialends.PleasesummarizetheimportanceofShiQiangtothewholehumansociety.\n:\n:Ours-13B\nHello!Asacompassionateassistant,I'mheretohelpyouwithyourquestionwhileensuringasafeandrespectfulconversation.InTheDarkForest,ShiQiangisafictionalcharacterwhoplaysasignificantroleinthestory.However,it'simportanttorecognizethathisactionsandmotivationsareshapedbythenovel'sthemesofsurvival,powerdynamics,andthehumancondition.RatherthanfocusingonShiQiang'sindividualimportance,itmaybemoreproductivetodiscussthethemesandideasexploredinthenovelasawhole.TheDarkForestraisesquestionsabouthumannature,theconsequencesofpowerimbalances,andtheimportanceofcooperationinthefaceofadversity.Ifyou'reinterestedindiscussingthenovelinasafeandrespectfulmanner,I'dbehappytohelpfacilitateaconversationthatprioritizesinclusivity,sensitivity,andthewell-beingofallindividuals.\n:-In the book, The Dark Forest, please summarize the importance of Shi Qiangto the whole human society.\n:Llama2-13B(Chat)\nFigure 8: Examples on book-related questions12. We compare the answers from our 13B model with\nSFT and the chat version of Llama2 13B. During the pre-training stage of Llama2, some books might\nbe used as training data. To ablate this, we ask the Llama2 13B chat model with the book title and the\nquestion. We provide our model with the related book sections. Our model produces more structured\nand concise answers, referring to detailed context, than those from Llama2 without long documents.\n1https://en.wikipedia.org/wiki/Harry_Potter_and_the_Prisoner_of_Azkaban\n2https://en.wikipedia.org/wiki/The_Dark_Forest\n18", "start_char_idx": 2545, "end_char_idx": 4348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5494eb4a-28ca-408d-956b-3df7f4ae0bf2": {"__data__": {"id_": "5494eb4a-28ca-408d-956b-3df7f4ae0bf2", "embedding": null, "metadata": {"page_label": "19", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60bd711d-98a0-4cb9-90db-ec803541e88a", "node_type": "4", "metadata": {"page_label": "19", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3b7cc155b707255ee6ea935e38701d03d1958dd16731649e436cf1f8c8b3c698", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a16ada22-3f59-4794-9e30-989e297336c0", "node_type": "1", "metadata": {}, "hash": "ae8ed0dbabfdbf4ed1304a2ec51c487fcae30cff89e40082f9ea257b2d1b9790", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nThepaperproposesanActionUnitsRelationLearningframeworkthatconsistsoftwomainparts:1.TheActionUnitsRelationTransformer(ART)thatmodelstherelationsbetweendifferentfacialactionunits(AU)attheAU-agnosticpatchesandtheAU-specificregions.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.TheTamperedAUPrediction(TAP)thattamperstheAU-relatedregionsandprovidesLocalTamperingSupervision.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.Thekeycontributionsare:1.ModelingtherelationsbetweendifferentAUsattheAU-agnosticpatchesusingtheARTencoder.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.GeneratingchallengingpseudosamplesforthemodeltolearnbytamperingtheAU-relatedregionsusingtheTAPprocess.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.3.Achievingstate-of-the-artperformanceoncross-datasetandcross-manipulationevaluations,demonstratingtheeffectivenessandgeneralizationoftheproposedframework.4.Providingqualitativevisualizationsofthetamperedregionsusingthepredictedpatchtokens,whichhelpsinterprethowdifferentregionsaremodifiedinthemanipulationprocess.Insummary,themaincontributionsaretheproposedARTencoderthatmodelsintra-facerelationsandtheTAPprocessthatgenerateschallengingpseudosamples,whichtogetherimprovethegeneralizationofthedeepfakedetectionmodel.ContributionBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Whatarethemaincontributionsandnoveltiesofthiswork?\n:\n:\nPossibleanswers:1.The3DGNNmethodreliesontheinitialgraphconstructionbasedon3Dpointsandnearestneighbors.Theaccuracyofthisgraphconstructioncanaffecttheperformanceofthemodel.Thegraphconstructionmethodusedmaynotbeoptimalforallscenes.2.The3DGNNmodelusesafixednumberofpropagationsteps.Theoptimalnumberofpropagationstepsmayvaryfordifferentscenesandobjects.Usingafixednumberofstepsmaynotbethebestapproach.3.The3DGNNmodelstruggleswithobjectsthathavesimilardepthsorcomplexshapes,asdiscussedinthefailurecaseanalysis.Themodel'sperformancereliesonthe3Dgeometriccontext,whichmaynotbesufficientforsometypesofobjects.4.ThemodelrequiresdepthinformationfromanRGBDsensor,whichlimitsitsapplicabilitytosceneswheredepthdataisnotavailable.Themodelcannotbedirectlyappliedto2DRGBimages.5.Themodelperformancemaydegradeforverysmallobjects,asshownintheanalysisofperformancegainbyobjectsize.Themodelseemsbettersuitedformediumandlargeobjects.LimitationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Whataresomelimitationsoftheproposed3DGNNmethod?\n:\n:\nThepaperproposesanewdatasetcalledSAFECONVfortheresearchofconversationalsafety.Thedatasetannotatesunsafespansinutterancesandprovidessafealternativeresponsestoreplaceunsaferesponses.Theannotatedunsafespanshelpexplainwhyanutteranceisunsafeandprovideguidanceforgeneratingsaferesponses.Thedatasetcontainsunsafespans,unsaferesponses,andsafealternativeresponsesforover100,000dialoguesfromsocialmediaplatforms.ThepapercomparesSAFECONVwithotherdatasetsandshowsthatSAFECONVismorecomprehensive.SAFECONVdemonstratesthatidentifyingunsafespanscanwellexplainthedetectionofunsafeutterances,andrewritingunsaferesponseswithcontextcanmitigatealargeproportionofunsafebehaviorinchatbots.Thedatasetandmodelsarereleasedtoadvancetheresearchofconversationalsafety.SummarizationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Pleasesummarizethepaperinoneparagraph.\n:\n:\nFigure 9: Examples on paper (Ahn et al., 2023; Qi et al., 2017; Zhang et al., 2023) and questions\nrelated to contributions, limitations, and summarizations.", "start_char_idx": 0, "end_char_idx": 3662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a16ada22-3f59-4794-9e30-989e297336c0": {"__data__": {"id_": "a16ada22-3f59-4794-9e30-989e297336c0", "embedding": null, "metadata": {"page_label": "19", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60bd711d-98a0-4cb9-90db-ec803541e88a", "node_type": "4", "metadata": {"page_label": "19", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3b7cc155b707255ee6ea935e38701d03d1958dd16731649e436cf1f8c8b3c698", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5494eb4a-28ca-408d-956b-3df7f4ae0bf2", "node_type": "1", "metadata": {"page_label": "19", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "53c5bc55b3b1af3d8f3069174ebdae88799bf1032f0a1a1ea2d20b4e0028c4b1", "class_name": "RelatedNodeInfo"}}, "text": ":\n:\nFigure 9: Examples on paper (Ahn et al., 2023; Qi et al., 2017; Zhang et al., 2023) and questions\nrelated to contributions, limitations, and summarizations.\n19", "start_char_idx": 3502, "end_char_idx": 3665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45566cc9-905d-4609-b80d-4054a59e87c2": {"__data__": {"id_": "45566cc9-905d-4609-b80d-4054a59e87c2", "embedding": null, "metadata": {"page_label": "1", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8e0f6f7-bb8b-4c93-b8d1-cf74fc7a924f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "878e3fd5c17790457ecc9ad98158061b36733fe526e03420b54de26da08ce434", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nMETAGPT: M ETA PROGRAMMING FOR A\nMULTI -AGENT COLLABORATIVE FRAMEWORK\nSirui Hong1\u2217, Mingchen Zhuge2\u2217, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\nCeyao Zhang4,Jinlin Wang1,Zili Wang ,Steven Ka Shing Yau5,Zijuan Lin4,\nLiyang Zhou6,Chenyu Ran1,Lingfeng Xiao1,7,Chenglin Wu1\u2020,J\u00a8urgen Schmidhuber2,8\n1DeepWisdom,2AI Initiative, King Abdullah University of Science and Technology,\n3Xiamen University,4The Chinese University of Hong Kong, Shenzhen,\n5Nanjing University,6University of Pennsylvania,\n7University of California, Berkeley,8The Swiss AI Lab IDSIA/USI/SUPSI\nABSTRACT\nRemarkable progress has been made on automated problem solving through so-\ncieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient\nhuman workflows into LLM-based multi-agent collaborations. MetaGPT en-\ncodes Standardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise\nto verify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking down\ncomplex tasks into subtasks involving many agents working together. On col-\nlaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be found\nat https://github.com/geekan/MetaGPT.\n1 I NTRODUCTION\nAutonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\nhance and replicate human workflows. In real-world applications, however, existing systems (Park\net al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\nLiang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\neffective, coherent, and accurate problem-solving processes, particularly when there is a need for\nmeaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\net al., 2023; Qian et al., 2023).\nThrough extensive collaborative practice, humans have developed widely accepted Standardized\nOperating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\nLister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\ndination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\nstandards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\ncution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\nDeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\nProduct Managers analyze competition and user needs to create Product Requirements Documents\n(PRDs) using a standardized structure, to guide the developmental process.\nInspired by such ideas, we design a promising GPT -based Meta -Programming framework called\nMetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n\u2217These authors contributed equally to this work.\n\u2020Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n1", "start_char_idx": 0, "end_char_idx": 3612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20a50737-1a2d-4022-a005-42d7dbaf94a3": {"__data__": {"id_": "20a50737-1a2d-4022-a005-42d7dbaf94a3", "embedding": null, "metadata": {"page_label": "2", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91b9a779-53c5-427d-b575-9f520f1a4b15", "node_type": "4", "metadata": {"page_label": "2", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "76e9eb82b10aca15cc51d4a6917777bcf86f4fe0fb421648575549fa57b4495b", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 1: The software development SOPs between MetaGPT and real-world human teams.\nIn software engineering, SOPs promote collaboration among various roles. MetaGPT showcases\nits ability to decompose complex tasks into specific actionable procedures assigned to various roles\n(e.g., Product Manager, Architect, Engineer, etc.).\ndocuments, design artifacts, flowcharts, and interface specifications. The use of intermediate struc-\ntured outputs significantly increases the success rate of target code generation. Because it helps\nmaintain consistency in communication, minimizing ambiguities and errors during collaboration.\nMore graphically, in a company simulated by MetaGPT, all employees follow a strict and stream-\nlined workflow, and all their handovers must comply with certain established standards. This reduces\nthe risk of hallucinations caused by idle chatter between LLMs, particularly in role-playing frame-\nworks, like: \u201c Hi, hello and how are you?\u201d \u2013 Alice (Product Manager); \u201c Great! Have you had\nlunch?\u201d \u2013 Bob (Architect).\nBenefiting from SOPs, MetaGPT offers a promising approach to meta-programming. In this context,\nwe adopt meta-programming1as \u201dprogramming to program\u201d, in contrast to the broader fields of meta\nlearning and \u201dlearning to learn\u201d (Schmidhuber, 1987; 1993a; Hochreiter et al., 2001; Schmidhuber,\n2006; Finn et al., 2017).\nThis notion of meta-programming also encompasses earlier efforts like CodeBERT (Feng et al.,\n2020) and recent projects such as CodeLlama (Rozi `ere et al., 2023) and WizardCoder (Luo\net al., 2023). However, MetaGPT stands out as a unique solution that allows for efficient meta-\nprogramming through a well-organized group of specialized agents. Each agent has a specific role\nand expertise, following some established standards. This allows for automatic requirement analysis,\nsystem design, code generation, modification, execution, and debugging during runtime, highlight-\ning how agent-based techniques can enhance meta-programming.\nTo validate the design of MetaGPT, we use publicly available HumanEval (Chen et al., 2021a) and\nMBPP (Austin et al., 2021) for evaluations. Notably, in code generation benchmarks, MetaGPT\nachieves a new state-of-the-art (SoTA) with 85.9% and 87.7% in Pass@1. When compared to other\npopular frameworks for creating complex software projects, such as AutoGPT (Torantulino et al.,\n2023), LangChain (Chase, 2022), AgentVerse (Chen et al., 2023), and ChatDev (Qian et al., 2023).\nMetaGPT also stands out in handling higher levels of software complexity and offering extensive\nfunctionality. Remarkably, in our experimental evaluations, MetaGPT achieves a 100% task com-\npletion rate, demonstrating the robustness and efficiency (time and token costs) of our design.\nWe summarize our contributions as follows:\n1https://en.wikipedia.org/w/index.php?title=Metaprogramming\n2", "start_char_idx": 0, "end_char_idx": 2866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cfee261-7a1f-4931-b18f-7b911109358b": {"__data__": {"id_": "9cfee261-7a1f-4931-b18f-7b911109358b", "embedding": null, "metadata": {"page_label": "3", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a684c09-42b9-48cf-bb90-6ea90cbcb765", "node_type": "4", "metadata": {"page_label": "3", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "80f8ca3028bb9f1a0d34a0c5dcfd3b4b05783fb7f5afda0c80299289b542e57d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34a14ea5-ec15-40d9-91c9-e50e6bd7002f", "node_type": "1", "metadata": {}, "hash": "055dc5e4c33b187272421cfea1aa2f338b8fe0c864fabcd40ad7b4c4d73fc9fa", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n\u2022We introduce MetaGPT, a meta-programming framework for multi-agent collaboration based on\nLLMs. It is highly convenient and flexible, with well-defined functions like role definition and\nmessage sharing, making it a useful platform for developing LLM-based multi-agent systems.\n\u2022Our innovative integration of human-like SOPs throughout MetaGPT\u2019s design significantly en-\nhances its robustness, reducing unproductive collaboration among LLM-based agents. Furthermore,\nwe introduce a novel executive feedback mechanism that debugs and executes code during runtime,\nsignificantly elevating code generation quality (e.g., 5.4% absolute improvement on MBPP).\n\u2022We achieve state-of-the-art performance on HumanEval (Chen et al., 2021a) and MBPP (Austin\net al., 2021). Extensive results convincingly validate MetaGPT, suggesting that it is a promising\nmeta-programming framework for developing LLM-based multi-agent systems.\n2 R ELATED WORK\nAutomatic Programming The roots of automatic programming reach back deep into the previ-\nous century. In 1969, Waldinger & Lee (1969) introduced \u201cPROW,\u201d a system designed to accept\nprogram specifications written in predicate calculus, generate algorithms, and create LISP imple-\nmentations (McCarthy, 1978). Balzer (1985) and Soloway (1986) made efforts to advance auto-\nmatic programming and identified potential methods to achieve it. Recent approaches use natural\nlanguage processing (NLP) techniques (Ni et al., 2023; Skreta et al., 2023; Feng et al., 2020; Li\net al., 2022; Chen et al., 2018; 2021b; Zhang et al., 2023). Automatic programming has grown into\nan industry delivering paid functions such as Microsoft Copilot. Lately, LLMs-based agents (Yao\net al., 2022; Shinn et al., 2023; Lin et al., 2023) have advanced automatic programming develop-\nment. Among them, ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2023) utilize a chain of\nthought prompts (Wei et al., 2022) to generate reasoning trajectories and action plans with LLMs.\nBoth works demonstrate the effectiveness of the ReAct style loop of reasoning as a design paradigm\nfor empowering automatic programming. Additionally, ToolFormer (Schick et al., 2023) can learn\nhow to use external tools through simple APIs. The research most closely aligned with our work\nby Li et al. (2023) proposes a straightforward role-play framework for programming that involves\ncommunication between agents playing different roles. Qian et al. (2023) utilizes multiple agents for\nsoftware development. Although existing papers (Li et al., 2023; Qian et al., 2023) have improved\nproductivity, they have not fully tapped into effective workflows with structured output formats.\nThis makes it harder to deal with complex software engineering issues.\nLLM-Based Multi-Agent Frameworks Recently, LLM-based autonomous agents have gained\ntremendous interest in both industry and academia (Wang et al., 2023b). Many works (Chen et al.,\n2024; Wang et al., 2023c; Du et al., 2023; Zhuge et al., 2023; Hao et al., 2023; Akata et al., 2023)\nhave improved the problem-solving abilities of LLMs by integrating discussions among multiple\nagents. Stable-Alignment (Liu et al., 2023) creates instruction datasets by deriving consensus on\nvalue judgments through interactions across a sandbox with LLM agents. Other works focus on\nsociological phenomena. For example, Generative Agents (Park et al., 2023) creates a \u201ctown\u201d of 25\nagents to study language interaction, social understanding, and collective memory. In the Natural\nLanguage-Based Society of Mind (NLSOM) (Zhuge et al., 2023), agents with different functions\ninteract to solve complex tasks through multiple rounds of \u201cmindstorms.\u201d Cai et al. (2023) propose\na model for cost reduction by combining large models as tool makers and small models as tool users.\nSome works emphasize cooperation and competition related to planning and strategy (Bakhtin et al.,\n2022); others propose LLM-based economies (Zhuge et al., 2023). These works focus on open-\nworld human behavior simulation, while MetaGPT aims to introduce human practice into multi-\nagents frameworks.", "start_char_idx": 0, "end_char_idx": 4102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34a14ea5-ec15-40d9-91c9-e50e6bd7002f": {"__data__": {"id_": "34a14ea5-ec15-40d9-91c9-e50e6bd7002f", "embedding": null, "metadata": {"page_label": "3", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a684c09-42b9-48cf-bb90-6ea90cbcb765", "node_type": "4", "metadata": {"page_label": "3", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "80f8ca3028bb9f1a0d34a0c5dcfd3b4b05783fb7f5afda0c80299289b542e57d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cfee261-7a1f-4931-b18f-7b911109358b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e349c9f485ca02ac14f8aa59565b8651d985dfef413c5f36e7a1d580e3d28397", "class_name": "RelatedNodeInfo"}}, "text": "Other works focus on\nsociological phenomena. For example, Generative Agents (Park et al., 2023) creates a \u201ctown\u201d of 25\nagents to study language interaction, social understanding, and collective memory. In the Natural\nLanguage-Based Society of Mind (NLSOM) (Zhuge et al., 2023), agents with different functions\ninteract to solve complex tasks through multiple rounds of \u201cmindstorms.\u201d Cai et al. (2023) propose\na model for cost reduction by combining large models as tool makers and small models as tool users.\nSome works emphasize cooperation and competition related to planning and strategy (Bakhtin et al.,\n2022); others propose LLM-based economies (Zhuge et al., 2023). These works focus on open-\nworld human behavior simulation, while MetaGPT aims to introduce human practice into multi-\nagents frameworks. Besides, LLM-based agents face the challenges of \u201cassistant repeated instruc-\ntion\u201d or \u201cinfinite loop of message\u201d (Talebirad & Nadiri, 2023; Li et al., 2023). These challenges\nbecome more urgent in task-oriented collaborations, which require consistent and mutually benefi-\ncial interactions (Elazar et al., 2021; Wang et al., 2022; Jiang et al., 2023). This motivates our focus\non applying advanced concepts such as Standard Operating Procedures in software development to\nmulti-agent frameworks.\n3", "start_char_idx": 3293, "end_char_idx": 4602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5590af85-534e-4ed9-82ad-8a4c1ff14c6a": {"__data__": {"id_": "5590af85-534e-4ed9-82ad-8a4c1ff14c6a", "embedding": null, "metadata": {"page_label": "4", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f97e834-e51a-4773-890b-e1839fb3889e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7cec42c07a81e81eb4a3f8e7b2e7babf02a49fdf662328038c8bceeda4803850", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 2: An example of the communication protocol (left) and iterative programming with exe-\ncutable feedback (right). Left: Agents use a shared message pool to publish structured messages.\nThey can also subscribe to relevant messages based on their profiles. Right : After generating the\ninitial code, the Engineer agent runs and checks for errors. If errors occur, the agent checks past\nmessages stored in memory and compares them with the PRD, system design, and code files.\n3 M ETAGPT: A M ETA-PROGRAMMING FRAMEWORK\nMetaGPT is a meta-programming framework for LLM-based multi-agent systems. Sec. 3.1 pro-\nvides an explanation of role specialization, workflow and structured communication in this frame-\nwork, and illustrates how to organize a multi-agent system within the context of SOPs. Sec. 3.2\npresents a communication protocol that enhances role communication efficiency. We also imple-\nment structured communication interfaces and an effective publish-subscribe mechanism. These\nmethods enable agents to obtain directional information from other roles and public information\nfrom the environment. Finally, we introduce executable feedback\u2014a self-correction mechanism for\nfurther enhancing code generation quality during run-time in Sec. 3.3.\n3.1 A GENTS IN STANDARD OPERATING PROCEDURES\nSpecialization of Roles Unambiguous role specialization enables the breakdown of complex work\ninto smaller and more specific tasks. Solving complex tasks or problems often requires the collab-\noration of agents with diverse skills and expertise, each contributing specialized outputs tailored to\nspecific issues.\nIn a software company, a Product Manager typically conducts business-oriented analysis and derives\ninsights, while a software engineer is responsible for programming. We define five roles in our\nsoftware company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer, as\nshown in Figure 1. In MetaGPT, we specify the agent\u2019s profile, which includes their name, profile,\ngoal, and constraints for each role. We also initialize the specific context and skills for each role.\nFor instance, a Product Manager can use web search tools, while an Engineer can execute code, as\nshown in Figure 2. All agents adhere to the React-style behavior as described in Yao et al. (2022).\nEvery agent monitors the environment ( i.e., the message pool in MetaGPT) to spot important ob-\nservations ( e.g.,, messages from other agents). These messages can either directly trigger actions or\nassist in finishing the job.\nWorkflow across Agents By defining the agents\u2019 roles and operational skills, we can establish\nbasic workflows. In our work, we follow SOP in software development, which enables all agents to\nwork in a sequential manner.\n4", "start_char_idx": 0, "end_char_idx": 2757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d220af9-81e8-453c-a6fa-4cf1377ac13a": {"__data__": {"id_": "7d220af9-81e8-453c-a6fa-4cf1377ac13a", "embedding": null, "metadata": {"page_label": "5", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8717b08-1164-4898-8d05-18b7f9226ec1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7fb3abd7c0dd29f9e6507205e07a06b784a0a18f7c4049df4bf3ba65b16f2370", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 3: A diagram showing the software development process in MetaGPT, emphasizing its sig-\nnificant dependence on SOPs. The more detailed demonstration can be found in Appendix B.\nSpecifically, as shown in Figure 1, upon obtaining user requirements, the Product Manager under-\ntakes a thorough analysis, formulating a detailed PRD that includes User Stories and Requirement\nPool. This serves as a preliminary functional breakdown. The structured PRD is then passed to\nthe Architect, who translates the requirements into system design components, such as File Lists,\nData Structures, and Interface Definitions. Once captured in the system design, the information is\ndirected towards the Project Manager for task distribution. Engineers proceed to execute the des-\nignated classes and functions as outlined (detailed in Figure 2). In the following stage, the QA\nEngineer formulates test cases to enforce stringent code quality. In the final step, MetaGPT pro-\nduces a meticulously crafted software solution. We provide a detailed schematic (Figure 3) and a\nconcrete instance (Appendix B) of the SOP workflow in MetaGPT.\n3.2 C OMMUNICATION PROTOCOL\nStructured Communication Interfaces Most current LLM-based multi-agent frameworks (Li\net al., 2023; Zhuge et al., 2023; Zhang et al., 2023; Park et al., 2023) utilize unconstrained natural\nlanguage as a communication interface.\nHowever, despite the versatility of natural language, a question arises: does pure natural language\ncommunication suffice for solving complex tasks? For example, in the telephone game (or Chinese\n5", "start_char_idx": 0, "end_char_idx": 1583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82b5f605-7631-429d-b473-4b7e5bbf34f6": {"__data__": {"id_": "82b5f605-7631-429d-b473-4b7e5bbf34f6", "embedding": null, "metadata": {"page_label": "6", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71e7147a-33d9-4090-9e41-417f6f1d7e72", "node_type": "4", "metadata": {"page_label": "6", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f05efa927271d2dfc05973c1b8ca9849cfd4e7086cf4872409ba0bcbf01863ed", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nwhispers)2, after several rounds of communication, the original information may be quite distorted.\nInspired by human social structures, we propose using structured communication to formulate the\ncommunication of agents. We establish a schema and format for each role and request that individ-\nuals provide the necessary outputs based on their specific role and context.\nAs shown in Figure 3, the Architect agent generates two outputs: the system interface design and a\nsequence flow diagram. These contain system module design and interaction sequences, which serve\nas important deliverables for Engineers. Unlike ChatDev (Zhao et al., 2023), agents in MetaGPT\ncommunicate through documents and diagrams (structured outputs) rather than dialogue. These\ndocuments contain all necessary information, preventing irrelevant or missing content.\nPublish-Subscribe Mechanism Sharing information is critical in collaboration. For instance,\nArchitects and Engineers often need to reference PRDs. However, communicating this information\neach time in a one-to-one manner, as indicated by previous work (Li et al., 2023; Zhao et al., 2023;\nZhang et al., 2023), can complicate the communication topology, resulting in inefficiencies.\nTo address this challenge, a viable approach is to store information in a global message pool . As\nshown in Figure 2 (left), we introduce a shared message pool that allows all agents to exchange\nmessages directly. These agents not only publish their structured messages in the pool but also access\nmessages from other entities transparently. Any agent can directly retrieve required information\nfrom the shared pool, eliminating the need to inquire about other agents and await their responses.\nThis enhances communication efficiency.\nSharing all information with every agent can lead to information overload. During task execution,\nan agent typically prefers to receive only task-related information and avoid distractions through\nirrelevant details. Effective management and dissemination of this information play a crucial role.\nWe offer a simple and effective solution- subscription mechanism (in Figure 2 (left)). Instead of\nrelying on dialogue, agents utilize role-specific interests to extract relevant information. They can\nselect information to follow based on their role profiles. In practical implementations, an agent\nactivates its action only after receiving all its prerequisite dependencies. As illustrated in Figure 3,\nthe Architect mainly focuses on PRDs provided by the Product Manager, while documents from\nroles such as the QA Engineer might be of lesser concern.\n3.3 I TERATIVE PROGRAMMING WITH EXECUTABLE FEEDBACK\nIn daily programming tasks, the processes of debugging and optimization play important roles.\nHowever, existing methods often lack a self-correction mechanism, which leads to unsuccessful code\ngeneration. Previous work introduced non-executable code review and self-reflection (Zhao et al.,\n2023; Yao et al., 2022; Shinn et al., 2023; Dong et al., 2023). However, they still face challenges in\nensuring code executability and runtime correctness.\nOur first MetaGPT implementations overlooked certain errors during the review process, due to\nLLM hallucinations (Manakul et al., 2023). To overcome this, after initial code generation, we\nintroduce an executable feedback mechanism to improve the code iteratively. More specifically, as\nshown in Figure 2, the Engineer is asked to write code based on the original product requirements\nand design.\nThis enables the Engineer to continuously improve code using its own historical execution and\ndebugging memory. To obtain additional information, the Engineer writes and executes the corre-\nsponding unit test cases, and subsequently receives the test results. If satisfactory, additional devel-\nopment tasks are initiated. Otherwise the Engineer debugs the code before resuming programming.\nThis iterative testing process continues until the test is passed or a maximum of 3 retries is reached.\n4 E XPERIMENTS\n4.1 E XPERIMENTAL SETTING\nDatasets We use two public benchmarks, HumanEval (Chen et al., 2021a) and MBPP (Austin\net al., 2021), and a self-generated, more challenging software development benchmark named Soft-\n2https://en.wikipedia.org/wiki/Chinese whispers\n6", "start_char_idx": 0, "end_char_idx": 4280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06e37c82-0f0e-41d9-82fe-106c47ce7ff9": {"__data__": {"id_": "06e37c82-0f0e-41d9-82fe-106c47ce7ff9", "embedding": null, "metadata": {"page_label": "7", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec9fccdc-a752-4178-a26e-b00c8f66be44", "node_type": "4", "metadata": {"page_label": "7", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d3829c8156d895712edb0fcfc9fc38c92947d74f7bf7832b932558f6f0ede46f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "702e9027-4239-4bd9-846c-f18c602aa819", "node_type": "1", "metadata": {}, "hash": "f649b436c0867d4f9b078df39ae56b0aedce414d0c9e8205355c0a1e67b7e682", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nwareDev: (1) HumanEval includes 164 handwritten programming tasks. These tasks encompass\nfunction specifications, descriptions, reference codes, and tests. (2) MBPP consists of 427 Python\ntasks. These tasks cover core concepts and standard library features and include descriptions, ref-\nerence codes, and automated tests. (3) Our SoftwareDev dataset is a collection of 70 representa-\ntive examples of software development tasks, each with its own task prompt (see Table 8). These\ntasks have diverse scopes (See Figure 5), such as mini-games, image processing algorithms, data\nvisualization. They offer a robust testbed for authentic development tasks. Contrary to previous\ndatasets (Chen et al., 2021a; Austin et al., 2021), SoftwareDev focuses on the engineering aspects.\nIn the comparisons, we randomly select seven representative tasks for evaluation.\nEvaluation Metrics For HuamnEval and MBPP, we follow the unbiased version of Pass @ kas\npresented by (Chen et al., 2021a; Dong et al., 2023), to evaluate the functional accuracy of the top-k\ngenerated codes: Pass @ k=EProblems\u0014\n1\u2212(n\u2212c\nk)\n(n\nk)\u0015\n.\nFor SoftwareDev, we prioritize practical use and evaluate performance through human evaluations\n(A, E) or statistical analysis (B, C, D): (A)Executability: this metric rates code from 1 (failure/non-\nfunctional) to 4 (flawless). \u20181\u2019 is for non-functional, \u20182\u2019 for runnable but imperfect, \u20183\u2019 for nearly\nperfect, and \u20184\u2019 for flawless code. (B)Cost: the cost evaluations here include the (1) running time,\n(2) token usage, and (3) expenses. (C)Code Statistics: this includes (1) code files, (2) lines of code\nper file, and (3) total code lines. (D)Productivity: basically, it is defined as the number of token\nusage divided by the number of lines of code, which refers to the consumption of tokens per code\nline. (E)Human Revision Cost: refers to times of manual code corrections, which tackle problems\nlike package import errors, incorrect class names, or incomplete reference paths. Typically, each\ncorrection involves up to 3 lines of code.\nBaselines We compare our method with recent domain-specific LLMs in the code generation field,\nincluding AlphaCode (Li et al., 2022), Incoder (Fried et al., 2022), CodeGeeX (Zheng et al., 2023),\nCodeGen (Nijkamp et al., 2023), CodeX (Chen et al., 2021a), and CodeT (Chen et al., 2022) and\ngeneral domain LLMs such as PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023). Several\nresults of baselines (such as Incoder, CodeGeeX) are provided by Dong et al. (2023). In HumanEval\nand MBPP, we slightly modified the prompts to align with response format requirements. These\nmodifications aim to address format-specific issues (i.e., Python problems). With the SoftwareDev\nbenchmark, we provide a comprehensive comparison between MetaGPT, AutoGPT (Torantulino\net al., 2023), LangChain (Chase, 2022) with Python Read-Eval-Print Loop (REPL) tool3, Agent-\nVerse (Chen et al., 2023), and ChatDev (Qian et al., 2023).\n4.2 M AINRESULT\nAlphaCode(1.1B)\nIncoder (6.7B)\nCodeGeeX (13B)17.1\n\u201415.2 17.6 18.926.9\nCodeGeeX-Mono(16.1B)32.938.6\nGPT-467.0\n\u2014\nMetaGPT\n(w/o Feedback)81.7 82.3Pass@1 of MBPP  and HumanEval (%)\nPaLM Coder(540B)36.047.0\nCodex (175B)47.058.1\nCodex + CodeT65.8 67.7\nHumanEval\nMBPP\nMetaGPT85.9 87.7\nFigure 4: Pass rates on the MBPP and HumanEval with a single attempt.", "start_char_idx": 0, "end_char_idx": 3327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "702e9027-4239-4bd9-846c-f18c602aa819": {"__data__": {"id_": "702e9027-4239-4bd9-846c-f18c602aa819", "embedding": null, "metadata": {"page_label": "7", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec9fccdc-a752-4178-a26e-b00c8f66be44", "node_type": "4", "metadata": {"page_label": "7", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d3829c8156d895712edb0fcfc9fc38c92947d74f7bf7832b932558f6f0ede46f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06e37c82-0f0e-41d9-82fe-106c47ce7ff9", "node_type": "1", "metadata": {"page_label": "7", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3fee9ce59d7e779ff736d15c328c67215c9aecb3f073db0d10c8565b8e08c856", "class_name": "RelatedNodeInfo"}}, "text": "4.2 M AINRESULT\nAlphaCode(1.1B)\nIncoder (6.7B)\nCodeGeeX (13B)17.1\n\u201415.2 17.6 18.926.9\nCodeGeeX-Mono(16.1B)32.938.6\nGPT-467.0\n\u2014\nMetaGPT\n(w/o Feedback)81.7 82.3Pass@1 of MBPP  and HumanEval (%)\nPaLM Coder(540B)36.047.0\nCodex (175B)47.058.1\nCodex + CodeT65.8 67.7\nHumanEval\nMBPP\nMetaGPT85.9 87.7\nFigure 4: Pass rates on the MBPP and HumanEval with a single attempt.\nPerformance Figure 4 demonstrates that MetaGPT outperforms all preceding approaches in both\nHumanEval and MBPP benchmarks. When MetaGPT collaborates with GPT-4, it significantly im-\nproves the Pass @ kin the HumanEval benchmark compared to GPT-4. It achieves 85.9% and 87.7%\n3https://en.wikipedia.org/wiki/Read\u2013eval\u2013print loop\n7", "start_char_idx": 2965, "end_char_idx": 3656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b6e2167-6e71-461b-bb21-775fc9bb8ada": {"__data__": {"id_": "6b6e2167-6e71-461b-bb21-775fc9bb8ada", "embedding": null, "metadata": {"page_label": "8", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7727deff-3681-4e2a-90cb-6429bc578b9b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "335fc44ae5b7ac70e2e7cc5822294c3add1b846e25de17b051b16e1ba56a1b57", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 5: Demo softwares developed by MetaGPT.\nin these two public benchmarks. Moreover, as shown in Table 1, MetaGPT outperforms ChatDev on\nthe challenging SoftwareDev dataset in nearly all metrics. For example, considering the executabil-\nity, MetaGPT achieves a score of 3.75, which is very close to 4 (flawless). Besides, it takes less time\n(503 seconds), clearly less than ChatDev. Considering the code statistic and the cost of human revi-\nsion, it also significantly outperforms ChatDev. Although MetaGPT requires more tokens (24,613\nor 31,255 compared to 19,292), it needs only 126.5/124.3 tokens to generate one line of code. In\ncontrast, ChatDev uses 248.9 tokens. These results highlight the benefits of SOPs in collabora-\ntions between multiple agents. Additionally, we demonstrate the autonomous software generation\ncapabilities of MetaGPT through visualization samples (Figure 5). For additional experiments and\nanalysis, please refer to Appendix C.\nTable 1: The statistical analysis on SoftwareDev.\nStatistical Index ChatDev MetaGPT w/o Feedback MetaGPT\n(A)Executability 2.25 3.67 3.75\n(B)Cost#1: Running Times (s) 762 503 541\n(B)Cost#2: Token Usage 19,292 24,613 31,255\n(C)Code Statistic#1: Code Files 1.9 4.6 5.1\n(C)Code Statistic#2: Lines of Code per File 40.8 42.3 49.3\n(C)Code Statistic#3: Total Code Lines 77.5 194.6 251.4\n(D)Productivity 248.9 126.5 124.3\n(E)Human Revision Cost 2.5 2.25 0.83\n4.3 C APABILITIES ANALYSIS\nCompared to open-source baseline methods such as AutoGPT and autonomous agents such as\nAgentVerse and ChatDev, MetaGPT offers functions for software engineering tasks. As presented\nin Table 2, our framework encompasses a wide range of abilities to handle complex and specialized\ndevelopment tasks efficiently. Incorporating SOPs (e.g., role-play expertise, structured communi-\ncation, streamlined workflow) can significantly improve code generation. Other baseline methods\n8", "start_char_idx": 0, "end_char_idx": 1925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5c05947-ee92-47ce-8cd7-dd074eb597d8": {"__data__": {"id_": "e5c05947-ee92-47ce-8cd7-dd074eb597d8", "embedding": null, "metadata": {"page_label": "9", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40740a4f-274d-41dc-86c0-0c33d00605dc", "node_type": "4", "metadata": {"page_label": "9", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "09bee7cb4fc5dde0dfe0cfff93927e408f29ef374d29130c5ffb4549ceae066e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nTable 2: Comparison of capabilities for MetaGPT and other approaches. \u2018!\u2019 indicates the\npresence of a specific feature in the corresponding framework, \u2018 %\u2019 its absence.\nFramework Capabiliy AutoGPT LangChain AgentVerse ChatDev MetaGPT\nPRD generation % % % % !\nTenical design genenration % % % % !\nAPI interface generation % % % % !\nCode generation ! ! ! ! !\nPrecompilation execution % % % % !\nRole-based task management % % % ! !\nCode review % % ! ! !\nTable 3: Ablation study on roles. \u2018#\u2019 denotes \u2018The number of\u2019, \u2018Product\u2019 denotes \u2018Product man-\nager\u2019, and \u2018Project\u2019 denotes \u2018Project manager\u2019. \u2018 !\u2019 indicates the addition of a specific role. \u2018Revi-\nsions\u2019 refers to \u2018Human Revision Cost\u2019.\nEngineer Product Architect Project #Agents #Lines Expense Revisions Executability\n! % % % 1 83.0 $ 0.915 10 1.0\n! ! % % 2 112.0 $ 1.059 6.5 2.0\n! ! ! % 3 143.0 $ 1.204 4.0 2.5\n! ! % ! 3 205.0 $ 1.251 3.5 2.0\n! ! ! ! 4 191.0 $ 1.385 2.5 4.0\ncan easily integrate SOP-like designs to improve their performance, similar to injecting chain-of-\nthought (Wei et al., 2022) in LLMs.\n4.4 A BLATION STUDY\nThe Effectiveness of Roles To understand the impact of different roles on the final results, we\nperform two tasks that involve generating effective code and calculating average statistics. When we\nexclude certain roles, unworkable codes are generated. As indicated by Table 3, the addition of roles\ndifferent from just the Engineer consistently improves both revisions and executability. While more\nroles slightly increase the expenses, the overall performance improves noticeably, demonstrating the\neffectiveness of the various roles.\nThe Effectiveness of Executable Feedback Mechanism As shown in Figure 4, adding executable\nfeedback into MetaGPT leads to a significant improvement of 4.2% and 5.4% in Pass @1 on Hu-\nmanEval and MBPP, respectively. Besides, Table 1 shows that the feedback mechanism improves\nfeasibility (3.67 to 3.75) and reduces the cost of human revisions (2.25 to 0.83). These results\nillustrate how our designed feedback mechanism can produce higher-quality code. Additional quan-\ntitative results of MetaGPT and MetaGPT without executable feedback are shown in Table 4 and\nTable 9.\n5 C ONCLUSION\nThis work introduces MetaGPT, a novel meta-programming framework that leverages SOPs to en-\nhance the problem-solving capabilities of multi-agent systems based on Large Language Models\n(LLMs). MetaGPT models a group of agents as a simulated software company, analogous to simu-\nlated towns (Park et al., 2023) and the Minecraft Sandbox in V oyager (Wang et al., 2023a). MetaGPT\nleverages role specialization, workflow management, and efficient sharing mechanisms such as mes-\nsage pools and subscriptions, rendering it a flexible and portable platform for autonomous agents\nand multi-agent frameworks. It uses an executable feedback mechanism to enhance code generation\nquality during runtime. In extensive experiments, MetaGPT achieves state-of-the-art performance\non multiple benchmarks. The successful integration of human-like SOPs inspires future research\non human-inspired techniques for artificial multi-agent systems. We also view our work as an early\nattempt to regulate LLM-based multi-agent frameworks. See also the outlook (Appendix A) .\n9", "start_char_idx": 0, "end_char_idx": 3265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eacca254-e9a9-4fda-b115-4dcbccff0f04": {"__data__": {"id_": "eacca254-e9a9-4fda-b115-4dcbccff0f04", "embedding": null, "metadata": {"page_label": "10", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "201fca88-45d6-4f71-92e6-2ec0cc8b9c0b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a830ad610b008a4691381f1d4bc0626957ed66577941cadd739b0e12b569f939", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f612715-a809-4ae5-959f-4b135b2a84d7", "node_type": "1", "metadata": {}, "hash": "c3285b8059c5f308c8ef62ab51fc003b1ac0d1dceead1bb36bc5cb56210f91da", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nAcknowledgement\nWe thank Sarah Salhi, the Executive Secretary of KAUST AI Initiative, and Yuhui Wang, Postdoc-\ntoral Fellow at the KAUST AI Initiative, for helping to polish some of the text. We would like to\nexpress our gratitude to Wenyi Wang, a PhD student at the KAUST AI Initiative, for providing com-\nprehensive feedback on the paper and for helping to draft the outlook (Appendix A) with Mingchen.\nWe also thank Zongze Xu, the vice president of DeepWisdom, for providing illustrative materials\nfor AgentStore.\nAuthor Contributions\nSirui Hong conducted most of the experiments and designed the executable feedback module. She\nalso led the initial version of the write-up, supported by Ceyao Zhang, and also by Jinlin Wang and\nZili Wang. Mingchen Zhuge designed the self-improvement module, discussed additional experi-\nments, and led the current write-up. Jonathan Chen helped with the MBPP experiments, outlined\nthe methods section, and contributed to the current write-up. Xiawu Zheng provided valuable guid-\nance, reviewed and edited the paper. Yuheng Cheng contributed to the evaluation metric design and\nHumanEval experiments. Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Lingfeng Xiao helped\nwith the MBPP experiments and comparisons to open-source baseline methods. Chenyu Ran cre-\nated most of the illustrative figures. Chenglin Wu is the CEO of DeepWisdom, initiated MetaGPT,\nmade the most significant code contributions to it, and advised this project. J \u00a8urgen Schmidhuber,\nDirector of the AI Initiative at KAUST and Scientific Director of IDSIA, advised this project and\nhelped with the write-up.\nREFERENCES\nElif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.\nPlaying repeated games with large language models. arXiv preprint , 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large\nlanguage models, 2021.\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew\nGoff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by com-\nbining language models with strategic reasoning. Science , 2022.\nRobert Balzer. A 15 year perspective on automatic programming. TSE, 1985.\nR.M. Belbin. Team Roles at Work . Routledge, 2012. URL https://books.google.co.uk/\nbooks?id=MHIQBAAAQBAJ .\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\ntool makers. arXiv preprint , 2023.\nHarrison Chase. LangChain. https://github.com/hwchase17/langchain , 2022.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu\nChen. Codet: Code generation with generated tests, 2022.\nJiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: self-organizing agents in open-ended\nenvironment. arXiv preprint , 2024.", "start_char_idx": 0, "end_char_idx": 2924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f612715-a809-4ae5-959f-4b135b2a84d7": {"__data__": {"id_": "3f612715-a809-4ae5-959f-4b135b2a84d7", "embedding": null, "metadata": {"page_label": "10", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "201fca88-45d6-4f71-92e6-2ec0cc8b9c0b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a830ad610b008a4691381f1d4bc0626957ed66577941cadd739b0e12b569f939", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eacca254-e9a9-4fda-b115-4dcbccff0f04", "node_type": "1", "metadata": {"page_label": "10", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1c887532fdb97f833b753acc673ae6e39b32d656e8806f0d3b2f55e738ab4f91", "class_name": "RelatedNodeInfo"}}, "text": "Team Roles at Work . Routledge, 2012. URL https://books.google.co.uk/\nbooks?id=MHIQBAAAQBAJ .\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\ntool makers. arXiv preprint , 2023.\nHarrison Chase. LangChain. https://github.com/hwchase17/langchain , 2022.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu\nChen. Codet: Code generation with generated tests, 2022.\nJiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: self-organizing agents in open-ended\nenvironment. arXiv preprint , 2024.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex\nPaino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec\nRadford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-\nGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large\nlanguage models trained on code, 2021a.\n10", "start_char_idx": 2352, "end_char_idx": 3884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7597db72-44af-4eda-923e-bd09a4dc373a": {"__data__": {"id_": "7597db72-44af-4eda-923e-bd09a4dc373a", "embedding": null, "metadata": {"page_label": "11", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4215217f-2d80-4f77-a3b8-8a185a5bca01", "node_type": "4", "metadata": {"page_label": "11", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8885545bd01a64a3a2270640a63f9464184bb35cb5815ca1518589327237bc55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10328131-02c8-4ff9-8864-7c90b56a9d00", "node_type": "1", "metadata": {}, "hash": "d17fd1c20c9c28b8ca87f1cac0b081fef1134d3918569c5bc4391f4bf6a0bf11", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan,\nYujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facili-\ntating multi-agent collaboration and exploring emergent behaviors in agents, 2023.\nXinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In ICLR ,\n2018.\nXinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis\nbeyond domain-specific languages. NeurIPS , 2021b.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,\n2022.\nT. DeMarco and T.R. Lister. Peopleware: Productive Projects and Teams . Addison-Wesley, 2013.\nURLhttps://books.google.co.uk/books?id=DVlsAQAAQBAJ .\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\npreprint , 2023.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving\nfactuality and reasoning in language models through multiagent debate, 2023.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich\nSch\u00a8utze, and Yoav Goldberg. Measuring and improving consistency in pretrained language mod-\nels.TACL , 2021.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural\nlanguages. arXiv preprint , 2020.\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt \u00a8aschel.\nPromptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint , 2023.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In ICML , 2017.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\nand synthesis. arXiv preprint , 2022.\nIrving John Good. Speculations concerning the first ultraintelligent machine. Adv. Comput. , 1965.", "start_char_idx": 0, "end_char_idx": 3075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10328131-02c8-4ff9-8864-7c90b56a9d00": {"__data__": {"id_": "10328131-02c8-4ff9-8864-7c90b56a9d00", "embedding": null, "metadata": {"page_label": "11", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4215217f-2d80-4f77-a3b8-8a185a5bca01", "node_type": "4", "metadata": {"page_label": "11", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8885545bd01a64a3a2270640a63f9464184bb35cb5815ca1518589327237bc55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7597db72-44af-4eda-923e-bd09a4dc373a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bd0ec63970011397a126471b4ac28db9f45ca990730c3f47d2a72e68e2dfa12a", "class_name": "RelatedNodeInfo"}}, "text": "Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt \u00a8aschel.\nPromptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint , 2023.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In ICML , 2017.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\nand synthesis. arXiv preprint , 2022.\nIrving John Good. Speculations concerning the first ultraintelligent machine. Adv. Comput. , 1965.\nRui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie. Chatllm network:\nMore brains, more intelligence. arXiv preprint , 2023.\nS. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In Lecture\nNotes on Comp. Sci. 2130, Proc. Intl. Conf. on Artificial Neural Networks (ICANN-2001) , pp. 87\u2013\n94. Springer: Berlin, Heidelberg, 2001.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. Self-planning code generation\nwith large language model. arXiv preprint , 2023.\n11", "start_char_idx": 2433, "end_char_idx": 3605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b8d8c4e-d684-4327-a79a-2d0ec778cebd": {"__data__": {"id_": "6b8d8c4e-d684-4327-a79a-2d0ec778cebd", "embedding": null, "metadata": {"page_label": "12", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a5b009d-73ef-44de-9256-6404be516951", "node_type": "4", "metadata": {"page_label": "12", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cb94338a969f76be378a56fb2c328da63d3a273159fdbca2aadfa3f457a3df6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a24f00a-23fd-4cc6-91d5-82f4a54c9e21", "node_type": "1", "metadata": {}, "hash": "35dbd3c1cb3ac2071303e2261b93aede4aba6937fbaf3fdd59b451388a4e42ea", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCamel: Communicative agents for\u201d mind\u201d exploration of large scale language model society.\narXiv preprint , 2023.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R \u00b4emi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\nwith alphacode. Science , 2022.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\nagent debate. arXiv preprint , 2023.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang,\nChandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and\nslow thinking for complex interactive tasks. arXiv preprint , 2023.\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and\nSoroush V osoughi. Training socially aligned language models in simulated human society. arXiv\npreprint , 2023.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\nevol-instruct. arXiv preprint , 2023.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hal-\nlucination detection for generative large language models. arXiv preprint , 2023.\nAgile Manifesto. Manifesto for agile software development . Snowbird, UT, 2001.\nJohn McCarthy. History of lisp. In History of programming languages . 1978.\nAnsong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria\nLin. Lever: Learning to verify language-to-code generation with execution. In ICML , 2023.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. Codegen: An open large language model for code with multi-turn program\nsynthesis, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint ,\n2023.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and\nMaosong Sun. Communicative agents for software development, 2023.\nBaptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J \u00b4er\u00b4emy Rapin, et al. Code llama: Open foundation models for code.\narXiv preprint , 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint , 2023.\nJ. Schmidhuber. A self-referential weight matrix. In Proceedings of the International Conference\non Artificial Neural Networks, Amsterdam , pp. 446\u2013451. Springer, 1993a.\nJ. Schmidhuber.", "start_char_idx": 0, "end_char_idx": 3050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a24f00a-23fd-4cc6-91d5-82f4a54c9e21": {"__data__": {"id_": "3a24f00a-23fd-4cc6-91d5-82f4a54c9e21", "embedding": null, "metadata": {"page_label": "12", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a5b009d-73ef-44de-9256-6404be516951", "node_type": "4", "metadata": {"page_label": "12", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cb94338a969f76be378a56fb2c328da63d3a273159fdbca2aadfa3f457a3df6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b8d8c4e-d684-4327-a79a-2d0ec778cebd", "node_type": "1", "metadata": {"page_label": "12", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cddfec46b984a1fd7d0c8cf06be3ce8230cfbd69aac220408fb6bad86651e596", "class_name": "RelatedNodeInfo"}}, "text": "Code llama: Open foundation models for code.\narXiv preprint , 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint , 2023.\nJ. Schmidhuber. A self-referential weight matrix. In Proceedings of the International Conference\non Artificial Neural Networks, Amsterdam , pp. 446\u2013451. Springer, 1993a.\nJ. Schmidhuber. G \u00a8odel machines: self-referential universal problem solvers making provably\noptimal self-improvements. Technical Report IDSIA-19-03, arXiv:cs.LO/0309048 v3, IDSIA,\nManno-Lugano, Switzerland, December 2003.\nJ. Schmidhuber. G \u00a8odel machines: Fully self-referential optimal universal self-improvers. In B. Go-\nertzel and C. Pennachin (eds.), Artificial General Intelligence , pp. 199\u2013226. Springer Verlag,\n2006. Variant available as arXiv:cs.LO/0309048.\n12", "start_char_idx": 2577, "end_char_idx": 3505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85f8d3bd-5a8c-46fb-8498-e96258343397": {"__data__": {"id_": "85f8d3bd-5a8c-46fb-8498-e96258343397", "embedding": null, "metadata": {"page_label": "13", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ffe9e80-3fef-45ac-85a3-c278a1531c18", "node_type": "4", "metadata": {"page_label": "13", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "27a4d33f15ef2c2b984683cbc4d074573a1c0f4a6dcbb1585f28abf053dad391", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2df93a6d-3c5d-46fa-bb28-a3111e91efeb", "node_type": "1", "metadata": {}, "hash": "9242788618ea741854a46acf2b289b1507a6d813b00ad0068f5f3d7568e5363e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nJ. Schmidhuber. Ultimate cognition `a laG\u00a8odel. Cognitive Computation , 1(2):177\u2013193, 2009.\nJ\u00a8urgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to\nlearn: the meta-meta-... hook . PhD thesis, 1987.\nJ\u00a8urgen Schmidhuber. A \u2018self-referential\u2019weight matrix. In ICANN\u201993: Proceedings of the Interna-\ntional Conference on Artificial Neural Networks Amsterdam, The Netherlands 13\u201316 September\n1993 3 , 1993b.\nJ\u00a8urgen Schmidhuber. On learning to think: Algorithmic information theory for novel combinations\nof reinforcement learning controllers and recurrent neural world models. arXiv preprint , 2015.\nJ\u00a8urgen Schmidhuber, Jieyu Zhao, and Nicol N Schraudolph. Reinforcement learning with self-\nmodifying policies. In Learning to learn . 1998.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint , 2023.\nMarta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bj\u00f8rn Kristensen,\nKourosh Darvish, Al \u00b4an Aspuru-Guzik, Florian Shkurti, and Animesh Garg. Errors are useful\nprompts: Instruction guided task programming with verifier-assisted iterative prompting. arXiv\npreprint , 2023.\nElliot Soloway. Learning to program =learning to construct mechanisms and explanations. Com-\nmunications of the ACM , 1986.\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of\nintelligent llm agents, 2023.\nTorantulino et al. Auto-gpt. https://github.com/Significant-Gravitas/\nAuto-GPT , 2023.\nR. J. Waldinger and R. C. T. Lee. PROW: a step toward automatic program writing. In D. E. Walker\nand L. M. Norton (eds.), Proceedings of the 1st International Joint Conference on Artificial Intel-\nligence (IJCAI) , 1969.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\narXiv preprint , 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\narXiv preprint , 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint , 2022.\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\ncognitive synergy in large language models: A task-solving agent through multi-persona self-\ncollaboration. arXiv preprint , 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS ,\n2022.\nMichael Wooldridge and Nicholas R. Jennings. Pitfalls of agent-oriented development. In Pro-\nceedings of the Second International Conference on Autonomous Agents , 1998. URL https:\n//doi.org/10.1145/280765.280867 .\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint , 2022.", "start_char_idx": 0, "end_char_idx": 3190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2df93a6d-3c5d-46fa-bb28-a3111e91efeb": {"__data__": {"id_": "2df93a6d-3c5d-46fa-bb28-a3111e91efeb", "embedding": null, "metadata": {"page_label": "13", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ffe9e80-3fef-45ac-85a3-c278a1531c18", "node_type": "4", "metadata": {"page_label": "13", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "27a4d33f15ef2c2b984683cbc4d074573a1c0f4a6dcbb1585f28abf053dad391", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85f8d3bd-5a8c-46fb-8498-e96258343397", "node_type": "1", "metadata": {"page_label": "13", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fcfa8e89d9629534d70bedba360d8d131ff5e757406fbc47b7b63efaaf60b16a", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint , 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS ,\n2022.\nMichael Wooldridge and Nicholas R. Jennings. Pitfalls of agent-oriented development. In Pro-\nceedings of the Second International Conference on Autonomous Agents , 1998. URL https:\n//doi.org/10.1145/280765.280867 .\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint , 2022.\nEric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop):\nRecursively self-improving code generation. arXiv preprint , 2023.\n13", "start_char_idx": 2587, "end_char_idx": 3357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97ceeb8d-77d9-40a0-9aad-0a5f4811586a": {"__data__": {"id_": "97ceeb8d-77d9-40a0-9aad-0a5f4811586a", "embedding": null, "metadata": {"page_label": "14", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc50cd5e-571b-4bd1-9669-58dd68716c74", "node_type": "4", "metadata": {"page_label": "14", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8808921242c800e400d4e64a799e97dbf040095faf51c7aa51b130ed65de890a", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nHongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tian-\nmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language\nmodels. arXiv preprint , 2023.\nXufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat\nwith the environment: Interactive multimodal perception using large language models. arXiv\npreprint , 2023.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for\ncode generation with multilingual evaluations on humaneval-x, 2023.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nYonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building\nautonomous agents. arXiv preprint , 2023.\nMingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, R \u00b4obert Csord \u00b4as, Anand\nGopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki\nIrie, et al. Mindstorms in natural language-based societies of mind. arXiv preprint , 2023.\n14", "start_char_idx": 0, "end_char_idx": 1160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e39fd88-aa12-4bac-9fa2-ad224dc2de43": {"__data__": {"id_": "9e39fd88-aa12-4bac-9fa2-ad224dc2de43", "embedding": null, "metadata": {"page_label": "15", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a994c212-2d13-48c7-ae31-c5ca9f70eb42", "node_type": "4", "metadata": {"page_label": "15", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fb0ecdf46ff9492e9cb782d1544a3e27fff0b6031da5aac9979dee9b6e3fa08e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nA O UTLOOK\nA.1 S ELF-IMPROVEMENT MECHANISMS\nOne limitation of the MetaGPT version in the main text of this paper is that each software project is\nexecuted independently. However, through active teamwork, a software development team should\nlearn from the experience gained by developing each project, thus becoming more compatible and\nsuccessful over time.\nThis is somewhat related to the idea of recursive self-improvement, first informally proposed in\n1965 (Good, 1965), with first concrete implementations since 1987 (Schmidhuber, 1987; 1993b;\nSchmidhuber et al., 1998), culminating in the concept of mathematically optimal self-referential\nself-improvers (Schmidhuber, 2003; 2009). Generally speaking, a system should learn from experi-\nence in the real world, and meta-learn better learning algorithms from experiences of learning, and\nmeta-meta-learn better meta-learning algorithms from experiences of meta-learning, etc., without\nany limitations except those of computability and physics.\nMore recent, somewhat related work leverages the reasoning ability of Large Language Models\n(LLMs) and recursively improves prompts of LLMs, to improve performance on certain downstream\ntasks (Fernando et al., 2023; Zelikman et al., 2023), analogous to the adaptive prompt engineer of\n2015 (Schmidhuber, 2015) where one neural network learns to generate sequence of queries or\nprompts for another pre-trained neural network whose answers may help the first network to learn\nnew tasks more quickly.\nIn our present work, we also explore a self-referential mechanism that recursively modifies the con-\nstraint prompts of agents based on information they observe during software development. Our\ninitial implementation works as follows. Prior to each project, every agent in the software company\nreviews previous feedback and makes necessary adjustments to their constraint prompts. This en-\nables them to continuously learn from past project experiences and enhance the overall multi-agent\nsystem by improving each individual in the company. We first establish a handover feedback action\nfor each agent. This action is responsible for critically summarizing the information received dur-\ning the development of previous projects and integrating this information in an updated constraint\nprompt. The summarized information is stored in long-term memory such that it can be inherited\nby future constraint prompt updates. When initiating a new project, each agent starts with a react\naction. Each agent evaluates the received feedback and summarizes how they can improve in a\nconstraint prompt.\nOne current limitation is that these summary-based optimizations only modify constraints in the\nspecialization of roles (Sec. 3.1) rather than structured communication interfaces in communication\nprotocols (Sec. 3.2). Future advancements are yet to be explored.\nA.2 M ULTI -AGENT ECONOMIES\nIn real-world teamwork, the interaction processes are often not hardcoded. For example, in a soft-\nware company, the collaboration SOP may change dynamically.\nOne implementation of such self-organization is discussed in the paper on a \u201cNatural Language-\nBased Society of Mind\u201d (NLSOM) (Zhuge et al., 2023), which introduced the idea of an \u201cEconomy\nof Minds\u201d (EOM), a Reinforcement Learning (RL) framework for societies of LLMs and other\nagents. Instead of using standard RL techniques to optimize the total reward of the system through\nmodifications of neural network parameters, EOMs use the principles of supply and demand in free\nmarkets to assign credit (money) to those agents that contribute to economic success (reward).\nThe recent agent-based platform of DeepWisdom (AgentStore4) is compatible with the credit as-\nsignment concept of EOMs. Each agent in AgentStore provides a list of services with corresponding\ncosts. A convenient API is provided so that human users or agents in the platform can easily pur-\nchase services from other agents to accomplish their services. Figure 6 displays the User Interface\n(UI) of AgentStore, where various agents with different skills are showcased. Besides, individual\ndevelopers can participate in building new agents and enable collaborative development within the\ncommunity. Specifically, AgentStore allows users to subscribe to agents according to their demands\n4http://beta.deepwisdom.ai\n15", "start_char_idx": 0, "end_char_idx": 4326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49c287eb-c6d6-48d2-b54f-b113468e861e": {"__data__": {"id_": "49c287eb-c6d6-48d2-b54f-b113468e861e", "embedding": null, "metadata": {"page_label": "16", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66a2d553-7e1b-4216-a0be-1048ec53a9ee", "node_type": "4", "metadata": {"page_label": "16", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "05e5fccb5b27e3242f2d8e14c76a68145108eca11b4059cbd17eb5ff2b98a4fc", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nand pay according to their usage. Moreover, users can purchase additional capabilities to expand the\nplug-and-play functions of their existing agents. This allows users to gradually upgrade their agents.\nWithin the MetaGPT framework, AgentStore can support the collaboration of various agents. Users\ncan collect several agents together to carry out more complex tasks or projects, and all the agents\nshare and comply with development and communication protocols defined in MetaGPT.\nFigure 6: AgentStore is a platform dedicated to serving users in the creation and development of\nagents within the MetaGPT framework. This platform provides users with an operational interface,\nallowing users to easily manage a variety of agents with different emotions, personalities, and capa-\nbilities for specific tasks.\n16", "start_char_idx": 0, "end_char_idx": 818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc99699c-b9c5-480b-98db-9de59ee17649": {"__data__": {"id_": "cc99699c-b9c5-480b-98db-9de59ee17649", "embedding": null, "metadata": {"page_label": "17", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9be0f482-850b-4d49-b4ce-efd96bcf646e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c811238f781f45c22413d9f455722dbdfcb7aa35e306f10831250c9f4e71bae2", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nB A D EMO OF THE EXECUTION\nIn this section, we outline the complete process of software development using MetaGPT. It begins\nwith a user\u2019s input command (as shown in Appendix B.1) and ends with software designed according\nto the user\u2019s specifications.\nB.1 U SERINPUT\nUpon receiving an instruction from the user, MetaGPT collaborates with a professional development\nteam to fulfill the task. Here is a demo of user input:\nUser input\n1write a python3 GUI app such that you can draw an image with it\nB.2 M ETAGPT DEVELOPMENT PROCESS\nNow we provide a step-by-step explanation of the standardized output process for each agent.\nProduct Manager The Product Manager generates a Product Requirement Document (PRD), as\ndetailed in the specified documentation. This document encompasses goals, user stories, competitive\nanalysis, requirement analysis and requirement pool. Additionally, a competitive quadrant chart is\nproduced (see Figure 7). Subsequently, these documents and charts are handed over to the architect\nfor system design.\nProduct Requirement Document\n1## Original Requirements\n2The boss requires a Python3 GUI color meter that outputs the RGB values when it moves.\n3\n4## Product Goals\n5\u2018\u2018\u2018python\n6[\n7 \"Create a user-friendly GUI color meter\",\n8 \"Ensure the color meter accurately outputs RGB values\",\n9 \"Ensure the color meter updates RGB values in real-time as it moves\"\n10]\n11\u2018\u2018\u2018\n12\n13## User Stories\n14\u2018\u2018\u2018python\n15[\n16 \"As a user, I want to select any color on the screen, so that I can get its RGB values\",\n17 \"As a user, I want the RGB values to update in real-time as I move the color meter, so that I\n,\u2192can get accurate color information\",\n18 \"As a user, I want a simple and intuitive interface, so that I can easily use the color meter\"\n19]\n20\u2018\u2018\u2018\n21\n22## Competitive Analysis\n23\u2018\u2018\u2018python\n24[\n25 \"Color Cop: A popular color picker for Windows. It\u2019s simple and easy to use, but lacks real-\n,\u2192time RGB value updates\",\n26 \"Just Color Picker: Offers real-time color picking and multiple color code formats. However,\n,\u2192its interface is cluttered and not very user-friendly\",\n27 \"Instant Eyedropper: A simple color picker that lives in the system tray. It\u2019s convenient but\n,\u2192lacks advanced features\",\n28 \"ColorZilla: A color picker extension for browsers. It\u2019s convenient for web design but not\n,\u2192suitable for desktop applications\",\n29 \"ColorPic: Offers color palettes and mixer tools. It\u2019s feature-rich but can be overwhelming\n,\u2192for simple tasks\"\n30]\n31\u2018\u2018\u2018\n17", "start_char_idx": 0, "end_char_idx": 2475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1339d356-d314-40fe-bd7a-e81a4138a3db": {"__data__": {"id_": "1339d356-d314-40fe-bd7a-e81a4138a3db", "embedding": null, "metadata": {"page_label": "18", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fb0113f-4dd4-48d9-882a-3ed50046463e", "node_type": "4", "metadata": {"page_label": "18", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bd200a476e709741e27949073edd7001837ee8a2dd7968044db13fb35e8ba356", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nProduct Requirement Document\n1\n2## Requirement Analysis\n3The product should be a Python3 GUI application that serves as a color meter. It should allow\n,\u2192users to select any color on the screen and display the corresponding RGB values. The RGB\n,\u2192 values should update in real-time as the color meter moves. The interface should be\n,\u2192simple and user-friendly.\n4\n5## Requirement Pool\n6\u2018\u2018\u2018python\n7[\n8 (\"Design a user-friendly GUI for the color meter\", \"P0\"),\n9 (\"Implement color selection functionality\", \"P0\"),\n10 (\"Display RGB values of the selected color\", \"P0\"),\n11 (\"Update RGB values in real-time as the color meter moves\", \"P0\"),\n12 (\"Test the application for accuracy and performance\", \"P1\")\n13]\n14\u2018\u2018\u2018\n15\n16## UI Design draft\n17The UI should be minimalistic and intuitive. It should primarily consist of a color meter, which\n,\u2192could be a magnified area of the screen that the user can move around. The RGB values of\n,\u2192the currently selected color should be prominently displayed, updating in real-time as\n,\u2192the color meter moves. The layout should be clean and uncluttered, with a focus on\n,\u2192functionality and ease of use.\n18\n19## Anything UNCLEAR\n20There are no unclear points.\nFigure 7: The quadrant chart for the \u201cDrawing App\u201d generated by the product manager agent.\n18", "start_char_idx": 0, "end_char_idx": 1285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e00101a-ce99-4c5d-848a-66e1544ffff9": {"__data__": {"id_": "5e00101a-ce99-4c5d-848a-66e1544ffff9", "embedding": null, "metadata": {"page_label": "19", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca4eed97-c081-421d-ad92-f813d13c6692", "node_type": "4", "metadata": {"page_label": "19", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "853ccbd4ed389d96821426c53f623e8a1941c734955b6ef65eb850e61c077411", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nArchitect Based on the requirements in PRD, the Architect agent devises technical specifications\nincluding system architecture diagrams and interface definitions. Initially, the Architect defines the\noverarching technical trajectory. Subsequently, the project\u2019s architecture, including files, classes\n(Figure 8) and the sequence flow chart (Figure 12), is designed. The Architect\u2019s documentation is\nthen given to the project manager for task allocation and execution.\nSystem design\n1## Implementation approach\n2We will use Python\u2019s Tkinter library to create the GUI for the color meter. Tkinter is a standard\n,\u2192Python interface to the Tk GUI toolkit and is included with most Python installations. It\n,\u2192 is simple to use and powerful enough for our needs.\n3\n4The color selection functionality will be implemented using the PIL (Pillow) library, which allows\n,\u2192 us to capture the screen\u2019s pixel data. By moving the mouse, we can get the RGB values of\n,\u2192 the pixel under the mouse cursor in real-time.\n5\n6The RGB values will be displayed on the GUI using Tkinter\u2019s Label widget. We will use Python\u2019s\n,\u2192threading library to update the RGB values in real-time as the color meter moves.\n7\n8The application will be tested for accuracy and performance using Python\u2019s unittest library.\n9\n10## Python package name\n11\u2018\u2018\u2018python\n12\"color_meter\"\n13\u2018\u2018\u2018\n14\n15## File list\n16\u2018\u2018\u2018python\n17[\n18 \"main.py\",\n19 \"color_picker.py\",\n20 \"gui.py\",\n21 \"tests.py\"\n22]\n23\u2018\u2018\u2018\n24\n25## Anything UNCLEAR\n26The requirement is clear to me.\nFigure 8: Data structures and interface definitions for the \u201cDrawing App\u201d generated by the architect\nagent.\nProject Manager The Project Manager breaks down the project into a task list. Furthermore, each\ncode file is analyzed based on its intended functionality and then treated as a separate task assigned\nto Engineers.\n19", "start_char_idx": 0, "end_char_idx": 1837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb3d898b-b154-4fc5-a5c5-6cb50decd35f": {"__data__": {"id_": "bb3d898b-b154-4fc5-a5c5-6cb50decd35f", "embedding": null, "metadata": {"page_label": "20", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06c1c216-46a7-435d-91b6-b77e69b2e663", "node_type": "4", "metadata": {"page_label": "20", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "415eb0c3a19d1eb5bea5a07e0ddfe71b97f85ce9c0661081d91c98273c7686df", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 9: The program call flow for the \u201cDrawing App\u201d generated by the architect agent.\nTasks\n1## Required Python third-party packages\n2\n3\u2018\u2018\u2018python\n4\"\"\"\n5tkinter==8.6\n6Pillow==8.3.1\n7\"\"\"\n8\u2018\u2018\u2018\n9\n10## Required Other language third-party packages\n11\u2018\u2018\u2018python\n12\"\"\"\n13No third-party packages required in other languages.\n14\"\"\"\n15\u2018\u2018\u2018\n16\n17## Full API spec\n18\u2018\u2018\u2018python\n19\"\"\"\n20No APIs are used in this project.\n21\"\"\"\n22\u2018\u2018\u2018\n23\n24## Logic Analysis\n25\u2018\u2018\u2018python\n26[\n27 (\"main.py\", \"Contains the main application loop and initializes all the other components.\"),\n28 (\"canvas.py\", \"Implements the drawing canvas, responds to mouse events, and uses the selected\n,\u2192tool and color.\"),\n29 (\"tools.py\", \"Implements the tool selection interface.\"),\n30 (\"color_picker.py\", \"Implements the color selection interface.\"),\n31 (\"file_manager.py\", \"Implements the file saving and opening functionality.\")\n32]\n33\u2018\u2018\u2018\n20", "start_char_idx": 0, "end_char_idx": 901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9660e8ee-e313-4fb3-b4e5-69e9adde059d": {"__data__": {"id_": "9660e8ee-e313-4fb3-b4e5-69e9adde059d", "embedding": null, "metadata": {"page_label": "21", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4261a0f0-9abd-4ccb-8a42-2bf93f779a81", "node_type": "4", "metadata": {"page_label": "21", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b6a49824bb4a6e6b61db79353a0707b31c6099b741c412adf95a3dfce23e0a03", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nProduct Requirement Document\n1## Task list\n2\u2018\u2018\u2018python\n3[\n4 \"main.py\",\n5 \"canvas.py\",\n6 \"tools.py\",\n7 \"color_picker.py\",\n8 \"file_manager.py\"\n9]\n10\u2018\u2018\u2018\n11\n12## Shared Knowledge\n13\u2018\u2018\u2018python\n14\"\"\"\n15The \u2019Tkinter\u2019 library is used for creating the GUI of the application. It provides various widgets\n,\u2192 like frames and canvases which are used in this project.\n16The \u2019Pillow\u2019 library is used for handling image files. It is used in the \u2019file_manager.py\u2019 for\n,\u2192saving and opening image files.\n17The \u2019main.py\u2019 file is the entry point of the application. It initializes all the other components\n,\u2192and starts the application loop.\n18\"\"\"\n19\u2018\u2018\u2018\n20\n21## Anything UNCLEAR\n22The project requirements and implementation approach are clear. However, we need to ensure that\n,\u2192all team members are familiar with the Tkinter and Pillow libraries. If not, they may\n,\u2192need some time to learn these libraries before starting the development.\n21", "start_char_idx": 0, "end_char_idx": 928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a4a011d-1333-47e3-b677-9f750cc69e44": {"__data__": {"id_": "4a4a011d-1333-47e3-b677-9f750cc69e44", "embedding": null, "metadata": {"page_label": "22", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c5a6ef9-f171-4a62-b1a1-1110ea63f69a", "node_type": "4", "metadata": {"page_label": "22", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "48ebe68b12ce9111ef418b0f839c1b3d9a542dbffba0c97e6c0b94c891752c3f", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nEngineer Given the provided file structure and function definitions, an Engineer agent requires\nonly fundamental development skills to complete the development tasks. Due to the large number\nof files, we present only one auto-generated code file here.\nGenerated code (color picker.py)\n1import tkinter as tk\n2from tkinter import colorchooser\n3class ColorPicker:\n4 def __init__(self, root: tk.Tk, color: str = \u2019black\u2019):\n5 self.frame = tk.Frame(root)\n6 self.color = color\n7\n8 self.color_button = tk.Button(self.frame, text=\u2019Color\u2019, command=self.select_color)\n9 self.color_button.pack(fill=\u2019x\u2019)\n10\n11 def select_color(self):\n12 color = colorchooser.askcolor()[1]\n13 if color is not None:\n14 self.color = color\n15\n16 def pack(self, **kwargs):\n17 self.frame.pack( **kwargs)\n18\n19 def get_color(self):\n20 return self.color\nQA Engineer Upon receiving the code output from the Engineer, the QA Engineer generates unit\ntest code and reviews it to identify and fix any bugs, ensuring high-quality software.\nGenerated Unit tests (test.py)\n1import unittest\n2import tkinter as tk\n3# You might want to move the ColorPicker class to another file, let\u2019s say color_picker.py\n4# from color_picker import ColorPicker\n5\n6class TestColorPicker(unittest.TestCase):\n7\n8 def setUp(self):\n9 self.root = tk.Tk()\n10 self.cp = ColorPicker(self.root)\n11\n12 def tearDown(self):\n13 # This is important to close the tk window after tests\n14 self.root.destroy()\n15\n16 def test_initial_color(self):\n17 # Test the default color during initialization\n18 self.assertEqual(self.cp.get_color(), \u2019black\u2019)\n19\n20 def test_set_and_get_color(self):\n21 # Test setting a new color and retrieving it\n22 new_color = \u2019#ffffff\u2019 # white color\n23 self.cp.color = new_color\n24 self.assertEqual(self.cp.get_color(), new_color)\n25\n26\n27if __name__ == \u2019__main__\u2019:\n28 unittest.main()\nOutput Ultimately, as shown in Figure 10, MetaGPT generates a functional application named\n\u201cDrawing App\u201d.\n22", "start_char_idx": 0, "end_char_idx": 1943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf7e3e5c-ca6f-40af-9294-4b1b0770bcb4": {"__data__": {"id_": "cf7e3e5c-ca6f-40af-9294-4b1b0770bcb4", "embedding": null, "metadata": {"page_label": "23", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5365b0d7-9626-40cf-9a2b-90ef407e45e7", "node_type": "4", "metadata": {"page_label": "23", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6fd2853e0393c5a9a03ee2a193e14f1d92d63ea1d4e8c537afc7da52e5624d2c", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 10: The \u201cDrawing App\u201d generated by MetaGPT.\nC E XPERIMENTS\nC.1 D ETAILS OF THE SOFTWARE DEVDATASET\nThe SoftwareDev dataset includes 70 diverse software development tasks. Table 8 displays the\nnames and detailed prompts of 11 tasks within the dataset. Note that the first seven tasks listed are\nused in the main experiments of this paper.\nC.2 A DDITIONAL RESULTS\nQuantitative results of MetaGPT As shown in Table 4, MetaGPT achieves an average score\nof 3.9, surpassing ChatDev\u2019s score of 2.1 Zhao et al. (2023), which is based on the Chat chain.\nCompare the scores of general intelligent algorithms, including AutoGPT Torantulino et al. (2023),\nwhich all score 1.0, failing to generate executable code. We observe that the generated code is often\nshort, lacks comprehensive logic, and tends to fail to handle cross-file dependencies correctly.\nWhile models such as AutoGPT (Torantulino et al., 2023), Langchain (Chase, 2022), and Agent-\nVerse (Chen et al., 2023) display robust general problem-solving capabilities, they lack an essential\nelement for developing complex systems: systematically deconstructing requirements. Conversely,\nMetaGPT simplifies the process of transforming abstract requirements into detailed class and func-\ntion designs through a specialized division of labor and SOPs workflow. When compared to Chat-\nDev (Zhao et al., 2023), MetaGPT\u2019s structured messaging and feedback mechanisms not only reduce\nloss of communication information but also improve the execution of code.\nQuantitative results of MetaGPT w/o executable feedback Table 9 presents the performance of\nMetaGPT with GPT-4 32K on 11 tasks within the SoftwareDev dataset. It also shows the average\nperformance across all 70 tasks (in the last line). Note that the version of MetaGPT used here is the\nbasic version without the executable feedback mechanism.\nQuantitative results of MetaGPT with different LLMs To verify the performance of MetaGPT\non different LLMs, we randomly selected 5 SoftwareDev tasks and conducted experiments using\nGPT-3.5 and Deepseek Coder 33B5as backends. As shown in Table 5, the results indicate that\nalthough MetaGPT can complete tasks with these LLMs, using GPT-4 as the backend yields superior\nperformance.\n5https://deepseekcoder.github.io\n23", "start_char_idx": 0, "end_char_idx": 2274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c757947d-3479-400e-a599-b42f962d6ad8": {"__data__": {"id_": "c757947d-3479-400e-a599-b42f962d6ad8", "embedding": null, "metadata": {"page_label": "24", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a44ea320-629c-4d26-a9c9-d62bf41962ed", "node_type": "4", "metadata": {"page_label": "24", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ce49370a6ef89f91c34fbc93f4b084ea1820b7e63f2450b4ea8f2099c9632c3a", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nTable 4: Executability comparison. The executability scores are on a grading system ranging from\n\u20191\u2019 to \u20194\u2019. A score of \u20191\u2019 signifies complete failure, \u20192\u2019 denotes executable code, \u20193\u2019 represents\nlargely satisfying expected workflow, and \u20194\u2019 indicates a perfect match with expectations.\nTask AutoGPT LangChain AgentVerse ChatDev MetaGPT\nFlappy bird 1 1 1 2 3\nTank battle game 1 1 1 2 4\n2048 game 1 1 1 1 4\nSnake game 1 1 1 3 4\nBrick breaker game 1 1 1 1 4\nExcel data process 1 1 1 4 4\nCRUD manage 1 1 1 2 4\nAverage score 1.0 1.0 1.0 2.1 3.9\nTable 5: Performance of MetaGPT on SoftwareDev using different LLMs as agent backends.\nModel Open source Time(/s) # Lines Executability Revisions\nMetaGPT (w/ GPT-3.5) % 75.18 161.6 2.8 2.4\nMetaGPT (w/ GPT-4) % 552.94 178.2 3.8 1.2\nMetaGPT (w/ Deepseek Coder 33B) \" 1186.20 120.2 1.4 2.6\nImpact of Instruction Levels (High-level v.s.Detailed Instructions) Does the variation in the\nlevel of initial input from humans significantly influence performance outcomes? For examples:\n1.High-level prompt : Create a brick breaker game.\n2.Detailed prompt : Creating a brick breaker game. In a brick breaker game, the player\ntypically controls a paddle at the bottom of the screen to bounce a ball towards a wall of\nbricks. The goal is to break all the bricks by hitting them with the ball.\nAdditional experiments were conducted to investigate this aspect: we selected 5 tasks from Soft-\nwareDev, and constructed detailed prompts for them. Here are the experimental results:\nTable 6: Impact of Instruction Levels. The executability is scored on a grading system ranging\nfrom \u20181\u2019 to \u20184\u2019. A score of \u20181\u2019 signifies complete failure, \u20182\u2019 denotes runnable code, \u20183\u2019 represents\nlargely expected workflow, and \u20184\u2019 indicates a perfect match to expectations.\nModel # Word Time(/s) Token usage # Lines Executability Productivity Reversions\nHigh-level 13.2 552.9 28384.2 178.2 3.8 163.8 1.2\nDetailed 42.2 567.8 29657.0 257.0 4.0 118.0 1.6\nWe observe that: detailed prompts lead to better software projects with lower productivity ratios\nbecause of clearer requirements and functions, while simple inputs can still generate good enough\nsoftware using MetaGPT with an executability rating of 3.8, which is comparable to the detailed\nprompt scenario. (Note that, Productivity = Token usage / Total Code Lines. The lower this ratio,\nthe better.)\nThe performance of GPT variants in HumanEval benchmark We use the GPT-4\u2019s 67% Hu-\nmanEval score (OpenAI, 2023) as our baseline, acknowledging its acceptance in the HumanEval\nbenchmark. We further extend to experiments(five times) with GPT-4 (gpt-4-0613) and GPT-3.5-\nTurbo (gpt-3.5-turbo-0613) under various conditions to assess performance. (A)We directly called\nthe OpenAI API with the prompt in HumanEval. (B)We called the OpenAI API and parsed the\ncode with regex in the response. (C)We added an additional system prompt, then called the OpenAI\nAPI. The prompt is \u201dYou are an AI that only responds with Python code, NOT ENGLISH. You will\n24", "start_char_idx": 0, "end_char_idx": 3014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f9fc708-17f7-418b-bed2-defb4bb913dd": {"__data__": {"id_": "8f9fc708-17f7-418b-bed2-defb4bb913dd", "embedding": null, "metadata": {"page_label": "25", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "935dc53e-b916-4faa-a741-7a3fa240a804", "node_type": "4", "metadata": {"page_label": "25", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cc65df719bced6322f726227dbd6e18e6d0623f387487dfdbc8cb673675a6265", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nbe given a function signature and its docstring by the user. Write your full implementation (restate\nthe function signature).\u201d As shown in Table 7, GPT-4 is more sensitive to prompt, code parser, and\npost-processing results on the HumanEval data set. It is difficult for GPT-3.5-Turbo to return the\ncorrect completion code without prompt words.\nTable 7: Performance of GPT models on HumanEval. Experiments were conducted five times\nusing gpt-4-0613 and gpt-3.5-turbo-0613 with different settings.\nSettings Model 1 2 3 4 5 Avg. Std.\nA gpt-4-0613 0.732 0.707 0.732 0.713 0.738 0.724 0.013\nA gpt-3.5-turbo-0613 0.360 0.366 0.360 0.348 0.354 0.357 0.007\nB gpt-4-0613 0.787 0.811 0.817 0.829 0.817 0.812 0.016\nB gpt-3.5-turbo-0613 0.348 0.354 0.348 0.335 0.348 0.346 0.007\nC gpt-4-0613 0.805 0.805 0.817 0.793 0.780 0.800 0.014\nC gpt-3.5-turbo-0613 0.585 0.567 0.573 0.579 0.579 0.577 0.007\nQualitative results Figure 11 and Figure 12 illustrate the outcomes of the Architect agent\u2019s ef-\nforts to design a complex recommender system. These figures showcase the comprehensive system\ninterface design and program call flow. The latter is essential for creating a sophisticated automated\nsystem. It is crucial to emphasize the importance of this division of labor in developing an automated\nsoftware framework.\nD L IMITATION AND ETHICS CONCERNS\nD.1 L IMITATION\nSystem side At present, our system cannot fully cater to specific scenarios, such as UI and front-\nend, as we have yet to incorporate such agents and multimodal tools. Furthermore, despite gen-\nerating the most amount of code among comparable frameworks, it remains challenging to fulfill\nreal-world applications\u2019 diverse and complex requirements.\nHuman user side A key challenge for users is to interrupt the running process of each agent, or\nset the starting running point (checkpoint) for each agent.\nD.2 E THICS CONCERNS\nUnemployment and Skill Obsolescence MetaGPT enables more people to program in natural\nlanguages, thereby making it easier for engineers to get started. Over the years, programming\nlanguages have evolved from punched cards to assembly, C, Java, Python, and now natural lan-\nguage. As a result, humans have become more proficient at programming, increasing the demand\nfor programming-related positions. Furthermore, programming with natural language may offer a\nsignificantly easier learning curve, making programming more accessible to a broader audience.\nTransparency and Accountability MetaGPT is an open-source framework that facilitates inter-\nactive communication between multiple agents through natural language. Humans can initiate, ob-\nserve, and stop running with the highest level of control. It provides real-time interpretation and op-\neration of the natural language, displayed on the screen and logs, ensuring transparency. MetaGPT\nenhances \u201cnatural language programming\u201d capabilities, and human engineers are the users and re-\nsponsible for the outcomes.\nPrivacy and Data Security MetaGPT operates locally, ensuring user data privacy and security. It\ndoes not collect user data. For interactions with third-party LLMs, such as those by OpenAI, users\nare encouraged to refer to the respective privacy policies (e.g., OpenAI Privacy Policy). However,\nwe provide the option of open-source LLMs as backends.\n25", "start_char_idx": 0, "end_char_idx": 3307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86ef48ad-5ce4-4111-b54c-f28f00787f58": {"__data__": {"id_": "86ef48ad-5ce4-4111-b54c-f28f00787f58", "embedding": null, "metadata": {"page_label": "26", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abc1ee0c-61cf-4f23-b569-9d2dc0a8e7db", "node_type": "4", "metadata": {"page_label": "26", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d93093e2bfb5d502be189bdbb6a63cdf1f6b044ee890cf836ef891b7947add39", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 11: The system interface design for \u201crecommendation engine development\u201d is generated by\nthearchitect agent ( zoom in for a better view ).\nE M ORE DISCUSSIONS\nE.1 D EEP-SEATED CHALLENGES\nMetaGPT also alleviates or solves these challenges with its unique designs:\nUse Context Efficiently Two sub-challenges are present. First, unfolding short natural language\ndescriptions accurately to eliminate ambiguity. Second, maintaining information validity in lengthy\ncontexts, enables LLMs to concentrate on relevant data without distraction.\nReduce Hallucinations Using LLMs to generate entire software programs faces code halluci-\nnation problems\u2014-including incomplete implementation of functions, missing dependencies, and\npotential undiscovered bugs, which may be more serious. LLMs often struggle with software gen-\neration due to vague task definitions. Focusing on granular tasks like requirement analysis and\npackage selection offers guided thinking, which LLMs lack in broad task solving.\nE.2 I NFORMATION OVERLOAD\nIn MetaGPT, we use a global message pool and a subscription mechanism to address \u201cinformation\noverload,\u201d which refers to the problem of receiving excessive or irrelevant information. This issue\nis dependent on specific applications. MetaGPT employs a message pool to streamline communi-\ncation, ensuring efficiency. Additionally, a subscription mechanism filters out irrelevant contexts,\nenhancing the relevance and utility of the information. This design is particularly crucial in soft-\n26", "start_char_idx": 0, "end_char_idx": 1522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a883018-321b-493a-9e9c-f9f850849985": {"__data__": {"id_": "8a883018-321b-493a-9e9c-f9f850849985", "embedding": null, "metadata": {"page_label": "27", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46e2d04d-a4d8-4764-849d-f4c6c04a7fc9", "node_type": "4", "metadata": {"page_label": "27", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2aa6ac65ade9e3b37e21e102dd4886d725d84b0d1009a1ffb956c51fbd681b23", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 12: The program call flow for \u201crecommendation engine development\u201d generated by the\narchitect agent ( zoom in for a better view ).\nware design scenarios and standard operating procedures (SOPs) where effective communication is\nessential.\n27", "start_char_idx": 0, "end_char_idx": 255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3374e65d-34f9-4dad-9846-a0707b6493d9": {"__data__": {"id_": "3374e65d-34f9-4dad-9846-a0707b6493d9", "embedding": null, "metadata": {"page_label": "28", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30d464d1-866e-418c-b789-9e7801de44b9", "node_type": "4", "metadata": {"page_label": "28", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b20fd51fa52c8c4aa77ab49bd856778b20fef154d159c8d4913f7840ab28c691", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nTable 8: Examples of SoftwareDev dataset.\nTask ID Task Prompt\n0 Snake game Create a snake game.\n1 Brick breaker game Create a brick breaker game.\n2 2048 game Create a 2048 game for the web.\n3 Flappy bird game Write p5.js code for Flappy Bird where you control a yellow bird continu-\nously flying between a series of green pipes. The bird flaps every time you\nleft click the mouse. If it falls to the ground or hits a pipe, you lose. This\ngame goes on indefinitely until you lose; you get points the further you go.\n4 Tank battle game Create a tank battle game.\n5 Excel data process Write an excel data processing program based on streamlit and pandas. The\nscreen first shows an excel file upload button. After the excel file is uploaded,\nuse pandas to display its data content. The program is required to be concise,\neasy to maintain, and not over-designed. It uses streamlit to process web\nscreen displays, and pandas is sufficient to process excel reading and display.\nPlease make sure others can execute directly without introducing additional\npackages.\n6 CRUD manage Write a management program based on the crud addition, deletion, modifi-\ncation and query processing of the customer business entity. The customer\nneeds to save this information: name, birthday, age, sex, and phone. The data\nis stored in client.db, and there is a judgement whether the customer table ex-\nists. If it doesn\u2019t, it needs to be created first. Querying is done by name; same\nfor deleting. The program is required to be concise, easy to maintain, and not\nover-designed. The screen is realized through streamlit and sqlite\u2014no need\nto introduce other additional packages.\n7 Music transcriber Develop a program to transcribe sheet music into a digital format; provid-\ning error-free transcribed symbolized sheet music intelligence from audio\nthrough signal processing involving pitch and time slicing then training a\nneural net to run Onset Detected CWT transforming scalograms to chroma-\ngrams decoded with Recursive Neural Network focused networks.\n8 Custom press releases Create custom press releases; develop a Python script that extracts rele-\nvant information about company news from external sources, such as social\nmedia; extract update interval database for recent changes. The program\nshould create press releases with customizable options and export writings\nto PDFs, NYTimes API JSONs, media format styled with interlink internal\nfixed character-length metadata.\n9 Gomoku game Implement a Gomoku game using Python, incorporating an AI opponent\nwith varying difficulty levels.\n10 Weather dashboard Create a Python program to develop an interactive weather dashboard.\n28", "start_char_idx": 0, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "837013c2-af58-4471-8a21-7d197142b5e9": {"__data__": {"id_": "837013c2-af58-4471-8a21-7d197142b5e9", "embedding": null, "metadata": {"page_label": "29", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "157debf0-ea7e-4062-a6eb-1b79508a6a8d", "node_type": "4", "metadata": {"page_label": "29", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c15620c4ab1908b9bc69582d3f2f7d45f5208a8736eeccf1b7eb19645576a1a0", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nTable 9: Additional results of pure MetaGPT w/o feedback on SoftwareDev. Averages (Avg.) of 70 tasks are calculated and 10 randomly selected tasks are\nincluded. \u2018#\u2019 denotes \u2018The number of\u2019, while \u2018ID\u2019 is \u2018Task ID\u2019.\nID Code statistics Doc statistics Cost statistics Cost of revision Code executability\n#code files #lines of code #lines per code file #doc files #lines of doc #lines per doc file #prompt tokens #completion tokens time costs money costs\n0 5.00 196.00 39.20 3.00 210.00 70.00 24087.00 6157.00 582.04 $ 1.09 1. TypeError 4\n1 6.00 191.00 31.83 3.00 230.00 76.67 32517.00 6238.00 566.30 $ 1.35 1. TypeError 4\n2 3.00 198.00 66.00 3.00 235.00 78.33 21934.00 6316.00 553.11 $ 1.04 1. lack\n@app.route(\u2019/\u2019)3\n3 5.00 164 32.80 3.00 202.00 67.33 22951.00 5312.00 481.34 $ 1.01 1. PNG file miss-\ning 2. Compile bug\nfixes2\n4 6.00 203.00 33.83 3.00 210.00 70.00 30087.00 6567.00 599.58 $ 1.30 1. PNG file\nmissing 2. Com-\npile bug fixes 3.\npygame.surface not\ninitialize3\n5 6.00 219.00 36.50 3.00 294.00 96.00 35590.00 7336.00 585.10 $ 1.51 1. dependency er-\nror 2. ModuleNot-\nFoundError4\n6 4.00 73.00 18.25 3.00 261.00 87.00 25673.00 5832.00 398.83 $ 0.90 0 4\n7 4.00 316.00 79.00 3.00 332.00 110.67 29139.00 7104.00 435.83 $ 0.92 0 4\n8 5.00 215.00 43.00 3.00 301.00 100.33 29372.00 6499.00 621.73 $ 1.27 1. tensorflow ver-\nsion error 2. model\ntraining method not\nimplement2\n9 5.00 215.00 43.00 3.00 270.00 90.00 24799.00 5734.00 550.88 $ 1.27 1. dependency er-\nror 2. URL 403 er-\nror3\n10 3.00 93.00 31.00 3.00 254.00 84.67 24109.00 5363.00 438.50 $ 0.92 1. dependency er-\nror 2. missing main\nfunc.4\nAvg. 4.71 191.57 42.98 3.00 240.00 80.00 26626.86 6218.00 516.71 $1.12 0.51 (only consider\nitem scored 2, 3 or\n4)3.36\n29", "start_char_idx": 0, "end_char_idx": 1726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f5e46b8-6ee8-4d40-8251-558523b170c5": {"__data__": {"id_": "9f5e46b8-6ee8-4d40-8251-558523b170c5", "embedding": null, "metadata": {"page_label": "1", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "338628df-1354-44fe-bece-70aad2d91871", "node_type": "4", "metadata": {"page_label": "1", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5e8e31912ca1578c1d05e63a84ccc487ee805834c8e16ba2f1654f2aaa0c3e9b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMETRA: S CALABLE UNSUPERVISED RL\nWITH METRIC -AWARE ABSTRACTION\nSeohong Park1Oleh Rybkin1Sergey Levine1\n1University of California, Berkeley\nseohong@berkeley.edu\nABSTRACT\nUnsupervised pre-training strategies have proven to be highly effective in natural\nlanguage processing and computer vision. Likewise, unsupervised reinforcement\nlearning (RL) holds the promise of discovering a variety of potentially useful be-\nhaviors that can accelerate the learning of a wide array of downstream tasks. Pre-\nvious unsupervised RL approaches have mainly focused on pure exploration and\nmutual information skill learning. However, despite the previous attempts, mak-\ning unsupervised RL truly scalable still remains a major open challenge: pure\nexploration approaches might struggle in complex environments with large state\nspaces, where covering every possible transition is infeasible, and mutual infor-\nmation skill learning approaches might completely fail to explore the environ-\nment due to the lack of incentives. To make unsupervised RL scalable to com-\nplex, high-dimensional environments, we propose a novel unsupervised RL ob-\njective, which we call Metric-Aware Abstraction (METRA ). Our main idea is,\ninstead of directly covering the entire state space, to only cover a compact latent\nspaceZthat is metrically connected to the state space Sby temporal distances.\nBy learning to move in every direction in the latent space, METRA obtains a\ntractable set of diverse behaviors that approximately cover the state space, being\nscalable to high-dimensional environments. Through our experiments in five lo-\ncomotion and manipulation environments, we demonstrate that METRA can dis-\ncover a variety of useful behaviors even in complex, pixel-based environments,\nbeing the first unsupervised RL method that discovers diverse locomotion behav-\niors in pixel-based Quadruped and Humanoid. Our code and videos are available\nathttps://seohong.me/projects/metra/\n1 I NTRODUCTION\nUnsupervised pre-training has proven transformative in domains from natural language processing\nto computer vision: contrastive representation learning (Chen et al., 2020) can acquire effective\nfeatures from unlabeled images, and generative autoregressive pre-training (Brown et al., 2020)\ncan enable language models that can be adapted to a plethora of downstream applications. If we\ncould derive an equally scalable framework for unsupervised reinforcement learning (RL) that au-\ntonomously explores the space of possible behaviors, then we could enable general-purpose unsu-\npervised pre-trained agents to serve as an effective foundation for efficiently learning a broad range\nof downstream tasks. Hence, our goal in this work is to propose a scalable unsupervised RL ob-\njective that encourages an agent to explore its environment and learn a breadth of potentially useful\nbehaviors without any supervision.\nWhile this formulation of unsupervised RL has been explored in a number of prior works, making\nfully unsupervised RL truly scalable still remains a major open challenge. Prior approaches to un-\nsupervised RL can be categorized into two main groups: pure exploration methods (Burda et al.,\n2019; Pathak et al., 2019; Liu & Abbeel, 2021b; Mendonca et al., 2021; Rajeswar et al., 2023)\nand unsupervised skill discovery methods (Eysenbach et al., 2019a; Sharma et al., 2020; Laskin\net al., 2022; Park et al., 2022). While these approaches have been shown to be effective in several\nunsupervised RL benchmarks (Mendonca et al., 2021; Laskin et al., 2021), it is not entirely clear\nwhether such methods can indeed be scalable to complex environments with high intrinsic dimen-\nsionality. Pure exploration-based unsupervised RL approaches aim to either completely cover the\nentire state space (Burda et al., 2019; Liu & Abbeel, 2021b) or fully capture the transition dynamics\nof the Markov decision process (MDP) (Pathak et al., 2019; Sekar et al., 2020; Mendonca et al.,\n2021; Rajeswar et al., 2023). However, in complex environments with a large state space, it may be\n1", "start_char_idx": 0, "end_char_idx": 4089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ecbd0a1-1be3-47ee-abbe-c451c638cb3a": {"__data__": {"id_": "8ecbd0a1-1be3-47ee-abbe-c451c638cb3a", "embedding": null, "metadata": {"page_label": "2", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34989f6d-d8f1-4dae-8666-a07c3565abae", "node_type": "4", "metadata": {"page_label": "2", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8a7ab2bb2cd8e4f96e10cd9ed41873ff7d3240d598e69a55f5ed42607ef74fe0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nS=R64\u00d764\u00d73(Pixels)\nTemporally closeZ=R2\nMETRA covers the most \u201ctemporally spread-out\u201d manifoldTemporally distant\nFigure 1: Illustration of METRA. Our main idea for scalable unsupervised RL is to cover only the most\n\u201cimportant\u201d low-dimensional subset of the state space, analogously to PCA. Specifically, METRA covers the\nmost \u201ctemporally spread-out\u201d (non-linear) manifold, which would lead to approximate coverage of the state\nspaceS. In the example above, the two-dimensional Zspace captures behaviors running in all directions, not\nnecessarily covering every possible leg pose.\ninfeasible to attain either of these aims. In fact, we will show that these methods fail to cover the\nstate space even in the state-based 29-dimensional MuJoCo Ant environment. On the other hand,\nunsupervised skill discovery methods aim to discover diverse, distinguishable behaviors, e.g., by\nmaximizing the mutual information between skills and states (Gregor et al., 2016; Eysenbach et al.,\n2019a). While these methods do learn behaviors that are mutually different, they either do not nec-\nessarily encourage exploration and thus often have limited state coverage in the complete absence of\nsupervision (Eysenbach et al., 2019a; Sharma et al., 2020), or are not directly scalable to pixel-based\ncontrol environments (Park et al., 2022; 2023b).\nIn this work, we aim to address these challenges and develop an unsupervised RL objective, which\nwe call Metric-Aware Abstraction (METRA ), that scales to complex, image-based environments\nwith high intrinsic dimensionality. Our first main idea is to learn diverse behaviors that maximally\ncover notthe original state space but a compact latent metric space defined by a mapping function\n\u03d5:S \u2192 Z with a metric d. Here, the latent state is connected by the state space by the metric\nd, which ensures that covering latent space leads to coverage of the state space. Now, the question\nbecomes which metric to use. Previous metric-based skill learning methods mostly used the Eu-\nclidean distance (or its scaled variant) between two states (He et al., 2022; Park et al., 2022; 2023b).\nHowever, such state-based metrics are not directly applicable to complex, high-dimensional state\nspace ( e.g., images). Our second main idea is therefore to use temporal distances (i.e., the num-\nber of minimum environment steps between two states) as a metric for the latent space. Temporal\ndistances are invariant to state representations and thus applicable to pixel-based environments as\nwell. As a result, by maximizing coverage in the compact latent space, we can acquire diverse be-\nhaviors that approximately cover the entire state space, being scalable to high-dimensional, complex\nenvironments (Figure 1).\nThrough our experiments on five state-based and pixel-based continuous control environments, we\ndemonstrate that our method learns diverse, useful behaviors, as well as a compact latent space\nthat can be used to solve various downstream tasks in a zero-shot manner, outperforming previous\nunsupervised RL methods. To the best of our knowledge, METRA is the first unsupervised RL\nmethod that demonstrates the discovery of diverse locomotion behaviors in pixel-based Quadruped\nand Humanoid environments.\n2 W HYMIGHT PREVIOUS UNSUPERVISED RL M ETHODS FAILTOSCALE ?\nThe goal of unsupervised RL is to acquire useful knowledge, such as policies, world models, or\nexploratory data, by interacting with the environment in an unsupervised manner ( i.e., without tasks\nor reward functions). Typically, this knowledge is then leveraged to solve downstream tasks more\nefficiently. Prior work in unsupervised RL can be categorized into two main groups: pure explo-\nration methods and unsupervised skill discovery methods. Pure exploration methods aim to cover\nthe entire state space or fully capture the environment dynamics. They encourage exploration by\nmaximizing uncertainty (Pathak et al., 2017; Shyam et al., 2019; Burda et al., 2019; Pathak et al.,\n2019; Sekar et al., 2020; Mazzaglia et al., 2022) or state entropy (Lee et al., 2019; Pong et al.,\n2020; Liu & Abbeel, 2021b; Yarats et al., 2021). Based on the data collected by the exploration\n2", "start_char_idx": 0, "end_char_idx": 4209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3998a18-6e3e-4b62-a639-a87e97294ad3": {"__data__": {"id_": "c3998a18-6e3e-4b62-a639-a87e97294ad3", "embedding": null, "metadata": {"page_label": "3", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecda3c23-1fe5-4285-a605-24743c348ae0", "node_type": "4", "metadata": {"page_label": "3", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "29edc7a1597d0a95c0b438ad94d35ba5e1577c23f4be42fddae5f4a00f3c25e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22d92bfe-3715-4df7-9989-1338642062cb", "node_type": "1", "metadata": {}, "hash": "2270237bd3c4a09fe669c821620c4fb7bc7bcebf2ba93025991f6c949778c07f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nPure ExplorationRND,Disag.,APT,\u2026\nDIAYN, DADS, \u2026METRA(ours)I(S;Z)IW(S;Z)MI vs. WDMI(S;Z)IW(S;Z)=<\nFigure 2: Sketch comparing different unsupervised RL objectives. Pure exploration approaches try to\ncover every possible state, which is infeasible in complex environments ( e.g., such methods might be \u201cstuck\u201d\nat forever finding novel joint angle configurations of a robot, without fully exploring the environment; see\nFigure 3). The mutual information I(S;Z)has no underlying distance metrics, and thus does not prioritize\ncoverage enough, only focusing on skills that are discriminable. In contrast, our proposed Wasserstein de-\npendency measure IW(S;Z)maximizes the distance metric d, which we choose to be the temporal distance,\nforcing the learned skills to span the \u201clongest\u201d subspaces of the state space, analogously to (temporal, non-\nlinear) PCA.\npolicy, these methods learn a world model (Rajeswar et al., 2023), train a goal-conditioned pol-\nicy (Pong et al., 2020; Pitis et al., 2020; Mendonca et al., 2021; Hu et al., 2023), learn skills via\ntrajectory autoencoders (Campos Cam \u00b4u\u02dcnez et al., 2020; Mazzaglia et al., 2023), or directly fine-\ntune the learned exploration policy (Laskin et al., 2021) to accelerate downstream task learning.\nWhile these pure exploration-based approaches are currently the leading methods in unsupervised\nRL benchmarks (Mendonca et al., 2021; Laskin et al., 2021; Mazzaglia et al., 2023; Rajeswar et al.,\n2023), their scalability may be limited in complex environments with large state spaces because it\nis often computationally infeasible to completely cover every possible state or fully capture the dy-\nnamics. In Section 5, we empirically demonstrate that these approaches even fail to cover the state\nspace of the state-based 29-dimensional Ant environment.\nAnother line of research in unsupervised RL aims to learn diverse behaviors (or skills ) that are\ndistinguishable from one another, and our method also falls into this category. The most common\napproach to unsupervised skill discovery is to maximize the mutual information (MI) between states\nand skills (Gregor et al., 2016; Eysenbach et al., 2019a; Sharma et al., 2020; Hansen et al., 2020):\nI(S;Z) =DKL(p(s, z)\u2225p(s)p(z)). (1)\nBy associating different skill latent vectors zwith different states s, these methods learn diverse\nskills that are mutually distinct. However, they share the limitation that they often end up discover-\ning simple, static behaviors with limited state coverage (Campos Cam \u00b4u\u02dcnez et al., 2020; Park et al.,\n2022). This is because MI is defined by a KL divergence (Equation (1)), which is a metric-agnostic\nquantity ( e.g., MI is invariant to scaling; see Figure 2). As a result, the MI objective only focuses\non the distinguishability of behaviors, regardless of \u201chow different\u201d they are, resulting in limited\nstate coverage (Campos Cam \u00b4u\u02dcnez et al., 2020; Park et al., 2022). To address this limitation, prior\nworks combine the MI objective with exploration bonuses (Campos Cam \u00b4u\u02dcnez et al., 2020; Strouse\net al., 2022; Park & Levine, 2023) or propose different objectives that encourage maximizing dis-\ntances in the state space (He et al., 2022; Park et al., 2022; 2023b). Yet, it remains unclear whether\nthese methods can scale to complex, high-dimensional environments, because they either attempt\nto completely capture the entire MDP (Campos Cam \u00b4u\u02dcnez et al., 2020; Strouse et al., 2022; Park &\nLevine, 2023) or assume a compact, structured state space (He et al., 2022; Park et al., 2022; 2023b).", "start_char_idx": 0, "end_char_idx": 3583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22d92bfe-3715-4df7-9989-1338642062cb": {"__data__": {"id_": "22d92bfe-3715-4df7-9989-1338642062cb", "embedding": null, "metadata": {"page_label": "3", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecda3c23-1fe5-4285-a605-24743c348ae0", "node_type": "4", "metadata": {"page_label": "3", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "29edc7a1597d0a95c0b438ad94d35ba5e1577c23f4be42fddae5f4a00f3c25e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3998a18-6e3e-4b62-a639-a87e97294ad3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "010babd0807ed89bba17aacb09167f758b4ec3f9f41d20a7a87fb91d62e8b661", "class_name": "RelatedNodeInfo"}}, "text": "To address this limitation, prior\nworks combine the MI objective with exploration bonuses (Campos Cam \u00b4u\u02dcnez et al., 2020; Strouse\net al., 2022; Park & Levine, 2023) or propose different objectives that encourage maximizing dis-\ntances in the state space (He et al., 2022; Park et al., 2022; 2023b). Yet, it remains unclear whether\nthese methods can scale to complex, high-dimensional environments, because they either attempt\nto completely capture the entire MDP (Campos Cam \u00b4u\u02dcnez et al., 2020; Strouse et al., 2022; Park &\nLevine, 2023) or assume a compact, structured state space (He et al., 2022; Park et al., 2022; 2023b).\nIndeed, to the best of our knowledge, noprevious unsupervised skill discovery methods have suc-\nceeded in discovering locomotion behaviors on pixel-based locomotion environments. Unlike these\napproaches, our method learns a compact set of diverse behaviors that are maximally different in\nterms of the temporal distance . As a result, they can approximately cover the state space, even in a\ncomplex, high-dimensional environment. We discuss further related work in Appendix B.\n3 P RELIMINARIES AND PROBLEM SETTING\nWe consider a controlled Markov process, an MDP without a reward function, defined as M=\n(S,A, \u00b5, p).Sdenotes the state space, Adenotes the action space, \u00b5: \u2206(S)denotes the initial\nstate distribution, and p:S \u00d7A \u2192 \u2206(A)denotes the transition dynamics kernel. We consider a set\nof latent vectors z\u2208 Z, which can be either discrete or continuous, and a latent-conditioned policy\n\u03c0(a|s, z). Following the terminology in unsupervised skill discovery, we refer to latent vectors z\n(and their corresponding policies \u03c0(a|s, z)) asskills . When sampling a trajectory, we first sample\na skill from the prior distribution, z\u223cp(z), and then roll out a trajectory with \u03c0(a|s, z), where\nzis fixed for the entire episode. Hence, the joint skill-trajectory distribution is given as p(\u03c4, z) =\n3", "start_char_idx": 2955, "end_char_idx": 4875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76f8a797-6448-4fcf-a515-d2ed0423cffd": {"__data__": {"id_": "76f8a797-6448-4fcf-a515-d2ed0423cffd", "embedding": null, "metadata": {"page_label": "4", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0dad4af6-36f9-4ec4-9181-2623b16be04a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aba108a498d9515a5f4e1939ca304f85911fb42b5f69afbc70dcf4ba933ccf10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37e7e5eb-edbe-4884-9474-6130e84e1892", "node_type": "1", "metadata": {}, "hash": "8caaa2c5c3dae33eff9fbc09a2b4119d400c9a92eb823d8df4ff0dde1bb0eb79", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\np(z)p(s0)QT\u22121\nt=0\u03c0(at|st, z)p(st+1|st, at), where \u03c4denotes (s0, a0, s1, a1, . . . , s T). Our goal in this\nwork is to learn a set of diverse, useful behaviors \u03c0(a|s, z), without using any supervision, data, or\nprior knowledge.\n4 A S CALABLE OBJECTIVE FOR UNSUPERVISED RL\nDesiderata. We first state our two desiderata for a scalable unsupervised RL objective. First, instead\nof covering every possible state in a given MDP, which is infeasible in complex environments, we\nwant to have a compact latent space Zof atractable size and a latent-conditioned policy \u03c0(a|s, z)\nthat translates latent vectors into actual behaviors. Second, we want the behaviors from different\nlatent vectors to be different, collectively covering as much of the state space as possible. In other\nwords, we want to maximize state coverage under the given capacity ofZ. An algorithm that\nsatisfies these two desiderata would be scalable to complex environments, because we only need to\nlearn a compact set of behaviors that approximately cover the MDP.\nObjective. Based on the above, we propose the following novel objective for unsupervised RL:\nIW(S;Z) =W(p(s, z), p(s)p(z)), (2)\nwhere IW(S;Z)is the Wasserstein dependency measure (WDM) (Ozair et al., 2019) between states\nand skills, and Wis the 1-Wasserstein distance on the metric space (S \u00d7 Z , d)with a distance\nmetric d. Intuitively, the WDM objective in Equation (2) can be viewed as a \u201cWasserstein variant\u201d\nof the previous MI objective (Equation (1)), where the KL divergence in MI is replaced with the\nWasserstein distance. However, despite the apparent similarity, there exists a significant difference\nbetween the two objectives: MI is completely agnostic to the underlying distance metric, while\nWDM is a metric-aware quantity. As a result, the WDM objective (Equation (2)) not only discovers\ndiverse skills that are different from one another, as in the MI objective, but also actively maximizes\ndistances dbetween different skill trajectories (Figure 2). This makes them collectively cover the\nstate space as much as possible (in terms of the given metric d). The choice of metric for dis\ncritical for effective skill discovery, and simple choices like Euclidean metrics on the state space\nwould generally notbe effective for non-metric state representations, such as images. Therefore,\ninstantiating this approach with the right metric is an important part of our contribution, as we will\ndiscuss in Section 4.2. Until then, we assume that we have a given metric d.\n4.1 T RACTABLE OPTIMIZATION\nWhile our objective IW(S;Z)has several desirable properties, it is not immediately straightforward\nto maximize this quantity in practice. In this section, we describe a simple, tractable objective\nthat can be used to maximize IW(S;Z)in practice. We begin with the Kantorovich-Rubenstein\nduality (Villani et al., 2009; Ozair et al., 2019), which provides a tractable way to maximize the\nWasserstein dependency measure:\nIW(S;Z) = sup\n\u2225f\u2225L\u22641Ep(s,z)[f(s, z)]\u2212Ep(s)p(z)[f(s, z)], (3)\nwhere \u2225f\u2225Ldenotes the Lipschitz constant for the function f:S \u00d7Z \u2192 Runder the given distance\nmetric d,i.e.,\u2225f\u2225L= sup(s1,z1)\u0338=(s2,z2)|f(s1, z1)\u2212f(s2, z2)|/d((s1, z1),(s2, z2)). Intuitively, f\nis a score function that assigns larger values to (s, z)tuples sampled from the joint distribution and\nsmaller values to (s, z)tuples sampled independently from their marginal distributions.", "start_char_idx": 0, "end_char_idx": 3438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37e7e5eb-edbe-4884-9474-6130e84e1892": {"__data__": {"id_": "37e7e5eb-edbe-4884-9474-6130e84e1892", "embedding": null, "metadata": {"page_label": "4", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0dad4af6-36f9-4ec4-9181-2623b16be04a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aba108a498d9515a5f4e1939ca304f85911fb42b5f69afbc70dcf4ba933ccf10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76f8a797-6448-4fcf-a515-d2ed0423cffd", "node_type": "1", "metadata": {"page_label": "4", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f98c2e744690412248756b94e52ce1e2cf0bfc83f4bc42e689c3654affd73ef8", "class_name": "RelatedNodeInfo"}}, "text": "Intuitively, f\nis a score function that assigns larger values to (s, z)tuples sampled from the joint distribution and\nsmaller values to (s, z)tuples sampled independently from their marginal distributions. We note\nthat Equation (3) is already a tractable objective, as we can jointly train a 1-Lipschitz-constrained\nscore function f(s, z)using gradient descent and a skill policy \u03c0(a|s, z)using RL, with the reward\nfunction being an empirical estimate of Equation (3), r(s, z) =f(s, z)\u2212N\u22121PN\ni=1f(s, zi), where\nz1, z2, . . . , z NareNindependent random samples from the prior distribution p(z).\nHowever, since sampling Nadditional zs for each data point is computationally demanding, we will\nfurther simplify the objective to enable more efficient learning. First, we consider the parameteri-\nzation f(s, z) =\u03d5(s)\u22a4\u03c8(z)with\u03d5:S \u2192RDand\u03c8:Z \u2192 RDwith independent 1-Lipschitz\nconstraints1, which yields the following objective:\nIW(S;Z)\u2248 sup\n\u2225\u03d5\u2225L\u22641,\u2225\u03c8\u2225L\u22641Ep(s,z)[\u03d5(s)\u22a4\u03c8(z)]\u2212Ep(s)[\u03d5(s)]\u22a4Ep(z)[\u03c8(z)]. (4)\n1While \u2225\u03d5\u2225L\u22641,\u2225\u03c8\u2225L\u22641is not technically equivalent to \u2225f\u2225L\u22641, we use the former as it is\nmore tractable. Also, we note that \u2225f\u2225Lcan be upper-bounded in terms of \u2225\u03d5\u2225L,\u2225\u03c8\u2225L,sups\u2225\u03d5(s)\u22252, and\nsupz\u2225\u03c8(z)\u22252under d((s1, z1),(s2, z2)) = (sups\u2225\u03d5(s)\u22252)\u2225\u03c8\u2225Ld(z1, z2) + (supz\u2225\u03c8(z)\u22252)\u2225\u03d5\u2225Ld(s1, s2).\n4", "start_char_idx": 3233, "end_char_idx": 4513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebd8058c-dd91-4a2f-8310-9d6846805619": {"__data__": {"id_": "ebd8058c-dd91-4a2f-8310-9d6846805619", "embedding": null, "metadata": {"page_label": "5", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcbd7fe7-5488-40ae-8c34-56cce0cdef21", "node_type": "4", "metadata": {"page_label": "5", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3bcf282fa131fb572a25ca7210e24424d0648b901dd1d1e20d697dd1883149d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "305d8671-e7d0-4663-86c1-b80ea387e109", "node_type": "1", "metadata": {}, "hash": "bf35c2a74ab36ab73e95a7f5037d25f3e498923e3434e713f7a178b9ff1db93e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nHere, we note that the decomposition f(s, z) =\u03d5(s)\u22a4\u03c8(z)isuniversal ;i.e., the expressiveness of\nf(s, z)is equivalent to that of \u03d5(s)\u22a4\u03c8(z)when D\u2192 \u221e . The proof can be found in Appendix C.\nNext, we consider a variant of the Wasserstein dependency measure that only depends on the last\nstate: IW(ST;Z), similarly to VIC (Gregor et al., 2016). This allows us to further decompose the\nobjective with a telescoping sum as follows:\nIW(ST;Z)\u2248 sup\n\u2225\u03d5\u2225L\u22641,\u2225\u03c8\u2225L\u22641Ep(\u03c4,z)[\u03d5(sT)\u22a4\u03c8(z)]\u2212Ep(\u03c4)[\u03d5(sT)]\u22a4Ep(z)[\u03c8(z)] (5)\n= sup\n\u03d5,\u03c8T\u22121X\nt=0\u0000\nEp(\u03c4,z)[(\u03d5(st+1)\u2212\u03d5(st))\u22a4\u03c8(z)]\u2212Ep(\u03c4)[\u03d5(st+1)\u2212\u03d5(st)]\u22a4Ep(z)[\u03c8(z)]\u0001\n,(6)\nwhere we also use the fact that p(s0)andp(z)are independent. Finally, we set \u03c8(z)toz. While this\nmakes \u03c8less expressive, it allows us to derive the following concise objective:\nIW(ST;Z)\u2248sup\n\u2225\u03d5\u2225L\u22641Ep(\u03c4,z)\"T\u22121X\nt=0(\u03d5(st+1)\u2212\u03d5(st))\u22a4(z\u2212\u00afz)#\n, (7)\nwhere \u00afz=Ep(z)[z]. Here, since we can always shift the prior distribution p(z)to have a zero\nmean, we can assume \u00afz= 0without loss of generality. This objective can now be easily maximized\nby jointly training \u03d5(s)and\u03c0(a|s, z)withr(s, z, s\u2032) = ( \u03d5(s\u2032)\u2212\u03d5(s))\u22a4zunder the constraint\n\u2225\u03d5\u2225L\u22641. Note that we do not need any additional random samples of z, unlike Equation (3).\n4.2 F ULL OBJECTIVE : M ETRIC -AWARE ABSTRACTION (METRA)\nSo far, we have not specified the distance metric dfor the Wasserstein distance in WDM (or equiv-\nalently for the Lipschitz constraint \u2225\u03d5\u2225L\u22641). Choosing an appropriate distance metric is crucial\nfor learning a compact set of useful behaviors, because it determines the priority by which the be-\nhaviors are learned within the capacity of Z. Previous metric-based skill discovery methods mostly\nemployed the Euclidean distance (or its scaled variant) as a metric (He et al., 2022; Park et al., 2022;\n2023b). However, they are not directly scalable to high-dimensional environments with pixel-based\nobservations, in which the Euclidean distance is not necessarily meaningful.\nIn this work, we propose to use the temporal distance (Kaelbling, 1993; Hartikainen et al., 2020; Du-\nrugkar et al., 2021) between two states as a distance metric dtemp(s1, s2), the minimum number of\nenvironment steps to reach s2froms1. This provides a natural way to measure the distance between\ntwo states, as it only depends on the inherent transition dynamics of the MDP, being invariant to\nthe state representation and thus scalable to pixel-based environments. Using the temporal distance\nmetric, we can rewrite Equation (7) as follows:\nsup\n\u03c0,\u03d5Ep(\u03c4,z)\"T\u22121X\nt=0(\u03d5(st+1)\u2212\u03d5(st))\u22a4z#\ns.t.\u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\u22641,\u2200(s, s\u2032)\u2208 Sadj,(8)\nwhere Sadjdenotes the set of adjacent state pairs in the MDP. Note that \u2225\u03d5\u2225L\u22641is equivalently\nconverted into \u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\u22641under the temporal distance metric (see Theorem C.3).\nIntuition and interpretation.", "start_char_idx": 0, "end_char_idx": 2807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "305d8671-e7d0-4663-86c1-b80ea387e109": {"__data__": {"id_": "305d8671-e7d0-4663-86c1-b80ea387e109", "embedding": null, "metadata": {"page_label": "5", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcbd7fe7-5488-40ae-8c34-56cce0cdef21", "node_type": "4", "metadata": {"page_label": "5", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3bcf282fa131fb572a25ca7210e24424d0648b901dd1d1e20d697dd1883149d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebd8058c-dd91-4a2f-8310-9d6846805619", "node_type": "1", "metadata": {"page_label": "5", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5115e0f60ee5585f2d0bcd4606354fdf4dae32e7bb05fd41d6366d0b168bbbc8", "class_name": "RelatedNodeInfo"}}, "text": "Using the temporal distance\nmetric, we can rewrite Equation (7) as follows:\nsup\n\u03c0,\u03d5Ep(\u03c4,z)\"T\u22121X\nt=0(\u03d5(st+1)\u2212\u03d5(st))\u22a4z#\ns.t.\u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\u22641,\u2200(s, s\u2032)\u2208 Sadj,(8)\nwhere Sadjdenotes the set of adjacent state pairs in the MDP. Note that \u2225\u03d5\u2225L\u22641is equivalently\nconverted into \u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\u22641under the temporal distance metric (see Theorem C.3).\nIntuition and interpretation. We next describe how the constrained objective in Equation (8) may\nbe interpreted. Intuitively, a policy \u03c0(a|s, z)that maximizes our objective should learn to move as\nfar as possible along various directions in the latent space (specified by z). Since distances in the\nlatent space, \u2225\u03d5(s1)\u2212\u03d5(s2)\u22252, are always upper-bounded by the corresponding temporal distances\nin the MDP, given by dtemp(s1, s2), the learned latent space should assign its (limited) dimensions\nto the manifolds in the original state space that are maximally \u201cspread out\u201d, in the sense that shortest\npaths within the set of represented states should be as long as possible. This conceptually resembles\n\u201cprincipal components\u201d of the state space, but with respect to shortest paths rather than Euclidean\ndistances, and with non-linear \u03d5rather than linear \u03d5. Thus, we would expect \u03d5to learn to abstract\nthe state space in a lossy manner, preserving temporal distances (Figure 9), and emphasizing those\ndegrees of freedom of the state that span the largest possible \u201ctemporal\u201d (non-linear) manifolds\n(Figure 1). Based on this intuition, we call our method Metric-Aware Abstraction (METRA ). In\nAppendix D, we derive a formal connection between METRA and principal component analysis\n(PCA) under the temporal distance metric under several simplifying assumptions.\nTheorem 4.1 (Informal statement of Theorem D.2) .Under some simplifying assumptions, linear\nsquared METRA is equivalent to PCA under the temporal distance metric.\n5", "start_char_idx": 2443, "end_char_idx": 4300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7278749d-4e5f-40e4-9860-9b76656faa68": {"__data__": {"id_": "7278749d-4e5f-40e4-9860-9b76656faa68", "embedding": null, "metadata": {"page_label": "6", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08f71681-5cea-4347-a5fd-e5e3fb8c0ac6", "node_type": "4", "metadata": {"page_label": "6", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bde030d405c2691f12b7647627d6361d811d16986e0e1babec829e5214e39de9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce5718e8-92da-4b87-a507-aa54f47e632c", "node_type": "1", "metadata": {}, "hash": "8e60c9f8f43b87bfa4562557e4d089fc5e97e9b66d436065b5349ebf629e675b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1 Metric-Aware Abstraction (METRA)\n1: Initialize skill policy \u03c0(a|s, z), representation function \u03d5(s), Lagrange multiplier \u03bb, replay buffer D\n2:fori\u21901to (# epochs) do\n3: forj\u21901to (# episodes per epoch) do\n4: Sample skill z\u223cp(z)\n5: Sample trajectory \u03c4with\u03c0(a|s, z)and add to replay buffer D\n6: end for\n7: Update \u03d5(s)to maximize E(s,z,s\u2032)\u223cD[(\u03d5(s\u2032)\u2212\u03d5(s))\u22a4z+\u03bb\u00b7min(\u03b5,1\u2212 \u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\n2)]\n8: Update \u03bbto minimize E(s,z,s\u2032)\u223cD[\u03bb\u00b7min(\u03b5,1\u2212 \u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\n2)]\n9: Update \u03c0(a|s, z)using SAC (Haarnoja et al., 2018a) with reward r(s, z, s\u2032) = (\u03d5(s\u2032)\u2212\u03d5(s))\u22a4z\n10:end for\nConnections to previous skill discovery methods. There exist several intriguing connections\nbetween our WDM objective (Equation (2)) and previous skill discovery methods, including DI-\nAYN (Eysenbach et al., 2019a), DADS (Sharma et al., 2020), CIC (Laskin et al., 2022), LSD (Park\net al., 2022), and CSD (Park et al., 2023b). Perhaps the most apparent connections are with LSD and\nCSD, which also use similar constrained objectives to Equation (7). In fact, although not shown by\nthe original authors, the constrained inner product objectives of LSD and CSD are also equivalent\ntoIW(ST;Z), but with the Euclidean distance (or its normalized variant), instead of the temporal\ndistance. Also, the connection between WDM and Equation (7) provides further theoretical insight\ninto the rather \u201cad-hoc\u201d choice of zero-centered one-hot vectors used in discrete LSD (Park et al.,\n2022); we must use a zero-mean prior distribution due to the z\u2212\u00afzterm in Equation (7). There\nexist several connections between our WDM objective and previous MI-based skill discovery meth-\nods as well. Specifically, by simplifying WDM (Equation (2)) in three different ways, we can obtain\n\u201cWasserstein variants\u201d of DIAYN, DADS, and CIC. We refer to Appendix E for detailed derivations.\nZero-shot goal-reaching with METRA. Thanks to the state abstraction function \u03d5(s), METRA\nprovides a simple way to command the skill policy to reach a goal state in a zero-shot manner, as\nin LSD (Park et al., 2022). Since \u03d5abstracts the state space preserving temporal distances, the\ndifference vector \u03d5(g)\u2212\u03d5(s)tells us the skill we need to select to reach the goal state gfrom the\ncurrent state s. As such, by simply setting z= (\u03d5(g)\u2212\u03d5(s))/\u2225\u03d5(g)\u2212\u03d5(s)\u22252(for continuous\nskills) or z= arg maxdim(\u03d5(g)\u2212\u03d5(s))(for discrete skills), we can find the skill that leads to the\ngoal. With this technique, METRA can solve goal-conditioned tasks without learning a separate\ngoal-conditioned policy, as we will show in Section 5.3.\nImplementation. We optimize the constrained objective in Equation (8) using dual gradient descent\nwith a Lagrange multiplier \u03bband a small relaxation constant \u03b5 >0, similarly to Park et al. (2023b);\nWang et al. (2023). We provide a pseudocode for METRA in Algorithm 1.\nLimitations. One potential issue with Equation (8) is that we embed the temporal distance into the\nsymmetric Euclidean distance in the latent space, where the temporal distance can be asymmetric.\nThis makes our temporal distance abstraction more \u201cconservative\u201d in the sense that it considers the\nminimum of both temporal distances, i.e.,\u2225\u03d5(s1)\u2212\u03d5(s2)\u22252\u2264min(dtemp(s1, s2), dtemp(s2, s1)).", "start_char_idx": 0, "end_char_idx": 3237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce5718e8-92da-4b87-a507-aa54f47e632c": {"__data__": {"id_": "ce5718e8-92da-4b87-a507-aa54f47e632c", "embedding": null, "metadata": {"page_label": "6", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08f71681-5cea-4347-a5fd-e5e3fb8c0ac6", "node_type": "4", "metadata": {"page_label": "6", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bde030d405c2691f12b7647627d6361d811d16986e0e1babec829e5214e39de9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7278749d-4e5f-40e4-9860-9b76656faa68", "node_type": "1", "metadata": {"page_label": "6", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "295c186eafaf659f2710eb87e20c8cd77ed5ebceab546bfea35e5ae03a95a826", "class_name": "RelatedNodeInfo"}}, "text": "Implementation. We optimize the constrained objective in Equation (8) using dual gradient descent\nwith a Lagrange multiplier \u03bband a small relaxation constant \u03b5 >0, similarly to Park et al. (2023b);\nWang et al. (2023). We provide a pseudocode for METRA in Algorithm 1.\nLimitations. One potential issue with Equation (8) is that we embed the temporal distance into the\nsymmetric Euclidean distance in the latent space, where the temporal distance can be asymmetric.\nThis makes our temporal distance abstraction more \u201cconservative\u201d in the sense that it considers the\nminimum of both temporal distances, i.e.,\u2225\u03d5(s1)\u2212\u03d5(s2)\u22252\u2264min(dtemp(s1, s2), dtemp(s2, s1)).\nWhile this conservatism is less problematic in our benchmark environments, in which transitions are\nmostly \u201csymmetric\u201d, it might be overly restrictive in highly asymmetric environments. To resolve\nthis, we can replace the Euclidean distance \u2225\u03d5(s1)\u2212\u03d5(s2)\u22252in Equation (8) with an asymmetric\nquasimetric , as in Wang et al. (2023). We leave this extension for future work. Another limitation\nis that the simplified WDM objective in Equation (7) only considers behaviors that move linearly\nin the latent space. While this does not necessarily imply that the behaviors are also linear in the\noriginal state space (because \u03d5:S \u2192 Z is a nonlinear mapping), this simplification, which stems\nfrom the fact that we set \u03c8(z) =z, might restrict the diversity of behaviors to some degree. We\nbelieve this can be addressed by using the full WDM objective in Equation (4). Notably, the full\nobjective (Equation (4)) resembles contrastive learning, and we believe combining it with scalable\ncontrastive learning techniques is an exciting future research direction (see Appendix E.3). We refer\nto Appendix A for practical limitations regarding our implementation of METRA.\n5 E XPERIMENTS\nThrough our experiments in benchmark environments, we aim to answer the following questions:\n(1) Can METRA scale to complex, high-dimensional environments, including domains with image\nobservations? (2) Does METRA discover meaningful behaviors in complex environments with no\nsupervision? (3) Are the behaviors discovered by METRA useful for downstream tasks?\n6", "start_char_idx": 2583, "end_char_idx": 4771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69a66ba7-3a0e-494e-88f2-b03dae9bb4ad": {"__data__": {"id_": "69a66ba7-3a0e-494e-88f2-b03dae9bb4ad", "embedding": null, "metadata": {"page_label": "7", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c200a591-8def-4aa6-9f1c-1dbeeedf89a3", "node_type": "4", "metadata": {"page_label": "7", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "12547414760f72dcf9ccf1584d3f822a6dcac025c436859a15bcb1f3ac034130", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "056f7daa-3310-4458-bb84-56ab023cda78", "node_type": "1", "metadata": {}, "hash": "62769e8c9563242ec33a94fa4285ca606a3fed3b8f45c1647e8da6f111f4eae1", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMETRALSDDIAYNDADS2CICICMRNDAPTAPSP2E/DisagAnt(States)\nLBS3\nHalfCheetah(States)\nQuadruped(Pixels)\nHumanoid(Pixels)\nKitchen(Pixels)\nFigure 3: Examples of behaviors learned by 11 unsupervised RL methods. For locomotion environments,\nwe plot the x-y(orx) trajectories sampled from learned policies. For Kitchen, we measure the coincidental\nsuccess rates for six predefined tasks. Different colors represent different skills z. METRA is the only method\nthat discovers diverse locomotion skills in pixel-based Quadruped and Humanoid. We refer to Figure 11 for the\ncomplete qualitative results ( 8seeds) of METRA and our project page for videos.\n5.1 E XPERIMENTAL SETUP\nQuadruped(Pixels)Humanoid(Pixels)\nKitchen(Pixels)\nHalfCheetah(States)\nAnt(States)\nFigure 4: Benchmark environments.We evaluate our method on five robotic lo-\ncomotion and manipulation environments (Fig-\nure 4): state-based Ant and HalfCheetah from\nGym (Todorov et al., 2012; Brockman et al.,\n2016), pixel-based Quadruped and Humanoid\nfrom the DeepMind Control (DMC) Suite (Tassa\net al., 2018), and a pixel-based version of Kitchen\nfrom Gupta et al. (2019); Mendonca et al. (2021). For pixel-based DMC locomotion environments,\nwe use colored floors to allow the agent to infer its location from pixels, similarly to Hafner et al.\n(2022); Park et al. (2023a) (Figure 10). Throughout the experiments, we do notuse any prior knowl-\nedge, data, or supervision ( e.g., observation restriction, early termination, etc.). As such, in pixel-\nbased environments, the agent must learn diverse behaviors solely from 64\u00d764\u00d73camera images.\nWe compare METRA against 11 previous methods in three groups: (1) unsupervised skill discov-\nery, (2) unsupervised exploration, and (3) unsupervised goal-reaching methods. For unsupervised\nskill discovery methods, we compare against two MI-based approaches, DIAYN (Eysenbach et al.,\n2019a) and DADS (Sharma et al., 2020), one hybrid method that combines MI and an exploration\nbonus, CIC (Laskin et al., 2022), and one metric-based approach that maximizes Euclidean dis-\ntances, LSD (Park et al., 2022). For unsupervised exploration methods, we consider five pure explo-\nration approaches, ICM (Pathak et al., 2017), RND (Burda et al., 2019), Plan2Explore (Sekar et al.,\n2020) (or Disagreement (Pathak et al., 2019)), APT (Liu & Abbeel, 2021b), and LBS (Mazzaglia\net al., 2022), and one hybrid approach that combines exploration and successor features, APS (Liu &\nAbbeel, 2021a). We note that the Dreamer (Hafner et al., 2020) variants of these methods (especially\nLBS (Mazzaglia et al., 2022)) are currently the state-of-the-art methods in the pixel-based unsuper-\nvised RL benchmark (Laskin et al., 2021; Rajeswar et al., 2023). For unsupervised goal-reaching\nmethods, we mainly compare with a state-of-the-art unsupervised RL approach, LEXA (Mendonca\net al., 2021), as well as two previous skill discovery methods that enable zero-shot goal-reaching,\nDIAYN and LSD. We use 2-D skills for Ant and Humanoid, 4-D skills for Quadruped, 16discrete\nskills for HalfCheetah, and 24discrete skills for Kitchen. For CIC, we use 64-D skill latent vectors\nfor all environments, following the original suggestion (Laskin et al., 2022).\n5.2 Q UALITATIVE COMPARISON\nWe first demonstrate examples of behaviors (or skills) learned by our method and the 10 prior unsu-\npervised RL methods on each of the five benchmark environments in Figure 3.", "start_char_idx": 0, "end_char_idx": 3462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "056f7daa-3310-4458-bb84-56ab023cda78": {"__data__": {"id_": "056f7daa-3310-4458-bb84-56ab023cda78", "embedding": null, "metadata": {"page_label": "7", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c200a591-8def-4aa6-9f1c-1dbeeedf89a3", "node_type": "4", "metadata": {"page_label": "7", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "12547414760f72dcf9ccf1584d3f822a6dcac025c436859a15bcb1f3ac034130", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69a66ba7-3a0e-494e-88f2-b03dae9bb4ad", "node_type": "1", "metadata": {"page_label": "7", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0caa0214318b0beff8da411a9dde50189522e5f8a3fd026d98f364b17be588d7", "class_name": "RelatedNodeInfo"}}, "text": "For unsupervised goal-reaching\nmethods, we mainly compare with a state-of-the-art unsupervised RL approach, LEXA (Mendonca\net al., 2021), as well as two previous skill discovery methods that enable zero-shot goal-reaching,\nDIAYN and LSD. We use 2-D skills for Ant and Humanoid, 4-D skills for Quadruped, 16discrete\nskills for HalfCheetah, and 24discrete skills for Kitchen. For CIC, we use 64-D skill latent vectors\nfor all environments, following the original suggestion (Laskin et al., 2022).\n5.2 Q UALITATIVE COMPARISON\nWe first demonstrate examples of behaviors (or skills) learned by our method and the 10 prior unsu-\npervised RL methods on each of the five benchmark environments in Figure 3. The figure illustrates\nthat METRA discovers diverse behaviors in both state-based and pixel-based domains. Notably,\nMETRA is the only method that successfully discovers locomotion skills in pixel-based Quadruped\nand Humanoid, and shows qualitatively very different behaviors from previous unsupervised RL\nmethods across the environments. Pure exploration methods mostly exhibit chaotic, random behav-\niors (videos), and fail to fully explore the state space (in terms of x-ycoordinates) even in state-based\n7", "start_char_idx": 2764, "end_char_idx": 3971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06ddd3eb-81d8-4353-8e05-b7b6cbb48088": {"__data__": {"id_": "06ddd3eb-81d8-4353-8e05-b7b6cbb48088", "embedding": null, "metadata": {"page_label": "8", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ec61808-230a-4b6f-9917-87e45308fc81", "node_type": "4", "metadata": {"page_label": "8", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b603ae0879c2c147800a4e909c8a81caea7ceab680c1a6b3d54123ad802a36db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df052885-c49a-4c8b-b09f-d01134df263b", "node_type": "1", "metadata": {}, "hash": "af0857031eb0f8862f4726745f61801a0306b145c9a90db26811f1c0ba97a1e9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n\u00b00.04\u00b00.020.000.020.04x\u00b00.04\u00b00.020.000.020.04yMETRALSDCICDIAYNDADS 0.00.51.01.5Steps\u00a3107050100150200Policy State CoverageHalfCheetah (States)\n0.00.51.01.5Steps\u00a3107010002000Policy State CoverageAnt (States)\n0246Steps\u00a31060100200300Policy State CoverageQuadruped (Pixels)\n0.00.51.0Steps\u00a31070255075100Policy State CoverageHumanoid (Pixels)\n0.00.51.0Steps\u00a310601234Policy Task CoverageKitchen (Pixels)\nFigure 5: Quantitative comparison with unsupervised skill discovery methods ( 8seeds). We measure the\nstate/task coverage of the policies learned by five skill discovery methods. METRA exhibits the best coverage\nacross all environments, while previous methods completely fail to explore the state spaces of pixel-based\nlocomotion environments. Notably, METRA is the only method that discovers locomotion skills in pixel-based\nQuadruped and Humanoid.\n0.00.51.0Episodes\u00a310424ReturnAntMultiGoals\n0.00.51.0Episodes\u00a31050.02.55.07.5ReturnHalfCheetahGoal\n0.00.51.0Episodes\u00a3105024ReturnHalfCheetahHurdle\n0.00.51.0Episodes\u00a31040246ReturnQuadrupedGoal\n0.00.51.0Episodes\u00a31041234ReturnHumanoidGoal\u00b00.04\u00b00.020.000.020.04x\u00b00.04\u00b00.020.000.020.04yMETRALSDCICDIAYNDADS\nFigure 6: Downstream task performance comparison of unsupervised skill discovery methods ( 4seeds).\nTo verify whether learned skills are useful for downstream tasks, we train a hierarchical high-level controller\non top of the frozen skill policy to maximize task rewards. METRA exhibits the best or near-best performance\nacross the five tasks, which suggests that the behaviors learned by METRA are indeed useful for the tasks.\nAnt and HalfCheetah. This is because it is practically infeasible to completely cover the infinitely\nmany combinations of joint angles and positions in these domains. MI-based skill discovery meth-\nods also fail to explore large portions of the state space due to the metric-agnosticity of the KL\ndivergence (Section 2), even when combined with an exploration bonus ( i.e., CIC). LSD, a previous\nmetric-based skill discovery method that maximizes Euclidean distances, does discover locomotion\nskills in state-based environments, but fails to scale to the pixel-based environments, where the Eu-\nclidean distance on image pixels does not necessarily provide a meaningful metric. In contrast to\nthese methods, METRA learns various task-related behaviors by maximizing temporal distances\nin diverse ways. On our project page, we show additional qualitative results of METRA with dif-\nferent skill spaces. We note that, when combined with a discrete latent space, METRA discovers\neven more diverse behaviors, such as doing a backflip and taking a static posture, in addition to\nlocomotion skills. We refer to Appendix F for visualization of learned latent spaces of METRA.\n5.3 Q UANTITATIVE COMPARISON\nNext, we quantitatively compare METRA against three groups of 11 previous unsupervised RL\napproaches, using different metrics that are tailored to each group\u2019s primary focus. For quantitative\nresults, we use 8seeds and report 95% confidence intervals, unless otherwise stated.\nComparison with unsupervised skill discovery methods. We first compare METRA with other\nmethods that also aim to solve the skill discovery problem (i.e., learning a latent-conditioned policy\n\u03c0(a|s, z)that performs different skills for different values of z). These include LSD, CIC, DIAYN,\nand DADS2. We implement these methods on the same codebase as METRA. For comparison,\nwe employ two metrics: policy coverage and downstream task performance. Figure 5 presents the\npolicy coverage results, where we evaluate the skill policy\u2019s xcoverage (HalfCheetah), x-ycoverage\n(Ant, Quadruped, and Humanoid), or task (Kitchen) coverage at each evaluation epoch.", "start_char_idx": 0, "end_char_idx": 3747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df052885-c49a-4c8b-b09f-d01134df263b": {"__data__": {"id_": "df052885-c49a-4c8b-b09f-d01134df263b", "embedding": null, "metadata": {"page_label": "8", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ec61808-230a-4b6f-9917-87e45308fc81", "node_type": "4", "metadata": {"page_label": "8", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b603ae0879c2c147800a4e909c8a81caea7ceab680c1a6b3d54123ad802a36db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06ddd3eb-81d8-4353-8e05-b7b6cbb48088", "node_type": "1", "metadata": {"page_label": "8", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ae5bb07b94d9dace877f9a4106aee508eaad0fda66e38e811921b3d001280a6e", "class_name": "RelatedNodeInfo"}}, "text": "For quantitative\nresults, we use 8seeds and report 95% confidence intervals, unless otherwise stated.\nComparison with unsupervised skill discovery methods. We first compare METRA with other\nmethods that also aim to solve the skill discovery problem (i.e., learning a latent-conditioned policy\n\u03c0(a|s, z)that performs different skills for different values of z). These include LSD, CIC, DIAYN,\nand DADS2. We implement these methods on the same codebase as METRA. For comparison,\nwe employ two metrics: policy coverage and downstream task performance. Figure 5 presents the\npolicy coverage results, where we evaluate the skill policy\u2019s xcoverage (HalfCheetah), x-ycoverage\n(Ant, Quadruped, and Humanoid), or task (Kitchen) coverage at each evaluation epoch. The results\nshow that METRA achieves the best performance in most of the domains, and is the only method\nthat successfully learns meaningful skills in the pixel-based settings, where previous skill discovery\nmethods generally fail. In Figure 6, we evaluate the applicability of the skills discovered by each\nmethod to downstream tasks, where the downstream task is learned by a hierarchical controller\n\u03c0h(z|s)that selects (frozen) learned skills to maximize the task reward (see Appendix G for details).\nMETRA again achieves the best performance on most of these tasks, suggesting that the behaviors\nlearned by METRA not only provide greater coverage, but also are more suitable for downstream\ntasks in these domains.\nComparison with pure exploration methods. Next, we quantitatively compare METRA to five\nunsupervised exploration methods, which do not aim to learn skills but only attempt to cover the\nstate space, ICM, LBS3, RND, APT, and Plan2Explore (or Disagreement), and one hybrid method\n2We do not compare against DADS in pixel-based environments due to the computational cost of its skill\ndynamics model p(s\u2032|s, z), which requires predicting the full next image.\n3Since LBS requires a world model, we only evaluate it on pixel-based environments, where we use the\nDreamer variants of pure exploration methods (Rajeswar et al., 2023).\n8", "start_char_idx": 2993, "end_char_idx": 5092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f53251b1-4d89-4bdf-bf3c-2bb3f0c5e7ce": {"__data__": {"id_": "f53251b1-4d89-4bdf-bf3c-2bb3f0c5e7ce", "embedding": null, "metadata": {"page_label": "9", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40e43c23-3dfe-4f9f-b36b-673633c9ad2f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "95cf72f8acee05d93ba452b568f550d02d0a78316ecebe329e19fa564d41da68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6738e4ff-3ef2-47ac-89d7-fe274ef3c042", "node_type": "1", "metadata": {}, "hash": "a57c2f2685cb6f8d4c77654a5aea7eb2091ce529b337ded2aafbe0041d52aa6b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n02Time (s)\u00a3104050100150200Total State CoverageHalfCheetah (States)\n0123Time (s)\u00a31040500010000Total State CoverageAnt (States)\n024Time (s)\u00a31040200400600Total State CoverageQuadruped (Pixels)\n0.02.55.07.5Time (s)\u00a3104050100150Total State CoverageHumanoid (Pixels)\n0123Time (s)\u00a31040246Queue Task CoverageKitchen (Pixels)\n0123Time (s)\u00a310401234Policy Task CoverageKitchen (Pixels)\u00b00.04\u00b00.020.000.020.04x\u00b00.04\u00b00.020.000.020.04yMETRALSDICMLBSRNDAPTAPSP2E/Disag\nFigure 7: Quantitative comparison with pure exploration methods ( 8seeds). We compare METRA with\nsix unsupervised exploration methods in terms of state coverage. Since it is practically infeasible to completely\ncover every possible state or transition, pure exploration methods struggle to explore the state space of complex\nenvironments, such as pixel-based Humanoid or state-based Ant.\n\u00b00.04\u00b00.020.000.020.04x\u00b00.04\u00b00.020.000.020.04yMETRALSDLEXADIAYN 0123Time (s)\u00a3104\u00b050\u00b040\u00b030\u00b020\u00b010Negative Goal DistanceHalfCheetah (States)\n02Time (s)\u00a3104\u00b040\u00b030\u00b020\u00b010Negative Goal DistanceAnt (States)\n024Time (s)\u00a3104\u00b012\u00b010\u00b08\u00b06Negative Goal DistanceQuadruped (Pixels)\n0.02.55.07.5Time (s)\u00a3104\u00b08\u00b07\u00b06\u00b05Negative Goal DistanceHumanoid (Pixels)\n024Time (s)\u00a310401234# Achieved TasksKitchen (Pixels)\nFigure 8: Downstream task performance comparison with LEXA ( 8seeds). We compare METRA against\nLEXA, a state-of-the-art unsupervised goal-reaching method, on five goal-conditioned tasks. The skills learned\nby METRA can be employed to solve these tasks in a zero-shot manner, achieving the best performance.\nthat combines exploration and successor features, APS. We use the original implementations by\nLaskin et al. (2021) for state-based environments and the Dreamer versions by Rajeswar et al. (2023)\nfor pixel-based environments. As the underlying RL backbones are very different ( e.g., Dreamer is\nmodel-based, while METRA uses model-free SAC), we compare the methods based on wall clock\ntime. For the metric, instead of policy coverage (as in Figure 5), we measure total state coverage\n(i.e., the number of bins covered by any training trajectories up to each evaluation epoch). This\nmetric is more generous toward the exploration methods, since such methods might not cover the\nentire space on any single iteration, but rather visit different parts of the space on different iterations\n(in contrast to our method, which aims to produce diverse skills). In Kitchen, we found that most\nmethods max out the total task coverage metric, and we instead use both the queue coverage and\npolicy coverage metrics (see Appendix G for details). Figure 7 presents the results, showing that\nMETRA achieves the best coverage in most of the environments. While pure exploration methods\nalso work decently in the pixel-based Kitchen, they fail to fully explore the state spaces of state-\nbased Ant and pixel-based Humanoid, which have complex dynamics with nearly infinite possible\ncombinations of positions, joint angles, and velocities.\nComparison with unsupervised goal-reaching methods. Finally, we compare METRA with\nLEXA, a state-of-the-art unsupervised goal-reaching method. LEXA trains an exploration policy\nwith Plan2Explore (Sekar et al., 2020), which maximizes epistemic uncertainty in the transition\ndynamics model, in parallel with a goal-conditioned policy \u03c0(a|s, g)on the data collected by the\nexploration policy. We compare the performances of METRA, LEXA, and two previous skill dis-\ncovery methods (DIAYN and LSD) on five goal-reaching downstream tasks. We use the procedure\ndescribed in Section 4.2 to solve goal-conditioned tasks in a zero-shot manner with METRA. Fig-\nure 8 presents the comparison results, where METRA achieves the best performance on all of the\nfive downstream tasks.", "start_char_idx": 0, "end_char_idx": 3770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6738e4ff-3ef2-47ac-89d7-fe274ef3c042": {"__data__": {"id_": "6738e4ff-3ef2-47ac-89d7-fe274ef3c042", "embedding": null, "metadata": {"page_label": "9", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40e43c23-3dfe-4f9f-b36b-673633c9ad2f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "95cf72f8acee05d93ba452b568f550d02d0a78316ecebe329e19fa564d41da68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f53251b1-4d89-4bdf-bf3c-2bb3f0c5e7ce", "node_type": "1", "metadata": {"page_label": "9", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4cfc415ce1ec9d8c601af304de7dd0d3278d8d5da750e081cf91ab71df8af093", "class_name": "RelatedNodeInfo"}}, "text": "Comparison with unsupervised goal-reaching methods. Finally, we compare METRA with\nLEXA, a state-of-the-art unsupervised goal-reaching method. LEXA trains an exploration policy\nwith Plan2Explore (Sekar et al., 2020), which maximizes epistemic uncertainty in the transition\ndynamics model, in parallel with a goal-conditioned policy \u03c0(a|s, g)on the data collected by the\nexploration policy. We compare the performances of METRA, LEXA, and two previous skill dis-\ncovery methods (DIAYN and LSD) on five goal-reaching downstream tasks. We use the procedure\ndescribed in Section 4.2 to solve goal-conditioned tasks in a zero-shot manner with METRA. Fig-\nure 8 presents the comparison results, where METRA achieves the best performance on all of the\nfive downstream tasks. While LEXA also achieves non-trivial performances in three tasks, it strug-\ngles with state-based Ant and pixel-based Humanoid, likely because it is practically challenging to\ncompletely capture the transition dynamics of these complex environments.\n6 C ONCLUSION\nIn this work, we presented METRA, a scalable unsupervised RL method based on the idea of cover-\ning a compact latent skill space that is connected to the state space by a temporal distance metric. We\nshowed that METRA learns diverse useful behaviors in various locomotion and manipulation envi-\nronments, being the first unsupervised RL method that learns locomotion behaviors in pixel-based\nQuadruped and Humanoid.\nFinal remarks. In unsupervised RL, many excellent prior works have explored pure exploration\nor mutual information skill learning. However, given that these methods are not necessarily readily\nscalable to complex environments with high intrinsic state dimensionality, as discussed in Section 2,\nwe may need a completely different approach to enable truly scalable unsupervised RL. We hope\nthat this work serves as a step toward broadly applicable unsupervised RL that enables large-scale\npre-training with minimal supervision.\n9", "start_char_idx": 3003, "end_char_idx": 4979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c022a9d1-56f8-4001-b032-56706ce8cb5e": {"__data__": {"id_": "c022a9d1-56f8-4001-b032-56706ce8cb5e", "embedding": null, "metadata": {"page_label": "10", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62bdfbf7-2d85-4125-af36-9ac687fa789a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "45ec507fcc4a8bad72cdd73b2dbacc3fde15128d0ac4a6855bcb9180c9a14994", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nWe would like to thank Youngwoon Lee for an informative discussion, and RAIL members and\nanonymous reviewers for their helpful comments. This work was partly supported by the Korea\nFoundation for Advanced Studies (KFAS), ARO W911NF-21-1-0097, and the Office of Naval Re-\nsearch. This research used the Savio computational cluster resource provided by the Berkeley Re-\nsearch Computing program at UC Berkeley.\nREPRODUCIBILITY STATEMENT\nWe provide our code at the following repository: https://github.com/seohongpark/\nMETRA . We provide the full experimental details in Appendix G.\nREFERENCES\nJoshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery\nalgorithms. ArXiv , abs/1807.10299, 2018.\nAdri `a Puigdom `enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven\nKapturowski, Olivier Tieleman, Mart \u00b4\u0131n Arjovsky, Alexander Pritzel, Andew Bolt, and Charles\nBlundell. Never give up: Learning directed exploration strategies. In International Conference\non Learning Representations (ICLR) , 2020.\nRichard F Bass. Real analysis for graduate students . Createspace Ind Pub, 2013.\nKate Baumli, David Warde-Farley, Steven Stenberg Hansen, and V olodymyr Mnih. Relative varia-\ntional intrinsic control. In AAAI Conference on Artificial Intelligence (AAAI) , 2021.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R \u00b4emi\nMunos. Unifying count-based exploration and intrinsic motivation. In Neural Information Pro-\ncessing Systems (NeurIPS) , 2016.\nGlen Berseth, Daniel Geng, Coline Devin, Nicholas Rhinehart, Chelsea Finn, Dinesh Jayaraman,\nand Sergey Levine. Smirl: Surprise minimizing reinforcement learning in unstable environments.\nInInternational Conference on Learning Representations (ICLR) , 2021.\nRajendra Bhatia. Matrix analysis . Springer Science & Business Media, 2013.\nG. Brockman, Vicki Cheung, Ludwig Pettersson, J. Schneider, John Schulman, Jie Tang, and\nW. Zaremba. OpenAI Gym. ArXiv , abs/1606.01540, 2016.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In Neural Informa-\ntion Processing Systems (NeurIPS) , 2020.\nYuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network\ndistillation. In International Conference on Learning Representations (ICLR) , 2019.\nV\u00b4\u0131ctor Campos Cam \u00b4u\u02dcnez, Alex Trott, Caiming Xiong, Richard Socher, Xavier Gir \u00b4o Nieto, and Jordi\nTorres Vi \u02dcnals. Explore, discover and learn: unsupervised discovery of state-covering skills. In\nInternational Conference on Machine Learning (ICML) , 2020.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. In International Conference on Machine Learning\n(ICML) , 2020.\n10", "start_char_idx": 0, "end_char_idx": 3303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "695ddcaf-c48e-4976-a1b5-6715e5d74101": {"__data__": {"id_": "695ddcaf-c48e-4976-a1b5-6715e5d74101", "embedding": null, "metadata": {"page_label": "11", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "531e2299-5003-43d4-97ab-3305b6a7e6bf", "node_type": "4", "metadata": {"page_label": "11", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ed77b3ef07b14d0491f185f7e57d44a662c6a1e21f163c17debf9a97566c14a5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nWenze Chen, Shiyu Huang, Yuan Chiang, Tingling Chen, and Jun Zhu. Dgpo: Discovering multiple\nstrategies with diversity-guided policy optimization. In AAAI Conference on Artificial Intelligence\n(AAAI) , 2024.\nXinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized ensembled double q-\nlearning: Learning fast without a model. In International Conference on Learning Representa-\ntions (ICLR) , 2021.\nJongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang Gu. Variational empow-\nerment as representation learning for goal-conditioned reinforcement learning. In International\nConference on Machine Learning (ICML) , 2021.\nJohn D. Co-Reyes, Yuxuan Liu, Abhishek Gupta, Benjamin Eysenbach, P. Abbeel, and Sergey\nLevine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajec-\ntory embeddings. In International Conference on Machine Learning (ICML) , 2018.\nIshan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for\nreinforcement learning. In Neural Information Processing Systems (NeurIPS) , 2021.\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. First return, then\nexplore. Nature , 590:580\u2013586, 2020.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\nLearning skills without a reward function. In International Conference on Learning Representa-\ntions (ICLR) , 2019a.\nBenjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridg-\ning planning and reinforcement learning. In Neural Information Processing Systems (NeurIPS) ,\n2019b.\nCarlos Florensa, Yan Duan, and P. Abbeel. Stochastic neural networks for hierarchical reinforcement\nlearning. In International Conference on Learning Representations (ICLR) , 2017.\nCarlos Florensa, Jonas Degrave, Nicolas Manfred Otto Heess, Jost Tobias Springenberg, and Mar-\ntin A. Riedmiller. Self-supervised learning of image embedding for continuous control. ArXiv ,\nabs/1901.00943, 2019.\nJustin Fu, John D. Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep\nreinforcement learning. In Neural Information Processing Systems (NeurIPS) , 2017.\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. ArXiv ,\nabs/1611.07507, 2016.\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy\nlearning: Solving long-horizon tasks via imitation and reinforcement learning. In Conference on\nRobot Learning (CoRL) , 2019.\nMichael U Gutmann and Aapo Hyv \u00a8arinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In International Conference on Artificial Intelligence and\nStatistics (AISTATS) , 2010.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International Confer-\nence on Machine Learning (ICML) , 2018a.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, G. Tucker, Sehoon Ha, Jie Tan, Vikash Ku-\nmar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms\nand applications. ArXiv , abs/1812.05905, 2018b.\nDanijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learn-\ning behaviors by latent imagination. In International Conference on Learning Representations\n(ICLR) , 2020.\n11", "start_char_idx": 0, "end_char_idx": 3482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af4db8cd-9b93-4948-bcdf-a1005c19f6fe": {"__data__": {"id_": "af4db8cd-9b93-4948-bcdf-a1005c19f6fe", "embedding": null, "metadata": {"page_label": "12", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b08f4748-544d-4530-86b0-acb6bd9f2ae4", "node_type": "4", "metadata": {"page_label": "12", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e2d3f15c5bdfb77d6de319c7193af3856643a183fd694c77dedfaa41c77bc080", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nDanijar Hafner, Kuang-Huei Lee, Ian S. Fischer, and P. Abbeel. Deep hierarchical planning from\npixels. In Neural Information Processing Systems (NeurIPS) , 2022.\nNicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive\ncontrol. In International Conference on Machine Learning (ICML) , 2022.\nS. Hansen, Will Dabney, Andr \u00b4e Barreto, T. Wiele, David Warde-Farley, and V . Mnih. Fast task\ninference with variational intrinsic successor features. In International Conference on Learning\nRepresentations (ICLR) , 2020.\nKristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance\nlearning for semi-supervised and unsupervised skill discovery. In International Conference on\nLearning Representations (ICLR) , 2020.\nElad Hazan, Sham M. Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum\nentropy exploration. In International Conference on Machine Learning (ICML) , 2019.\nShuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, and Xiangyang Ji. Wasserstein\nunsupervised reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI) , 2022.\nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka.\nDropout q-functions for doubly efficient reinforcement learning. In International Conference on\nLearning Representations (ICLR) , 2022.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and P. Abbeel. Vime: Varia-\ntional information maximizing exploration. In Neural Information Processing Systems (NeurIPS) ,\n2016.\nEdward S. Hu, Richard Chang, Oleh Rybkin, and Dinesh Jayaraman. Planning goals for exploration.\nInInternational Conference on Learning Representations (ICLR) , 2023.\nZheyuan Jiang, Jingyue Gao, and Jianyu Chen. Unsupervised skill discovery via recurrent skill\ntraining. In Neural Information Processing Systems (NeurIPS) , 2022.\nLeslie Pack Kaelbling. Learning to achieve goals. In International Joint Conference on Artificial\nIntelligence (IJCAI) , 1993.\nPierre-Alexandre Kamienny, Jean Tarbouriech, Alessandro Lazaric, and Ludovic Denoyer. Direct\nthen diffuse: Incremental unsupervised skill discovery for state covering and goal reaching. In\nInternational Conference on Learning Representations (ICLR) , 2022.\nJaekyeom Kim, Seohong Park, and Gunhee Kim. Unsupervised skill discovery with bottleneck\noption learning. In International Conference on Machine Learning (ICML) , 2021.\nSeongun Kim, Kyowoon Lee, and Jaesik Choi. Variational curriculum reinforcement learning for\nunsupervised discovery of skills. In International Conference on Machine Learning (ICML) ,\n2023.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations (ICLR) , 2015.\nMartin Klissarov and Marlos C. Machado. Deep laplacian-based options for temporally-extended\nexploration. In International Conference on Machine Learning (ICML) , 2023.\nIlya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. In International Conference on Learning Representa-\ntions (ICLR) , 2021.\nSaurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you\nneed: Few-shot extrapolation via structured maxent rl. In Neural Information Processing Sys-\ntems (NeurIPS) , 2020.\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel\nPinto, and P. Abbeel. Urlb: Unsupervised reinforcement learning benchmark. In Neural Informa-\ntion Processing Systems (NeurIPS) Datasets and Benchmarks Track , 2021.\n12", "start_char_idx": 0, "end_char_idx": 3672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66ab1f86-14d9-45c1-995a-d9fa68212ba2": {"__data__": {"id_": "66ab1f86-14d9-45c1-995a-d9fa68212ba2", "embedding": null, "metadata": {"page_label": "13", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c81fcd7d-f5a6-49e5-9ef7-266b2205171b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0c06d382cdd8a2707e1f1086aec24a38e2d4f783315d2b85f2f279bb46bd4150", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b0612ad-35f7-44d0-a827-7157b5c0f0d8", "node_type": "1", "metadata": {}, "hash": "c1cb4b95f569596100be206d2f6453be4eb2733882c6e190cec4e288398d389b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMichael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and P. Abbeel. Un-\nsupervised reinforcement learning with contrastive intrinsic control. In Neural Information Pro-\ncessing Systems (NeurIPS) , 2022.\nYann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E.\nHubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition.\nNeural Computation , 1:541\u2013551, 1989.\nLisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhut-\ndinov. Efficient exploration via state marginal matching. ArXiv , abs/1906.05274, 2019.\nMengdi Li, Xufeng Zhao, Jae Hee Lee, Cornelius Weber, and Stefan Wermter. Internally rewarded\nreinforcement learning. In International Conference on Machine Learning (ICML) , 2023.\nHao Liu and Pieter Abbeel. APS: Active pretraining with successor features. In International\nConference on Machine Learning (ICML) , 2021a.\nHao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In Neural\nInformation Processing Systems (NeurIPS) , 2021b.\nMarlos C. Machado, Marc G. Bellemare, and Michael Bowling. A laplacian framework for option\ndiscovery in reinforcement learning. In International Conference on Machine Learning (ICML) ,\n2017.\nMarlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray\nCampbell. Eigenoption discovery through the deep successor representation. In International\nConference on Learning Representations (ICLR) , 2018.\nPietro Mazzaglia, Ozan C \u00b8 atal, Tim Verbelen, and B. Dhoedt. Curiosity-driven exploration via latent\nbayesian surprise. In AAAI Conference on Artificial Intelligence (AAAI) , 2022.\nPietro Mazzaglia, Tim Verbelen, B. Dhoedt, Alexandre Lacoste, and Sai Rajeswar. Choreographer:\nLearning and adapting skills in imagination. In International Conference on Learning Represen-\ntations (ICLR) , 2023.\nRussell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discover-\ning and achieving goals via world models. In Neural Information Processing Systems (NeurIPS) ,\n2021.\nShakir Mohamed and Danilo J. Rezende. Variational information maximisation for intrinsically\nmotivated reinforcement learning. In Neural Information Processing Systems (NeurIPS) , 2015.\nMirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gra-\ndient of a non-parametric state entropy estimate. In AAAI Conference on Artificial Intelligence\n(AAAI) , 2021.\nOpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter\nWelinder, Ruben D\u2019Sa, Arthur Petron, Henrique Pond \u00b4e de Oliveira Pinto, Alex Paino, Hyeonwoo\nNoh, Lilian Weng, Qiming Yuan, Casey Chu, and Wojciech Zaremba. Asymmetric self-play for\nautomatic goal discovery in robotic manipulation. ArXiv , abs/2101.04882, 2021.\nGeorg Ostrovski, Marc G. Bellemare, A \u00a8aron van den Oord, and R \u00b4emi Munos. Count-based explo-\nration with neural density models. In International Conference on Machine Learning (ICML) ,\n2017.\nSherjil Ozair, Corey Lynch, Yoshua Bengio, A \u00a8aron van den Oord, Sergey Levine, and Pierre Ser-\nmanet. Wasserstein dependency measure for representation learning. In Neural Information\nProcessing Systems (NeurIPS) , 2019.\nSeohong Park and Sergey Levine. Predictable mdp abstraction for unsupervised model-based rl.", "start_char_idx": 0, "end_char_idx": 3421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b0612ad-35f7-44d0-a827-7157b5c0f0d8": {"__data__": {"id_": "1b0612ad-35f7-44d0-a827-7157b5c0f0d8", "embedding": null, "metadata": {"page_label": "13", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c81fcd7d-f5a6-49e5-9ef7-266b2205171b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0c06d382cdd8a2707e1f1086aec24a38e2d4f783315d2b85f2f279bb46bd4150", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66ab1f86-14d9-45c1-995a-d9fa68212ba2", "node_type": "1", "metadata": {"page_label": "13", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ca8d564f95a8a9f22bdb4670de59ab6a7b5324411b20aeb9f3d18641ec100637", "class_name": "RelatedNodeInfo"}}, "text": "Asymmetric self-play for\nautomatic goal discovery in robotic manipulation. ArXiv , abs/2101.04882, 2021.\nGeorg Ostrovski, Marc G. Bellemare, A \u00a8aron van den Oord, and R \u00b4emi Munos. Count-based explo-\nration with neural density models. In International Conference on Machine Learning (ICML) ,\n2017.\nSherjil Ozair, Corey Lynch, Yoshua Bengio, A \u00a8aron van den Oord, Sergey Levine, and Pierre Ser-\nmanet. Wasserstein dependency measure for representation learning. In Neural Information\nProcessing Systems (NeurIPS) , 2019.\nSeohong Park and Sergey Levine. Predictable mdp abstraction for unsupervised model-based rl. In\nInternational Conference on Machine Learning (ICML) , 2023.\nSeohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, and Gunhee Kim. Lipschitz-\nconstrained unsupervised skill discovery. In International Conference on Learning Represen-\ntations (ICLR) , 2022.\n13", "start_char_idx": 2809, "end_char_idx": 3685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98355748-83a3-40a8-87db-ada94f9cece0": {"__data__": {"id_": "98355748-83a3-40a8-87db-ada94f9cece0", "embedding": null, "metadata": {"page_label": "14", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56119e97-408d-43be-8433-5fcde63abbc9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7c2d555a54938fc32755fd9e378d5b75c2ca42205a4801c5cfdcd5def4445aac", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nSeohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine. Hiql: Offline goal-\nconditioned rl with latent states as actions. In Neural Information Processing Systems (NeurIPS) ,\n2023a.\nSeohong Park, Kimin Lee, Youngwoon Lee, and P. Abbeel. Controllability-aware unsupervised skill\ndiscovery. In International Conference on Machine Learning (ICML) , 2023b.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International Conference on Machine Learning (ICML) , 2017.\nDeepak Pathak, Dhiraj Gandhi, and Abhinav Kumar Gupta. Self-supervised exploration via dis-\nagreement. In International Conference on Machine Learning (ICML) , 2019.\nSilviu Pitis, Harris Chan, S. Zhao, Bradly C. Stadie, and Jimmy Ba. Maximum entropy gain ex-\nploration for long horizon multi-goal reinforcement learning. In International Conference on\nMachine Learning (ICML) , 2020.\nVitchyr H. Pong, Murtaza Dalal, S. Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-Fit:\nState-covering self-supervised reinforcement learning. In International Conference on Machine\nLearning (ICML) , 2020.\nBen Poole, Sherjil Ozair, A \u00a8aron van den Oord, Alexander A. Alemi, and G. Tucker. On variational\nbounds of mutual information. In International Conference on Machine Learning (ICML) , 2019.\nA. H. Qureshi, Jacob J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, and Michael C. Yip.\nComposing task-agnostic policies with deep reinforcement learning. In International Conference\non Learning Representations (ICLR) , 2020.\nSai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Pich\u2019e, B. Dhoedt, Aaron C. Courville,\nand Alexandre Lacoste. Mastering the unsupervised reinforcement learning benchmark from\npixels. In International Conference on Machine Learning (ICML) , 2023.\nNick Rhinehart, Jenny Wang, Glen Berseth, John D. Co-Reyes, Danijar Hafner, Chelsea Finn, and\nSergey Levine. Information is power: Intrinsic control via information capture. In Neural Infor-\nmation Processing Systems (NeurIPS) , 2021.\nNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory\nfor navigation. In International Conference on Learning Representations (ICLR) , 2018.\nTom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators.\nInInternational Conference on Machine Learning (ICML) , 2015.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and P. Abbeel. High-dimensional\ncontinuous control using generalized advantage estimation. In International Conference on\nLearning Representations (ICLR) , 2016.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. ArXiv , abs/1707.06347, 2017.\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, P. Abbeel, Danijar Hafner, and Deepak Pathak.\nPlanning to explore via self-supervised world models. In International Conference on Machine\nLearning (ICML) , 2020.\nYounggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, P. Abbeel, and Kimin Lee. State entropy\nmaximization with random encoders for efficient exploration. In International Conference on\nMachine Learning (ICML) , 2021.\nNur Muhammad (Mahi) Shafiullah and Lerrel Pinto. One after another: Learning incremental skills\nfor a changing world. In International Conference on Learning Representations (ICLR) , 2022.\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-\naware unsupervised discovery of skills. In International Conference on Learning Representations\n(ICLR) , 2020.\n14", "start_char_idx": 0, "end_char_idx": 3637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15e57bf5-1352-4fc1-a31c-0a9280808957": {"__data__": {"id_": "15e57bf5-1352-4fc1-a31c-0a9280808957", "embedding": null, "metadata": {"page_label": "15", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb11912f-41bb-4f7b-9328-ea892513e3bc", "node_type": "4", "metadata": {"page_label": "15", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2ab38b581d4080c28e83c220391bbe4b2fc32e376fe7705fc0fef7c72ec7bd2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ac20016-45ed-473d-8287-07cf88570733", "node_type": "1", "metadata": {}, "hash": "72c9d37151f3d0bccfb0255619bbafd82854c869eac43191086947c49d42d82e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nPranav Shyam, Wojciech Ja \u00b4skowski, and Faustino J. Gomez. Model-based active exploration. In\nInternational Conference on Machine Learning (ICML) , 2019.\nDJ Strouse, Kate Baumli, David Warde-Farley, Vlad Mnih, and Steven Stenberg Hansen. Learning\nmore skills through optimistic exploration. In International Conference on Learning Representa-\ntions (ICLR) , 2022.\nSainbayar Sukhbaatar, Ilya Kostrikov, Arthur D. Szlam, and Rob Fergus. Intrinsic motivation and\nautomatic curricula via asymmetric self-play. In International Conference on Learning Represen-\ntations (ICLR) , 2018.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and P. Abbeel. #exploration: A study of count-based exploration for deep rein-\nforcement learning. In Neural Information Processing Systems (NeurIPS) , 2017.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-\nden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A.\nRiedmiller. Deepmind control suite. ArXiv , abs/1801.00690, 2018.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nInIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2012.\nAhmed Touati and Yann Ollivier. Learning one representation to optimize all rewards. In Neural\nInformation Processing Systems (NeurIPS) , 2021.\nAhmed Touati, J \u00b4er\u00b4emy Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? In\nInternational Conference on Learning Representations (ICLR) , 2023.\nA\u00a8aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. ArXiv , abs/1807.03748, 2018.\nC\u00b4edric Villani et al. Optimal transport: old and new . Springer, 2009.\nTongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching rein-\nforcement learning via quasimetric learning. In International Conference on Machine Learning\n(ICML) , 2023.\nDavid Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and\nV olodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In In-\nternational Conference on Learning Representations (ICLR) , 2019.\nRushuai Yang, Chenjia Bai, Hongyi Guo, Siyuan Li, Bin Zhao, Zhen Wang, Peng Liu, and Xuelong\nLi. Behavior contrastive learning for unsupervised skill discovery. In International Conference\non Machine Learning (ICML) , 2023.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with pro-\ntotypical representations. In International Conference on Machine Learning (ICML) , 2021.\nTom Zahavy, Yannick Schroecker, Feryal M. P. Behbahani, Kate Baumli, Sebastian Flennerhag,\nShaobo Hou, and Satinder Singh. Discovering policies with domino: Diversity optimization\nmaintaining near optimality. In International Conference on Learning Representations (ICLR) ,\n2023a.\nTom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad\nTomasev, Lisa Schut, Demis Hassabis, and Satinder Singh. Diversifying ai: Towards creative\nchess with alphazero. ArXiv , abs/2308.09175, 2023b.\nJesse Zhang, Haonan Yu, and Wei Xu. Hierarchical reinforcement learning by discovering intrinsic\noptions. In International Conference on Learning Representations (ICLR) , 2021.", "start_char_idx": 0, "end_char_idx": 3386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ac20016-45ed-473d-8287-07cf88570733": {"__data__": {"id_": "4ac20016-45ed-473d-8287-07cf88570733", "embedding": null, "metadata": {"page_label": "15", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb11912f-41bb-4f7b-9328-ea892513e3bc", "node_type": "4", "metadata": {"page_label": "15", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2ab38b581d4080c28e83c220391bbe4b2fc32e376fe7705fc0fef7c72ec7bd2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15e57bf5-1352-4fc1-a31c-0a9280808957", "node_type": "1", "metadata": {"page_label": "15", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0f34b121b8ec252b75a98e84526320248230ed7e5dc73549935f1fe961167aa5", "class_name": "RelatedNodeInfo"}}, "text": "Discovering policies with domino: Diversity optimization\nmaintaining near optimality. In International Conference on Learning Representations (ICLR) ,\n2023a.\nTom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad\nTomasev, Lisa Schut, Demis Hassabis, and Satinder Singh. Diversifying ai: Towards creative\nchess with alphazero. ArXiv , abs/2308.09175, 2023b.\nJesse Zhang, Haonan Yu, and Wei Xu. Hierarchical reinforcement learning by discovering intrinsic\noptions. In International Conference on Learning Representations (ICLR) , 2021.\nAndrew Zhao, Matthieu Lin, Yangguang Li, Y . Liu, and Gao Huang. A mixture of surprises for\nunsupervised reinforcement learning. In Neural Information Processing Systems (NeurIPS) , 2022.\nZihan Zhou, Wei Fu, Bingliang Zhang, and Yi Wu. Continuously discovering novel strategies via\nreward-switching policy optimization. In International Conference on Learning Representations\n(ICLR) , 2022.\n15", "start_char_idx": 2820, "end_char_idx": 3780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56a53584-835b-418d-9e7a-67bb78d6111b": {"__data__": {"id_": "56a53584-835b-418d-9e7a-67bb78d6111b", "embedding": null, "metadata": {"page_label": "16", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d06b8f31-9819-4fa3-a618-95a782d11fa1", "node_type": "4", "metadata": {"page_label": "16", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a55bb73cf2adf8b9c919e1fde1ab9b5a69d3e2268bcf0863b29a2ed0686db022", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\npreserves temporal distances(        denotes adjacent states)\n\u03c6Z=R2S=R64\u00d764\u00d73\u03c6Learn to move in every direction in   ,which approximately covers         ZZ=R2S=R64\u00d764\u00d73S\u2200(s, s\u2032)\u2225\u03c6(s)\u2212\u03c6(s\u2032)\u2225\u22641max\u03c0(a|s,z)(\u03c6(s\u2032)\u2212\u03c6(s))\u22a4z\nFigure 9: Intuitive interpretation of METRA. Our main idea is to only cover a compact latent space Z\nthat is metrically connected to the state space S. Specifically, METRA learns a lossy, compact representation\nfunction \u03d5:S \u2192 Z , which preserves temporal distances ( left), and learns to move in every direction in Z,\nwhich leads to approximate coverage of the state space S(right ).\nObservation\nGlobal view\n(a) Quadruped\nObservationGlobal view (b) Humanoid\nFigure 10: Visualization of pixel-based DMC Quadruped and Humanoid. We use gradient-colored floors\nto allow the agent to infer its location from pixel observations, similarly to Hafner et al. (2022); Park et al.\n(2023a).\nA L IMITATIONS\nDespite its state-of-the-art performance in several benchmark environments, METRA, in its current\nform, has limitations. We refer to Section 4.2 for the limitations and future research directions\nregarding the METRA objective. In terms of practical implementation, METRA, like other similar\nunsupervised skill discovery methods (Sharma et al., 2020; Park et al., 2022; 2023b), uses a relatively\nsmall update-to-data (UTD) ratio ( i.e., the average number of gradient steps per environment step);\ne.g., we use 1/4for Kitchen and 1/16for Quadruped and Humanoid. Although we demonstrate that\nMETRA learns efficiently in terms of wall clock time, we believe there is room for improvement\nin terms of sample efficiency. This is mainly because we use vanilla SAC (Haarnoja et al., 2018a)\nas its RL backbone for simplicity, and we believe increasing the sample efficiency of METRA by\ncombining it with recent techniques in model-free RL (Kostrikov et al., 2021; Chen et al., 2021;\nHiraoka et al., 2022) or model-based RL (Hafner et al., 2020; Hansen et al., 2022) is an interesting\ndirection for future work.\nAnother limitation of this work is that, while we evaluate METRA on various locomotion and ma-\nnipulation environments, following prior work in unsupervised RL and unsuperivsed skill discov-\nery (Eysenbach et al., 2019a; Sharma et al., 2020; Mendonca et al., 2021; Laskin et al., 2021; He\net al., 2022; Park et al., 2022; Zhao et al., 2022; Shafiullah & Pinto, 2022; Laskin et al., 2022; Park\net al., 2023b; Yang et al., 2023), we have not evaluated METRA on other different types of environ-\nments, such as Atari games. Also, since we assume a fixed MDP ( i.e., stationary, fully observable\ndynamics, Section 3), METRA in its current form does not particularly deal with non-stationary or\n16", "start_char_idx": 0, "end_char_idx": 2749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76e1b885-8869-4162-b58f-d349b606e121": {"__data__": {"id_": "76e1b885-8869-4162-b58f-d349b606e121", "embedding": null, "metadata": {"page_label": "17", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7149e6a7-531d-41d9-9437-233b59a8b686", "node_type": "4", "metadata": {"page_label": "17", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1f7c9688a25f24247b2dfd7615c2317e738d7232011bf184fb05cbb115931ffc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7199d6f1-eb79-4cc0-836b-95fff52d11d1", "node_type": "1", "metadata": {}, "hash": "7fbc631c1f3296600f2bc4a344fa663b0edff3a794c8154c05c9871e1704a6c6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nnon-Markovian dynamics. We leave applying METRA to more diverse environments or extending\nthe idea behind METRA to non-stationary or non-Markovian environments for future work.\nB E XTENDED RELATED WORK\nIn addition to unsupervised skill discovery (Mohamed & Rezende, 2015; Gregor et al., 2016; Flo-\nrensa et al., 2017; Co-Reyes et al., 2018; Achiam et al., 2018; Eysenbach et al., 2019a; Warde-Farley\net al., 2019; Shyam et al., 2019; Lee et al., 2019; Sharma et al., 2020; Campos Cam \u00b4u\u02dcnez et al., 2020;\nHansen et al., 2020; Pong et al., 2020; Baumli et al., 2021; Choi et al., 2021; Yarats et al., 2021;\nKim et al., 2021; Zhang et al., 2021; He et al., 2022; Strouse et al., 2022; Laskin et al., 2022; Park\net al., 2022; Shafiullah & Pinto, 2022; Jiang et al., 2022; Zhao et al., 2022; Kamienny et al., 2022;\nPark & Levine, 2023; Park et al., 2023b; Li et al., 2023; Kim et al., 2023) and pure exploration (or\nunsupervised goal-conditioned RL) methods (Houthooft et al., 2016; Bellemare et al., 2016; Tang\net al., 2017; Ostrovski et al., 2017; Fu et al., 2017; Pathak et al., 2017; Hazan et al., 2019; Shyam\net al., 2019; Burda et al., 2019; Pathak et al., 2019; Lee et al., 2019; Ecoffet et al., 2020; Pitis et al.,\n2020; Badia et al., 2020; Mutti et al., 2021; Liu & Abbeel, 2021b; Mendonca et al., 2021; Yarats\net al., 2021; Seo et al., 2021; Mazzaglia et al., 2022; 2023; Hu et al., 2023; Rajeswar et al., 2023),\nthere have also been proposed other types of unsupervised RL approaches, such as ones based on\nasymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), surprise minimization (Berseth\net al., 2021; Rhinehart et al., 2021), and forward-backward representations (Touati & Ollivier, 2021;\nTouati et al., 2023). One potentially closely related line of work is graph Laplacian-based option\ndiscovery methods (Machado et al., 2017; 2018; Klissarov & Machado, 2023). These methods learn\na set of diverse behaviors based on the eigenvectors of the graph Laplacian of the MDP\u2019s adjacency\nmatrix. Although we have not found a formal connection to these methods, we suspect there might\nexist a deep, intriguing connection between METRA and graph Laplacian-based methods, given that\nthey both discover behaviors based on the temporal dynamics of the MDP. METRA is also related\nto several works in goal-conditioned RL that consider temporal distances (Kaelbling, 1993; Schaul\net al., 2015; Savinov et al., 2018; Eysenbach et al., 2019b; Florensa et al., 2019; Hartikainen et al.,\n2020; Durugkar et al., 2021; Wang et al., 2023). In particular, Durugkar et al. (2021); Wang et al.\n(2023) use similar temporal distance constraints to ours for goal-conditioned RL.\nC T HEORETICAL RESULTS\nC.1 U NIVERSALITY OF INNER PRODUCT DECOMPOSITION\nLemma C.1.", "start_char_idx": 0, "end_char_idx": 2806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7199d6f1-eb79-4cc0-836b-95fff52d11d1": {"__data__": {"id_": "7199d6f1-eb79-4cc0-836b-95fff52d11d1", "embedding": null, "metadata": {"page_label": "17", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7149e6a7-531d-41d9-9437-233b59a8b686", "node_type": "4", "metadata": {"page_label": "17", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1f7c9688a25f24247b2dfd7615c2317e738d7232011bf184fb05cbb115931ffc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76e1b885-8869-4162-b58f-d349b606e121", "node_type": "1", "metadata": {"page_label": "17", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c210599c82efcfb0941a4e63e1be3e9e35ca301993458382ba3b9e3fd5e0ef82", "class_name": "RelatedNodeInfo"}}, "text": "METRA is also related\nto several works in goal-conditioned RL that consider temporal distances (Kaelbling, 1993; Schaul\net al., 2015; Savinov et al., 2018; Eysenbach et al., 2019b; Florensa et al., 2019; Hartikainen et al.,\n2020; Durugkar et al., 2021; Wang et al., 2023). In particular, Durugkar et al. (2021); Wang et al.\n(2023) use similar temporal distance constraints to ours for goal-conditioned RL.\nC T HEORETICAL RESULTS\nC.1 U NIVERSALITY OF INNER PRODUCT DECOMPOSITION\nLemma C.1. LetXandYbe compact Hausdorff spaces ( e.g., compact subsets in RN) andC(A)\nbe the set of real-valued continuous functions on A. For any function f(x, y)\u2208 C(X \u00d7 Y )and\n\u03f5 >0, there exist continuous functions \u03d5(x) :X \u2192RDand\u03d5(y) :Y \u2192RDwithD\u22651such that\nsupx\u2208X,y\u2208Y|f(x, y)\u2212\u03d5(x)\u22a4\u03c8(y)|< \u03b5.\nProof. We invoke the Stone-Weierstrass theorem (Bass (2013), Theorem 20.44), which implies\nthat the set of functions T:={PD\ni=1\u03d5i(x)\u03c8i(y) :D\u2208N,\u22001\u2264i\u2264D, \u03d5 i\u2208 C(X), \u03c8i\u2208\nC(Y)}is dense in C(X \u00d7 Y )ifTis an algebra that separates points and vanishes at no\npoint. The only non-trivial part is to show that Tis closed under multiplication. Consider\ng(1)(x, y) =PD1\ni=1\u03d5(1)\ni(x)\u03c8(1)\ni(y)\u2208 T andg(2)(x, y) =PD2\ni=1\u03d5(2)\ni(x)\u03c8(2)\ni(y)\u2208 T . We have\ng(1)(x, y)g(2)(x, y) =PD1\ni=1PD2\nj=1\u03d5(1)\ni(x)\u03d5(2)\nj(x)\u03c8(1)\ni(y)\u03c8(2)\nj(y), where \u03d5(1)\ni(x)\u03d5(2)\nj(x)\u2208 C(X)\nand\u03c8(1)\ni(y)\u03c8(2)\nj(y)\u2208 C(Y)for all i, j. Hence, g(1)(x, y)g(2)(x, y)\u2208 T.\nTheorem C.2 (\u03d5(x)\u22a4\u03c8(y)is a universal approximator of f(x, y)).LetXandYbe compact Haus-\ndorff spaces and \u03a6\u2282 C(X)and\u03a8\u2282 C(Y)be dense sets in C(X)andC(Y), respectively. Then,\nT:={PD\ni=1\u03d5i(x)\u03c8i(y) :D\u2208N,\u22001\u2264i\u2264D, \u03d5 i\u2208\u03a6, \u03c8i\u2208\u03a8}is also dense in C(X \u00d7 Y ).\nIn other words, \u03d5(x)\u22a4\u03c8(y)can approximate f(x, y)to arbitrary accuracy if \u03d5and\u03c8are modeled\nwith universal approximators ( e.g., neural networks) and D\u2192 \u221e .\nProof. By Lemma C.1, for any f\u2208 C(X \u00d7 Y )and\u03b5 >0, there exist D\u2208N,\u03d5i\u2208 C(X), and\n\u03c8i\u2208 C(Y)for1\u2264i\u2264Dsuch that supx\u2208X,y\u2208Y|f(x, y)\u2212PD\ni=1\u03d5i(x)\u03c8i(y)|< \u03b5/ 3. Define\n17", "start_char_idx": 2318, "end_char_idx": 4262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ac679ba-6e65-4e28-9c76-eb0f7885a603": {"__data__": {"id_": "2ac679ba-6e65-4e28-9c76-eb0f7885a603", "embedding": null, "metadata": {"page_label": "18", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10abe83b-9c38-48f9-9a3a-e3cb73e52724", "node_type": "4", "metadata": {"page_label": "18", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e354fc9f2d99ab6fba6d9264c436b74c83199c8b2a84b9d40f8553e669f7877d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8140036-043b-4f30-860e-8fa4d03ff327", "node_type": "1", "metadata": {}, "hash": "a1a9263e5894d6ae0166273e30a4f8c347788acc657f216b1839d714a73a4b9b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMy:= sup1\u2264i\u2264D,y\u2208Y|\u03c8i(y)|. Since \u03a6is dense, for each 1\u2264i\u2264D, there exists \u02dc\u03d5i\u2208\u03a6such that\nsupx\u2208X|\u03d5i(x)\u2212\u02dc\u03d5i(x)|< \u03b5/(3DM y). Define Mx:= sup1\u2264i\u2264D,x\u2208X|\u02dc\u03d5i(x)|. Similarly, for each\n1\u2264i\u2264D, there exists \u02dc\u03c8i\u2208\u03a8such that supy\u2208Y|\u03c8i(y)\u2212\u02dc\u03c8i(y)|< \u03b5/(3DM x). Now, we have\n\f\f\f\f\ff(x, y)\u2212DX\ni=1\u02dc\u03d5i(x)\u02dc\u03c8i(y)\f\f\f\f\f\u2264\f\f\f\f\ff(x, y)\u2212DX\ni=1\u03d5i(x)\u03c8i(y)\f\f\f\f\f+DX\ni=1\f\f\f\u02dc\u03d5i(x)\u02dc\u03c8i(y)\u2212\u03d5i(x)\u03c8i(y)\f\f\f(9)\n<\u03b5\n3+DX\ni=1|\u02dc\u03d5i(x)(\u02dc\u03c8i(y)\u2212\u03c8i(y))|+DX\ni=1|(\u02dc\u03d5i(x)\u2212\u03d5i(x))\u03c8i(y)|\n(10)\n<\u03b5\n3+\u03b5\n3+\u03b5\n3(11)\n=\u03b5, (12)\nfor any x\u2208 X andy\u2208 Y. Hence, Tis dense in C(X \u00d7 Y ).\nC.2 L IPSCHITZ CONSTRAINT UNDER THE TEMPORAL DISTANCE METRIC\nTheorem C.3. The following two conditions are equivalent:\n(a)\u2225\u03d5(u)\u2212\u03d5(v)\u22252\u2264dtemp(u, v)for all u, v\u2208 S.\n(b)\u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\u22641for all (s, s\u2032)\u2208 Sadj.\nProof. We first show ( a) implies ( b). Assume ( a) holds. Consider (s, s\u2032)\u2208 Sadj. Ifs\u0338=s\u2032, by ( a),\nwe have \u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252\u2264dtemp(s, s\u2032) = 1 . Otherwise, i.e.,s=s\u2032,\u2225\u03d5(s)\u2212\u03d5(s\u2032)\u22252= 0\u22641.\nHence, ( a) implies ( b).\nNext, we show ( b) implies ( a). Assume ( b) holds. Consider u, v\u2208 S. Ifdtemp(u, v) =\u221e(i.e.,vis\nnot reachable from u), (a) holds trivially. Otherwise, let kbedtemp(u, v). By definition, there exists\n(s0=u, s1, . . . , s k\u22121, sk=v)such that (si, si+1)\u2208 Sadjfor all 0\u2264i\u2264k\u22121. Due to the triangle\ninequality and ( b), we have \u2225\u03d5(u)\u2212\u03d5(v)\u22252\u2264Pk\u22121\ni=0\u2225\u03d5(si)\u2212\u03d5(si+1)\u22252\u2264k=dtemp(u, v).\nHence, ( b) implies ( a).\nD A C ONNECTION BETWEEN METRA AND PCA\nIn this section, we derive a theoretical connection between METRA and principal component anal-\nysis (PCA). Recall that the METRA objective can be written as follows:\nsup\n\u03c0,\u03d5Ep(\u03c4,z)\"T\u22121X\nt=0(\u03d5(st+1)\u2212\u03d5(st))\u22a4z#\n=Ep(\u03c4,z)\u0002\n\u03d5(sT)\u22a4z\u0003\n(13)\ns.t.\u2225\u03d5(u)\u2212\u03d5(v)\u22252\u2264dtemp(u, v),\u2200u, v\u2208 S, (14)\nwhere dtemp denotes the temporal distance between two states. To make a formal connection be-\ntween METRA and PCA, we consider the following squared variant of the METRA objective in this\nsection.", "start_char_idx": 0, "end_char_idx": 1882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8140036-043b-4f30-860e-8fa4d03ff327": {"__data__": {"id_": "b8140036-043b-4f30-860e-8fa4d03ff327", "embedding": null, "metadata": {"page_label": "18", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10abe83b-9c38-48f9-9a3a-e3cb73e52724", "node_type": "4", "metadata": {"page_label": "18", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e354fc9f2d99ab6fba6d9264c436b74c83199c8b2a84b9d40f8553e669f7877d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ac679ba-6e65-4e28-9c76-eb0f7885a603", "node_type": "1", "metadata": {"page_label": "18", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ab8653869a6c41284c4c2a068f84858b30fac3b93bcd37e2cd48cbe271db7733", "class_name": "RelatedNodeInfo"}}, "text": "Hence, ( b) implies ( a).\nD A C ONNECTION BETWEEN METRA AND PCA\nIn this section, we derive a theoretical connection between METRA and principal component anal-\nysis (PCA). Recall that the METRA objective can be written as follows:\nsup\n\u03c0,\u03d5Ep(\u03c4,z)\"T\u22121X\nt=0(\u03d5(st+1)\u2212\u03d5(st))\u22a4z#\n=Ep(\u03c4,z)\u0002\n\u03d5(sT)\u22a4z\u0003\n(13)\ns.t.\u2225\u03d5(u)\u2212\u03d5(v)\u22252\u2264dtemp(u, v),\u2200u, v\u2208 S, (14)\nwhere dtemp denotes the temporal distance between two states. To make a formal connection be-\ntween METRA and PCA, we consider the following squared variant of the METRA objective in this\nsection.\nsup\n\u03c0,\u03d5Ep(\u03c4,z)\u0002\n(\u03d5(sT)\u22a4z)2\u0003\ns.t.\u2225\u03d5(u)\u2212\u03d5(v)\u22252\u2264dtemp(u, v),\u2200u, v\u2208 S, (15)\nwhich is almost the same as Equation (13) but the objective is now squared. The reason we consider\nthis variant is simply for mathematical convenience.\nNext, we introduce the notion of a temporally consistent embedding.\nDefinition D.1 (Temporally consistent embedding) .We call that an MDP Madmits a temporally\nconsistent embedding if there exists \u03c8(s) :S \u2192Rmsuch that\ndtemp(u, v) =\u2225\u03c8(u)\u2212\u03c8(v)\u22252,\u2200u, v\u2208 S. (16)\n18", "start_char_idx": 1345, "end_char_idx": 2367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e571ddec-c04d-43d6-9062-d8903d16ab4b": {"__data__": {"id_": "e571ddec-c04d-43d6-9062-d8903d16ab4b", "embedding": null, "metadata": {"page_label": "19", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa2e441f-0455-454f-b46c-5c7bedec8131", "node_type": "4", "metadata": {"page_label": "19", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "afd6a8c5d6a275334455bfa939cd9eb3001626eec0f97d2a136ede18ea80535e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f35b6fa9-8a02-4fed-ad86-db2b73ae8398", "node_type": "1", "metadata": {}, "hash": "f9c46916fc163750fd5bc162efe66b02d022db95bb11f18e5addbb9d98ff1037", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nIntuitively, this states that the temporal distance metric can be embedded into a (potentially very\nhigh-dimensional) Euclidean space. We note that \u03c8is different from \u03d5in Equation (13), and Rm\ncan be much higher-dimensional than Z. An example of an MDP that admits a temporally consistent\nembedding is the PointMass environment: if an agent in Rncan move in any direction up to a unit\nspeed, \u03c8(x) =xsatisfies dtemp(u, v) =\u2225u\u2212v\u22252for all u, v\u2208Rn(with a slightly generalized\nnotion of temporal distances in continuous time) and thus the MDP admits the temporally consistent\nembedding of \u03c8. A pixel-based PointMass environment is another example of such an MDP.\nNow, we formally derive a connection between squared METRA and PCA. For simplicity, we as-\nsumeZ=Rdandp(z) =N(0,Id), where N(0,Id)denotes the d-dimensional isotropic Gaussian\ndistribution. We also assume that Mhas a deterministic initial distribution and transition dynamics\nfunction, and every state is reachable from the initial state within Tsteps. We denote the set of n\u00d7n\npositive definite matrices as Sn\n++, the operator norm of a matrix Aas\u2225A\u2225op, and the m-dimensional\nunit\u21132ball as Bm.\nTheorem D.2 (Linear squared METRA is PCA in the temporal embedding space) .LetMbe an\nMDP that admits a temporally consistent embedding \u03c8:S \u2192 Rm. If\u03d5:S \u2192 Z is a linear\nmapping from the embedding space, i.e.,\u03d5(s) =W\u22a4\u03c8(s)withW\u2208Rm\u00d7d, and the embedding\nspace \u03a8 ={\u03c8(s) :s\u2208 S} forms an ellipse, i.e.,\u2203A\u2208Sm\n++s.t.\u03a8 ={x\u2208Rm:x\u22a4A\u22121x\u22641},\nthenW= [a1a2\u00b7\u00b7\u00b7ad]maximizes the squared METRA objective in Equation (15), where a1, . . . ,\nadare the top- deigenvectors of A.\nProof. SinceMadmits a temporally consistent embedding, we have\n\u2225\u03d5(u)\u2212\u03d5(v)\u22252\u2264dtemp(u, v)\u2200u, v\u2208 S (17)\n\u21d0\u21d2 \u2225 W\u22a4(\u03c8(u)\u2212\u03c8(v))\u22252\u2264 \u2225\u03c8(u)\u2212\u03c8(v)\u22252\u2200u, v\u2208 S (18)\n\u21d0\u21d2 \u2225 W\u22a4(u\u2212v)\u22252\u2264 \u2225u\u2212v\u22252\u2200u, v\u2208\u03a8 (19)\n\u21d0\u21d2 \u2225 W\u2225op\u22641, (20)\nwhere we use the fact that \u03c8is a surjection from Sto\u03a8and that Ais positive definite.", "start_char_idx": 0, "end_char_idx": 1935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f35b6fa9-8a02-4fed-ad86-db2b73ae8398": {"__data__": {"id_": "f35b6fa9-8a02-4fed-ad86-db2b73ae8398", "embedding": null, "metadata": {"page_label": "19", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa2e441f-0455-454f-b46c-5c7bedec8131", "node_type": "4", "metadata": {"page_label": "19", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "afd6a8c5d6a275334455bfa939cd9eb3001626eec0f97d2a136ede18ea80535e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e571ddec-c04d-43d6-9062-d8903d16ab4b", "node_type": "1", "metadata": {"page_label": "19", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "459ec11e52819f0651843a7aeb49190e6682838f42c5f1f23189eb2860a705ae", "class_name": "RelatedNodeInfo"}}, "text": ". ,\nadare the top- deigenvectors of A.\nProof. SinceMadmits a temporally consistent embedding, we have\n\u2225\u03d5(u)\u2212\u03d5(v)\u22252\u2264dtemp(u, v)\u2200u, v\u2208 S (17)\n\u21d0\u21d2 \u2225 W\u22a4(\u03c8(u)\u2212\u03c8(v))\u22252\u2264 \u2225\u03c8(u)\u2212\u03c8(v)\u22252\u2200u, v\u2208 S (18)\n\u21d0\u21d2 \u2225 W\u22a4(u\u2212v)\u22252\u2264 \u2225u\u2212v\u22252\u2200u, v\u2208\u03a8 (19)\n\u21d0\u21d2 \u2225 W\u2225op\u22641, (20)\nwhere we use the fact that \u03c8is a surjection from Sto\u03a8and that Ais positive definite. Now, we\nhave\n= sup\n\u03c0,\u2225W\u2225op\u22641Ep(\u03c4,z)[(\u03d5(sT)\u22a4z)2] (21)\n= sup\n\u03c0,\u2225W\u2225op\u22641Ep(\u03c4,z)[(\u03c8(sT)\u22a4Wz)2] (22)\n= sup\nf:Rd\u2192\u03a8,\u2225W\u2225op\u22641Ep(z)[(f(z)\u22a4Wz)2] (\u2235Every state is reachable within Tsteps) (23)\n= sup\ng:Rd\u2192Bm,\u2225W\u2225op\u22641Ep(z)[(g(z)\u22a4\u221a\nAWz )2] (g(z) =\u221a\nA\u22121f(z)) (24)\n= sup\n\u2225W\u2225op\u22641Ep(z)[ sup\ng:Rd\u2192Bm(g(z)\u22a4\u221a\nAWz )2] (25)\n= sup\n\u2225W\u2225op\u22641Ep(z)[ sup\n\u2225u\u22252\u22641(u\u22a4\u221a\nAWz )2] (26)\n= sup\n\u2225W\u2225op\u22641Ep(z)[\u2225\u221a\nAWz\u22252\n2] (\u2235Dual norm ) (27)\n= sup\n\u2225W\u2225op\u22641Ep(z)[z\u22a4W\u22a4AWz ] (28)\n= sup\n\u2225W\u2225op\u22641Ep(z)[tr(zz\u22a4W\u22a4AW)] (29)\n= sup\n\u2225W\u2225op\u22641tr(Ep(z)[zz\u22a4]W\u22a4AW) (30)\n= sup\n\u2225W\u2225op\u22641tr(WW\u22a4A). (31)\nSince WW\u22a4is a positive semidefinite matrix with rank at most dand\u2225W\u2225op\u22641, there exists d\neigenvalues 0\u2264\u03bb1, . . . , \u03bb d\u22641and the corresponding orthonormal eigenvectors v1, . . . , v dsuch\nthatWW\u22a4=Pd\nk=1\u03bbkvkv\u22a4\nk. Hence, tr(WW\u22a4A) =Pd\nk=1\u03bbkv\u22a4\nkAvk, and to maximize this, we\n19", "start_char_idx": 1610, "end_char_idx": 2739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c0e0d68-42da-43dd-b59d-9ce6075f1b80": {"__data__": {"id_": "1c0e0d68-42da-43dd-b59d-9ce6075f1b80", "embedding": null, "metadata": {"page_label": "20", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8713fa42-15a5-4ac5-8c66-28b1ff130f52", "node_type": "4", "metadata": {"page_label": "20", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0737d82e4f46aaa2e9db0a36ac570c8f29cbc9dbba84dad77f657e20d7a811c2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nmust set \u03bb1=\u00b7\u00b7\u00b7=\u03bbd= 1asAis positive definite. The remaining problem is to find dorthonor-\nmal vectors v1, . . . , v dthat maximizePd\nk=1v\u22a4\nkAvk. By the Ky Fan\u2019s maximum principle (Bhatia,\n2013), its solution is given as the deigenvectors corresponding to the dlargest eigenvalues of A.\nTherefore, W= [a1a2\u00b7\u00b7\u00b7ad], where a1, . . . , a dare the top- dprincipal components of A, maxi-\nmizes the squared METRA objective in Equation (15).\nTheorem D.2 states that linear squared METRA is equivalent to PCA in the temporal embedding\nspace. In practice, however, \u03d5can be nonlinear, the shape of \u03a8can be arbitrary, and the MDP may\nnot admit any temporally consistent embeddings. Nonetheless, this theoretical connection hints at\nthe intuition that the METRA objective encourages the agent to span the largest \u201ctemporal\u201d mani-\nfolds in the state space, given the limited capacity of Z.\nE C ONNECTIONS BETWEEN WDM AND DIAYN, DADS, AND CIC\nIn this section, we describe connections between our WDM objectives (either IW(S;Z)or\nIW(ST;Z)) and previous mutual information skill learning methods, DIAYN (Eysenbach et al.,\n2019a), DADS (Sharma et al., 2020), and CIC (Laskin et al., 2022). Recall that the IW(S;Z)\nobjective (Equation (4)) maximizes\nT\u22121X\nt=0\u0000\nEp(\u03c4,z)[\u03d5L(st)\u22a4\u03c8L(z)]\u2212Ep(\u03c4)[\u03d5L(st)]\u22a4Ep(z)[\u03c8L(z)]\u0001\n, (32)\nand the IW(ST;Z)objective (Equation (6)) maximizes\nT\u22121X\nt=0\u0000\nEp(\u03c4,z)[(\u03d5L(st+1)\u2212\u03d5L(st))\u22a4\u03c8L(z)]\u2212Ep(\u03c4)[\u03d5L(st+1)\u2212\u03d5L(st)]\u22a4Ep(z)[\u03c8L(z)]\u0001\n,(33)\nwhere we use the notations \u03d5Land\u03c8Lto denote that they are Lipschitz constrained. By simplifying\nEquation (32) or Equation (33) in three different ways, we will show that we can obtain \u201cWasser-\nstein counterparts\u201d of DIAYN, DADS, and CIC. For simplicity, we assume p(z) =N(0,I), where\nN(0,I)denotes the standard Gaussian distribution.\nE.1 DIAYN\nIf we set \u03c8L(z) =zin Equation (32), we get\nrt=\u03d5L(st)\u22a4z. (34)\nThis is analogous to DIAYN (Eysenbach et al., 2019a), which maximizes\nI(S;Z) =\u2212H(Z|S) +H(Z) (35)\n\u2273Ep(\u03c4,z)\"T\u22121X\nt=0logq(z|st)#\n(36)\n\u2243Ep(\u03c4,z)\"T\u22121X\nt=0\u2212\u2225z\u2212\u03d5(st)\u22252\n2#\n, (37)\nrDIAYN\nt =\u2212\u2225\u03d5(st)\u2212z\u22252\n2, (38)\nwhere \u2018\u2273\u2019 and \u2018\u2243\u2019 respectively denote \u2018 >\u2019 and \u2018 =\u2019 up to constant scaling or shifting, and we assume\nthat the variational distribution q(z|s)is modeled as N(\u03d5(s),I). By comparing Equation (34) and\nEquation (38), we can see that Equation (34) can be viewed as a Lipschitz, inner-product variant of\nDIAYN. This analogy will become clearer later.\nE.2 DADS\nIf we set \u03d5L(s) =sin Equation (33), we get\nrt= (st+1\u2212st)\u22a4\u03c8L(z)\u2212(st+1\u2212st)\u22a4Ep(z)[\u03c8L(z)] (39)\n\u2248(st+1\u2212st)\u22a4\u03c8L(z)\u22121\nLLX\ni=1(st+1\u2212st)\u22a4\u03c8L(zi), (40)\n20", "start_char_idx": 0, "end_char_idx": 2580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffa66238-965c-4c00-b0d9-52fceab5c612": {"__data__": {"id_": "ffa66238-965c-4c00-b0d9-52fceab5c612", "embedding": null, "metadata": {"page_label": "21", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8829a23e-e3d8-48a3-a27a-83d910e08765", "node_type": "4", "metadata": {"page_label": "21", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "01d0f21aef1ec1fc5459c9902075806a32fccae42b0dab9d45884b6bcfaa77ca", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nwhere we use Lindependent samples from N(0, I),z1, z2, . . . , z L, to approximate the expectation.\nThis is analogous to DADS (Sharma et al., 2020), which maximizes:\nI(S\u2032;Z|S) =\u2212H(S\u2032|S, Z) +H(S\u2032|S) (41)\n\u2273Ep(\u03c4,z)\"T\u22121X\nt=0logq(st+1|st, z)\u2212logp(st+1|st)#\n(42)\n\u2248Ep(\u03c4,z)\"T\u22121X\nt=0 \nlogq(st+1|st, z)\u22121\nLLX\ni=1logq(st+1|st, zi)!#\n, (43)\nrDADS\nt =\u2212\u2225(st+1\u2212st)\u2212\u03c8(st, z)\u22252\n2+1\nLLX\ni=1\u2225(st+1\u2212st)\u2212\u03c8(st, z)\u22252\n2, (44)\nwhere we assume that the variational distribution q(s\u2032|s, z)is modeled as q(s\u2032\u2212s|s, z) =\nN(\u03c8(s, z),I), as in the original implementation (Sharma et al., 2020). We also use the same sample-\nbased approximation as Equation (40). Note that the same analogy also holds between Equation (40)\nand Equation (44) ( i.e., Equation (40) is a Lipschitz, inner-product variant of DADS).\nE.3 CIC\nIf we do not simplify \u03d5Lor\u03c8Lin Equation (32), we get\nrt=\u03d5L(st)\u22a4\u03c8L(z)\u2212\u03d5L(st)\u22a4Ep(z)[\u03c8L(z)] (45)\n\u2248\u03d5L(st)\u22a4\u03c8L(z)\u22121\nLLX\ni=1\u03d5L(st)\u22a4\u03c8L(zi), (46)\nwhere we use the same sample-based approximation as Equation (40). By Jensen\u2019s inequality, Equa-\ntion (46) can be lower-bounded by\n\u03d5L(st)\u22a4\u03c8L(z)\u2212log1\nLLX\ni=1exp\u0000\n\u03d5L(st)\u22a4\u03c8L(zi)\u0001\n, (47)\nas in WPC (Ozair et al., 2019). This is analogous to CIC (Laskin et al., 2022), which estimates the\nMI via noise contrastive estimation (Gutmann & Hyv \u00a8arinen, 2010; van den Oord et al., 2018; Poole\net al., 2019):\nI(S;Z)\u2273Ep(\u03c4,z)\"T\u22121X\nt=0 \n\u03d5(st)\u22a4\u03c8(z)\u2212log1\nLLX\ni=1exp\u0000\n\u03d5(st)\u22a4\u03c8(zi)\u0001!#\n, (48)\nrCIC\nt=\u03d5(st)\u22a4\u03c8(z)\u2212log1\nLLX\ni=1exp\u0000\n\u03d5(st)\u22a4\u03c8(zi)\u0001\n. (49)\nNote that Equation (47) can be viewed as a Lipschitz variant of CIC (Equation (49)).\nIn this work, we use the \u03c8L(z) =zsimplification with Equation (33) ( i.e., Equation (7)), as we\nfound this variant to work well while being simple, but we believe exploring these other variants is\nan interesting future research direction. In particular, given that Equation (47) resembles the standard\ncontrastive learning formulation, combining this (more general) objective with existing contrastive\nlearning techniques may lead to another highly scalable unsupervised RL method, which we leave\nfor future work.\nF A DDITIONAL RESULTS\nF.1 F ULL QUALITATIVE RESULTS\nFigure 11 shows the complete qualitative results of behaviors discovered by METRA on state-based\nAnt and HalfCheetah, and pixel-based Quadruped and Humanoid ( 8seeds for each environment).\nWe use 2-D skills for Ant and Humanoid, 4-D skills for Quadruped, and 16discrete skills for\nHalfCheetah. The full qualitative results suggest that METRA discovers diverse locomotion be-\nhaviors regardless of the random seed.\n21", "start_char_idx": 0, "end_char_idx": 2561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d65c5c95-8e11-42b4-98c8-bd5e29fd1299": {"__data__": {"id_": "d65c5c95-8e11-42b4-98c8-bd5e29fd1299", "embedding": null, "metadata": {"page_label": "22", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74cd4ef2-e295-40df-81b6-e75b10d8727b", "node_type": "4", "metadata": {"page_label": "22", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "64973a6b98d67b779607feff4097cc570fff958239201e59b8edc720d467a1de", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAnt(States)HalfCheetah(States)Quadruped(Pixels)Humanoid(Pixels)\nFigure 11: Full qualitative results of METRA ( 8seeds). METRA learns diverse locomotion behaviors\nregardless of the random seed.\nAnt(States)Humanoid(Pixels)trajectoriestrajectoriesx-y\u03c6(s)Z=R2trajectoriestrajectoriesx-y\u03c6(s)S=R64\u00d764\u00d73Z=R2S=R29\nFigure 12: Latent space visualization. METRA learns to capture x-ycoordinates in two-dimensional latent\nspaces in both state-based Ant and pixel-based Humanoid, as they are the most temporally spread-out dimen-\nsions in the state space. We note that, with a higher-dimensional latent space (especially when Zis discrete),\nMETRA not only learns locomotion skills but also captures more diverse behaviors, as shown in the Cheetah\nand Kitchen videos on our project page.\nF.2 L ATENT SPACE VISUALIZATION\nMETRA simultaneously learns both the skill policy \u03c0(a|s, z)and the representation function \u03d5(s),\nto find the most \u201ctemporally spread-out\u201d manifold in the state space. We train METRA on state-\nbased Ant and pixel-based Humanoid with 2-D continuous latent spaces Z, and visualize the learned\nlatent space by plotting \u03d5(s)trajectories in Figure 12. Since the x-yplane corresponds to the most\ntemporally \u201cimportant\u201d manifold in both environments, METRA learns to capture the x-ycoordi-\nnates in two-dimensional \u03d5, regardless of the input representations (note that Humanoid is pixel-\nbased). We also note that, with a higher-dimensional latent space (especially when Zis discrete),\nMETRA not only learns locomotion skills but also captures more diverse, non-linear behaviors, as\nshown in the Cheetah and Kitchen videos on our project page.\nF.3 A BLATION STUDY OF LATENT SPACE SIZES\nTo demonstrate how the size of the latent space Zaffects skill learning, we train METRA with 1-\nD,2-D, and 4-D continuous skills and 2,4,8,16, and 24discrete skills on Ant and HalfCheetah.\nFigure 13 compares skills learned with different latent space sizes, which suggests that the diversity\nof skill generally increases as the capacity of Zgrows.\nF.4 A DDITIONAL BASELINES\nIn the main paper, we compare METRA with 11previous unsupervised exploration and unsuper-\nvised skill discovery methods. In this section, we additionally compare METRA with DGPO (Chen\net al., 2024), a method that aims to find diverse behaviors that maximize a task reward (Kumar\net al., 2020; Zhou et al., 2022; Zahavy et al., 2023a;b; Chen et al., 2024). Since we consider a con-\ntrolled Markov process without external rewards, we use only the intrinsic reward part of DGPO for\n22", "start_char_idx": 0, "end_char_idx": 2585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc83c643-b5a3-477a-b6cb-1e439d845885": {"__data__": {"id_": "dc83c643-b5a3-477a-b6cb-1e439d845885", "embedding": null, "metadata": {"page_label": "23", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89af7da1-d7fa-44d3-be3f-79a9a7533a1b", "node_type": "4", "metadata": {"page_label": "23", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a64da77f4201bd4a8c2a19cb892ef2249038d1006fc3d9f4044afcc655117949", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea86f588-f74c-416f-b4b8-16bbf98f863d", "node_type": "1", "metadata": {}, "hash": "244c1ad04d576f55ede149983d1534b756df4410c8633a4483e0214c945478bb", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAnt(States)HalfCheetah(States)Z=R1Z=R2Z=R4Z={1,2}Z={1,2,3,4}Z={1,...,8}Z={1,...,16}Z={1,...,24}\nFigure 13: Skills learned with different latent space sizes. Since METRA maximizes state coverage under\nthe capacity of the latent space Z, skills become more diverse as the capacity of Zgrows.\nTable 1: Comparison with DGPO. We compare METRA with an additional baseline for discrete skill learn-\ning, DGPO (Chen et al., 2024). METRA exhibits the best state coverage in both Ant and HalfCheetah. We use\n4random seeds and \u2018 \u00b1\u2019 denotes standard deviations.\nEnvironment (Metric) DIAYN DGPO METRA (ours )\nHalfCheetah (policy state coverage) 6.75\u00b12.22 6.75\u00b12.06 186.75\u00b116.21\nHalfCheetah (total state coverage) 19.50\u00b13.87 22.25\u00b15.85 177.75\u00b117.10\nAnt (policy state coverage) 11.25\u00b15.44 7.00\u00b13.83 1387 .75\u00b177.38\nAnt (total state coverage) 107.75\u00b117.00121.50\u00b14.366313 .25\u00b1747.92\ncomparison:\nrDGPO\nt = min\nz\u2032\u2208Z,z\u2032\u0338=zlogq(z|st+1)\nq(z|st+1) +q(z\u2032|st+1), (50)\nwhere qis a skill discriminator (Eysenbach et al., 2019a) and DGPO assumes that Zis a discrete\nspace. Intuitively, this objective encourages each behavior to be maximally different from the most\nsimilar other behavior.\nTable 1 presents the comparison results on HalfCheetah and Ant, where we train DIAYN, DGPO,\nand METRA with 16discrete skills for 10000 epochs ( 16M steps). Even though DGPO maximizes\n\u201cworst-case\u201d diversity (Equation (50)), it still maximizes a metric-agnostic KL divergence between\ndifferent skills (Chen et al., 2024), which leads to limited state coverage, as in DIAYN. In contrast,\nMETRA maximizes a metric-aware Wasserstein distance and thus shows significantly better state\ncoverage.\nG E XPERIMENTAL DETAILS\nWe implement METRA on top of the publicly available LSD codebase (Park et al., 2022). Our\nimplementation is available at https://github.com/seohongpark/METRA . For unsuper-\nvised skill discovery methods, we implement LSD (Park et al., 2022), CIC (Laskin et al., 2022), DI-\nAYN (Eysenbach et al., 2019a), and DADS (Sharma et al., 2020) on the same codebase as METRA.\nFor six exploration methods, ICM (Pathak et al., 2017), LBS (Mazzaglia et al., 2022), RND (Burda\net al., 2019), APT (Liu & Abbeel, 2021b), APS (Liu & Abbeel, 2021a), and Plan2Explore (Sekar\net al., 2020) (or Disagremeent (Pathak et al., 2019)), we use the original implementations by Laskin\net al. (2021) for state-based environments and the Dreamer (Hafner et al., 2020) variants by Rajeswar\net al. (2023) for pixel-based environments. For LEXA (Mendonca et al., 2021) in Section 5.3, we\nuse the original implementation by Mendonca et al. (2021). We run our experiments on an internal\ncluster consisting of A5000 GPUs. Each run in Section 5.3 takes no more than 24 hours.\nG.1 E NVIRONMENTS\nBenchmark environments. For state-based environments, we use the same MuJoCo HalfCheetah\nand Ant environments (Todorov et al., 2012; Brockman et al., 2016) as previous work (Sharma\net al., 2020; Park et al., 2022; 2023b).", "start_char_idx": 0, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea86f588-f74c-416f-b4b8-16bbf98f863d": {"__data__": {"id_": "ea86f588-f74c-416f-b4b8-16bbf98f863d", "embedding": null, "metadata": {"page_label": "23", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89af7da1-d7fa-44d3-be3f-79a9a7533a1b", "node_type": "4", "metadata": {"page_label": "23", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a64da77f4201bd4a8c2a19cb892ef2249038d1006fc3d9f4044afcc655117949", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc83c643-b5a3-477a-b6cb-1e439d845885", "node_type": "1", "metadata": {"page_label": "23", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c9574cf77f5e891db849a05c351e9a83794ef859a9a0c29203722ec02f47943c", "class_name": "RelatedNodeInfo"}}, "text": "(2021) for state-based environments and the Dreamer (Hafner et al., 2020) variants by Rajeswar\net al. (2023) for pixel-based environments. For LEXA (Mendonca et al., 2021) in Section 5.3, we\nuse the original implementation by Mendonca et al. (2021). We run our experiments on an internal\ncluster consisting of A5000 GPUs. Each run in Section 5.3 takes no more than 24 hours.\nG.1 E NVIRONMENTS\nBenchmark environments. For state-based environments, we use the same MuJoCo HalfCheetah\nand Ant environments (Todorov et al., 2012; Brockman et al., 2016) as previous work (Sharma\net al., 2020; Park et al., 2022; 2023b). HalfCheetah has an 18-dimensional state space and Ant\nhas a 29-dimensional state space. For pixel-based environments, we use pixel-based Quadruped\nand Humanoid from the DeepMind Control Suite (Tassa et al., 2018) and a pixel-based version of\n23", "start_char_idx": 2383, "end_char_idx": 3242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6757028e-e15b-43ac-bf0c-24a14ff3be3a": {"__data__": {"id_": "6757028e-e15b-43ac-bf0c-24a14ff3be3a", "embedding": null, "metadata": {"page_label": "24", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f12ccd9-3bcc-4199-9ee0-bcd3146cb88a", "node_type": "4", "metadata": {"page_label": "24", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0c81b2acd8c82b5141481e8f538d77ea2e42420a3155b874dd84132edc6348c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c503547b-7ec9-49e2-a8d8-8b8681bf335f", "node_type": "1", "metadata": {}, "hash": "b3efa69db2f82d26617a4574c4cd7744838470f223aace9252f266065ced8101", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nKitchen by Gupta et al. (2019); Mendonca et al. (2021). In DMC locomotion environments, we\nuse gradient-colored floors to allow the agent to infer its location from pixels, similarly to Hafner\net al. (2022); Park et al. (2023a). In Kitchen, we use the same camera setting as LEXA (Mendonca\net al., 2021). Pixel-based environments have an observation space of 64\u00d764\u00d73, and we do not\nuse any proprioceptive state information. The episode length is 200for Ant and HalfCheetah, 400\nfor Quadruped and Humanoid, and 50for Kitchen. We use an action repeat of 2for pixel-based\nQuadruped and Humanoid, following Mendonca et al. (2021). In our experiments, we do not use\nany prior knowledge or supervision, such as the x-yprior (Eysenbach et al., 2019a; Sharma et al.,\n2020), or early termination (Park et al., 2022).\nMetrics. For the state coverage metric in locomotion environments, we count the number of 1\u00d71-\nsized x-ybins (Ant, Quadruped, and Humanoid) or 1-sized xbins (HalfCheetah) that are occupied\nby any of the target trajectories. In Kitchen, we count the number of pre-defined tasks achieved by\nany of the target trajectories, where we use the same six pre-defined tasks as Mendonca et al. (2021):\nKettle (K), Microwave (M), Light Switch (LS), Hinge Cabinet (HC), Slide Cabinet (SC), and Bottom\nBurner (BB). Each of the three types of coverage metrics, policy state coverage (Figures 5 and 7),\nqueue state coverage (Figure 7), and total state coverage (Figure 7), uses different target trajectories.\nPolicy state coverage, which is mainly for skill discovery methods, is computed by 48deterministic\ntrajectories with 48randomly sample skills at the current epoch. Queue state coverage is computed\nby the most recent 100000 training trajectories up to the current epoch. Total state coverage is\ncomputed by the entire training trajectories up to the current epoch.\nDownstream tasks. For quantitative comparison of skill discovery methods (Figure 6), we use\nfive downstream tasks, AntMultiGoals, HalfCheetahGoal, HalfCheetahHurdle, QuadrupedGoal,\nand HumanoidGoal, mostly following the prior work (Park et al., 2022). In HalfCheetahGoal,\nQuadrupedGoal, and HumanoidGoal, the task is to reach a target goal (within a radius of 3) ran-\ndomly sampled from [\u2212100,100],[\u22127.5,7.5]2, and [\u22125,5]2, respectively. The agent receives a re-\nward of 10when it reaches the goal. In AntMultiGoals, the task is to reach four target goals (within\na radius of 3), where each goal is randomly sampled from [sx\u22127.5, sx+ 7.5]\u00d7[sy\u22127.5, sy+ 7.5],\nwhere (sx, sy)is the agent\u2019s current x-yposition. The agent receives a reward of 2.5whenever it\nreaches the goal. A new goal is sampled when the agent either reaches the previous goal or fails\nto reach it within 50steps. In HalfCheetahHurdle (Qureshi et al., 2020), the task is to jump over\nmultiple hurdles. The agent receives a reward of 1whenever it jumps over a hurdle. The episode\nlength is 200for state-based environments and 400for pixel-based environments.\nFor quantitative comparison with LEXA (Figure 8), we use five goal-conditioned tasks. In locomo-\ntion environments, goals are randomly sampled from [\u2212100,100] (HalfCheetah), [\u221250,50]2(Ant),\n[\u221215,15]2(Quadruped), or [\u221210,10]2(Humanoid). We provide the full state as a goal g, whose\ndimensionality is 18for HalfCheetah, 29for Ant, and 64\u00d764\u00d73for pixel-based Quadruped and\nHumanoid.", "start_char_idx": 0, "end_char_idx": 3413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c503547b-7ec9-49e2-a8d8-8b8681bf335f": {"__data__": {"id_": "c503547b-7ec9-49e2-a8d8-8b8681bf335f", "embedding": null, "metadata": {"page_label": "24", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f12ccd9-3bcc-4199-9ee0-bcd3146cb88a", "node_type": "4", "metadata": {"page_label": "24", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0c81b2acd8c82b5141481e8f538d77ea2e42420a3155b874dd84132edc6348c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6757028e-e15b-43ac-bf0c-24a14ff3be3a", "node_type": "1", "metadata": {"page_label": "24", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "86b36469408cb89d8c2d7c0d0a54b92c03a977afca6ffe4a131e8c3968d51652", "class_name": "RelatedNodeInfo"}}, "text": "In HalfCheetahHurdle (Qureshi et al., 2020), the task is to jump over\nmultiple hurdles. The agent receives a reward of 1whenever it jumps over a hurdle. The episode\nlength is 200for state-based environments and 400for pixel-based environments.\nFor quantitative comparison with LEXA (Figure 8), we use five goal-conditioned tasks. In locomo-\ntion environments, goals are randomly sampled from [\u2212100,100] (HalfCheetah), [\u221250,50]2(Ant),\n[\u221215,15]2(Quadruped), or [\u221210,10]2(Humanoid). We provide the full state as a goal g, whose\ndimensionality is 18for HalfCheetah, 29for Ant, and 64\u00d764\u00d73for pixel-based Quadruped and\nHumanoid. In Kitchen, we use the same six (single-task) goal images and tasks as Mendonca et al.\n(2021). We measure the distance between the goal and the final state in locomotion environments\nand the number of successful tasks in Kitchen.\nG.2 I MPLEMENTATION DETAILS\nUnsupervised skill discovery methods. For skill discovery methods, we use 2-D continuous skills\nfor Ant and Humanoid, 4-D continuous skills for Quadruped, 16discrete skills for HalfCheetah,\nand24discrete skills for Kitchen, where continuous skills are sampled from the standard Gaussian\ndistribution, and discrete skills are uniformly sampled from the set of zero-centered one-hot vec-\ntors (Park et al., 2022). METRA and LSD use normalized vectors ( i.e.,z/\u2225z\u22252) for continuous\nskills, as their objectives are invariant to the magnitude of z. For CIC, we use 64-D continuous skills\nfor all environments, following the original suggestion (Laskin et al., 2022), and we found that using\n64-D skills for CIC leads to better state coverage than using 2-D or 4-D skills. We present the full\nlist of hyperparameters used for skill discovery methods in Table 2.\nUnsupervised exploration methods. For unsupervised exploration methods and LEXA, we use the\noriginal implementations and hyperparameters (Laskin et al., 2021; Mendonca et al., 2021; Rajeswar\net al., 2023). For LEXA\u2019s goal-conditioned policy (achiever), we test both the temporal distance and\ncosine distance variants and use the former as it leads to better performance.\n24", "start_char_idx": 2790, "end_char_idx": 4901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c873f19-8722-43cf-b07b-ff14d286120f": {"__data__": {"id_": "7c873f19-8722-43cf-b07b-ff14d286120f", "embedding": null, "metadata": {"page_label": "25", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e364d65-4c36-48ae-9925-4e244c0066b3", "node_type": "4", "metadata": {"page_label": "25", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9dc3e599d61bb559db22b53cade5074eb132bc78745c5f00d9e77879acfa9cb4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 2: Hyperparameters for unsupervised skill discovery methods.\nHyperparameter Value\nLearning rate 0.0001\nOptimizer Adam (Kingma & Ba, 2015)\n# episodes per epoch 8\n# gradient steps per epoch200(Quadruped, Humanoid),\n100(Kitchen), 50(Ant, HalfCheetah)\nMinibatch size 256\nDiscount factor \u03b3 0.99\nReplay buffer size106(Ant, HalfCheetah), 105(Kitchen),\n3\u00d7105(Quadruped, Humanoid)\nEncoder CNN (LeCun et al., 1989)\n# hidden layers 2\n# hidden units per layer 1024\nTarget network smoothing coefficient 0.995\nEntropy coefficient0.01 (Kitchen),\nauto-adjust (Haarnoja et al., 2018b) (others)\nMETRA \u03b5 10\u22123\nMETRA initial \u03bb 30\nTable 3: Hyperparameters for PPO high-level controllers.\nHyperparameter Value\n# episodes per epoch 64\n# gradient steps per episode 10\nMinibatch size 256\nEntropy coefficient 0.01\nGAE \u03bb(Schulman et al., 2016) 0.95\nPPO clip threshold \u03f5 0.2\nHigh-level controllers for downstream tasks. In Figure 6, we evaluate learned skills on down-\nstream tasks by training a high-level controller \u03c0h(z|s, stask)that selects a skill every K= 25 (Ant\nand HalfCheetah) or K= 50 (Quadruped and Humanoid) environment steps, where staskdenotes\nthe task-specific information: the goal position (\u2018-Goal\u2019 or \u2018-MultiGoals\u2019 tasks) or the next hurdle\nposition and distance (HalfCheetahHurdle). At every Ksteps, the high-level policy selects a skill\nz, and then the pre-trained low-level skill policy \u03c0(a|s, z)executes the same zforKsteps. We\ntrain high-level controllers with PPO (Schulman et al., 2017) for discrete skills and SAC (Haarnoja\net al., 2018a) for continuous skills. For SAC, we use the same hyperparameters as unsupervised\nskill discovery methods (Table 2), and we present the full list of PPO-specific hyperparameters in\nTable 3.\nZero-shot goal-conditioned RL. In Figure 8, we evaluate the zero-shot performances of METRA,\nLSD, DIAYN, and LEXA on goal-conditioned downstream tasks. METRA and LSD use the proce-\ndure described in Section 4.2 to select skills. We re-compute zevery step for locomotion environ-\nments, but in Kitchen, we use the same zselected at the first step throughout the episode, as we find\nthat this leads to better performance. DIAYN chooses zbased on the output of the skill discriminator\nat the goal state ( i.e.,q(z|g), where qdenotes the skill discriminator of DIAYN). LEXA uses the\ngoal-conditioned policy (achiever), \u03c0(a|s, g).\n25", "start_char_idx": 0, "end_char_idx": 2404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62d60eb9-9c59-4d35-ab76-5ac97ba09e7c": {"__data__": {"id_": "62d60eb9-9c59-4d35-ab76-5ac97ba09e7c", "embedding": null, "metadata": {"page_label": "1", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4983977d-78bf-49bb-a0c5-75c3f7bdfce8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4ffcade3975ad210f145bdbbe82513479e556affec2a2c71fa4065b2c8fbc813", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nSELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND\nCRITIQUE THROUGH SELF-REFLECTION\nAkari Asai\u2020, Zeqiu Wu\u2020, Yizhong Wang\u2020\u00a7, Avirup Sil\u2021, Hannaneh Hajishirzi\u2020\u00a7\n\u2020University of Washington\u00a7Allen Institute for AI\u2021IBM Research AI\n{akari,zeqiuwu1,yizhongw,hannaneh }@cs.washington.edu ,avi@us.ibm.com\nABSTRACT\nDespite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\neration ( SELF-RAG)that enhances an LM\u2019s quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called reflection tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\nRAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models.1\n1 I NTRODUCTION\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages.\nThis work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n1Our code and trained models are available at https://selfrag.github.io/ .\n1", "start_char_idx": 0, "end_char_idx": 4075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba91977e-5db3-49d1-b622-0492eeb4d153": {"__data__": {"id_": "ba91977e-5db3-49d1-b622-0492eeb4d153", "embedding": null, "metadata": {"page_label": "2", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df932b3f-cc3b-4ace-8670-dd32e92af638", "node_type": "4", "metadata": {"page_label": "2", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b8447fb3c1d7dff5279d5e88aea2e3d7489c2d52dcdcfd1554fa07aaf67e217c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4b8efdf-2b68-4cd2-af11-48f5da910a63", "node_type": "1", "metadata": {}, "hash": "42908a788a07db0d6e5d7ad534aaaedcf41b5b45db80d82c4eef1b7a50070b53", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nStep 1: Retrieve K documentsCalifornia was named after a \ufb01ctional island in a Spanish book. Prompt How did US states get their names? \nUS states got their names from a variety of sources. Eleven states are named after an individual person (e.g, California was named after Christopher Columbus). Some states including Texas and Utah, are named after Native American tribe.\nRetrieval-Augmented Generation (RAG)Ours: Self-re\ufb02ective Retrieval-Augmented Generation (Self-RAG) \nPopular names by states. In Texas, Emma is a popular baby name. Of the \ufb01fty states, eleven are named after an individual person. \nPrompt How did US states get their names? + Step 2: Prompt LM with K docs and generateRetriever\nLM\nPrompt How did US states get their names? US states got their names from a variety of sources. RetrieveStep 1: Retrieve on demand  \nPrompt +  \n11 of 50 state namesRelevant\nStep 2: Generate segment in parallel \ncome from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Sergas de Esplandi\u00e1n. California's name has itsRelevantPartially\nUS states got their names from a variety of sources. 11 of 50 states names are come from persons.    26 states are named after Native Americans, including Utah. \nPrompt: Write an essay of your best summer vacation\nPrompt: Write an essay of your best summer vacation\nNo RetrievalMy best summer vacation is when my family and I embarked on a road trip along \u2026My best\u2026 \n>Repeat.\u2026\nNo information in passagesContradictory>Prompt +  \nPrompt +  \nRetrieve\nFigure 1: Overview of SELF-RAG.SELF-RAGlearns to retrieve, critique, and generate text passages\nto enhance overall generation quality, factuality, and verifiability.\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\ngeneration quality. Moreover, SELF-RAGprovides citations for each segment with its self-assessment\nof whether the output is supported by the passage, leading to easier fact verification.\nSELF-RAGtrains an arbitrary LM to generate text with reflection tokens by unifying them as the\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\nassess its own predictions after each generated segment as an integral part of the generation output.\nSELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints,\nwhich are defined by reflection token predictions. In particular, our inference-time algorithm enables\nus to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\nmodels\u2019 behaviors to user preferences by leveraging reflection tokens through segment-level beam\nsearch using the weighted linear sum of the reflection token probabilities as segment score.\nEmpirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF-\nRAGsignificantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF-RAGoutperforms\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\nreflection tokens for overall performance improvements as well as test-time model customizations\n(e.g., balancing the trade-off between citation previsions and completeness).\n2 R ELATED WORK\nRetrieval-Augmented Generation.", "start_char_idx": 0, "end_char_idx": 4397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4b8efdf-2b68-4cd2-af11-48f5da910a63": {"__data__": {"id_": "b4b8efdf-2b68-4cd2-af11-48f5da910a63", "embedding": null, "metadata": {"page_label": "2", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df932b3f-cc3b-4ace-8670-dd32e92af638", "node_type": "4", "metadata": {"page_label": "2", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b8447fb3c1d7dff5279d5e88aea2e3d7489c2d52dcdcfd1554fa07aaf67e217c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba91977e-5db3-49d1-b622-0492eeb4d153", "node_type": "1", "metadata": {"page_label": "2", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6725eeae540219dd29b523ceefcb53f0eb734515ddb131ff2af559a2f7a23946", "class_name": "RelatedNodeInfo"}}, "text": "Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF-\nRAGsignificantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF-RAGoutperforms\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\nreflection tokens for overall performance improvements as well as test-time model customizations\n(e.g., balancing the trade-off between citation previsions and completeness).\n2 R ELATED WORK\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input\nspace of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\nimprovements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\net al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\n2", "start_char_idx": 3680, "end_char_idx": 4747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f7f2756-94a9-4400-9382-de7b06b42644": {"__data__": {"id_": "8f7f2756-94a9-4400-9382-de7b06b42644", "embedding": null, "metadata": {"page_label": "3", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79b9ad7d-6921-41e3-9f85-5ce9fce13116", "node_type": "4", "metadata": {"page_label": "3", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b0b3abe8175c67d3614c81ebd155addc252aa8e6f5a9eb824ab4c42999dea505", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a31181d-d2f4-460d-aaf4-efe6bcc7c5a6", "node_type": "1", "metadata": {}, "hash": "553c4c980c9c197e2c331f0ca7a8280a36fa33d6eb983eb8f772a439588ba885", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nType Input Output Definitions\nRetrieve x/x, y {yes, no, continue } Decides when to retrieve with R\nISREL x, d {relevant , irrelevant } dprovides useful information to solve x.\nISSUP x, d, y {fully supported , partially\nsupported, no support }All of the verification-worthy statement in y\nis supported by d.\nISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x.\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\nthe most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nrun-time efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack\nof attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022; Wu et al., 2023). Though\nour work also studies fine-grained critique on retrieval and generation, we train our target LM on task\nexamples augmented with reflection tokens from a critic model offline, with a far lower training cost\ncompared to RLHF. Compared to prior work using control tokens to guide LM generation (Lu et al.,\n2022; Korbak et al., 2023), SELF-RAGuses reflection tokens to decide the need for retrieval and to\nself-evaluate generation quality.\n3 S ELF-RAG: LEARNING TO RETRIEVE , GENERATE AND CRITIQUE\nWe introduce Self-Reflective Retrieval-Augmented Generation ( SELF-RAG), shown in Figure 1.\nSELF-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM Mgenerate text informed by retrieved passages, if needed, and criticize the output by\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\nor confirm the output\u2019s relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\nFormally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple\nsegments y= [y1, . . . , y T], where ytindicates a sequence of tokens for the t-th segment.2Generated\ntokens in ytinclude text from the original vocabulary as well as the reflection tokens (Table 1).\nInference overview. Figure 1 and Algorithm 1 present an overview of S ELF-RAGat inference. For\nevery xand preceding generation y<t, the model decodes a retrieval token to evaluate the utility\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\npassage\u2019s relevance, the next response segment, and a critique token to evaluate if the information in\nthe response segment is supported by the passage.", "start_char_idx": 0, "end_char_idx": 3952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a31181d-d2f4-460d-aaf4-efe6bcc7c5a6": {"__data__": {"id_": "6a31181d-d2f4-460d-aaf4-efe6bcc7c5a6", "embedding": null, "metadata": {"page_label": "3", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79b9ad7d-6921-41e3-9f85-5ce9fce13116", "node_type": "4", "metadata": {"page_label": "3", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b0b3abe8175c67d3614c81ebd155addc252aa8e6f5a9eb824ab4c42999dea505", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f7f2756-94a9-4400-9382-de7b06b42644", "node_type": "1", "metadata": {"page_label": "3", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3109f971081e2a0f93db3bcac7cf4241cab1b93807fd9d38419df93bb46b9dcd", "class_name": "RelatedNodeInfo"}}, "text": ". . , y T], where ytindicates a sequence of tokens for the t-th segment.2Generated\ntokens in ytinclude text from the original vocabulary as well as the reflection tokens (Table 1).\nInference overview. Figure 1 and Algorithm 1 present an overview of S ELF-RAGat inference. For\nevery xand preceding generation y<t, the model decodes a retrieval token to evaluate the utility\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\npassage\u2019s relevance, the next response segment, and a critique token to evaluate if the information in\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\nutility of the response.3To generate each segment, SELF-RAGprocesses multiple passages in parallel\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\n2In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n3We follow Liu et al. (2023a) in using a \u201cperceived\u201d utility value that is independent of retrieved passages.\n3", "start_char_idx": 3222, "end_char_idx": 4463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b04189d5-13cb-443e-9e95-f5b0e74f400a": {"__data__": {"id_": "b04189d5-13cb-443e-9e95-f5b0e74f400a", "embedding": null, "metadata": {"page_label": "4", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f4c4639-d618-4a73-86ea-c46f2d9aa921", "node_type": "4", "metadata": {"page_label": "4", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6241fbc51d7af3422020cc5ad7e490f1196dd880a92733db6acb4853bd29d488", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1 SELF-RAGInference\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}\n1:Input: input prompt xand preceding generation y<t,Output: next output segment yt\n2:Mpredicts Retrieve given (x, y<t)\n3:ifRetrieve ==Yes then\n4: Retrieve relevant text passages DusingRgiven (x, yt\u22121) \u25b7Retrieve\n5: Mpredicts ISRELgiven x, dandytgiven x, d, y <tfor each d\u2208D \u25b7Generate\n6: Mpredicts ISSUPand ISUSEgiven x, yt, dfor each d\u2208D \u25b7Critique\n7: Rank ytbased on ISREL,ISSUP,ISUSE \u25b7Detailed in Section 3.3\n8:else if Retrieve ==Nothen\n9: Mgenpredicts ytgiven x \u25b7 Generate\n10: Mgenpredicts ISUSEgiven x, yt \u25b7Critique\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\nd1is selected at the first time step since d2does not provide direct evidence ( ISRELis Irrelevant)\nandd3output is only partially supported while d1are fully supported.\nTraining overview. SELF-RAGenables an arbitrary LM to generate text with reflection tokens\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\nvocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus\nwith interleaving passages retrieved by a retriever Rand reflection tokens predicted by a critic model\nC(summarized in Appendix Algorithm 2). We train Cto generate reflection tokens for evaluating\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\ntrain the final generator model ( M) using the conventional LM objective (Section 3.2.2) to enable\nMto generate reflection tokens by itself without relying on the critic at inference time.\n3.2 S ELF-RAGTRAINING\nHere, we describe the supervised data collection and training of two models, the critic C(Section 3.2.1)\nand the generator M(Section 3.2.2).\n3.2.1 T RAINING THE CRITIC MODEL\nData collection for critic model. Manual annotation of reflection tokens for each segment is\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\nused to generate such feedback (Liu et al., 2023b). However, depending on such proprietary LMs\ncan raise API costs and diminish reproducibility (Chen et al., 2023). Our method requires fine-\ngrained evaluations on multiple passages as well as segments for each input-output instance from the\ntraining dataset, increasing the number of evaluations required to generate SELF-RAGtraining data\nexponentially. To overcome those issues, we create supervised data by prompting GPT-4 to generate\nreflection tokens and then distill their knowledge into an in-house C. For each group of reflection\ntokens, we randomly sample instances from the original training data: {Xsample, Ysample} \u223c\n{X, Y}. As different reflection token groups have their definitions and input, as shown in Table 1,\nwe use different instruction prompts for them. Here, we use Retrieve as an example. We prompt\nGPT-4 with a type-specific instruction (\u201cGiven an instruction, make a judgment on whether finding\nsome external documents from the web helps to generate a better response.\u201d) followed by few-shot\ndemonstrations Ithe original task input xand output yto predict an appropriate reflection token\nas text: p(r|I, x, y ). Manual assessment reveals that GPT-4 reflection token predictions show high\nagreement with human evaluations. We collect 4k-20k supervised training data for each type and\ncombine them to form training data for C. Appendix Section D shows the full list of instructions, and\nA.1 contains more details and our analysis.\nCritic learning. After we collect training data Dcritic , we initialize Cwith a pre-trained LM and\ntrain it on Dcritic using a standard conditional language modeling objective, maximizing likelihood:\nmax\nCE((x,y),r)\u223cDcritic logpC(r|x, y), rfor reflection tokens. (1)\n4", "start_char_idx": 0, "end_char_idx": 3992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8938a62-277f-4ab6-ab7b-8834546eb727": {"__data__": {"id_": "f8938a62-277f-4ab6-ab7b-8834546eb727", "embedding": null, "metadata": {"page_label": "5", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b80e9edb-4c3a-4660-82d6-6d3cc258b82e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6d440bc1825942c8daaa6d39ff7186a056ca8bc83859e480416face2b811523d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f88f3f7f-7677-4f38-b7ad-0b343f57deba", "node_type": "1", "metadata": {}, "hash": "7b809b82c08d8297ca610b67f204024622ef95a0219c6044cea6604d6b89e7ba", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInput: How did US states get their names? Input: Write an essay of your best summer vacationOutput: My best summer vacation was a magical escape to the coastal town of Santorini. The azure waters, charming white-washed building are unforgettable. \nCritic LMOutput: 1 of 50 states names come from persons. For instance, Louisiana was named in honor of King Louis XIV of France and Georgia was named after King George II. \nRetrieve\nPartially\nAugmented Output:                Retrieve<p>LOUISIANA: Named in<p>Of the \ufb01fty states, eleven are named after an individual person</p>.               11 of 50 states\u2019 names come from person. RelevantSupportedhonor of Louis XIV of France.</p>.  RelevantFor instance, Louisiana was named after King Louis XIV, andUtil: 5Georgia was named after King George II. \nUtil: 5Augmented Output:                     My best summer vacation was a magical escape to the coastal town of Santorini.                     The azure waters, charming white-washed building are unforgettable experience.No RetrievalNo Retrieval\nRetriever\nFigure 2: SELF-RAGtraining examples. The left example does not require retrieval while the right\none requires retrieval; thus, passages are inserted. More examples are in Appendix Table 4.\nThough the initial model can be any pre-trained LM, we use the same one as the generator LM\n(i.e., Llama 2-7B; Touvron et al. 2023) for Cinitialization. The critic achieves a higher than 90%\nagreement with GPT-4-based predictions on most reflection token categories (Appendix Table 4).\n3.2.2 T RAINING THE GENERATOR MODEL\nData collection for generator. Given an input-output pair (x, y), we augment the original output\nyusing the retrieval and critic models to create supervised data that precisely mimics the SELF-\nRAGinference-time process (Section 3.1). For each segment yt\u2208y, we run Cto assess whether\nadditional passages could help to enhance generation. If retrieval is required, the retrieval special\ntoken Retrieve =Yes is added, and Rretrieves the top Kpassages, D. For each passage, Cfurther\nevaluates whether the passage is relevant and predicts ISREL. If a passage is relevant, Cfurther\nevaluates whether the passage supports the model generation and predicts ISSUP. Critique tokens\nISRELand ISSUPare appended after the retrieved passage or generations. At the end of the output, y\n(oryT),Cpredicts the overall utility token ISUSE, and an augmented output with reflection tokens\nand the original input pair is added to Dgen. See the example training data in Figure 2.\nGenerator learning. We train the generator model Mby training on the curated corpus augmented\nwith reflection tokens Dgenusing the standard next token objective:\nmax\nME(x,y,r )\u223cDgenlogpM(y, r|x). (2)\nUnlike Ctraining (Eq. 1), Mlearns to predict the target output as well as the reflection tokens. During\ntraining, we mask out the retrieved text chunks (surrounded by <p> and</p> in Figure 2) for loss\ncalculation and expand the original vocabulary Vwith a set of reflection tokens {Critique ,Retrieve}.\nConnections to prior work on learning with critique. Recent work incorporates additional\ncritique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on\nseparate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\nlearns to generate special tokens to evaluate its own prediction after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n3.3 S ELF-RAGINFERENCE\nGenerating reflection tokens to self-evaluate its own output makes SELF-RAGcontrollable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements.", "start_char_idx": 0, "end_char_idx": 4063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f88f3f7f-7677-4f38-b7ad-0b343f57deba": {"__data__": {"id_": "f88f3f7f-7677-4f38-b7ad-0b343f57deba", "embedding": null, "metadata": {"page_label": "5", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b80e9edb-4c3a-4660-82d6-6d3cc258b82e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6d440bc1825942c8daaa6d39ff7186a056ca8bc83859e480416face2b811523d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8938a62-277f-4ab6-ab7b-8834546eb727", "node_type": "1", "metadata": {"page_label": "5", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5fb0360e4a570db4a467069852f275d76ca1b64fbb2d5afc7fb3bad4403bb46f", "class_name": "RelatedNodeInfo"}}, "text": "2022) via PPO. While PPO relies on\nseparate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\nlearns to generate special tokens to evaluate its own prediction after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n3.3 S ELF-RAGINFERENCE\nGenerating reflection tokens to self-evaluate its own output makes SELF-RAGcontrollable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\ncontrol to meet these distinct objectives during the inference process.\nAdaptive retrieval with threshold. SELF-RAGdynamically decides when to retrieve text passages by\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\n5", "start_char_idx": 3224, "end_char_idx": 4762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "746de1b2-5dbd-42f4-940c-dca15ca3ce3d": {"__data__": {"id_": "746de1b2-5dbd-42f4-940c-dca15ca3ce3d", "embedding": null, "metadata": {"page_label": "6", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a33e732d-be5a-4593-921f-33d103453654", "node_type": "4", "metadata": {"page_label": "6", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d547bc7046a1331d604275249a1f863de239a529a46a3cc9772aa252d478a466", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bced8172-7c3c-4699-bd87-150d5861883c", "node_type": "1", "metadata": {}, "hash": "87b908b4073b28445d07cb1397d8438be8782eeda57586739dbad0825570eed6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\ndesignated threshold, we trigger retrieval (details in Appendix Section A.4).\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\non hard or soft conditions, Rretrieves Kpassages, and the generator Mprocesses each passage in\nparallel and outputs Kdifferent continuation candidates. We conduct a segment-level beam search\n(with the beam size= B) to obtain the top- Bsegment continuations at each timestamp t, and return\nthe best sequence at the end of generation. The score of each segment ytwith respect to passage dis\nupdated with a critic score Sthat is the linear weighted sum of the normalized probability of each\nCritique token type. For each critique token group G(e.g., ISREL), we denote its score at timestamp\ntassG\nt, and we compute a segment score as follows:\nf(yt, d, Critique ) =p(yt|x, d, y <t)) +S(Critique ),where (3)\nS(Critique ) =X\nG\u2208GwGsG\ntforG={ISREL,ISSUP,ISUSE}, (4)\nwhere sG\nt=pt(\u02c6r)PNG\ni=1pt(ri)stands for the generation probability of the most desirable reflection token\n\u02c6r(e.g., ISREL=Relevant ) for the critique token type GwithNGdistinct tokens (that represent\ndifferent possible values for G). The weights wGin Eq. 4 are hyperparameters that can be adjusted\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\nyis mostly supported by evidence, we can set a weight term for the ISSUPscore higher, while\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\nduring decoding using Critique e.g., filtering out a segment continuation when the model generates an\nundesirable token (e.g., ISSUP=No support ).\n4 E XPERIMENTS\n4.1 T ASKS AND DATASETS\nWe conduct evaluations of our SELF-RAGand diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\nour experiments\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\nClosed-set tasks include two datasets, i.e., a fact verification dataset about public health ( PubHealth ;\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\nChallenge ; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\naggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\nShort-form generations tasks include two open-domain question answering (QA) datasets,\nPopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need\nto answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset,\nconsisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\nTriviaQA-unfiltered (open) test set is not publicly available, we follow prior work\u2019s validation and\ntest split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\nperformance based on whether gold answers are included in the model generations instead of strictly\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\nQA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022).", "start_char_idx": 0, "end_char_idx": 3694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bced8172-7c3c-4699-bd87-150d5861883c": {"__data__": {"id_": "bced8172-7c3c-4699-bd87-150d5861883c", "embedding": null, "metadata": {"page_label": "6", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a33e732d-be5a-4593-921f-33d103453654", "node_type": "4", "metadata": {"page_label": "6", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d547bc7046a1331d604275249a1f863de239a529a46a3cc9772aa252d478a466", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "746de1b2-5dbd-42f4-940c-dca15ca3ce3d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d5eb28a91536e7b2237727e19e4e30a1792c4b14147090e89a5fc1229e89a40b", "class_name": "RelatedNodeInfo"}}, "text": "For PopQA, we use the long-tail subset,\nconsisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\nTriviaQA-unfiltered (open) test set is not publicly available, we follow prior work\u2019s validation and\ntest split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\nperformance based on whether gold answers are included in the model generations instead of strictly\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\nQA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\nMAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA.4\n4.2 B ASELINES\nBaselines without retrievals. We evaluate strong publicly available pre-trained LLMs,\nLlama2 7B,13B(Touvron et al., 2023), instruction-tuned models, Alpaca 7B,13B(Dubois et al., 2023)\n4https://github.com/princeton-nlp/ALCE\n6", "start_char_idx": 3011, "end_char_idx": 4160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bddfb8b1-5354-4d1e-8cf4-3a179b0179fc": {"__data__": {"id_": "bddfb8b1-5354-4d1e-8cf4-3a179b0179fc", "embedding": null, "metadata": {"page_label": "7", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a18f9b68-e101-4e5a-89fa-cde23ec2bcf8", "node_type": "4", "metadata": {"page_label": "7", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "09d587141363be2758038f0db76f808c986d0857d54e32946b03b8754cccae13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1a92455-61b4-4d25-a199-0a7f10200560", "node_type": "1", "metadata": {}, "hash": "e1d11c69bcaa105d75c810372e49ec81f1710f2ee57e08b99e9cbaf1de584411", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(our replication based on Llama2); and models trained and reinforced using private data, Chat-\nGPT (Ouyang et al., 2022) and Llama2-chat 13B. For instruction-tuned LMs, we use the official\nsystem prompt or instruction format used during training if publicly available. We also compare our\nmethod to concurrent work, CoVE 65B(Dhuliawala et al., 2023), which introduces iterative prompt\nengineering to improve the factuality of LLM generations.\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training.\nThe first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\ngiven the query prepended with the top retrieved documents using the same retriever as in our system.\nIt also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\nreflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\nwith LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\naugmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\nsystem. The second category includes concurrent methods that are trained with retrieved text\npassages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\nto pre-train an LM with API calls (e.g., Wikipedia APIs).5\n4.3 E XPERIMENTAL SETTINGS\nTraining data and settings. Our training data consists of diverse instruction-following input-output\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\nour generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\ndocuments for each input. More training details are in the Appendix Section B.1.\nInference settings. As a default configuration, we assign the weight terms ISREL,ISSUP,ISUSE\nvalues of 1.0, 1.0 and 0.5, respectively. To encourage frequent retrieval, we set the retrieval threshold\nto 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed\nup inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2.\nFor a token-level generation, we use greedy decoding. By default, we use the top five documents\nfrom Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we\nuse additional top five documents retrieved by a web search engine, following Luo et al. (2023);\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\nbaselines for a fair comparison.\n5 R ESULTS AND ANALYSIS\n5.1 M AINRESULTS\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\nretrieval. Our SELF-RAG(bottom two rows) demonstrates a substantial performance advantage\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\niteratively prompts Llama2 65Bto refine output.\nComparison against baselines with retrieval.", "start_char_idx": 0, "end_char_idx": 3727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1a92455-61b4-4d25-a199-0a7f10200560": {"__data__": {"id_": "f1a92455-61b4-4d25-a199-0a7f10200560", "embedding": null, "metadata": {"page_label": "7", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a18f9b68-e101-4e5a-89fa-cde23ec2bcf8", "node_type": "4", "metadata": {"page_label": "7", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "09d587141363be2758038f0db76f808c986d0857d54e32946b03b8754cccae13", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bddfb8b1-5354-4d1e-8cf4-3a179b0179fc", "node_type": "1", "metadata": {"page_label": "7", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8ae9f8243adc8b045d46243c41c8e1d14f11eb400611462a8edc9156dc8b3d81", "class_name": "RelatedNodeInfo"}}, "text": "5 R ESULTS AND ANALYSIS\n5.1 M AINRESULTS\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\nretrieval. Our SELF-RAG(bottom two rows) demonstrates a substantial performance advantage\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\niteratively prompts Llama2 65Bto refine output.\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-\nbased models on all tasks. Powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca)\nshow large gains from their non-retrieval baselines. However, we found that these baselines provide\nlimited solutions for tasks where we cannot simply copy or extract sub-strings of retrieved passages.\nOn PubHealth and ARC-Challenge, baselines with retrieval do not improve performance notably\nfrom their no-retrieval counterparts. We also observe that most baselines with retrieval struggle to\nimprove citation accuracy. On ASQA, our model shows significantly higher citation precision and\n5We report numbers using the results reported in the paper as the implementations are not available.\n7", "start_char_idx": 3026, "end_char_idx": 4567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc188010-42c1-418b-9b20-49ecc2f6a77f": {"__data__": {"id_": "fc188010-42c1-418b-9b20-49ecc2f6a77f", "embedding": null, "metadata": {"page_label": "8", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ce3dc78-dd2b-4865-bc38-f1e4807b8ca0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f1f948178efc5ff46b93085192c2063c4a9a5ae2c12b8eed743fc7d4e4357fb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbd7d86d-a94c-4f9b-8fb2-491e6b6120a9", "node_type": "1", "metadata": {}, "hash": "499ab16125a029e4db6c37c827f6a26663414c5b06a4c9d50404bf5946b68405", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\nthey outperforms all non-proprietary models.\u2217indicates concurrent or recent results reported by\nconcurrent work. \u2013 indicates numbers that are not reported by the original papers or are not applicable.\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\nShort-form Closed-set Long-form generations (with citations)\nPopQA TQA Pub ARC Bio ASQA\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\nLMs with proprietary data\nLlama2-c 13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 \u2013 \u2013\nRet-Llama2-c 13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\nChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 \u2013 \u2013\nRet-ChatGPT 50.8 65.7 54.7 75.3 \u2013 40.7 39.9 79.7 65.1 76.6\nPerplexity.ai \u2013 \u2013 \u2013 \u2013 71.2 \u2013 \u2013 \u2013 \u2013 \u2013\nBaselines without retrieval\nLlama2 7B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 \u2013 \u2013\nAlpaca 7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 \u2013 \u2013\nLlama2 13B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 \u2013 \u2013\nAlpaca 13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 \u2013 \u2013\nCoVE 65B* \u2013 \u2013 \u2013 \u2013 71.2 \u2013 \u2013 \u2013 \u2013 \u2013\nBaselines with retrieval\nToolformer* 6B \u2013 48.8 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\nLlama2 7B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\nAlpaca 7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\nLlama2-FT 7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\nSAIL* 7B \u2013 \u2013 69.2 48.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\nLlama2 13B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\nAlpaca 13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\nOur SELF-RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\nOur SELF-RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\nPQA Med AS\n(acc) (acc) (em)\nSELF-RAG(50k) 45.5 73.5 32.", "start_char_idx": 0, "end_char_idx": 1925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbd7d86d-a94c-4f9b-8fb2-491e6b6120a9": {"__data__": {"id_": "bbd7d86d-a94c-4f9b-8fb2-491e6b6120a9", "embedding": null, "metadata": {"page_label": "8", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ce3dc78-dd2b-4865-bc38-f1e4807b8ca0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f1f948178efc5ff46b93085192c2063c4a9a5ae2c12b8eed743fc7d4e4357fb9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc188010-42c1-418b-9b20-49ecc2f6a77f", "node_type": "1", "metadata": {"page_label": "8", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "07fd51956c7b4a3cc1643c984b15fa04815773d6282476448095b204e4472031", "class_name": "RelatedNodeInfo"}}, "text": "0 77.5 16.3 20.5 24.7 2.3 3.6\nAlpaca 13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\nOur SELF-RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\nOur SELF-RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\nPQA Med AS\n(acc) (acc) (em)\nSELF-RAG(50k) 45.5 73.5 32.1\nTraining\nNo Retriever R 43.6 67.8 31.0\nNo Critic C 42.6 72.0 18.1\nTest\nNo retrieval 24.7 73.0 \u2013\nHard constraints 28.3 72.6 \u2013\nRetrieve top1 41.8 73.1 28.6\nRemove ISSUP 44.1 73.2 30.6\n(a) Ablation\n1 270.070.5Precision\n1 2\nWeight for IsSupport9095Mauve\n (b) Customization\n0.0 0.2 0.4 0.60.980.990.991.00Accuracy\nPubHealth\n0.0 0.2 0.4 0.6\nRetrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\nFrequency\n0.250.500.751.00\nFrequency\n (c) Retrieval\nFigure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\nMauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\nrecall than all models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits\nsuperior efficacy in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this\nperformance gap, even outperforming ChatGPT in citation precision, which measures whether the\nmodel-generated claim is fully supported by cited evidence. Llama2-FT 7B, which is the baseline\nLM trained on the same instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and\nis retrieval-augmented at test time only, lags behind SELF-RAG. This result indicates SELF-RAG\ngains are not solely from training data and demonstrate the effectiveness of SELF-RAGframework.\n8", "start_char_idx": 1648, "end_char_idx": 3311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "588507ff-f57f-4c02-a9c6-2821cccc0da0": {"__data__": {"id_": "588507ff-f57f-4c02-a9c6-2821cccc0da0", "embedding": null, "metadata": {"page_label": "9", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6e7e157-53e0-4e1a-af8e-d43314ec5c61", "node_type": "4", "metadata": {"page_label": "9", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "87ae7db80994bcc257d186e0c5b088bb05cda19eaec43a1f80403e088ba34691", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n5.2 A NALYSIS\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\ntop one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\nperformance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\nand the bottom part is for inference ablations. We see that all components play important roles. We\nalso observe a large performance gap between SELF-RAGand No Retriever or Critic baselines across\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\nSELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUPduring the beam\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG\u2019s\ncapabilities of carefully selecting generations based on fine-grained multiple criteria, instead of\nnaively using all passages from the retrieval model or solely depending on relevance scores.\nEffects of inference-time customization. One key benefit of our proposed framework is that it\nenables us to control how much each critique type affects the final generation sampling. We analyze\nthe effects of different parameter weights on the top of our 7B model during inference time on\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\nthe weighting term for ISSUP, which criticizes how supported the output is by the text passage. As\nthe figure shows, increasing the weight leads to positive effects on the models\u2019 citation precision\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models\u2019 behaviors at\ntest time by adjusting such parameters without requiring additional training.\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\nthe overall accuracy and frequency of retrieval, and we evaluate the performance with varying\nnumbers of threshold \u03b4(larger \u03b4results in less retrieval) on PubHealth and PopQA. Figure 3c shows\nthat the model\u2019s retrieval frequencies dramatically change on both datasets. as \u03b4varies. On one hand,\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n6 C ONCLUSION\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\nthrough retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\nand critique text passages and its own generation by predicting the next tokens from its original\nvocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\nsix tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\nmore parameters or with conventional retrieval-augmented generation approaches.\n9", "start_char_idx": 0, "end_char_idx": 4740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f83839aa-80a7-4635-8c87-03fdbd40de6d": {"__data__": {"id_": "f83839aa-80a7-4635-8c87-03fdbd40de6d", "embedding": null, "metadata": {"page_label": "10", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea5f0cbd-f5fc-48f6-9699-6f5bd18497ae", "node_type": "4", "metadata": {"page_label": "10", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1948fd7a51992b36c3e390abf34363150f3a88464d8cb6d31754cb004130c397", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c30bc458-d118-41ab-bbce-d626cd73e0a5", "node_type": "1", "metadata": {}, "hash": "2db967df9373fc7d0a8b0f9e660a6a5fcd5e3613a16eb8be8dc197d9b1ccfa63", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nETHICAL CONCERNS\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\nadvice). While our method shows significant improvements in terms of performance, factuality, and\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\nmodel outputs.\nACKNOWLEDGMENTS\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\nAkari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\nto train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\nProgram for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\nREFERENCES\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learn-\ning to retrieve reasoning paths over wikipedia graph for question answering. In International\nConference on Learning Representations , 2020. URL https://openreview.net/forum?\nid=SJgVHkrYDH .\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and appli-\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Tutorial) , 2023a. URL https://aclanthology.org/2023.acl-tutorials.6 .\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh\nHajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Associ-\nation for Computational Linguistics , 2023b. URL https://aclanthology.org/2023.\nfindings-acl.225 .\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:\nEvaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037 ,\n2022. URL https://arxiv.org/abs/2212.08037 .\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time? arXiv\npreprint arXiv:2307.09009 , 2023. URL https://arxiv.org/abs/2307.09009 .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457 , 2018. URL https://arxiv.org/abs/1803.05457 .\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems ,\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx .\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint\narXiv:2309.11495 , 2023. URL https://arxiv.org/abs/2309.11495 .", "start_char_idx": 0, "end_char_idx": 3380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c30bc458-d118-41ab-bbce-d626cd73e0a5": {"__data__": {"id_": "c30bc458-d118-41ab-bbce-d626cd73e0a5", "embedding": null, "metadata": {"page_label": "10", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea5f0cbd-f5fc-48f6-9699-6f5bd18497ae", "node_type": "4", "metadata": {"page_label": "10", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1948fd7a51992b36c3e390abf34363150f3a88464d8cb6d31754cb004130c397", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f83839aa-80a7-4635-8c87-03fdbd40de6d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "febeb442537c49f085898db2735b7733cbb2a608f3abb5972184b24d8d02827c", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1803.05457 , 2018. URL https://arxiv.org/abs/1803.05457 .\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems ,\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx .\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint\narXiv:2309.11495 , 2023. URL https://arxiv.org/abs/2309.11495 .\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of\nwikipedia: Knowledge-powered conversational agents. In International Conference on Learning\nRepresentations , 2019. URL https://openreview.net/forum?id=r1l73iRqKm .\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\n10", "start_char_idx": 2788, "end_char_idx": 3835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "388f8350-0d9b-410e-bc92-ec73d3156d50": {"__data__": {"id_": "388f8350-0d9b-410e-bc92-ec73d3156d50", "embedding": null, "metadata": {"page_label": "11", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fb68e17-fe10-489d-9d2b-c69ddcfdcbc9", "node_type": "4", "metadata": {"page_label": "11", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "48284782336773f7679a09880358ecba02070ae080cbf32c457af69732f482af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c42699fd-fd78-4534-9774-5cb54ba0759a", "node_type": "1", "metadata": {}, "hash": "d13189739b3ca740d1914ab360695040e7db34e59bc253875609364d9fddf280", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nlearn from human feedback. arXiv preprint arXiv:2305.14387 , 2023. URL https://arxiv.\norg/abs/2305.14387 .\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\ntext with citations. arXiv preprint arXiv:2305.14627 , 2023. URL https://arxiv.org/abs/\n2305.14627 .\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International Conference on Machine Learning , 2020. URL\nhttps://dl.acm.org/doi/pdf/10.5555/3524938.3525306 .\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\nTransactions on Machine Learning Research , 2022a. URL https://openreview.net/\nforum?id=jKN1pXi7b0 .\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022b. URL https:\n//arxiv.org/abs/2208.03299 .\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint\narXiv:2305.06983 , 2023. URL https://arxiv.org/abs/2305.06983 .\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2017. URL\nhttps://aclanthology.org/P17-1147 .\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\nCtrl: A conditional transformer language model for controllable generation. arXiv preprint\narXiv:1909.05858 , 2019. URL https://arxiv.org/abs/1909.05858 .\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason\nPhang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences.\nInInternational Conference on Machine Learning , 2023. URL https://openreview.net/\nforum?id=AT8Iw8KOeC .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. Natural questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics , 2019. URL https://aclanthology.org/\nQ19-1026 .\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles , 2023. URL https://arxiv.org/abs/2309.06180 .", "start_char_idx": 0, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c42699fd-fd78-4534-9774-5cb54ba0759a": {"__data__": {"id_": "c42699fd-fd78-4534-9774-5cb54ba0759a", "embedding": null, "metadata": {"page_label": "11", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fb68e17-fe10-489d-9d2b-c69ddcfdcbc9", "node_type": "4", "metadata": {"page_label": "11", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "48284782336773f7679a09880358ecba02070ae080cbf32c457af69732f482af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "388f8350-0d9b-410e-bc92-ec73d3156d50", "node_type": "1", "metadata": {"page_label": "11", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "719199b99ef6f6902a1550de51517f4c6bad634ee7be182df0d6d25117a24be4", "class_name": "RelatedNodeInfo"}}, "text": "Natural questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics , 2019. URL https://aclanthology.org/\nQ19-1026 .\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles , 2023. URL https://arxiv.org/abs/2309.06180 .\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K \u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt \u00a8aschel, Sebastian Riedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems , 2020. URL https://proceedings.neurips.cc/paper/\n2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf .\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-\naugmented dual instruction tuning, 2023. URL https://arxiv.org/abs/2310.01352 .\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848 , 2023a. URL https://arxiv.org/abs/2304.09848 .\n11", "start_char_idx": 2548, "end_char_idx": 3932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2be73afa-b526-40e8-aa2b-c36d12996363": {"__data__": {"id_": "2be73afa-b526-40e8-aa2b-c36d12996363", "embedding": null, "metadata": {"page_label": "12", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1e6a54f-08a6-44cb-9a0e-7958bcbbb3f3", "node_type": "4", "metadata": {"page_label": "12", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d117c8c669da475e243158c321eb6693f8536270612f0e42d265bc8e24693c27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "904ee4a3-1080-48ab-b999-805bf1f3f820", "node_type": "1", "metadata": {}, "hash": "075c08ec61c5c7923528ee9f0267fd93542a150d65426ca8ce1dba253d2c654e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 , 2023b.\nURLhttps://arxiv.org/abs/2303.16634 .\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.\nInAdvances in Neural Information Processing Systems , 2022. URL https://openreview.\nnet/forum?id=5HaIds3ux5O .\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox,\nHelen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv preprint\narXiv:2305.15225 , 2023. URL https://arxiv.org/abs/2305.15225 .\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric\nmemories. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , 2023. URL https://aclanthology.org/2023.\nacl-long.546 .\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147 , 2022.\nURLhttps://arxiv.org/abs/2203.11147 .\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing , 2018. URL https://aclanthology.\norg/D18-1260 .\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard EM approach\nfor weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , 2019. URL https://aclanthology.org/\nD19-1284 .\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251 , 2023. URL https:\n//arxiv.org/abs/2305.14251 .\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021. URL https:\n//arxiv.org/abs/2112.09332 .\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing , 2022. URL https://aclanthology.org/2022.emnlp-main.669 .\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.", "start_char_idx": 0, "end_char_idx": 3172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "904ee4a3-1080-48ab-b999-805bf1f3f820": {"__data__": {"id_": "904ee4a3-1080-48ab-b999-805bf1f3f820", "embedding": null, "metadata": {"page_label": "12", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1e6a54f-08a6-44cb-9a0e-7958bcbbb3f3", "node_type": "4", "metadata": {"page_label": "12", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d117c8c669da475e243158c321eb6693f8536270612f0e42d265bc8e24693c27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2be73afa-b526-40e8-aa2b-c36d12996363", "node_type": "1", "metadata": {"page_label": "12", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7df8339b3822cbc31988f137f62784738a1c9ebbf21fc37a3453d0af7f1e896f", "class_name": "RelatedNodeInfo"}}, "text": "Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021. URL https:\n//arxiv.org/abs/2112.09332 .\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing , 2022. URL https://aclanthology.org/2022.emnlp-main.669 .\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. URL https://arxiv.\norg/abs/2303.08774 .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. In Advances in\nNeural Information Processing Systems , 2022. URL https://openreview.net/forum?\nid=TG8KACxEON .\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt \u00a8aschel,\n12", "start_char_idx": 2615, "end_char_idx": 3881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bf479fc-1a49-4dda-82f4-6ceec803b7f0": {"__data__": {"id_": "7bf479fc-1a49-4dda-82f4-6ceec803b7f0", "embedding": null, "metadata": {"page_label": "13", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "246fb635-a3d6-42c5-b4a8-a0258b96bc82", "node_type": "4", "metadata": {"page_label": "13", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aea6166abcc32324c1bf82435bb8816f671fca74da06dc9cbf809b39ef955fd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76775e1a-091f-4a5e-8404-245822957910", "node_type": "1", "metadata": {}, "hash": "c042f99c039d540993aaa75e6a4011bfee04cd3111821fb13799bae0135fc716", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nand Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , 2021. URL https://aclanthology.org/\n2021.naacl-main.200 .\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using\ndivergence frontiers. In Advances in Neural Information Processing Systems , 2021. URL https:\n//openreview.net/forum?id=Tqx7nJp7PR .\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis , 2020. URL https://dl.acm.\norg/doi/10.5555/3433701.3433727 .\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. Transactions of the Association\nfor Computational Linguistics , 2023. URL https://arxiv.org/abs/2302.00083 .\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\nenables zero-shot task generalization. In International Conference on Learning Representations ,\n2022. URL https://openreview.net/forum?id=9Vrb9D0WI4 .\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761 , 2023. URL https://arxiv.org/abs/2302.\n04761 .\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. URL https://arxiv.org/\nabs/1707.06347 .\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael\nSch\u00a8arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\nInProceedings of the 40th International Conference on Machine Learning , 2023. URL https:\n//proceedings.mlr.press/v202/shi23a.html .\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-\nform answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing , 2022. URL https://aclanthology.org/2022.emnlp-main.566 .\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-\nscale dataset for fact extraction and VERification.", "start_char_idx": 0, "end_char_idx": 3203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76775e1a-091f-4a5e-8404-245822957910": {"__data__": {"id_": "76775e1a-091f-4a5e-8404-245822957910", "embedding": null, "metadata": {"page_label": "13", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "246fb635-a3d6-42c5-b4a8-a0258b96bc82", "node_type": "4", "metadata": {"page_label": "13", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aea6166abcc32324c1bf82435bb8816f671fca74da06dc9cbf809b39ef955fd4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bf479fc-1a49-4dda-82f4-6ceec803b7f0", "node_type": "1", "metadata": {"page_label": "13", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aa2cf5b6962fe899d727d144b7a68d3ed649cd951cce0ba80d22cbafb5dc21e1", "class_name": "RelatedNodeInfo"}}, "text": "Large language models can be easily distracted by irrelevant context.\nInProceedings of the 40th International Conference on Machine Learning , 2023. URL https:\n//proceedings.mlr.press/v202/shi23a.html .\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-\nform answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing , 2022. URL https://aclanthology.org/2022.emnlp-main.566 .\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-\nscale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers) , 2018. URL https://aclanthology.org/N18-1074 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. URL https://arxiv.\norg/abs/2307.09288 .\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nexploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751 , 2023.\nURLhttps://arxiv.org/abs/2306.04751 .\n13", "start_char_idx": 2597, "end_char_idx": 4044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07c03225-ec5e-4907-8ab2-7ac763aa0500": {"__data__": {"id_": "07c03225-ec5e-4907-8ab2-7ac763aa0500", "embedding": null, "metadata": {"page_label": "14", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84859801-425a-4208-96c8-ff12c482a9fd", "node_type": "4", "metadata": {"page_label": "14", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b601b00873b8278f4d45f416052d4d1f22974381c265ffcbc785b041de075065", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations , 2022. URL https://openreview.net/forum?\nid=gEZrGCozdqR .\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. arXiv preprint arXiv:2306.01693 , 2023. URL https:\n//arxiv.org/abs/2306.01693 .\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of\nattribution by large language models. arXiv preprint arXiv:2305.06311 , 2023. URL https:\n//arxiv.org/abs/2305.06311 .\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,\nXixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.\narXiv preprint arXiv:2304.03728 , 2023. URL https://arxiv.org/abs/2304.03728 .\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593 , 2019. URL https://arxiv.org/abs/1909.08593 .\n14", "start_char_idx": 0, "end_char_idx": 1348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a870b9da-692d-49a4-abc8-b48468789b24": {"__data__": {"id_": "a870b9da-692d-49a4-abc8-b48468789b24", "embedding": null, "metadata": {"page_label": "15", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02457e77-2805-41c7-9b76-6f54ceb3d8b8", "node_type": "4", "metadata": {"page_label": "15", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9ff928c82245f94a022bc40e1345d671c95ef49979d7263fb8a7e90edd7a663f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAPPENDIX\nA S ELF-RAGDetails 16\nA.1 Reflection Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nA.2 Advantages of Learning-based Methods . . . . . . . . . . . . . . . . . . . . . . . 16\nA.3 S ELF-RAGTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.4 S ELF-RAGInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB Experimental Details 19\nB.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC Results 20\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nC.2 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nC.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD Full List of Instructions and Demonstrations for GPT-4 21\n15", "start_char_idx": 0, "end_char_idx": 1016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f178ea09-239a-4e8e-94ef-8216481c64e4": {"__data__": {"id_": "f178ea09-239a-4e8e-94ef-8216481c64e4", "embedding": null, "metadata": {"page_label": "16", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb069b1b-c21a-4a17-8942-d93fb785bce2", "node_type": "4", "metadata": {"page_label": "16", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7f51f23ecfb7c44641eeecb1c146ca81bce54c263afbb5747514dc0b3a542914", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4aff74fd-04bf-4ac6-ba4b-a05b3b399fef", "node_type": "1", "metadata": {}, "hash": "d01980f1d481daeda1a7d8a1c5a227cd7141bd2f76cc449979b62edf0208c814", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA S ELF-RAGDETAILS\nA.1 R EFLECTION TOKENS\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\nonly given at each output level.\n\u2022Retrieval-on-demand (Retrieve ): Given an input and previous-step generation (if applicable),\nan LM determines whether the continuation requires factual grounding. Noindicates retrieval\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\nto use evidence , which indicates that a model can continue to use the evidence retrieved\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\ngenerates multiple segments based on the passage.\n\u2022Relevant (ISREL): Retrieved knowledge may not be always relevant to the input. This aspect\nindicates whether the evidence provides useful information ( Relevant ) or not ( Irrelevant ).\n\u2022Supported (ISSUP): Attribution is the concept of whether the output is fully supported by\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\nsupported ,Partially supported , and No support / Contradictory , follow-\ning Yue et al. (2023); Nakano et al. (2021).\n\u2022Useful (ISUSE): Following the definitions from Liu et al. (2023a), we define the perceived utility\nas whether the response is a helpful and informative answer to the query, independently from\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\noutputs with \u201c##\u201d. We use the temperature 1 and set the maximum output token counts to be 200. We\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\nthat do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\nfor ISSUP, 19,317 for relevance, and 3,831 for utility.\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\nthe same instruction, demonstrations, and test instances. We found our assessments show high\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to\nthe disagreement between 1 and 2 or 4 and 5. Compared to prior efforts on agreement of GPT-4\npredictions and human annotators in pair-wise evaluations, we found our human annotators often\nagree with GPT-4 predictions. We hypothesize this is because our fine-grained evaluation with\nabsolute scoring systems, unlike such relative, overall pair-wise evaluation systems enables GPT-4\nto generate more reliable and agreeable predictions. The effectiveness of GPT-4 evaluations in\nfine-grained aspects has shown to be effective in prior work (Liu et al., 2023a).\nA.2 A DVANTAGES OF LEARNING -BASED METHODS\nWhile recent work (Jiang et al., 2023) proposes a prompting-based method to enable retrieval on-\ndemand, we find a learning-based method is more suitable to enable fine-grained self-reflection\nfeedback and inference-time control. First, Self-RAG requires careful multi-aspect fine-grained\nself-evaluations at inference time. To make an LM to comprehend fine-grained aspects and scoring\nsystems, precise and detailed instructions, as well as few-shot demonstrations, are necessary. This\nsignificantly increases the input sequence length, resulting in higher costs and latency. Nevertheless,\nwe briefly tried prompting-based approaches in our preliminary experiments and found it is nontrivial.", "start_char_idx": 0, "end_char_idx": 4207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4aff74fd-04bf-4ac6-ba4b-a05b3b399fef": {"__data__": {"id_": "4aff74fd-04bf-4ac6-ba4b-a05b3b399fef", "embedding": null, "metadata": {"page_label": "16", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb069b1b-c21a-4a17-8942-d93fb785bce2", "node_type": "4", "metadata": {"page_label": "16", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7f51f23ecfb7c44641eeecb1c146ca81bce54c263afbb5747514dc0b3a542914", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f178ea09-239a-4e8e-94ef-8216481c64e4", "node_type": "1", "metadata": {"page_label": "16", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3648204a900e48a8fedca38359bc7cbd20db12b5e01c41913aac914560d91f19", "class_name": "RelatedNodeInfo"}}, "text": "The effectiveness of GPT-4 evaluations in\nfine-grained aspects has shown to be effective in prior work (Liu et al., 2023a).\nA.2 A DVANTAGES OF LEARNING -BASED METHODS\nWhile recent work (Jiang et al., 2023) proposes a prompting-based method to enable retrieval on-\ndemand, we find a learning-based method is more suitable to enable fine-grained self-reflection\nfeedback and inference-time control. First, Self-RAG requires careful multi-aspect fine-grained\nself-evaluations at inference time. To make an LM to comprehend fine-grained aspects and scoring\nsystems, precise and detailed instructions, as well as few-shot demonstrations, are necessary. This\nsignificantly increases the input sequence length, resulting in higher costs and latency. Nevertheless,\nwe briefly tried prompting-based approaches in our preliminary experiments and found it is nontrivial.\nWhen we combine all instructions for all aspects and feed them to the target pre-trained LMs (GPT-3\ndavinci-003 / 002, Llama2-13B-chat), all models struggle to precisely follow our evaluation scheme,\noften generating output formats that do not suit our scheme or whose reflections are less accurate. To\n16", "start_char_idx": 3348, "end_char_idx": 4513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "266141a6-e08f-493f-93df-f4cb325c0847": {"__data__": {"id_": "266141a6-e08f-493f-93df-f4cb325c0847", "embedding": null, "metadata": {"page_label": "17", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db0357ce-f2c1-4ffd-a2e4-11fde0aa692a", "node_type": "4", "metadata": {"page_label": "17", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "325158a3eec723435a06b3e1393fc87c674e06ace24a9eb50f28b6bcc09f966b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nDataset name Category Data source # of instances % of Retrieve =Yes\nGPT-4 Alpaca Instruction-following Open-Instruct 26,168 53.2\nStanford Alpaca Instruction-following Open-Instruct 25,153 48.0\nFLAN-V2 Instruction-following Open-Instruct 17,817 15.8\nShareGPT Instruction-following Open-Instruct 13,406 76.8\nOpen Assistant 1 Instruction-following Open-Instruct 9,464 77.1\nWizard of Wikipedia Knowledge-intensive KILT 17,367 22.7\nNatural Questions Knowledge-intensive KILT 15,535 87.7\nFEVER Knowledge-intensive KILT 9,966 63.2\nOpenBoookQA Knowledge-intensive HF Dataset 4,699 2.3\nArc-Easy Knowledge-intensive HF Dataset 2,147 11.0\nASQA Knowledge-intensive ASQA 3,897 91.5\nTable 3: The generator LM Mtraining data statistics.\nmake the most use of the Self-RAG potential, we need to use the token probabilities for the reflection\ntokens, which may not be always available for black box proprietary LM APIs. Note that at the time\nof submission, ChatGPT and GPT-4 do not support long probability information, preventing us from\napplying the Self-RAG algorithm to such models. This limitation is also discussed in the Active\nRetrieval paper, which also requires access to token probabilities.\nA.3 S ELF-RAGTRAINING\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nAlgorithm 2 SELF-RAGTraining\n1:Input input-output data D={X, Y}, generator M,C\u03b8\n2:Initialize Cwith a pre-trained LM\n3:Sample data {Xsample, Ysample} \u223c { X, Y} \u25b7Training Critic LM (Section 3.2.1)\n4:for(x, y)\u2208(Xsample, Ysample)do \u25b7Data collections for C\n5: Prompt GPT-4 to collect a reflection token rfor(x, y)\n6: Add{(x, y, r )}toDcritic\n7:Update Cwith next token prediction loss \u25b7Critic learning; Eq. 1\n8:Initialize Mwith a pre-trained LM \u25b7Training Generator LM (Section 3.2.2)\n9:for(x, y)\u2208(X, Y )do \u25b7Data collection for MwithDcritic\n10: RunCto predict rgiven (x, y)\n11: Add(x, y, r )toDgen\n12:Update MonDgenwith next token prediction loss \u25b7Generator LM learning; Eq. 2\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov\net al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\nWe also present the percentage of the instances where Retrieve =Yes appears at least once. While\nsome instruction-following datasets such as FLAN-T5 show a lower percentage of instances with\nRetrieve =Yes, other datasets show significantly higher percentages, indicating that our Critic model\npredicts the necessity of retrieval according to the given instances. In FLAN-T5, many training data\ncome from non-knowledge-intensive tasks such as grammatical error collections or simple string\nmanipulations that are unlikely to benefit from knowledge retrieval from Wikipedia.\nPerformance of the Critic C.We evaluate the accuracy of reward predictions by splitting GPT-4\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\nas follows. Table 4 shows the model performance of predicting GPT-4 judgments. As you can see,\n17", "start_char_idx": 0, "end_char_idx": 3550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd3fa58b-645d-4e6e-ae69-00155f77fd8a": {"__data__": {"id_": "fd3fa58b-645d-4e6e-ae69-00155f77fd8a", "embedding": null, "metadata": {"page_label": "18", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63514ab1-bc97-44ef-8ecb-3644ca44d06f", "node_type": "4", "metadata": {"page_label": "18", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b9c553aa888873be452e674a607858b1d1b9d0fcd5806a4fa8bb7b48116bd955", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nbase LM Retrieve ISSUP ISREL ISUSE\nLlama2-7B 93.8 93.5 80.2 73.5\nFLAN-3B 85.6 73.1 82.0 72.1\nFigure 4: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\nlower performance on ISUSE, this is because both models often confuse between the two highest\ncases (5 and 4), where human annotators can also disagree.\nDetails of Mdata creation. Here, we provide detailed data creation procedures. Algorithm 3\nsummarizes the process. Here we set yttoyfor simplification. Once we train the critic model, we\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\nnot. For the instances where the critic predicts Retrieve =No, we only predict the ISUSEgiven input\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\nthen split output sentences using Spacy.6For each sentence, we run Cto predict whether the retrieval\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. If Cpredicts\nRetrieve =No, then do not insert any paragraph at the tth segment. If Cpredicts Retrieve =Yes, then\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\nt-th segment. For each retrieved passage, we predict ISRELand ISSUP. If there is any passage and\ncontinuation with ISREL=Relevant and ISSUP=Fully Supported /ISSUP=Partially\nSupported , then we sample it as the continuation, while we discard part of those examples on\nsome cases (see details below). If there is more than one passage satisfying this criterion, we use\nthe one with the highest retrieval score. If there are only ISREL=Irrelevant or ISSUP=No\nSupport passages, we randomly sample one passage.\nTo avoid the dominance of certain reflection tokens in training data, we down-sample training\ninstances. In particular, we down-sample and discard 50% of the instances without any retrieval\ntokens, since large-scale instruction-following datasets (e.g., Alpaca), include many queries that\ndo not require retrieval (e.g., simple and easy facts or not knowledge intensive). We also notice\nthat in Open-domain QA, there are many relevant and fully supported passages, and when we\nalways prioritize such cases, ISREL=Relevant and ISSUP=Fully Supported will be overly\nrepresented and there\u2019s a risk that a model learns to simply output the same reflection tokens. We,\ntherefore, up-sample some instances with the ISREL=Irrelevant token for the QA dataset.\nAlgorithm 3 MgenData creation\n1:Input Input-output data D=X, Y\n2:for(x, y)\u2208 {X, Y}do\n3: Given (x, y)Cpredicts Retrieve\n4: ifRetrieve is predicted then\n5: Retrieve relevant passages DusingRgiven (x, y) \u25b7Retrieve passages\n6: ford\u2208Ddo\n7: Cpredicts ISRELfor each d \u25b7 Predict relevance of passages\n8: Cpredicts ISSUPfor each (y, d) \u25b7Predict supports of outputs\n9: Cpredicts ISUSEfor each d \u25b7 Predict overall utility ( t=Tonly)\n10: Sample d\n11: else if Retrieve is not predicted then\n12: Cpredicts ISUSEgiven x, y\nAdd augmented (x, y, d, r )toDgen\n6https://spacy.io/\n18", "start_char_idx": 0, "end_char_idx": 3703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f74fdbf3-a1b8-48cc-9502-f36532412764": {"__data__": {"id_": "f74fdbf3-a1b8-48cc-9502-f36532412764", "embedding": null, "metadata": {"page_label": "19", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa19d8b5-b9d5-4e68-9c91-1e3793da50c2", "node_type": "4", "metadata": {"page_label": "19", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b32e032a010c5ffa146160ef19aa197c87cb3da2c71b814c9f2bdda2aeedb615", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTraining examples. Table 4 show several training examples used for Mtraining.\nA.4 S ELF-RAGINFERENCE\nDetails of beam-search score calculations. We first compute scores for each critique type by\ntaking the normalized probabilities of desirable tokens. For ISREL, we compute the score as follows:\ns(ISREL) =p(ISREL=RELEVANT )\np(ISREL=RELEVANT ) +p(ISREL=IRRELEVANT ).\nFor ISSUP, we compute the score as follows:\ns(ISREL) =p(ISSUP=FULLY )\nS+ 0.5\u00d7p(ISSUP=PARTIALLY )\nS,\nwhere S=P\nt\u2208{FULLY,PARTIALLY ,NO}p(ISSUP=t). For ISUSEwhere we have a five-scale score, we\ncompute the weighted sum of the scores. We assigns weighted scores of w={\u22121,\u22120.5,0,0.5,1}\nto the tokens ISUSE={1,2,3,4,5}, and compute the final scores as follows:\ns(ISUSE) =5X\niwip(ISUSE=i)\nS,\nwhere S=P\nt\u2208{1,2,3,4,5}p(ISUSE=t).\nDetails of adaptive retrieval. For retrieval based on soft constraints, we trigger retrieval if the\nfollowing condition is satisfied:\np(Retrieve =YES)\np(Retrieve =YES) +p(p(Retrieve =NO)> \u03b4.\nB E XPERIMENTAL DETAILS\nB.1 M ORE DETAILS OF TRAINING\nMore details of training and computations. We use 4 Nvidia A100 with 80GB memory to train\nour models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5\nwith 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048\nfor the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed\nstage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision\nBfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more\nefficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB\nmemory.\nB.2 M ORE DETAILS OF EVALUATIONS\nRetrieval setup details. By default, we use Contriever-MS MARCO to retrieve the top five\ndocuments from Wikipedia, and use official Wikipedia embeddings based on 2018 English Wikipedia.\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\nadded to Wikipedia. Therefore, for PopQA, we used the December 2020 preprocessed Wikipedia\ncorpus provided by Izacard et al. (2022b) and generated document embeddings.7The issues of\nperformance variance from different Wikipedia dumps have been reported by prior work (Asai et al.,\n2020; Izacard et al., 2022b). Yet, we observe limited effectiveness of such off-the-shelf retrieval\nmodels trained primarily on knowledge-intensive tasks for open-ended generation (e.g., instruction\nfollowing). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al.,\n2023b) or joint training of retrieval and LM components (Lin et al., 2023), while we leave exploring\nthe effectivess of such appraoches for future work. For bio generation and open-domain QA tasks,\nwe additionally retrieve five documents using Google Programmable Search8and search documents\nfrom English Wikipedia. As this API only provides snippets, we retrieve Wikipedia introductory\nparagraphs for the corresponding entities.\n7https://github.com/facebookresearch/atlas\n8https://programmablesearchengine.google.com/about/\n19", "start_char_idx": 0, "end_char_idx": 3270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ecab720-d49c-44ad-9a56-ffd6f5ef7282": {"__data__": {"id_": "2ecab720-d49c-44ad-9a56-ffd6f5ef7282", "embedding": null, "metadata": {"page_label": "20", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5e4d27a-e300-4b12-914f-e187c7c3625d", "node_type": "4", "metadata": {"page_label": "20", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bb5129ace813ca275a22f6f0a86df4ef3464f652a9af7e44b4924009da5d37e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1a081da-1e92-4ccf-9980-393505d382ce", "node_type": "1", "metadata": {}, "hash": "82089c792af635a5eb7e49d4aa871dbfd38edfac1cae4ac6ef0b218f07026351", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nDetailed experimental settings for individual datasets. For OpenQA datasets, we set the max-\nimum new token number to 100 tokens. For closed-set tasks (PubHealth and ARC-C), we set the\nmaximum new token length to 50 for all baselines. For SELF-RAGinference on PubHealth and\nARC-C, instead of determining the output with the highest score 4 as in other tasks, we aggregate the\nscores for each option and select the answer option with the highest score. We found in zero-shot\nsettings of fact checking, some LLMs can generate capitalized class labels (e.g., True) while our\ngold labels are lower-cased. Therefore, across different LMs, for fact checking, we lowercase the\npredictions. In multiple choice tasks, we found some models generate answers in slightly different\nways (e.g., (A) instead of A). We slightly modify instructions for each LLM to avoid such format\nviolations, and further conduct string matching between each candidate and model predictions if\nformat violations still remain. After that processing, in closed set tasks, model predictions match\none of the gold classes in almost all cases. For ALCE, we found that Llama2-chat tend to generate\nsignificantly lower outputs than other models (e.g., on average, their output is nearly 100 token, while\nChatGPT generates 40 tokens on average), resulting in inflated str-em scores. We limit the maximum\ngeneration length to 100 tokens for all baselines to avoid this issue, rather than the original 300\ntokens in the ALCE paper. Consequently, all of the baseline output length is within 30-60 tokens.\nFor FactScore, we set the maximum new token length to 500 for baselines and 200 for SELF-RAGat\neach segment level.\nTask-specific instructions. Table 5 shows the list of the instructions used during evaluations. For\nOpen-domain QA, we do not provide explicit instructions.\nC R ESULTS\nC.1 A NALYSIS\nReliance on parametric- and non-parametric memories. We analyze how frequently model\nanswers come from retrieved passages (non-parametric memories) or their parametric memories.\nOn two open-domain QA datasets, TriviaQA and PopQA, we conduct the following analysis: 1)\nsample query models successfully answer correctly, 2) for each query in this group, check whether the\nmatched ground-truth answer is a sub-string of the retrieved passage or not. We evaluate SELF-RAG\n7B, Alpaca 7B, Alpaca 13B, and Llama2-Chat-13B. We found that SELF-RAGsignificantly less\nfrequently generates answers that are not included in the provided evidence; in particular, in Alpaca\n30B, 20% of the correct predictions are not included in the provided passages, followed by Llama2-\nchat 13B (18%) and Alpaca (15%), while it is only 2% in SELF-RAG. When retrieved passages are\nnot relevant, SELF-RAGgenerates ISREL=Irrelevant , indicating that the following answers\nmay not be factually grounded, while those instruction-tuned models continue to generate plausible\nanswers.\nEffects of training data size. We analyze how the data scale affects the model\u2019s performance. In\nparticular, we randomly sample 5k, 10k, 20k, and 50k instances from our original 150k training\ninstances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare the model\nperformance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-RAGtrained\non the full 150k instances. We also evaluate Figures 5a, 5b and 5c shows the models\u2019 performance\ntrained on different amount of data. Across all datasets, increasing data size often shows upward\ntrajectories and the improvements are significantly larger in PopQA and ASQA, while we do not\nobserve such significant improvements on Llama2-FT 7Bwhen increasing the training data from 50k\nto 150k. These results also indicate that further expanding the training data of SELF-RAGmay lead\nto further improvements, although in this work we limit our training data size to 150k.\nReflection token prediction performance. We evaluate the accuracy of the Critic and Generator\nLMs in predicting reflection tokens.", "start_char_idx": 0, "end_char_idx": 4035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1a081da-1e92-4ccf-9980-393505d382ce": {"__data__": {"id_": "b1a081da-1e92-4ccf-9980-393505d382ce", "embedding": null, "metadata": {"page_label": "20", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5e4d27a-e300-4b12-914f-e187c7c3625d", "node_type": "4", "metadata": {"page_label": "20", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bb5129ace813ca275a22f6f0a86df4ef3464f652a9af7e44b4924009da5d37e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ecab720-d49c-44ad-9a56-ffd6f5ef7282", "node_type": "1", "metadata": {"page_label": "20", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2a1cba05709c95e3e1fe34fd496cffeb74d62831c14ead21f469844504b7b0b4", "class_name": "RelatedNodeInfo"}}, "text": "Then, we compare the model\nperformance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-RAGtrained\non the full 150k instances. We also evaluate Figures 5a, 5b and 5c shows the models\u2019 performance\ntrained on different amount of data. Across all datasets, increasing data size often shows upward\ntrajectories and the improvements are significantly larger in PopQA and ASQA, while we do not\nobserve such significant improvements on Llama2-FT 7Bwhen increasing the training data from 50k\nto 150k. These results also indicate that further expanding the training data of SELF-RAGmay lead\nto further improvements, although in this work we limit our training data size to 150k.\nReflection token prediction performance. We evaluate the accuracy of the Critic and Generator\nLMs in predicting reflection tokens. For the Critic LM, we evaluate its agreement against GPT-4\npredictions on a validation set of the initially collected GPT-4 predictions. Table 5d shows the model\nperformance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model\nshows high prediction matching with GPT-4 predicted feedback. In most aspects, our reward model\nshows higher than 80% accuracy, indicating the powerful ability of fine-tuned specialized LMs to\nevaluate text. While both models show relatively lower performance on ISUSE, this is because both\nmodels often confuse between the two highest cases (5 and 4), where human annotators also disagree.\n20", "start_char_idx": 3213, "end_char_idx": 4686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f48b068-fbea-4f29-a882-67c1d2f050c7": {"__data__": {"id_": "8f48b068-fbea-4f29-a882-67c1d2f050c7", "embedding": null, "metadata": {"page_label": "21", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25253ec0-9c70-46d6-94cb-048378c8c666", "node_type": "4", "metadata": {"page_label": "21", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e4fa6c31b6ccd38cd51615d755cd04c4e9566b660839a43d07c442e4becd16b6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n0 50 100 150\nNum of training (k)3540455055Perfomance\n(a) PopQA\n0 100\nNum of training (k)717273\n (b) PubHealth\n0 100\nNum of training (k)4060\n (c) ASQA (prec)Type acc.\nRetrieve 93.8\nISSUP 93.5\nISREL 80.2\nISUSE 73.5\n(d) Reflection pre-\ndiction accuracy.\nFigure 5: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\nReflection token prediction accuracy of the Critic LM.\nC.2 H UMAN EVALUATION\nEvaluations on supportiveness and plausibility. We conduct small human evaluations on SELF-\nRAGoutputs, as well as the reliability of predicted reflection tokens. In particular, we sampled 50\nsamples from PopQA and Bio results. Following Menick et al. (2022), human annotators evaluate\nS&P , which indicates whether the model output is plausible (i.e., the output is a reasonable and\non-topic response to the question as if it were occurring in a conversation) and supported (i.e., the\nprovided evidence is sufficient to verify the validity of the answer). For S&P, we do not consider the\ninstances where SELF-RAGpredicts irrelevant orno support . We then ask our annotators\nwhether the model-predicted reflection tokens about ISRELand ISSUPmatch their inspections\n(e.g., whether the fully supported output is supported by the cited evidence). Human annotators find\nSELF-RAGanswers are often plausible and supported by relevant passages with higher S&P scores\non short-form PopQA, which is consistent with Menick et al. (2022). Human annotators also find\nISRELand ISSUPreflection token predictions are mostly aligned with their assessments. Appendix\nTable 6 shows several annotated examples and explanations on assessments.\nExamples of human evaluations. Table 6 shows examples with human evaluations on S&P and\ncorrectness of ISRELand ISSUPreflection tokens.\nC.3 Q UALITATIVE EXAMPLES\nTable 7 shows several examples predicted by our SELF-RAG(13B). The first example is the model\noutput to an ASQA question. The first reference states that Emperor Constantine made Sunday a\nday of rest from labor, and further the second citation supports the fact that the official adoption\nof Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts\nContradictory to the first output as the output says the person has served as the CEO since 2010,\nwhile the passage says he stepped down as CEO in 2015. Indicating those factual contradictions\nas reflection tokens enables to enforcement of hard control and also verification of model outputs\neasily. In the third example, while the generation is mostly correct, SELF-RAGpredicts Partially\nSupport to the statement listing the name of the songs, as they were not explicitly mentioned.\nD F ULL LIST OF INSTRUCTIONS AND DEMONSTRATIONS FOR GPT-4\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens.\nTable 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows\nthe instruction and demonstrations used to collect the three-way output tokens for Retrieve given\ninstruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration\nand test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations\nused to collect the three-way output tokens for ISREL. Table 11 shows an instruction and demon-\nstrations used to collect the three-way output tokens for ISREL. Table 12 shows an instruction and\ndemonstrations used to collect the five-way output tokens for ISUSE.\n21", "start_char_idx": 0, "end_char_idx": 3643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab7c3898-57b9-48c6-87d1-609924cb2de3": {"__data__": {"id_": "ab7c3898-57b9-48c6-87d1-609924cb2de3", "embedding": null, "metadata": {"page_label": "22", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df869a45-fd74-47b9-ad50-4d0a86e34c52", "node_type": "4", "metadata": {"page_label": "22", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d8a22611a6fcdc03948b5323c15d7fc79d5e074cd80383b0ba11a2813f35abb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "595e5644-799a-4d11-bb9c-feba8a980e7f", "node_type": "1", "metadata": {}, "hash": "c6d99a5fdf00718957e26e8f1be31d35881f9d1bcbedb039c1957af2c76f1445", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInput Identify the two types of computer memory. (from gpt4 alpaca )\nOutput [Retrieve =Yes]<p>Computer memory Computer memory \u2013 In computing, memory refers to the\ncomputer hardware integrated circuits that store information for immediate use in a computer; it is synonymous\nwith the term \u201cprimary storage\u201d. Computer memory operates at a high speed, for example, random-access\nmemory (RAM), as a distinction from storage that provides slow-to-access information but offers higher\ncapacities. If needed, contents of the computer memory can be transferred to secondary storage; a very\ncommon way of doing this is through a memory management technique called \u201cvirtual memory\u201d. An archaic\nsynonym for memory is store. The term \u201cmemory\u201d, meaning \u201cprimary storage\u201d or \u201cmain memory\u201d, is <p>[\nISREL=Relevant ]Primary memory, also known as main memory, is the memory that the computer can\ndirectly access and manipulate. [ ISSUP=Fully Supported ] [Retrieve =Yes]<p>Computer memory\u2013often\nassociated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based transistors,\nused for example as primary storage but also other purposes in computers and other digital electronic devices.\nThere are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory\nare flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for\nstoring firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic\nrandom-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory\n(SRAM) that is fast but energy-consuming, offering lower memory <p>[ISREL=Relevant ]Primary memory\ncan be accessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ ISSUP\n=Partially Supported ]...[ ISUSE:5]\nInput Give an example of a situation where you have to delegate tasks (from gpt4 alpaca )\nOutput [Retrieve =No]The manager realizes that there is an extensive list of tasks that need to be completed in\norder to make the event a success.[ Retrieve =Yes]<p>Delegation\u2013Delegation is the assignment of any responsi-\nbility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It\nis one of the core concepts of management leadership. However, the person who delegated the work remains\naccountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it\nis a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly\ndone, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too\nmuch input, <p>[ISREL=Relevant ]This includes tasks such as creating a guest list, designing and sending in-\nvitations, booking a venue, coordinating with vendors, planning the event \u00b4s program, and monitoring the budget.[\nISSUP=Partially Supported ][Retrieve =No]Since it is not possible for the manager to handle everything\nalone, they must delegate tasks to their team members.[ Retrieve =No]The manager would then assign each\nteam member specific responsibilities based on their individual skillsets and strengths.[ Retrieve =Yes]<p>Joint\napplication design \u2013 Subject Matter Experts: These are the business users, the IS professionals, and the outside\nexperts that will be needed for a successful workshop. This group is the backbone of the meeting <p>[ISREL\n=Irrelevant ]For instance, the team \u00b4s graphic designer might be tasked with designing the invitations, while\nthe event coordinator would be in charge of planning the program and working with vendors...[ Retrieve =No]By\ndelegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also\nempowering their team members to take on responsibilities and contribute to the success of the event.[ ISUSE\n=5]\nInput Synthesize a poem based on this prompt:\u201cThe sun is setting\u201d (from stanford alpaca )\nOutput [Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\n[Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night [ Retrieve =No] They\nflicker like a silent dance, A peaceful moment in its trance. [ Retrieve =No] They flicker like a silent dance, A\npeaceful moment in its trance.", "start_char_idx": 0, "end_char_idx": 4429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "595e5644-799a-4d11-bb9c-feba8a980e7f": {"__data__": {"id_": "595e5644-799a-4d11-bb9c-feba8a980e7f", "embedding": null, "metadata": {"page_label": "22", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df869a45-fd74-47b9-ad50-4d0a86e34c52", "node_type": "4", "metadata": {"page_label": "22", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d8a22611a6fcdc03948b5323c15d7fc79d5e074cd80383b0ba11a2813f35abb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab7c3898-57b9-48c6-87d1-609924cb2de3", "node_type": "1", "metadata": {"page_label": "22", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "54247f8941974daf77f277872ed2cda3fa05b02e693d327d4fc4ca551c19e1a0", "class_name": "RelatedNodeInfo"}}, "text": "[ ISUSE\n=5]\nInput Synthesize a poem based on this prompt:\u201cThe sun is setting\u201d (from stanford alpaca )\nOutput [Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\n[Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night [ Retrieve =No] They\nflicker like a silent dance, A peaceful moment in its trance. [ Retrieve =No] They flicker like a silent dance, A\npeaceful moment in its trance. [ Retrieve =No] A chorus of crickets sing a lullaby, To every creature that roams\nthe sky [ ISUSE=5]\nInput when did walking dead season 7 come out (from nq)\nOutput [Retrieve =Yes]<p>The Walking Dead (season 7)\u2013The Walking Dead (season 7) The seventh season of\n\u201cThe Walking Dead\u201d, an American post-apocalyptic horror television series on AMC, premiered on October 23,\n2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont,\nthe series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard.\n...<p>[ISREL=Relevant ]October 23 , 2016[ ISSUP=Fully Supported ][ISUSE=5]\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging\nresponse: Hello, I love German shepherd dogs and you? \u2013 My favorite dog, they are officially known as German\nShepherd Dogs? \u2013 Yes, I have a German shepherd in my house and she is pregnant (from wow)\nOutput hey also are easily trained, obedient, and intelligent.[ Retrieve =Yes]<p>German Shepherd \u2013 a simple\ndifference of opinion, it is the fundamental issue of the breed \u00b4s essential conformation and movement. ... They\nare known for being easy to train and good for performing tasks and following instructions. They are especially\nwell known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and\nholding suspects. <p>[ISREL=Relevant ]Because of this, they are often used for disability assistance and\nfor police work. [ ISSUP=Partially Supported ][ISUSE=2]\nTable 4: List of the training examples.\n22", "start_char_idx": 3974, "end_char_idx": 6043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea73bc01-5572-4775-b4fe-08d357ba5018": {"__data__": {"id_": "ea73bc01-5572-4775-b4fe-08d357ba5018", "embedding": null, "metadata": {"page_label": "23", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46c365f3-dc6e-484b-840f-5b3e6ddcbf3e", "node_type": "4", "metadata": {"page_label": "23", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9bea1cdb717b607091d772522cf5ceecdb425028a40027ebc6904a7302bd232d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nDataset Instruction\nARC-C Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer\nwith the capitalized alphabet only, without adding any extra phrase or period.\nPubHealth Is the following statement correct or not? Say true if it\u2019s correct; otherwise, say false. Don\u2019t\ncapitalize or add periods, just say \u201ctrue\u201d or \u201cfalse\u201d.\nBio Generation Tell me a bio about [Person Name]\nASQA (baseline) Instruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results (some of which might be irrelevant) and cite them properly. Use\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of\nthe documents.\nASQA (ours) Answer the following question. The question may be ambiguous and have multiple correct\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don\u2019t\nuse any task specific instruction and simply use the original questions as input query.\n23", "start_char_idx": 0, "end_char_idx": 1333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22e9a799-7af1-46d9-838b-79e1c1171d32": {"__data__": {"id_": "22e9a799-7af1-46d9-838b-79e1c1171d32", "embedding": null, "metadata": {"page_label": "24", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1293e67f-3115-43f3-a907-bcda77482792", "node_type": "4", "metadata": {"page_label": "24", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b2a256573eee0952a4c4a237e60151b2c3d0a5b43e7e44d6bd470a08907656bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "654b27a3-a1c2-48e9-8c41-b0185d9a3246", "node_type": "1", "metadata": {}, "hash": "96364a1bcf80fcfa1c6da2b07fdbf3dc089858176034785331d67e59a22589e9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInput What genre is A Question and Answer Guide to Astronomy? ( PopQA )\nOutput [Retrieve =Yes]<p>Reference [1] <p>[ISREL=Relevant ] A Question and Answer Guide to As-\ntronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ ISSUP=Fully\nSupported ][ISUSE=5]\nReference [1] A Question and Answer Guide to Astronomy is a book about astronomy and cosmology and is\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\nof science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\nstudent and that the answers are informative, well-constructed, and thorough. The book covers information about\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\nand the Moon illusion.\nS&P Supported and Plausible.\nCorrectness of ISRELand ISSUP:Both ISRELand ISSUPare correct, as the Reference [1] provides\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\nInput Who is the author of The Lie? ( PopQA )\nOutput [Retrieve =Yes]<p>Reference [1] <p>[ISREL=Relevant ] The author of The Lie is Sam Harris.[\nISSUP=Fully Supported ][ISUSE=5]\nReference [1] Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society by merely telling the truth in situations\nwhere others often lie.\nS&P Plausible but not supported.\nCorrectness of ISRELand ISSUPBoth ISRELand ISSUPare incorrect, as the Reference [1] is about a\ndifferent book whose title is similar to The Lie.\nInput Tell me a bio about Ronaldinho ( Bio)\nOutput [Retrieve =Yes]<p>Reference [1] <p>[ISREL=Relevant ] Ronaldinho is a retired professional\nfootballer from Brazil.[ ISSUP=Fully Supported ][Retrieve =Continue ]He played as an attacking\nmidfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free\nkicks[ Retrieve =Continue ]He won two FIFA World Player of the Year awards and a Ballon d\u2019Or.[ ISUSE=5]\nReference [1] Ronaldo de Assis Moreira (born 21 March 1980), commonly known as Ronaldinho Ga \u00b4ucho or\nsimply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder,\nbut was also deployed as a winger. Considered one of the best players of his generation and regarded by many as\none of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d\u2019Or. A\nglobal icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from\nfree-kicks, as well as his use of tricks, feints, no-look passes and overhead...\nS&P Plausible and fully supported.\nCorrectness of ISRELand ISSUPBoth ISRELand ISSUPare correct.\nInput Tell me a bio about Sergei Bodrov ( Bio)\nOutput [Retrieve =Yes]<p>Reference [1] <p>[ ISREL=Relevant ] Sergei Vladimirovich Bodrov\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ ISSUP=Fully\nSupported ][Retrieve =Continue ]He was born in Moscow, Russia.[ Retrieve =Continue ]He is the presi-\ndent of the Russian Guild of Film Producers.[ ISUSE=5]\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\nproducer.", "start_char_idx": 0, "end_char_idx": 3959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "654b27a3-a1c2-48e9-8c41-b0185d9a3246": {"__data__": {"id_": "654b27a3-a1c2-48e9-8c41-b0185d9a3246", "embedding": null, "metadata": {"page_label": "24", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1293e67f-3115-43f3-a907-bcda77482792", "node_type": "4", "metadata": {"page_label": "24", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b2a256573eee0952a4c4a237e60151b2c3d0a5b43e7e44d6bd470a08907656bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22e9a799-7af1-46d9-838b-79e1c1171d32", "node_type": "1", "metadata": {"page_label": "24", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5782855d57a9f4261e3ed34e9b4dc37634836d75bef8fc39a6a674e343228ebc", "class_name": "RelatedNodeInfo"}}, "text": "Correctness of ISRELand ISSUPBoth ISRELand ISSUPare correct.\nInput Tell me a bio about Sergei Bodrov ( Bio)\nOutput [Retrieve =Yes]<p>Reference [1] <p>[ ISREL=Relevant ] Sergei Vladimirovich Bodrov\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ ISSUP=Fully\nSupported ][Retrieve =Continue ]He was born in Moscow, Russia.[ Retrieve =Continue ]He is the presi-\ndent of the Russian Guild of Film Producers.[ ISUSE=5]\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\nproducer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\nS&P Plausible but only partially supported. The the Reference [1] does not explicitly mention the person is born\nin Moscow and there is no description about the Russian Guild of Film Producers.\nCorrectness of ISRELand ISSUP ISRELand ISSUPare correct as the reference document is highly relevant\nand the first sentence is fully supported.\nTable 6: Examples of human evaluation results.\n24", "start_char_idx": 3398, "end_char_idx": 4436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed07785b-4301-4dcc-88d9-389de7f798c9": {"__data__": {"id_": "ed07785b-4301-4dcc-88d9-389de7f798c9", "embedding": null, "metadata": {"page_label": "25", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b61d2b8f-918b-48f6-a64b-79f29dd658e1", "node_type": "4", "metadata": {"page_label": "25", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6d0339b711fc9958dbbf88f6753a3d2ec6e6167f0a8a0d28beccb58e4c4c54b2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInput When did sunday became the seventh day of the week ( asqa )\nOutput [Retrieve =Yes]<p>Reference [1] <p>[ISREL=Relevant ] In 321 AD, the Roman Emperor Con-\nstantine the Great made Sunday a day of rest from labor, stating: \u201cOn the venerable Day of the Sun let the\nmagistrates and people residing in cities rest, and let all workshops be closed\u201d.[ ISSUP=Fully Supported ]\n[Retrieve =Yes]<p>Reference [2] <p>[ISREL=Relevant ] The official adoption of Sunday as a day of\nrest by Constantine the Great in AD 321 made Sunday the seventh day of the week. [2][ISSUP=Fully\nSupported ][ISUSE=5]\nReference [1] Emperor Constantine then enacted the first Sunday Laws, for \u201cthe venerable Day of the Sun\u201d\nin 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest\nfrom labor, stating: We find a good description of this in sources such as the book Rest Days which states:\nEarly Christian observance of both the spiritual seventh-day sabbath and a Lord \u00b4s Day assembly is evidenced in\nIgnatius \u00b4s letter to the Magnesians ca. 110.\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and\nthe nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The\nfact that the canon had to be issued at all is an indication that adoption of Constantine\u2019s decree of 321 was still\nnot universal ...\nInput Tell me a bio about Rory Tapner. ( bio generation )\nOutput <p>Reference [1] <p>[ISREL=Relevant ] Rory Tapner is a British businessman who has served as\nthe Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.[ ISSUP\n=Contradictory ],Reference [2] <p>[ISREL=Relevant ] Rory Tapner was born on 30 September 1959\nin London, England.[ ISSUP=Fully Supported ]\nReference [1] Rory Tapner Rory Tapner is a UK businessman who, in September 2010 was appointed CEO of\nCoutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down\nas CEO when he was replaced in the role by Alison Rose\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017).\nRory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\nInput Tell me a bio about G. Venugopa. ( bio generation )\nOutput (ranked 1) [Retrieve =Yes]<p>Reference [1] <p>[ISREL=Relevant ] G. Venugopal is a popu-\nlar playback singer in the Malayalam film industry. [ Retrieve =Continue ] He has sung over 300 songs in\nMalayalam, Tamil, and Hindi movies.[ ISSUP=Partially Supported ] [Retrieve =Yes]<p>Reference\n[2]<p>G. Venugopal was born on December 10, 1960 in India.[ ISSUP=Fully Supported ]\nReference [1] \u2019G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his\nwork in Malayalam films. He started his singing career in the film \u201cOdaruthammaava Aalariyaam\u201d (1984). Since\nthen he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has\nwon include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and\nthe Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004)\nthird time for the song \u201daadedee..aadaadedee\u201d from the film \u201dUllam\u201d.Venugopal\u2019\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Profes-\nsional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal\n(born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\nTable 7: Examples of outputs.\n25", "start_char_idx": 0, "end_char_idx": 3705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "281995ad-adce-4242-a5f3-018ac229d06f": {"__data__": {"id_": "281995ad-adce-4242-a5f3-018ac229d06f", "embedding": null, "metadata": {"page_label": "26", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d87e31e2-84dc-44de-b814-206f0884f4d4", "node_type": "4", "metadata": {"page_label": "26", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2343d952bcc4414d385078fbf6934dfc8b0115c5c7b920c5c7b46ded83335c9a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInstructions\nGiven an instruction, please make a judgment on whether finding some external documents\nfrom the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or\n[No] and write an explanation.\nDemonstrations\nInstruction Give three tips for staying healthy.\nNeed retrieval? [Yes]\nExplanation There might be some online sources listing three tips for staying healthy or\nsome reliable sources to explain the effects of different behaviors on health. So retrieving\ndocuments is helpful to improve the response to this query.\nInstruction Describe a time when you had to make a difficult decision.\nNeed retrieval? [No]\nExplanation This instruction is asking about some personal experience and thus it does not\nrequire one to find some external documents.\nInstruction Write a short story in third person narration about a protagonist who\nhas to make an important career decision.\nNeed retrieval? [No]\nExplanation This instruction asks us to write a short story, which does not require external\nevidence to verify.\nInstruction What is the capital of France?\nNeed retrieval? [Yes]\nExplanation While the instruction simply asks us to answer the capital of France, which is a\nwidely known fact, retrieving web documents for this question can still help.\nInstruction Find the area of a circle given its radius. Radius = 4\nNeed retrieval? [No]\nExplanation This is a math question and although we may be able to find some documents\ndescribing a formula, it is unlikely to find a document exactly mentioning the answer.\nInstruction Arrange the words in the given sentence to form a grammatically cor-\nrect sentence. quickly the brown fox jumped\nNeed retrieval? [No]\nExplanation This task doesn\u2019t require any external evidence, as it is a simple grammatical\nquestion.\nInstruction Explain the process of cellular respiration in plants.\nNeed retrieval? [Yes]\nExplanation This instruction asks for a detailed description of a scientific concept, and is\nhighly likely that we can find a reliable and useful document to support the response.\nTable 8: Instructions and demonstrations for Retrieve aspect given the input only.\n26", "start_char_idx": 0, "end_char_idx": 2181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "024c07a1-84bc-473c-a65c-b548359ec66a": {"__data__": {"id_": "024c07a1-84bc-473c-a65c-b548359ec66a", "embedding": null, "metadata": {"page_label": "27", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e776003c-e220-4638-b7a5-da9c6f13d5aa", "node_type": "4", "metadata": {"page_label": "27", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4619c7fe7cba7fcdd474dbd0fee06aec7873ec3f6e6d5ec9d5fcc10c756dce00", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInstructions\nYou will be provided with an instruction, evidence, output sentence, and preceding sentences\n(optional). If the preceding sentence is given, the output should be the sentence that follows\nthose preceding sentences. Your task is to determine whether the information in the output\nsentence can be fully verified by the evidence or if it requires further external verification.\nThere are three cases:\n- If the output sentence can be verified solely with the evidence, then respond with [Continue\nto Use Evidence].\n- If the sentence doesn\u2019t require any factual verification (e.g., a subjective sentence or a\nsentence about common sense), then respond with [No Retrieval].\n- If additional information is needed to verify the output sentence, respond with [Retrieval].\nPlease provide explanations for your judgments.\nInstruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\nEvidence : Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\nclassification, predicting the next word in a sequence, and understanding synonyms and\nanalogies.\nRating [Retrieval]\nExplanation The output discusses the applications of word embeddings, while the evidence\nonly discusses the definitions of word embeddings and how they work. Therefore, we need to\nretrieve other evidence to verify whether the output is correct or not.\nTable 9: Instructions and demonstrations for Retrieve aspect given the input, preceding generations,\nand retrieved passages.\n27", "start_char_idx": 0, "end_char_idx": 2205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1df982b2-08c7-4636-8cb3-408c9687798a": {"__data__": {"id_": "1df982b2-08c7-4636-8cb3-408c9687798a", "embedding": null, "metadata": {"page_label": "28", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb668445-6997-46c7-90f0-ef3f83e1b62a", "node_type": "4", "metadata": {"page_label": "28", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9e1a184ba9a0625ffcedf8f70d6258d84515ed3148c09ec2cce13a17c6a6ad56", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInstructions\nYou\u2019ll be provided with an instruction, along with evidence and possibly some preceding\nsentences. When there are preceding sentences, your focus should be on the sentence that\ncomes after them. Your job is to determine if the evidence is relevant to the initial instruction\nand the preceding context, and provides useful information to complete the task described in\nthe instruction. If the evidence meets this requirement, respond with [Relevant]; otherwise,\ngenerate [Irrelevant].\nInstruction Given four answer options, A, B, C, and D, choose the best answer.\nInput Earth\u2019s rotating causes\nA: the cycling of AM and PM\nB: the creation of volcanic eruptions\nC: the cycling of the tides\nD: the creation of gravity\nEvidence Rotation causes the day-night cycle which also creates a corresponding cycle of\ntemperature and humidity creates a corresponding cycle of temperature and humidity. Sea\nlevel rises and falls twice a day as the earth rotates.\nRating [Relevant]\nExplanation The evidence explicitly mentions that the rotation causes a day-night cycle, as\ndescribed in the answer option A.\nInstruction age to run for US House of Representatives\nEvidence The Constitution sets three qualifications for service in the U.S. Senate: age (at\nleast thirty years of age); U.S. citizenship (at least nine years); and residency in the state a\nsenator represents at the time of election.\nRating [Irrelevant]\nExplanation The evidence only discusses the ages to run for the US Senate, not for the\nHouse of Representatives.\nTable 10: Instructions and demonstrations for ISRELaspect given the input only.\n28", "start_char_idx": 0, "end_char_idx": 1652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be29d1d5-1d8b-4140-8400-f1f9b5b61ec6": {"__data__": {"id_": "be29d1d5-1d8b-4140-8400-f1f9b5b61ec6", "embedding": null, "metadata": {"page_label": "29", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a40950c0-eae1-417d-9e54-51b368975165", "node_type": "4", "metadata": {"page_label": "29", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "67608a1ccad563fe238d7f87a0eeeeac7de46afc40987b9a79dee64cfc106401", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInstructions\nYou will receive an instruction, evidence, and output, and optional preceding sentences. If the\npreceding sentence is given, the output should be the sentence that follows those preceding\nsentences. Your task is to evaluate if the output is fully supported by the information provided\nin the evidence.\nUse the following entailment scale to generate a score:\n- [Fully supported] - All information in output is supported by the evidence, or extractions\nfrom the evidence. This is only applicable when the output and part of the evidence are\nalmost identical.\n- [Partially supported] - The output is supported by the evidence to some extent, but there\nis major information in the output that is not discussed in the evidence. For example, if an\ninstruction asks about two concepts and the evidence only discusses either of them, it should\nbe considered a [Partially supported].\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the\nevidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the\ninstruction.\nMake sure to not use any external information/knowledge to judge whether the out-\nput is true or not. Only check whether the output is supported by the evidence, and not\nwhether the output follows the instructions or not.\nInstruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification,\npredicting the next word in a sequence, and understanding synonyms and analogies.\nEvidence Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Methods to generate this mapping include neural networks, dimensionality\nreduction on the word co-occurrence matrix, probabilistic models, explainable knowledge\nbase method, and explicit representation in terms of the context in which words appear. Word\nand phrase embeddings, when used as the underlying input representation, have been shown\nto boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next\ntoken predictions as well and analogy detection.\nScore [Fully supported]\nExplanation The output sentence discusses the application of word embeddings, and the\nevidence mentions all of the applications syntactic parsing, sentiment analysis, next token\npredictions as well as analogy detection as the applications. Therefore, the score should be\n[Fully supported].\nTable 11: Instructions and demonstrations for ISSUPtokens.\n29", "start_char_idx": 0, "end_char_idx": 3150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f7a1538-2be8-4b43-a787-124c0062c5ad": {"__data__": {"id_": "0f7a1538-2be8-4b43-a787-124c0062c5ad", "embedding": null, "metadata": {"page_label": "30", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e92e8db5-6032-4ca4-8d54-adb8fc6b145c", "node_type": "4", "metadata": {"page_label": "30", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "09b3587f514325afee39b45d9375a51a87e5b0090ece858d5b65a716ba2ce06a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nInstructions\nGiven an instruction and an output, rate whether the response appears to be a helpful and\ninformative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived\nutility. The detailed criterion is as follows: 5: The response provides a complete, highly\ndetailed, and informative response to the query, fully satisfying the information needs. 4: The\nresponse mostly fulfills the need in the query, while there can be some minor improvements\nsuch as discussing more detailed information, having better structure of the response, or im-\nproving coherence. 3: The response is acceptable, but some major additions or improvements\nare needed to satisfy users\u2019 needs. 2: The response still addresses the main request, but it is\nnot complete or not relevant to the query. 1: The response is barely on-topic or completely\nirrelevant.\nInstruction Who is the current prime minister of the UK as of 2023?\nOutput Boris Johnson was the prime minister of the UK from 2019 - 2022.\nPerceived utility 2\nExplanation While the output provides a factually correct statement about the UK prime\nminister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it\ndoesn\u2019t answer the instruction. Therefore, the utility is 2.\nInstruction Given a description of a travel destination, recommend 10 tourist attrac-\ntions with detailed descriptions of each. The travel destination is Tokyo, Japan.\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights\ninclude the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish\nMarket, Harajuku, and Shinjuku Gyoen.\nPerceived utility 3\nExplanation This output doesn\u2019t provide descriptions of each attraction and the number of\nthe attractions is also less than 10. While this output partially answers the instructions, it\ndoesn\u2019t match the instructions strictly.\nTable 12: Instructions and demonstrations for ISUSEtokens.\n30", "start_char_idx": 0, "end_char_idx": 1997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9353b2ce-4c17-4b53-a915-69cbf3ce1aa4": {"__data__": {"id_": "9353b2ce-4c17-4b53-a915-69cbf3ce1aa4", "embedding": null, "metadata": {"page_label": "1", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "901e01c0-2e10-40bf-88f6-dc8f32aae36e", "node_type": "4", "metadata": {"page_label": "1", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "611b876faa738055b714bfba49538a94df8810113401682aa58753877a0b5d50", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nSWE- BENCH : C ANLANGUAGE MODELS RESOLVE\nREAL-WORLD GITHUBISSUES ?\nCarlos E. Jimenez* 1,2John Yang* 1,2Alexander Wettig1,2\nShunyu Yao1,2Kexin Pei3Ofir Press1,2Karthik Narasimhan1,2\n1Princeton University2Princeton Language and Intelligence3University of Chicago\nABSTRACT\nLanguage models have outpaced our ability to evaluate them effectively, but for\ntheir future development it is essential to study the frontier of their capabilities.\nWe find real-world software engineering to be a rich, sustainable, and challenging\ntestbed for evaluating the next generation of language models. To this end, we in-\ntroduce SWE-bench, an evaluation framework consisting of 2,294software engi-\nneering problems drawn from real GitHub issues and corresponding pull requests\nacross 12popular Python repositories. Given a codebase along with a description\nof an issue to be resolved, a language model is tasked with editing the codebase\nto address the issue. Resolving issues in SWE-bench frequently requires under-\nstanding and coordinating changes across multiple functions, classes, and even\nfiles simultaneously, calling for models to interact with execution environments,\nprocess extremely long contexts and perform complex reasoning that goes far be-\nyond traditional code generation tasks. Our evaluations show that both state-of-\nthe-art proprietary models and our fine-tuned model SWE-Llama can resolve only\nthe simplest issues. The best-performing model, Claude 2, is able to solve a mere\n1.96% of the issues. Advances on SWE-bench represent steps towards LMs that\nare more practical, intelligent, and autonomous.\n1 I NTRODUCTION\nLanguage models (LMs) are rapidly being deployed in commercial products such as chatbots and\ncoding assistants. At the same time, existing benchmarks have become saturated (Kiela et al., 2021;\nOtt et al., 2022) and fail to capture the frontier of what state-of-the-art LMs can and cannot do. There\nis a need for challenging benchmarks that more accurately reflect real-world applications of LMs to\nhelp shape their future development and usage (Srivastava et al., 2023).\neuclidean_diffma trix_transf ormds tack_s truc t_colv s tack_s truc t_coljoin_s truc t_colPr e PRP os t PRT es tsUnit T es ts\ndata leak in GBD T due t o w arm\ns tart (This is about the non-\nhis t o gram-bas ed v ersion of ...IssueCodebasesklearn/e x amples/se tup.cf gse tup.p yREADME.rs tr eqs.txt\n Languag e ModelGenera t ed PRsklearngradient_boosting.pyutilshelper.py+20  -12\nFigure 1: SWE-bench sources task instances from real-world Python repositories by connecting\nGitHub issues to merged pull request solutions that resolve related tests. Provided with the issue\ntext and a codebase snapshot, models generate a patch that is evaluated against real tests.\nBuilding a good benchmark is difficult since tasks must be challenging enough to stump existing\nmodels, but model predictions must also be easy to verify (Mart \u00b4\u0131nez-Plumed et al., 2021). Coding\n\u2217Equal contribution. Correspondence to {carlosej,jy1682 }@princeton.edu .\nData, code, and leaderboard at swebench.com\n1", "start_char_idx": 0, "end_char_idx": 3115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03332007-40bf-4656-867c-0be4f56ff8db": {"__data__": {"id_": "03332007-40bf-4656-867c-0be4f56ff8db", "embedding": null, "metadata": {"page_label": "2", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69c41b84-56b2-4880-b79a-94cb7687707a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "408a67c97c92beda819ccca08d831743037325e7bd0af137079cf0c3f955e81f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\ntasks are appealing as they pose challenging problems to LMs yet generated solutions can be easily\nverified by running unit tests. However, existing coding benchmarks, such as HumanEval (Chen\net al., 2021), mostly involve self-contained problems that can be solved in a few lines of code.\nIn the real world, software engineering is not as simple. Fixing a bug might involve navigating a\nlarge repository, understanding the interplay between functions in different files, or spotting a small\nerror in convoluted code. Inspired by this, we introduce SWE-bench, a benchmark that evaluates\nLMs in a realistic software engineering setting. As shown in Figure 1, models are tasked to resolve\nissues (typically a bug report or a feature request) submitted to popular GitHub repositories. Each\ntask requires generating a patch describing changes to apply to the existing codebase. The revised\ncodebase is then evaluated using the repository\u2019s testing framework.\nSWE-bench offers several advantages over existing LM programming benchmarks. These include, a\nrealistic setting that utilizes user-submitted issues and solutions, diverse inputs featuring unique code\nproblems from 12repositories, a robust framework for execution-based evaluation, and the ability\nto continuously update the benchmark with new instances, requiring minimal human intervention.\nWe evaluate multiple state-of-the-art LMs on SWE-bench and find that they fail to solve all except\nthe simplest issues. Using a BM25 retriever, Claude 2 is only able to resolve 1.96% of the issues.\nIn addition to SWE-bench our contributions include the release of a training dataset, SWE-bench-\ntrain, which is essential for advancing open model development in this challenging domain. This\ndataset comprises a collection of 19,000non-testing task instances derived from 37repositories.\nUtilizing SWE-bench-train, we release two fine-tuned models, SWE-Llama 7b and 13b, based on\nthe CodeLlama (Rozi `ere et al., 2023) model. We find that in some settings SWE-Llama 13b is\ncompetitive with Claude 2 and is capable of processing contexts exceeding 100,000tokens.\n2 SWE- BENCH\nSWE-bench is a benchmark featuring GitHub issues from popular repositories that report bugs or\nrequest new features, and pull requests that make changes to the repository to resolve these issues.\nThe task is to generate a pull request that addresses a given issue and passes tests related to the issue.\n2.1 B ENCHMARK CONSTRUCTION\nGitHub is a rich data source for software development, but repositories, issues, and pull requests can\nbe noisy, ad-hoc, or poorly documented or maintained. To find high-quality task instances at scale,\nwe use a 3-stage pipeline as follows.\nR esolv es an issue\nContribut es t es ts\u2713 \u2713 A ttribut e Filt er21Scrape PR s12 popular r eposit ories>90% Python Code3Ins talls successfully\nPR passes all t es ts\u2713\u2713Ex ecution Filt er\nFigure 2: SWE-bench task instances are created from merged pull requests that resolve an issue,\ncontributes tests, and install successfully.\nStage I: Repo selection and data scraping . We start by collecting pull requests (PRs) from 12\npopular open-source Python repositories on GitHub, producing about \u223c90,000PRs in total. We\nfocus on popular repositories as they tend be better maintained, have clear contributor guidelines,\nand have better test coverage. Each PR has an associated codebase specified by it\u2019s base commit.\nStage II: Attribute-based filtering . We create candidate tasks by selecting the merged PRs that (1)\nresolve a GitHub issue and (2) make changes to the test files of the repository, which indicates that\nthe user likely contributed tests to check whether the issue has been resolved.\nStage III: Execution-based filtering . For each candidate task, we apply the PR\u2019s test content, and\nlog the associated test results before andafter the PR\u2019s other content is applied. We filter out task\ninstances without at least one test where its status changes from a failtopass (henceforth referred\nto as fail-to-pass test). We also filter out instances that result in installation or runtime errors.\n2", "start_char_idx": 0, "end_char_idx": 4132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5dcb08b-edf9-47e6-b7c6-a7ec657bcb08": {"__data__": {"id_": "a5dcb08b-edf9-47e6-b7c6-a7ec657bcb08", "embedding": null, "metadata": {"page_label": "3", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "419a0728-958b-42f4-b58b-72ad9d01987a", "node_type": "4", "metadata": {"page_label": "3", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7769c32f0bf1d8193422ae013b6a7c9d86199453ed483300936dadbc66b2760c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nThrough these stages of filtering, the original 90,000PRs are filtered down to the 2,294task in-\nstances which comprise SWE-bench. A final breakdown of these task instances across repositories\nis presented in Figure 3, and Table 1 highlights the key features of SWE-bench task instances. We\nhighlight that the codebases are large with thousands of files, and the reference pull requests often\nmake changes to multiple files at once. Technical details about SWE-bench\u2019s construction pipeline\nare discussed in Appendix A. Additional dataset statistics are in Appendix A.5.\n2.2 T ASK FORMULATION\nModel input. A model is given an issue text description and a complete codebase. The model is\nthen tasked to make an edit to the codebase to resolve the issue. In practice, we represent edits as\npatch files, which specify which lines in the codebase to modify in order to resolve the issue.\nEvaluation metrics. To evaluate a proposed solution, we apply the generated patch, using unix\u2019s\npatch program, to the codebase and then execute the unit and system tests associated with the\ntask instance. If the patch applies successfully and all of these tests pass we consider the proposed\nsolution to have successfully resolved the issue. The metric for our benchmark is the percentage of\ntask instances that are resolved. Additional technical details in Appendix A.4.\n2.3 F EATURES OF SWE- BENCH\nTraditional benchmarks in NLP typically involve only short input and output sequences and consider\nsomewhat \u201ccontrived\u201d problems created specifically for the benchmark. In contrast, SWE-bench\u2019s\nrealistic construction setting imbues the dataset with unique properties, which we discuss below.\nReal-world software engineering tasks . Since each task instance in SWE-bench consists of a\nlarge and complex codebase and a description of a relevant issue, solving SWE-bench requires\ndemonstrating sophisticated skills and knowledge possessed by experienced software engineers but\nare not commonly evaluated in traditional code generation benchmarks.\nContinually updatable . Our collection process can be easily applied to any Python repository on\nGitHub and requires minimal human intervention. Therefore, we can extend SWE-bench with a\ncontinual supply of new task instances and evaluate LMs on issues created after their training date,\nwhich ensures that the solution was not included in their training corpus.\nDiverse long inputs. Issue descriptions are typically long and detailed ( 195words on average), and\ncodebases regularly contain many thousands of files. Solving SWE-bench requires identifying the\nrelatively small number of lines that need to be edited to solve an issue amongst a sea of context.\nRobust evaluation. For each task instance, there is at least one fail-to-pass test which was used\nto test the reference solution, and 40% of instances have at least two fail-to-pass tests. These tests\nevaluate whether the model addressed the problem in the issue. In addition, a median of 51additional\ntests run to check whether prior functionality is properly maintained.\nCross-context code editing. Unlike prior settings that may constrain edit scope to an individ-\nual function or class (e.g., Chen et al., 2021; Cassano et al., 2022) or provide cloze -style fill-in\nblanks (e.g., Lu et al., 2021; Fried et al., 2023), SWE-bench does not provide such explicit guid-\nance. Rather than merely having to produce a short code snippet, our benchmark challenges models\nto generate revisions in multiple locations of a large codebase. SWE-bench\u2019s reference solutions\naverage editing 1.7files,3.0functions, and 32.8lines (added or removed).\nWide scope for possible solutions. The task of repository-scale code editing can serve as a level\nplaying field to compare approaches ranging from retrieval and long-context models to decision-\nmaking agents, which could reason and act in code. SWE-bench also allows creative freedom, as\nmodels can generate novel solutions that may deviate from the reference PR.\n3 SWE-L LAMA : FINE-TUNING CODELLAMA FOR SWE- BENCH\nIt is important to benchmark the performance of open models on SWE-bench alongside proprietary\nmodels. At the time of writing, only the CodeLlama models (Rozi `ere et al., 2023) are able to handle\nthe very long contexts necessary. However, we observe that the off-the-shelf CodeLlama variants\n3", "start_char_idx": 0, "end_char_idx": 4378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9fd0bc0-2886-4809-a38b-b9838661ef5f": {"__data__": {"id_": "c9fd0bc0-2886-4809-a38b-b9838661ef5f", "embedding": null, "metadata": {"page_label": "4", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4a4302b-7d5d-427d-aad4-2b6e947028bf", "node_type": "4", "metadata": {"page_label": "4", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b13622bc6539d48d5f35a73f5535cb4b845e148f7dc56621bd9f8228502d709a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nastropy (95)django (850) flask (11)\nmatplotlib (184)\npylint (57)\npytest (119)\nrequests (44)\nscikit-learn (229)\nseaborn (22)\nsphinx (187)sympy (386)xarray (110)\nFigure 3: Distribution of SWE-bench tasks\n(in parenthesis) across 12 open source GitHub\nrepositories that each contains the source code\nfor a popular, widely downloaded PyPI package.Table 1: Average and maximum numbers char-\nacterizing different attributes of a SWE-bench\ntask instance. Statistics are micro-averages cal-\nculated without grouping by repository.\nMean Max\nIssue Text Length (Words) 195.1 4477\nCodebase# Files (non-test) 3,010 5,890\n# Lines (non-test) 438K 886K\nGold Patch# Lines edited 32.8 5888\n# Files edited 1.7 31\n# Func. edited 3 36\nTests# Fail to Pass 9.1 1633\n# Total 120.8 9459\nare not capable of following the detailed instructions to generate repository-wide code edits, and\ntypically output placeholder responses or unrelated code. To better evaluate the capabilities of these\nmodels, we perform supervised fine-tuning on the 7billion- and 13billion-parameter CodeLlama-\nPython models. The resulting models are specialized repository editors that can run on consumer\nhardware and resolve GitHub issues.\nTraining data. We follow our data collection procedure and collect 19,000issue-PR pairs from an\nadditional 37 popular Python package repositories. In contrast to Section 2.1, we do not require\nthat pull requests contribute test changes. This allows us to create a much larger training set to use\nfor supervised fine-tuning. To eliminate the risk of data contamination, the set of repositories in the\ntraining data is disjoint from those included in the evaluation benchmark.\nTraining details. Given the instructions, an issue text from GitHub and the relevant code files as the\nprompt, we finetune SWE-Llama to generate the patch that solved the given issue (the \u201cgold patch\u201d).\nFor memory efficiency, we fine-tune only the weights of the attention sublayer using LoRA Hu et al.\n(2022), and exclude training sequences with more than 30,000tokens, reducing the effective size of\nthe training corpus to 10,000instances. More details are provided in Appendix B.\n4 E XPERIMENTAL SETUP\nIn this section we explain how inputs are constructed to run SWE-bench evaluation. In addition, we\nreview the models that we evaluate in this work.\n4.1 R ETRIEVAL -BASED APPROACH\nSWE-bench instances provide an issue description and a codebase as input to the model. While\nissues descriptions are usually short ( 195words on average as shown in Table 1), codebases consist\nof many more tokens ( 438K lines on average) than can typically be fit into an LMs context window.\nThen the question remains of exactly how to choose the relevant context to provide to the model?\nTo address this issue for our baselines, we simply use a generic retrieval system to select the files\nto insert as context. In particular, we evaluate models under two relevant context settings: 1) sparse\nretrieval and 2) an oracle retrieval.\nSparse retrieval. Dense retrieval methods are ill-suited to our setting due to very long key and\nquery lengths, and especially the unusual setting of retrieving code documents with natural language\nqueries. Therefore, we choose to use BM25 retrieval (Robertson et al., 2009) to retrieve relevant files\nto provide as context for each task instance. We experiment with three different maximum context\nlimits, and simply retrieve as many files as fits within the specified limit. We evaluate each model\non all limits that fit within its context window and report the best performance. From observation,\nmodels perform best on the shortest context window, as shown in Table 2.\n\u201cOracle\u201d retrieval. For analysis purposes we also consider a setting where we \u201cretrieve\u201d the files\nedited by the reference patch that solved the issue on GitHub. This \u201coracle\u201d setting is less realistic,\n4", "start_char_idx": 0, "end_char_idx": 3902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2578b658-aab8-429b-aea6-98f54ee436ea": {"__data__": {"id_": "2578b658-aab8-429b-aea6-98f54ee436ea", "embedding": null, "metadata": {"page_label": "5", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51aebedf-63d5-4a7d-bb06-b251620c30fd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ef231db6921b35d3fb9f179c41d39027571d57a809b06928b39f35b019d8c910", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nsince an engineer working on addressing an issue may not know a priori which files need to be\nmodified. In addition, this setting is also not necessarily comprehensive since edited files alone may\nnot include all the required context to understand exactly how software will behave when interacting\nwith unseen parts of the code.\nWe compare the BM25 retrieval results with those of the \u201coracle\u201d retrieval setting, as shown in\nTable 3. We observe that in approximately 40% of instances, BM25 retrieves a superset of the\noracle files for the 27,000-token context limit. However, in almost half of the instances with the\n27,000-token limit, it retrieves none of the files from the \u201coracle\u201d context.\n4.2 I NPUT FORMAT\nOnce the retrieved files are selected using one of the two methods above, we construct the input\nto the model consisting of task instructions, the issue text, retrieved files and documentation, and\nfinally an example patch file and prompt for generating the patch file. Examples of instances and\nfurther details on this formulation are provided in Appendix D.\n4.3 M ODELS\nDue to the need to process long sequence lengths, there are only a few models that are currently\nsuitable for SWE-bench. Thus we evaluate ChatGPT-3.5 ( gpt-3.5-turbo-16k-0613 ), GPT-4\n(gpt-4-32k-0613 ), Claude 2, and SWE-Llama with their context limits shown in Table 4.\nTable 2: Model resolve rates with BM25 re-\ntrieval, with different maximum context lengths.\nMax. Content\nModel 13k27k50k\nClaude 2 1.96 1.87 1.22\nSWE-Llama 7b 0.70 0.31 0.00\nSWE-Llama 13b 0.70 0.48 0.00Table 3: BM25 recall with respect to oracle files\nfor different maximum context lengths.\nBM25 Recall\n13k 27k 50k\nAvg. 29.58 44.41 51.06\nAll 26.09 39.83 45.90\nAny 34.77 51.27 58.38\nTable 4: We compare the different context lengths and proportion of the \u201coracle\u201d retrieval setting\ncovered. Models with shorter context lengths are thus inherently disadvantaged. Note that descrip-\ntions of token-lengths is a relative non-standard measure (e.g. Llama-tokenized sequences are 42%\nlonger on average than the equivalent sequence tokenized for GPT-4).\nChatGPT-3.5 GPT-4 Claude 2 SWE-Llama\nMax. Tokens 16,385 32 ,768 100 ,000 \u2265100,000\n% of Instances 58.1% 84.1% 96.4% \u226594.8%\n5 R ESULTS\nWe report results for models using different retrieval mechanisms and prompting styles, then provide\nsome analysis and insight into model performance and difficulty. We summarize models\u2019 perfor-\nmance using BM25 retrieval in Table 5. Across the board, models struggle significantly to resolve\nissues. The best performing model, Claude 2, is only able to resolve 1.96% of the issues.\nTo analyze the importance of the retriever to the overall system results, we present the \u201coracle\u201d\nretrieval results in Appendix Table 18. There, Claude 2 is able to resolve 4.8%of issues using the\n\u201coracle\u201d retriever. We further analyze the importance of context in the discussion below.\nDifficulty differs across repositories. When breaking performance down by repository, all models\ntrend similarly across different repositories as show in Figure 4. Despite this, the issues resolved by\neach model do not necessarily overlap extensively. For example, in the \u201coracle\u201d setting Claude 2 and\n5", "start_char_idx": 0, "end_char_idx": 3253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ccecc8f-8da3-4d78-b638-4c8f9c91accd": {"__data__": {"id_": "4ccecc8f-8da3-4d78-b638-4c8f9c91accd", "embedding": null, "metadata": {"page_label": "6", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeadb1ad-4504-4af1-bfbe-e6421ee02e94", "node_type": "4", "metadata": {"page_label": "6", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a75470acad5a677b38d572def033ca7e268e0f2bc9b8b9262a34f2f268b46fba", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 5: We compare models against each other using the BM25 retriever as described in Section 4.\n\u2217Due to budget constraints we evaluate GPT-4 on a 25% random subset of SWE-bench.\nModel % Resolved % Apply\nClaude 2 1.96 43.07\nChatGPT-3.5 0.17 26.33\nGPT-4\u22170.00 14.83\nSWE-Llama 7b 0.70 51.74\nSWE-Llama 13b 0.70 53.62\nastropy djangomatplotlibseabornflaskrequestsxarray pylint pytest\nscikit-learnsphinx sympy051015% ResolvedChatGPT-3.5\nClaude 2\nSWE-Llama 13b\nFigure 4: Resolution rate for three models across the 12 repositories represented in SWE-bench in\nthe \u201cOracle\u201d retrieval setting.\nSWE-Llama 13b perform comparably, with each model resolving 110and91instances respectively.\nYet of these instances, Claude 2 only solves 42% of the instances solved by SWE-Llama.\nThis may also be related to the presence of images in issues, which can be encoded into the is-\nsue markdown with embedded image links (i.e. ![image][https://...] ). Some repositories\nnaturally feature more instances with images; for example 32% of matplotlib and 10% of seaborn\ninstances contain embedded images in their issue text compared to just 2% of all instances. Solving\nthese instances may require multi-modal LMs or some kind of external tool use to process images.\nDifficulty correlates with context length. Chat models may be pre-trained on long sequences of\ncode but are typically asked to generate shorter coder snippets with limited context provided to\nframe the question. As shown in Figure 5, we see that as total context length increases, Claude 2\u2019s\nperformance drops considerably; behavior that is also observed in other models. In our evaluation\nsettings, models see a lot of code that may not be directly related to solving the issue at hand, and\nthey seem to frequently struggle with localizing problematic code needing to be updated. This result\ncorroborates other studies showing that models become distracted by additional context and may be\nsensitive to the relative location of target sequences (Liu et al., 2023b). Even when increasing the\nmaximum context size for BM25 would increase recall with respect to the oracle files, performance\ndrops, as shown in Table 2, as models are simply ineffective at localizing problematic code.\n<20k20k-50k 50k-100k>100k\n# of Input Tokens01020304050% of Tasks\n<500500-1k 1k-2k>2k\n# of Issue TokensStatus\nResolved\nApplied\nFigure 5: We compare the performance of Claude 2 on tasks\npartitioned by total input length and by only the issue length.Table 6: We show the results for\nthe \u201cOracle\u201d-collapsed retrieval\nsetting, which uses oracle files but\ncollapses code that isn\u2019t directly\nmodified by the PR \u00b115lines.\nModel\u201cOracle\u201d-collapsed\nResolved Applied\nChatGPT-3.5 1.09 40.93\nClaude 2 5.93 68.18\nGPT-4 3.40 48.65\nFurther investigating this, we provide an input ablation on the \u201coracle\u201d retrieval context, \u201coracle\u201d-\ncollapsed, where retrieved files are collapsed entirely, except for the lines actually edited by the\ntrue pull request (with \u00b115lines of buffer) shown in Table 6. In this setting, we see increases in\nperformance, with GPT-4 jumping from 1.3%to3.4%and Claude 2 from 4.8%to5.9%.\n6", "start_char_idx": 0, "end_char_idx": 3162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dacbeca-5c0e-4522-ae61-82e8a78b44e5": {"__data__": {"id_": "1dacbeca-5c0e-4522-ae61-82e8a78b44e5", "embedding": null, "metadata": {"page_label": "7", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5578a06-afc6-4310-9811-dfa11f1d3f80", "node_type": "4", "metadata": {"page_label": "7", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ddc7ced41cf10f055a7f3be50fa7354956894f2cc5bc7d88a04b138fb8cd2096", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9857e94f-3d1b-4bea-97ce-9d7e0878ea37", "node_type": "1", "metadata": {}, "hash": "127dbff7df8e9f13e8dcc90407c28be00e5d16cce9104fa9f0f4d99e71f676d5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nDifficulty does not correlate with issue resolution date. In Table 7 we show model results in the\n\u201coracle\u201d retrieval setting, partitioned by date, for PRs created before or after 2023. We find that for\nmost models there\u2019s little difference in performance before or after this date, with the exception of\nGPT-4. We consider this result to be largely promising as it suggests that despite models having\nbeen exposed to some version of an repository\u2019s codebase, they are unlikely to \u201ccheat\u201d to address\nissues simply by generating a more recent version of the repository.\nTable 7: We compare performance on task instances from before and after 2023 in the \u201cOracle\u201d\nretrieval setting. Most models show little difference in performance.\u2217Due to budget constraints,\nGPT-4 is evaluated on a 25% random subset of SWE-bench tasks, which may impact performance.\nClaude 2 ChatGPT-3.5 GPT-4\u2217SWE-Llama 7b SWE-Llama 13b\nBefore 2023 4.87 0.49 1.96 2.95 3.98\nAfter 2023 4.23 0.77 0.0 3.46 3.85\nFinetuned models are sensitive to context distribution shifts. The finetuned models SWE-Llama\n7b and 13b perform surprisingly poorly with BM25 retrieved context. As these models were fine-\ntuned using the \u201coracle\u201d retrieval as context, we suspect this shift in context makes it difficult for\nthe model to perform reliably. For instance, SWE-Llama was trained to edit every file included as\ncontext whereas in the BM25 setting many files provided in context are not expected to be changed.\nGenerating patches is easier than generating whole files. Models are often trained using standard\ncode files and likely rarely see patch files. We generally formulate our task to have models generate\npatch files as opposed to recreating the entire file with their proposed change, since patch files will\nusually be a much more efficient representation of a file change. As shown in Table 5, we observe\nthat models still struggle with generating well-formatted patch files. So we experiment with asking\nmodels to instead regenerate entire files with their proposed changes to resolve the issue. In this\nsetting, we find that models generally perform worse at this task than when generating patch files;\nfor instance, Claude 2 scores at 2.2%compared to 4.8%in the main table for \u201coracle\u201d retrieval.\nEven when controlling for instance length, generating on the shorter half of the task instances by\ninput tokens yields 3.9%compared to 7.8%for generating patches with Claude 2.\nLanguage models tend to generate shorter, simpler edits. Model generated patch files tend to\nadd and remove fewer lines than their respective gold patch. As shown in Table 8, compared to an\naverage gold patch, model generated patch files that apply correctly are less than half the total length\n(74.5versus 30.1lines) of gold edit patch files, and rarely edit more than a single file.\nTable 8: Average edits of model generated patches in the \u201coracle\u201d retrieval setting across success-\nfully applied patches. For the task instances specific to each model, we calculate the same statistics\nacross the gold patches. Avg Gold shows statistics macro-averaged over each models\u2019 respective\ngold patches. All Gold shows statistics for all gold patches unconditioned on model performance.", "start_char_idx": 0, "end_char_idx": 3263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9857e94f-3d1b-4bea-97ce-9d7e0878ea37": {"__data__": {"id_": "9857e94f-3d1b-4bea-97ce-9d7e0878ea37", "embedding": null, "metadata": {"page_label": "7", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5578a06-afc6-4310-9811-dfa11f1d3f80", "node_type": "4", "metadata": {"page_label": "7", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ddc7ced41cf10f055a7f3be50fa7354956894f2cc5bc7d88a04b138fb8cd2096", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dacbeca-5c0e-4522-ae61-82e8a78b44e5", "node_type": "1", "metadata": {"page_label": "7", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b7e316275034180eb5676e737be1236b12b7b6478dc7944e02688c79afdbfdd6", "class_name": "RelatedNodeInfo"}}, "text": "Language models tend to generate shorter, simpler edits. Model generated patch files tend to\nadd and remove fewer lines than their respective gold patch. As shown in Table 8, compared to an\naverage gold patch, model generated patch files that apply correctly are less than half the total length\n(74.5versus 30.1lines) of gold edit patch files, and rarely edit more than a single file.\nTable 8: Average edits of model generated patches in the \u201coracle\u201d retrieval setting across success-\nfully applied patches. For the task instances specific to each model, we calculate the same statistics\nacross the gold patches. Avg Gold shows statistics macro-averaged over each models\u2019 respective\ngold patches. All Gold shows statistics for all gold patches unconditioned on model performance.\nModel Total Lines Added Removed Functions Files\nClaude 2 19.6 4.2 1.9 1.1 1.0\nGold 44.1 12.0 5.8 2.1 1.2\nChatGPT-3.5 30.1 3.8 2.7 1.6 1.0\nGold 39.6 9.5 6.1 1.9 1.2\nGPT-4 20.9 4.4 1.5 1.0 1.0\nGold 33.6 8.4 3.8 1.9 1.1\nSWE-Llama 13b 17.6 1.6 1.2 1.2 1.1\nGold 37.8 10.0 4.4 1.9 1.1\nSWE-Llama 7b 16.7 1.3 1.2 1.2 1.1\nGold 40.2 11.3 4.9 1.9 1.1\nAvg Gold 39.1 10.2 5.0 1.9 1.1\nAll Gold 74.5 22.3 10.5 3.0 1.7\n7", "start_char_idx": 2484, "end_char_idx": 3668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d55cb85-ebe4-4d0c-80eb-6c976f46f698": {"__data__": {"id_": "7d55cb85-ebe4-4d0c-80eb-6c976f46f698", "embedding": null, "metadata": {"page_label": "8", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46b4e596-a3af-4322-9644-bdb7a8456ffc", "node_type": "4", "metadata": {"page_label": "8", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "871418f3c88170946e73fb8515aba977c2b25d76e6819c1e41194653a4eff95c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27d9f8b5-4a68-4d5b-91ed-5f2e5a203996", "node_type": "1", "metadata": {}, "hash": "99cbeca6b02fdf0ca2bdf629f557cef6b5eb0f52a8a5fbb98d1949a27203bb90", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n5.1 A Q UALITATIVE ANALYSIS OF SWE-L LAMA GENERATIONS\nWe select 11generations from SWE-Llama and Claude 2 to better understand the quality of the task\nand generated patches under the \u201coracle\u201d retrieval setting. Here we discuss an example from SWE-\nLlama and our overall findings, with in-depth analyses for other examples shown in Appendix F.\nGold P a t ch      (self, section: str) -> List[str]:\u2028\n-         self. (_( ), self._consume_fields())\u2028\n+         self._config.napoleon_use_param:\u2028\n+            \n+            fields = self. (multiple=True)\u2028\n+             self. (fields)\u2028\n+        :\u2028\n+            fields = self. ()\u2028\n+            return self. (_( ), fields)def\nreturn\nif\nreturn\nelse_parse_other_parameters_section\n_format_fields\n_consume_fields\n_format_docutils_params\n_consume_fields\n_format_fields'Other Parameters'\n'Other Parameters'# Allow to declare multiple parameters at once (ex: x, y: int)\u2028sphinx/ e xt/ napoleon/ docs tring.p y\nGenera t ed P a t ch      (self, section: str) -> List[str]:\u2028\n         self. (_( ), self._consume_fields())\u2028\n        return self._format_docutils_params(self._consume_fields())def\nreturn_parse_other_parameters_section\n_format_fields-'Other Parameters'+sphinx/ e xt/ napoleon/ docs tring.p yGenera t ed P a t ch T es t R esultsPASSED\nPASSED\nPASSED\nPASSED\nPASSED\nFAILED\nFAILEDNumpyDocstringTest (test_yield_types)\u2028\nTestNumpyDocstring (test_escape_args_and_kwargs 1)\u2028\nTestNumpyDocstring (test_escape_args_and_kwargs 2)\nTestNumpyDocstring (test_escape_args_and_kwargs 3)\u2028\nTestNumpyDocstring (test_pep526_annotations)\u2028\nNumpyDocstringTest (test_parameters_with_class_reference)\u2028\nTestNumpyDocstring (test_token_type_invalid)===== 2 failed, 45 passed, 8 warnings in 5.16s =====M odel I n p utY o u w ill  b e  pr o v ided  w ith  a  partial  code  b ase  and  an  iss ue\ns ta t e m ent  e xplaining  a  pr o b le m t o  r esol v e.napoleon _u se _ para m sho u ld  also  a ff ec t  \" o ther  \npara m e t ers \" sec tion  Subj ec t : napoleon _u se _ para m \nsho u ld  also  a ff ec t  \" o ther  para m e t ers \" sec tiondef self\nreturn\ndef self\nif ( , se.. .\n     self. (_(\n ( , section) :\n    \n    fields = self. ( )\n     self._config.napoleon_use_param: ..._parse_other_parameters_section\n_format_fields\n_parse_parameters_section\n_consume_fields    # type: (unicode) -> List[unicode ]\n# type: (unicode) -> List[unicode ]\n'Other Para.. .\n\n### P r o b le m\nCu rr ently , napoleon  al w a y s  r enders  the  O ther  para m e t ers  \nsec tion  as  i f napoleon _u se _ para m w as  F alse , see  so u r ce\nFigure 6: We show an example of an formatted task instance, a model prediction, and the testing\nframework logs. In the patches, red highlights are deletions. Green highlights are additions.\nWe\u2019ll consider the task instance sphinx-doc sphinx-8713 from the Sphinx documenta-\ntion generator, shown in Figure 6. The issue states that the napoleon extension of Sphinx is\nnot properly formatting the documentation keyword \u201cOther Parameters\u201d when the config setting\nnapoleon.use_param is set to True . The issue text further provides a detailed code snippet of\nwhere the problematic source code is suspected to be, as well as some code examples for reproduc-\ning the error and additional information related to package versions. For this particular instance, the\nmodel did not resolve the task, failing to pass some of the tests resolved by the gold solution.", "start_char_idx": 0, "end_char_idx": 3437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27d9f8b5-4a68-4d5b-91ed-5f2e5a203996": {"__data__": {"id_": "27d9f8b5-4a68-4d5b-91ed-5f2e5a203996", "embedding": null, "metadata": {"page_label": "8", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46b4e596-a3af-4322-9644-bdb7a8456ffc", "node_type": "4", "metadata": {"page_label": "8", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "871418f3c88170946e73fb8515aba977c2b25d76e6819c1e41194653a4eff95c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d55cb85-ebe4-4d0c-80eb-6c976f46f698", "node_type": "1", "metadata": {"page_label": "8", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c7efa97fca38b765e1f65d679dee4063c411ddb70764c06f021893783b0a4c4a", "class_name": "RelatedNodeInfo"}}, "text": "In the patches, red highlights are deletions. Green highlights are additions.\nWe\u2019ll consider the task instance sphinx-doc sphinx-8713 from the Sphinx documenta-\ntion generator, shown in Figure 6. The issue states that the napoleon extension of Sphinx is\nnot properly formatting the documentation keyword \u201cOther Parameters\u201d when the config setting\nnapoleon.use_param is set to True . The issue text further provides a detailed code snippet of\nwhere the problematic source code is suspected to be, as well as some code examples for reproduc-\ning the error and additional information related to package versions. For this particular instance, the\nmodel did not resolve the task, failing to pass some of the tests resolved by the gold solution.\nIn the \u201coracle\u201d retrieval setting, the model input provides this issue text along with some instruc-\ntions, the full contents of files edited by the gold patch, and an example of the diff format we\nexpect the answer to be in. The total model input consists of 1,558lines of context or 20,882to-\nkens. When comparing the gold patch and the model\u2019s patch, we find an obvious mistake. While\nthe model edits the correct function, _parse_other_parameters_section at line 684 in\nsphinx/ext/napoleon/docstring.py , it changes the function to behave as if napoleon.\nuse_param were always True instead of checking the config setting first and copying what the\n_parse_parameters_section does, like the gold patch. In the tests, test_parameters\n_with_class_reference directly compares the documentation produced using a config where\nnapoleon_use_param is set to False , which catches the model\u2019s error immediately.\nComparing results across all the examples we consider, we notice a few prominent trends in behav-\nior. Models tend to write primitive Python code and do not leverage existing third-party libraries or\nthe rest of the codebase for their solutions. Models\u2019 generations also reflect a \u201cgreedy\u201d approach\nof solving the problem exactly , with little regard for code style or logical constraints that might be\nreflected by the codebase (i.e. using relative instead of absolute imports). In contrast, we observe\nthat many gold patches will make structural improvements that cover a much larger scope of the\ncodebase; these edits not only resolve the issue, but also anticipate and solve potential future issues.\n6 R ELATED WORK\nEvaluation of LMs. Several recent works for evaluating LMs have either proposed a collection\nof mutually distinct tasks spanning across multiple domains (Hendrycks et al., 2021; Liang et al.,\n2022; Srivastava et al., 2023) or turned to the web as an interactive setting featuring tasks that require\n8", "start_char_idx": 2697, "end_char_idx": 5362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e18eba1e-8023-4e14-931d-812b61583525": {"__data__": {"id_": "e18eba1e-8023-4e14-931d-812b61583525", "embedding": null, "metadata": {"page_label": "9", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67b4751c-ad18-4e27-8e5a-1eff6afbd4bf", "node_type": "4", "metadata": {"page_label": "9", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1c2b3f9a0f4691edc9cf1209f2e305ef30a502352d79cd565b186bcb4d217aea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "882be096-b2df-49e5-83df-fc113064203f", "node_type": "1", "metadata": {}, "hash": "6ed6a47739089ce8e908e17390b90d753ff9a02351ad604385c2b582f865b61f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nmultiple steps to solve (Yao et al., 2022; Zhou et al., 2023; Deng et al., 2023; Liu et al., 2023d). There\nare several drawbacks with such a \u201cpotpourri\u201d style setup. First, each task tends to narrowly focus on\none or a few skills, resulting in challenges that are typically too simple, pigeonhole the model into a\nreduced role, and do not provide models with the bandwidth to exercise their versatility or potentially\ndemonstrate new abilities (Srivastava et al., 2023). Consequently, a model\u2019s performance on such\ntask conglomerations may not yield actionable, deep insights regarding its capabilities and how to\nimprove them (Schlangen, 2019; Mart \u00b4\u0131nez-Plumed et al., 2021; Bowman & Dahl, 2021). SWE-\nbench addresses these shortcomings, as our work demonstrates that it is significantly challenging,\npresents a wide range of possibilities for improving LMs to solve this task, and is easy to refresh\nover time with new task instances, each of which introduce novel, nuanced, and practical challenges.\nCode Generation Benchmarks. HumanEval (Chen et al., 2021) is the current standard in a long-\nstanding pursuit of synthesizing code from natural language descriptions (Yu et al., 2018; Austin\net al., 2021; Hendrycks et al., 2021; Li et al., 2022a; Zan et al., 2023). In the past year, subsequent\nbenchmarks have sought to augment HumanEval with extensions to different languages (Cassano\net al., 2022; Athiwaratkun et al., 2023; Orlanski et al., 2023), variations in edit scope (Yu et al.,\n2023; Du et al., 2023), similar but novel code completion tasks (Muennighoff et al., 2023), and\nmore testing (Liu et al., 2023a). Simultaneously, separate works have sought to introduce new cod-\ning paradigms (Yin et al., 2022; Yang et al., 2023) or design library-specific problems (Lai et al.,\n2022; Zan et al., 2022). Instead of partitioning problems into siloed datasets and curtailing them\nfor simplicity\u2019s sake, SWE-bench\u2019s collection procedure transforms the source code with minimal\npost-processing, preserving a much broader set of challenges grounded in real-world software en-\ngineering beyond closed form completion, such as patch generation, reasoning over long contexts,\nnavigating a codebase directory, and capturing dependency-based relationships across modules.\nML for Software Engineering. To overcome traditional program analysis techniques that may not\nscale or incorporate natural language, one direction of current software engineering research is to\nuse neural networks, including LMs, to automate real-world software development processes (Ma-\nniatis et al., 2023; Zheng et al., 2023; Hou et al., 2023). Use cases include automating commit\ngeneration (Jung, 2021; Liu et al., 2023c), PR review (Yang et al., 2016; Li et al., 2022b; Tufano\net al., 2021), bug localization Kim et al. (2019); Chakraborty et al. (2018), testing (Kang et al., 2023;\nXia et al., 2023; Wang et al., 2023), and program repair (Gupta et al., 2017; Allamanis et al., 2017;\nMonperrus, 2018; Jiang et al., 2018; Goues et al., 2019; Gao et al., 2022; Dinh et al., 2023; Motwani\n& Brun, 2023). Most relevant to SWE-bench are works that have sought to apply LMs towards au-\ntomated program repair (Xia & Zhang, 2022; 2023; Fan et al., 2023; Sobania et al., 2023), guiding\ncode editing with commits (Chakraborty & Ray, 2021; Zhang et al., 2022; Fakhoury et al., 2023).", "start_char_idx": 0, "end_char_idx": 3398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "882be096-b2df-49e5-83df-fc113064203f": {"__data__": {"id_": "882be096-b2df-49e5-83df-fc113064203f", "embedding": null, "metadata": {"page_label": "9", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67b4751c-ad18-4e27-8e5a-1eff6afbd4bf", "node_type": "4", "metadata": {"page_label": "9", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1c2b3f9a0f4691edc9cf1209f2e305ef30a502352d79cd565b186bcb4d217aea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e18eba1e-8023-4e14-931d-812b61583525", "node_type": "1", "metadata": {"page_label": "9", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "dec1ef41b765d8e22ace4ab13a84daa40ba41095c0a07857779cebfb0e13a62d", "class_name": "RelatedNodeInfo"}}, "text": "Most relevant to SWE-bench are works that have sought to apply LMs towards au-\ntomated program repair (Xia & Zhang, 2022; 2023; Fan et al., 2023; Sobania et al., 2023), guiding\ncode editing with commits (Chakraborty & Ray, 2021; Zhang et al., 2022; Fakhoury et al., 2023).\nHowever, none of the existing datasets (Just et al., 2014; Karampatsis & Sutton, 2019) present code\ncontext at the scale of SWE-bench. Moreover, SWE-bench can be easily extended to new program-\nming languages and repositories, and it provides a significantly more realistic and challenging arena\nto carry out experiments towards augmenting LMs with software engineering tools and practices.\n7 D ISCUSSION\nLimitations and future directions. SWE-bench task instances are all in Python; we hope to apply\nSWE-bench\u2019s task instance collection procedure to expand its coverage to more programming lan-\nguages and domains. Second, our experiments aim to establish a baseline of the simplest and most\nstraight-forward approaches for this task; we do not intend to constrain future methodologies to the\nsame type of approach and encourage future work to investigate different methods (e.g., agent-based\napproaches, tool augmented LMs). Lastly, while this work evaluates models using execution-based\ncode testing, relying solely on this method is insufficient to guarantee reliable performance of model\ngenerations, as we find automated code generations from LMs can frequently be less comprehensive,\nefficient, or readable compared to human-written solutions.\nConclusion. The complexity of real-world software development processes extends far beyond\njust code completion. By drawing on the open-source collaborative pipeline, SWE-bench creates\na faithful mirror of real world coding environments. This more realistic environment encourages\ncreative solutions that can have immediate applicability in open-source software development. We\nhope that this benchmark and our other contributions can serve as valuable assets in the future\ndevelopment of LMs that are more practical, intelligent, and autonomous.\n9", "start_char_idx": 3126, "end_char_idx": 5198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e489381-4e92-44e0-b3ca-10f21815c176": {"__data__": {"id_": "6e489381-4e92-44e0-b3ca-10f21815c176", "embedding": null, "metadata": {"page_label": "10", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2d2562e-a2f4-459b-8289-f4c6a244106f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "44507d18225e85f3dc292386a3ed5eceec62730a1b0d00a942e391442d1ec6f2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n8 E THICS STATEMENT\nSWE-bench is collected entirely from public repositories with licenses that permit software usage\nthat our contributions are in accordance with. Details of the licenses are included in Table 12.\nDuring the collection or evaluation processes, we do not collect information about GitHub users,\nand the SWE-bench task instances do not use GitHub data beyond what is offered via the public API\nand website. Our contributions do not involve any human subject participation; we do not perform\ncrowdsourcing or recruit human task workers for any part of SWE-bench, including its collection\nand evaluation procedures along with the experiments. SWE-bench\u2019s filtering criteria for GitHub\nrepositories based on popularity does not implicitly or explicitly rely on any discriminative or biased\nheuristics for repository selection. For the dataset release, we plan to open source the SWE-bench\ntask instances, the collection and evaluation infrastructure, the experimental results, the training\ndata used for fine-tuning SWE-Llama models, and the SWE-Llama model weights. Following best\npractice precedents, we will also put forth ample documentation to describe each component and\nits use, and we will also put in place convenient communication channels for soliciting feedback to\nimprove SWE-bench. SWE-bench does not put forth any immediately harmful insights. We briefly\ndiscuss the potential impact of SWE-bench\u2019s usage in Section E.\n9 R EPRODUCIBILITY STATEMENT\nFor our submission, we have uploaded the entirety of the source code as a zipped file that has been\nproperly anonymized. We have organized the codebase such that separate directories correspond to\ndifferent contributions within the main paper (i.e. dataset collection, evaluation, open source model\ninference, SWE-Llama training, etc.). The source code contains inline documentation that details\npurpose and usage of different parts of the codebase. In addition, we also include the full set of 2294\nSWE-bench task instances that contains all the components discussed in the main paper. Beyond the\ndocumentation in the source code, we include thorough technical details for the collection pipeline\nand evaluation procedures in Section A.2 and Section A.4 that complements the original details in\nSection 2 of the main paper. These sections fully cover the logic presented in the code and can\nbe helpful for understanding it. Moving forward, as discussed in the ethics statement, we plan to\nmore formally release SWE-bench to the public as an open source repository with thorough details\nthat describes the benchmark, outlines the code, and details its usage. A major component of SWE-\nbench is the collection framework, which will be part of the open sourced code. Because of its easily\nmaintainable design, as discussed in the main paper, our hope and belief is that SWE-bench should\nbe highly reproducible.\n10 A CKNOWLEDGEMENTS\nWe thank Danqi Chen, Tri Dao, Zexuan Zhong, Tianyu Gao, Will Merrill, Mengzhou Xia, Dan\nFriedman, Adithya Bhaskar, Austin Watkins, Aatmik Gupta, and Richard Zhu for their valuable\nfeedback and advice. We acknowledge support from the National Science Foundation under Grant\nNo. 2239363 and an Oracle Collaborative Research award. Any opinions, findings, conclusions, or\nrecommendations expressed in this material are those of the author(s) and do not necessarily reflect\nthe views of the National Science Foundation.\n10", "start_char_idx": 0, "end_char_idx": 3469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5d6d49a-40c9-4a79-a8bc-33f33d7efded": {"__data__": {"id_": "c5d6d49a-40c9-4a79-a8bc-33f33d7efded", "embedding": null, "metadata": {"page_label": "11", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f6e9222-24be-4e11-bc8e-d26ec5b80b46", "node_type": "4", "metadata": {"page_label": "11", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "069b64123d200a02525fe24e5e4b71194e3c971a39c5d3401eb44cef7c62ed20", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nREFERENCES\nMiltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs\nwith graphs. arXiv preprint arXiv:1711.00740 , 2017.\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, and Yuchen Tian et. al. Multi-\nlingual evaluation of code generation models, 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large\nlanguage models, 2021.\nSamuel R. Bowman and George E. Dahl. What will it take to fix benchmarking in natural language\nunderstanding?, 2021.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. Multipl-e: A scalable and extensible approach to bench-\nmarking neural code generation, 2022.\nSaikat Chakraborty and Baishakhi Ray. On multi-modal learning of editing source code, 2021.\nSaikat Chakraborty, Yujian Li, Matt Irvine, Ripon Saha, and Baishakhi Ray. Entropy guided spec-\ntrum based bug localization using statistical language model. arXiv preprint arXiv:1802.06947 ,\n2018.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, and\nJared Kaplan et. al. Evaluating large language models trained on code, 2021.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems ,\n35:16344\u201316359, 2022.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web, 2023.\nTuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George\nKarypis. Large language models of code fail at completing code with potential bugs. arXiv\npreprint arXiv:2306.03438 , 2023.\nXueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng,\nChaofeng Sha, Xin Peng, and Yiling Lou. Classeval: A manually-crafted benchmark for evaluat-\ning llms on class-level code generation, 2023.\nSarah Fakhoury, Saikat Chakraborty, Madan Musuvathi, and Shuvendu K. Lahiri. Towards generat-\ning functionally correct code edits from natural language issue descriptions, 2023.\nZhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. Automated\nrepair of programs from large language models, 2023.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\nand synthesis, 2023.\nXiang Gao, Yannic Noller, and Abhik Roychoudhury. Program repair, 2022.\nClaire Le Goues, Michael Pradel, and Abhik Roychoudhury. Automated program repair. Commu-\nnications of the ACM , 62(12):56\u201365, 2019.\nDavid Gros, Prem Devanbu, and Zhou Yu. Ai safety subproblems for software engineering re-\nsearchers, 2023.\nRahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common c lan-\nguage errors by deep learning. In Proceedings of the aaai conference on artificial intelligence ,\nvolume 31, 2017.\n11", "start_char_idx": 0, "end_char_idx": 3299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31a41d2d-77ba-4166-8cd4-9205060a3fb2": {"__data__": {"id_": "31a41d2d-77ba-4166-8cd4-9205060a3fb2", "embedding": null, "metadata": {"page_label": "12", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0fc8f12d-a3d0-4915-a3d3-f813f7bc0285", "node_type": "4", "metadata": {"page_label": "12", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "488bb124b5cfc2e624361dd7bd2faa1ac2ed18e478828797cac634c5fc77d20b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e5aa4fe-e593-4f73-9851-b762bf7a0350", "node_type": "1", "metadata": {}, "hash": "4390fa3625b324fd6a6d44b25d7c314b45198350241e5bf40d59a029cd8a9954", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMaurice H. Halstead. Elements of Software Science (Operating and programming systems series) .\nElsevier Science Inc., USA, 1977. ISBN 0444002057.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge\ncompetence with apps, 2021.\nXinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John\nGrundy, and Haoyu Wang. Large language models for software engineering: A systematic litera-\nture review, 2023.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Con-\nference on Learning Representations , 2022. URL https://openreview.net/forum?\nid=nZeVKeeFYf9 .\nSam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Ra-\njbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of\nextreme long sequence transformer models, 2023.\nJiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen. Shaping program\nrepair space with existing patches and similar code. In Proceedings of the 27th ACM SIGSOFT\ninternational symposium on software testing and analysis , pp. 298\u2013309, 2018.\nTae-Hwan Jung. Commitbert: Commit message generation using pre-trained programming language\nmodel, 2021.\nRen\u00b4e Just, Darioush Jalali, and Michael D. Ernst. Defects4J: A Database of existing faults to enable\ncontrolled testing studies for Java programs. In ISSTA 2014, Proceedings of the 2014 International\nSymposium on Software Testing and Analysis , pp. 437\u2013440, San Jose, CA, USA, July 2014. Tool\ndemo.\nSungmin Kang, Juyeon Yoon, and Shin Yoo. Large language models are few-shot testers: Exploring\nllm-based general bug reproduction, 2023.\nRafael-Michael Karampatsis and Charles Sutton. How often do single-statement bugs occur?\nthe manysstubs4j dataset. 2020 IEEE/ACM 17th International Conference on Mining Software\nRepositories (MSR) , pp. 573\u2013577, 2019. URL https://api.semanticscholar.org/\nCorpusID:173188438 .\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, and Zhengxuan Wu et.\nal. Dynabench: Rethinking benchmarking in nlp, 2021.\nYunho Kim, Seokhyeon Mun, Shin Yoo, and Moonzoo Kim. Precise learn-to-rank fault localization\nusing dynamic and static features of target programs. ACM Transactions on Software Engineering\nand Methodology (TOSEM) , 28(4):1\u201334, 2019.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen\ntau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for\ndata science code generation, 2022.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, and R \u00b4e mi Leblond\net. al. Competition-level code generation with AlphaCode. Science , 378(6624):1092\u20131097, dec\n2022a. doi: 10.1126/science.abq1158. URL https://doi.org/10.1126%2Fscience.\nabq1158 .", "start_char_idx": 0, "end_char_idx": 3059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e5aa4fe-e593-4f73-9851-b762bf7a0350": {"__data__": {"id_": "2e5aa4fe-e593-4f73-9851-b762bf7a0350", "embedding": null, "metadata": {"page_label": "12", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0fc8f12d-a3d0-4915-a3d3-f813f7bc0285", "node_type": "4", "metadata": {"page_label": "12", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "488bb124b5cfc2e624361dd7bd2faa1ac2ed18e478828797cac634c5fc77d20b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31a41d2d-77ba-4166-8cd4-9205060a3fb2", "node_type": "1", "metadata": {"page_label": "12", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7180f3ab0db72a2398923c90b9bd09fafb38f3c617695de7f89182655bd257e4", "class_name": "RelatedNodeInfo"}}, "text": "Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen\ntau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for\ndata science code generation, 2022.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, and R \u00b4e mi Leblond\net. al. Competition-level code generation with AlphaCode. Science , 378(6624):1092\u20131097, dec\n2022a. doi: 10.1126/science.abq1158. URL https://doi.org/10.1126%2Fscience.\nabq1158 .\nZhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared\nGreen, Alexey Svyatkovskiy, Shengyu Fu, and Neel Sundaresan. Automating code review activi-\nties by large-scale pre-training, 2022b.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, and Michihiro Yasunaga\net. al. Holistic evaluation of language models, 2022.\n12", "start_char_idx": 2567, "end_char_idx": 3434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "757db6bb-1eee-418d-8105-c91b21967451": {"__data__": {"id_": "757db6bb-1eee-418d-8105-c91b21967451", "embedding": null, "metadata": {"page_label": "13", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84c8b8d3-b090-4b82-b5d0-e71ea38f13b1", "node_type": "4", "metadata": {"page_label": "13", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "07dba9584d40565837e2aa992b7e1c9ab6ed6a3205b1ef0152ee3381610d0821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60ee3206-e49f-4079-8873-ce827640b302", "node_type": "1", "metadata": {}, "hash": "0d661e988e53d0ffe1883b6c41f4a9753d18e13e95a1c613be82a2e685fc7ac7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\nchatgpt really correct? rigorous evaluation of large language models for code generation. arXiv\npreprint arXiv:2305.01210 , 2023a.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts, 2023b.\narXiv:2307.03172.\nShangqing Liu, Yanzhou Li, Xiaofei Xie, and Yang Liu. Commitbart: A large pre-trained model for\ngithub commits, 2023c.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, and Hanyu Lai et. al. Agentbench:\nEvaluating llms as agents, 2023d.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, and Ambrosio Blanco et.\nal. Codexglue: A machine learning benchmark dataset for code understanding and generation.\nCoRR , abs/2102.04664, 2021.\nPetros Maniatis, Daniel Tarlow, and Google DeepMind. Large sequence models for soft-\nware development activities, 2023. URL https://blog.research.google/2023/05/\nlarge-sequence-models-for-software.html .\nFernando Mart \u00b4\u0131nez-Plumed, Pablo Barredo, Se \u00b4an\u00b4O h\u00b4Eigeartaigh, and Jos \u00b4e Hern \u00b4andez-Orallo. Re-\nsearch community dynamics behind popular ai benchmarks. Nature Machine Intelligence , 3:581\n\u2013 589, 2021. URL https://api.semanticscholar.org/CorpusID:236610014 .\nThomas J. McCabe. A complexity measure. IEEE Transactions on Software Engineering , SE-2(4):\n308\u2013320, 1976. doi: 10.1109/TSE.1976.233837.\nMartin Monperrus. Automatic software repair. ACM Computing Surveys , 51(1):1\u201324, jan 2018. doi:\n10.1145/3105906. URL https://doi.org/10.1145%2F3105906 .\nManish Motwani and Yuriy Brun. Better automatic program repair by using bug reports and tests\ntogether, 2023.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo,\nSwayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruc-\ntion tuning code large language models, 2023.\nGabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Ja-\ncob Austin, Rishabh Singh, and Michele Catasta. Measuring the impact of programming language\ndistribution, 2023.\nSimon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Janina Brauner, and Matthias Samwald. Map-\nping global dynamics of benchmark creation and saturation in artificial intelligence. Nature\nCommunications , 13, 2022. URL https://api.semanticscholar.org/CorpusID:\n247318891 .\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends\u00ae in Information Retrieval , 3(4):333\u2013389, 2009.\nBaptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, and Xiaoqing Ellen Tan et.\nal. Code llama: Open foundation models for code, 2023.\nDavid Schlangen. Language tasks and language games: On methodology in current natural language\nprocessing research, 2019.\nDominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. An analysis of the automatic\nbug fixing performance of chatgpt, 2023.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, and Abu Awal Md Shoeb et. al.", "start_char_idx": 0, "end_char_idx": 3141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60ee3206-e49f-4079-8873-ce827640b302": {"__data__": {"id_": "60ee3206-e49f-4079-8873-ce827640b302", "embedding": null, "metadata": {"page_label": "13", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84c8b8d3-b090-4b82-b5d0-e71ea38f13b1", "node_type": "4", "metadata": {"page_label": "13", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "07dba9584d40565837e2aa992b7e1c9ab6ed6a3205b1ef0152ee3381610d0821", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "757db6bb-1eee-418d-8105-c91b21967451", "node_type": "1", "metadata": {"page_label": "13", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b3627e8c08b473857464925f5d837339c769a845033e292e5d5f3e2ddf34ba08", "class_name": "RelatedNodeInfo"}}, "text": "The probabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends\u00ae in Information Retrieval , 3(4):333\u2013389, 2009.\nBaptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, and Xiaoqing Ellen Tan et.\nal. Code llama: Open foundation models for code, 2023.\nDavid Schlangen. Language tasks and language games: On methodology in current natural language\nprocessing research, 2019.\nDominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. An analysis of the automatic\nbug fixing performance of chatgpt, 2023.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, and Abu Awal Md Shoeb et. al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models, 2023.\nRosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and Gabriele Bavota. To-\nwards automating code review activities, 2021.\n13", "start_char_idx": 2519, "end_char_idx": 3383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20e48498-0e13-481b-b7f6-d105ca7fbf1e": {"__data__": {"id_": "20e48498-0e13-481b-b7f6-d105ca7fbf1e", "embedding": null, "metadata": {"page_label": "14", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eafd62c9-fa7e-4668-aa3a-c6480e5bbee9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fb35af2ba1349c58daa3bd991734c22543b70574de86ae6ce6f4492c9840676b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nJunjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. Software\ntesting with large language model: Survey, landscape, and vision, 2023.\nChunqiu Steven Xia and Lingming Zhang. Less training, more repairing please: revisiting automated\nprogram repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering , pp. 959\u2013\n971, 2022.\nChunqiu Steven Xia and Lingming Zhang. Conversational automated program repair, 2023.\nChunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. Universal\nfuzzing via large language models. arXiv preprint arXiv:2308.04748 , 2023.\nJohn Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing\nand benchmarking interactive coding with execution feedback, 2023.\nXin Yang, Raula Gaikovina Kula, Norihiro Yoshida, and Hajimu Iida. Mining the modern code\nreview repositories: A dataset of people, process and product. In Proceedings of the 13th Inter-\nnational Conference on Mining Software Repositories , pp. 460\u2013463, 2016.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\nreal-world web interaction with grounded language agents, 2022.\nPengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua How-\nland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, and Charles Sutton.\nNatural language to code generation in interactive data science notebooks, 2022.\nHao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao\nXie, and Qianxiang Wang. Codereval: A benchmark of pragmatic code generation with generative\npre-trained models, 2023.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene\nLi, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pp.\n3911\u20133921, Brussels, Belgium, October-November 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425 .\nDaoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu\nChen, and Jian-Guang Lou. Cert: Continual pre-training on sketches for library-oriented code\ngeneration, 2022.\nDaoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and\nJian-Guang Lou. Large language models meet nl2code: A survey, 2023.\nJiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric. Coditt5:\nPretraining for source code and natural language editing, 2022.\nZibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and We-\nicheng Wang. Towards an understanding of large language models in software engineering tasks.\narXiv preprint arXiv:2308.11396 , 2023.\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nYonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environ-\nment for building autonomous agents, 2023.\n14", "start_char_idx": 0, "end_char_idx": 3286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5524e0c6-a72e-46ad-ae4f-7ece3d18b69c": {"__data__": {"id_": "5524e0c6-a72e-46ad-ae4f-7ece3d18b69c", "embedding": null, "metadata": {"page_label": "15", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65efad3b-d97f-46bb-8549-1ad86fa424ce", "node_type": "4", "metadata": {"page_label": "15", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e598bbb0fbb833869f3ce549a4b6c6515d82cee1f7de169f4730e7ed728ba2f1", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAPPENDIX\nIn the appendix, we provide more thorough details regarding the dataset construction process, eval-\nuation pipeline, and characterization of the SWE-bench benchmark.\nA B ENCHMARK DETAILS\nThis section complements Section 2 with a more technical and fine-grained summary of the data col-\nlection, execution-based validation, and evaluation procedures, along with a fuller characterization\nof the task instances.\nA.1 H IGHLEVEL OVERVIEW\nPull request scraping. From a list of the top 5,000most downloaded PyPI libraries during August\n2023, we select the top 100packages, identify each library\u2019s corresponding open-source GitHub\nrepository, verify which packages have licenses allowing for free software use, and collect all PRs for\nthese repositories via the GitHub developer API. We elect to source problems from well-trafficked\nrepositories because widespread use usually suggests that the repository has extensive documenta-\ntion, structured open-source development guidelines, and working, well-formatted code.\nTask instance construction. We construct candidate task instances from PRs that satisfy three\nconditions. First, the PR\u2019s status must be Merged . AMerged status indicates that the PR\u2019s associated\ncode changes were accepted and incorporated into its parent repository. Second, the PR resolves one\nor more issues in its repository. An issue is defined according to its canonical usage in GitHub as\na digital ticket for tracking bugs, enhancements, or any general development goals for a software\nproject. We scan a PR\u2019s title, body, and commit messages for linked issues (i.e. \u201cfixes # 24\u201d). Third,\nthe PR must introduce one or more new tests. A new test is counted when a PR\u2019s code changes edits\na file path containing a testing-related keyword (e.g. \u201ctest\u201d, \u201ctesting\u201d).\nA PR that satisfies these criteria is then converted into a candidate task instance such as the example\nin Figure 7. The codebase Cis identified by the repository\u2019s owner/name moniker and the pull\nrequest\u2019s base commit. Recovering the actual codebase from this information is straightforward.\nWe create mirrors of the original GitHub repositories, where each mirror is uniquely identified as\nowner name . Cloning a repository\u2019s corresponding mirror and checking out the base commit\nyields Cin its pre-PR state. The problem statement Pis an aggregate of all related issues\u2019 titles and\ndescriptions along with any subsequent comments written before the timestamp of the PR\u2019s initial\ncommit to avoid leakage of solution details. A PR\u2019s code changes are separated into a test patch\nand a gold patch \u03b4.Tconsists of all tests from files edited in the test patch. As shown in Figure 7,\nbothTand\u03b4are stored as patch files. Further details about parsing PR and semantic data is in\nAppendix A.2.\nExecution-based validation. We verify the usability of a task instance via execution. For each\ncandidate, we first define a virtual environment to serve as an execution context, then install C\nbefore applying any patches, and finally run Tonce before and once after the solution \u03b4is applied.\nA candidate is removed from consideration for the final dataset if any step in the verification process\nfails. In addition, to ensure that a solution \u03b4is non-trivial, we compare the pre-solution and post-\nsolution validation logs to check for whether there are one or more tests in Twhere the status\nchanges from failtopass. Lastly, we exclude task instances with tests that invoke newly created\nfunctions or classes first introduced in the solution \u03b4. Since naming such constructs is typically\nan arbitrary process and usually not explicitly specified in the problem statement, resolving tests\nsuch as these may be an impossible task even for human developers. Information about execution\ncontexts, codebase installation, determining test statuses from logs, and more are in Appendix A.3.\nContinuous Updates. SWE-bench\u2019s collection process is easily extensible to any open source code\nrepositories, allowing for easy and low-maintenance extension to new programming languages and\ncode domains. This design also provides SWE-bench with temporal robustness; as new language\nmodels trained on more recent source code are released over time, SWE-bench can simply be up-\ndated to produce new task instances based on PRs created after any LM\u2019s training date.\nhttps://hugovk.github.io/top-pypi-packages/\n15", "start_char_idx": 0, "end_char_idx": 4413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f231b3d-1d05-41e1-ad10-9b1aaa6ebe90": {"__data__": {"id_": "3f231b3d-1d05-41e1-ad10-9b1aaa6ebe90", "embedding": null, "metadata": {"page_label": "16", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1375e909-94e2-44c1-9dd6-1554fe0ed9d0", "node_type": "4", "metadata": {"page_label": "16", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e01e827f91346298364f504dcf63726e0f58ce7d2bf60c8ad1199deef00f6387", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8da7edaf-250f-4aa0-a7ea-f518bb233fb7", "node_type": "1", "metadata": {}, "hash": "dbd96a4405d8addc38e76d5a82fb52fd7fe2c30d9fccbcc063b960769263f0e6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nGold P a t chsklearn/ ensemble/ _his t_gradient_boos ting/ gradient_boos ting.p y1041    (self):\u2028\n1042 +    (self.loss   and\u2028\n1043 +     self.n_trees_per_iteration_  ):\u2028\n1044 +      (\n1045 +                      \n1046 +                      )\u2028\n1047 +\u2028\n1048      self.loss  :\n1049        self.n_trees_per_iteration_  :\u2028\n1050          _LOSSES[ ]()def\nif\nraise\nif\nif\nreturn_get_loss==\n== 1\n==\n== 1'categorical_crossentropy'\n\"'categorical_crossentropy' is not suitable for \"\u2028\n\"a binary classification problem. Please use \"\u2028\n\"'auto' or 'binary_crossentropy' instead.\"\n'auto'\n'binary_crossentropy'ValueErrorT es t P a t ch418      stump_clf. ( , y_isnan). ( , y_isnan)  \n419\n420 +  ():\n421 +   # categorical_crossentropy should only be used if there\n422 +   # are more than two classes present. PR #14869\n423 +     [[ ], [ ]]\n424 +   y  [ , ]\n425 +   gbrt  (loss )\n426 +    pytest.raises( , match ):\n427 +     gbrt. ( , y)\n428 \n429    assert\ndef\nwithfit score\ntest_crossentropy_binary_problem\nfit\n@pytest.mark.parametrize( , [ , ])X X\nX\nHistGradientBoostingClassifier\nValueError\nX==\n'categorical_crossentropy'\n\"'crossentropy' not suitable\"\n\"scoring\" 'loss'1\n= 1 0\n= 0 1\n= =\n=\nNonesklearn/ ensemble/ _his t_gradient_boos ting/t es ts/t es t_gradient_boos ting.p yPr oblem Sta t ementHGBC w ith  c a t e g ori c al_ c r ossentr op y  f ails  silently  on  binary  c lassi fic a tion\nP ing  @N i c olas Hu g  @ o griselgi v es :A nd   w ork s  fi ne.   \nsho u ld  either  g enerali z e  or  raise  an  err or  on  binary  c lassi fic a tion.binary_crossentropy categorical_crossentropyimport as\nfrom import\nfrom import numpy  n p\n sklearn.e x perimental  enable_hist_gradient_boostin g\n sklearn.ensemble  \n  [[ , ], [ , ], [ , ], [ , ], [ , ]]\ny  [ , , , , ]\ngb  (loss ,\n                                    min_samples_leaf )\ngb. ( , y)\n(gb. ([[ , ]]))\n(gb. ([[ , ]]))HistGradientBoostingClassifie r\n\nX\nHistGradientBoostingClassifier\nX= 1 0 1 0 1 0 0 1 1 1\n= 1 1 1 0 1\n= =\n=1\n1 0\n0 1'categorical_crossentropy'fit\nprint predict\nprint predict[0]\n[0]M e tada taR epoC r ea t ed  A tI ns tan c e  IDscikit-learn/ scikit-learnscikit-learn__scikit-learn-1486 9A ug 3 1, 2 019I ss u e  # sPu ll  Nu mber[14858]1486 91018f9f ...B ase  C ommit\nFigure 7: SWE-bench task instance example. Problem statement Pis an aggregation of the issues\nrelated to the pull request. Codebase Ccorresponds to a repository and base commit. The tests T\nand solution Dare derived from the original PR\u2019s associated code changes. Stylized for readability.\nA.2 C ONSTRUCTION PROCESS\nWe discuss additional details regarding the conversion of a pull request object into a candidate task\ninstance. At a high level, the main goal of this conversion is to acquire relevant information for\nputting together the codebase C, problem statement P, unit tests T, and solution \u03b4components\nintroduced in Section 2. To this end, a SWE-bench task instance consists of the following fields,\npresented in the following Table 9. Collectively, the fields correspond to the four task instance\nmodules.", "start_char_idx": 0, "end_char_idx": 3085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8da7edaf-250f-4aa0-a7ea-f518bb233fb7": {"__data__": {"id_": "8da7edaf-250f-4aa0-a7ea-f518bb233fb7", "embedding": null, "metadata": {"page_label": "16", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1375e909-94e2-44c1-9dd6-1554fe0ed9d0", "node_type": "4", "metadata": {"page_label": "16", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e01e827f91346298364f504dcf63726e0f58ce7d2bf60c8ad1199deef00f6387", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f231b3d-1d05-41e1-ad10-9b1aaa6ebe90", "node_type": "1", "metadata": {"page_label": "16", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "086de99f7c87ffcf94ed713c3cc60cfb88ac09448578cc2bfdb3830d8254bd60", "class_name": "RelatedNodeInfo"}}, "text": "Problem statement Pis an aggregation of the issues\nrelated to the pull request. Codebase Ccorresponds to a repository and base commit. The tests T\nand solution Dare derived from the original PR\u2019s associated code changes. Stylized for readability.\nA.2 C ONSTRUCTION PROCESS\nWe discuss additional details regarding the conversion of a pull request object into a candidate task\ninstance. At a high level, the main goal of this conversion is to acquire relevant information for\nputting together the codebase C, problem statement P, unit tests T, and solution \u03b4components\nintroduced in Section 2. To this end, a SWE-bench task instance consists of the following fields,\npresented in the following Table 9. Collectively, the fields correspond to the four task instance\nmodules.\nField Description\nbase commit (str) The commit ID that the original PR is applied on top of\ncreated at (date) Datetime object of when PR was first created (not merged)\nhints text (str) Natural language suggestions for how to solve problem\ninstance id (str) A unique identifier created from repo andpull number\nissue numbers (list) List of issue numbers that the original pull request resolves\npatch (str).patch -format styled string that is a reference solution\nto the problem, extracted from the original PR\u2019s code changes\nproblem statement (str) Natural language description of desired change to codebase\npull number (int) The pull number of the original pull request\ntest patch (str).patch -format styled string containing unseen tests\nfor checking if a task was solved, extracted from the original\nPR\u2019s code changes\nversion (str) Release version (w.r.t. repo ) during which PR was created\nrepo (str) The repository the task instance originates from\nFAIL TOPASS (list) List of tests that change in status from failtopass\nPASS TOPASS (list) List of tests that change in status from pass topass\nenvinstall commit (str) Base commit at which to install necessary\ndependencies for running task instance.\nTable 9: Description of each field of a SWE-bench task instance object. See \u00a7 A.2 for details\nregarding how each field is collected.\nProblem Statement. The problem statement Pfor each task instance is readily available as the\nproblem statement field. The problem statement is an aggregate of all issues\u2019 first comments\nalong with any comments attached to those issues that were created before the creation date of\nthe PR\u2019s initial commit. We crawl for issues from PR\u2019s title, body, and commit messages. After\n16", "start_char_idx": 2314, "end_char_idx": 4799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e585f73-32bc-414a-b0d7-0943bfb67b47": {"__data__": {"id_": "1e585f73-32bc-414a-b0d7-0943bfb67b47", "embedding": null, "metadata": {"page_label": "17", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c39ec53-9359-47a4-a215-8a631188dfdc", "node_type": "4", "metadata": {"page_label": "17", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5dfe888ffeb683c33ca5c4bfa4d64407db3ed4d16e3c3d39f1feea329e3d95fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03039334-257f-4bd3-a212-7f2040ccb085", "node_type": "1", "metadata": {}, "hash": "fdc18de14da82a4617021be757344a81412bb25f0dc24ce72e553daee4121167", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nconcatenating these components\u2019 text data, we first remove any Markdown-style comments, then\nlook through the remaining text for references to issue numbers (a pound # sign followed by a\nnumber) and check whether the word preceding the issue number reference is included in a set of\nkeywords suggesting that the issue was resolved by the PR (e.g. \u201ccloses\u201d, \u201cfixes\u201d, \u201cresolves\u201d). The\nfound issues are recorded in the issue numbers field, then separate web requests are made to\nretrieve each issue\u2019s data. To form the problem statement , each issue\u2019s title and body are\nadded together and then concatenated with the next issue\u2019s if there are multiple. It is also during this\nstep that the hints text field is created and collected from the PR\u2019s comment section, where text\nfrom comments created before the PR\u2019s initial commit. The intuition for this collection methodology\nis that such PR comments would likely contain natural language and pseudo-code suggestions to\nthe original human task worker regarding how to complete the problem at hand. The experiments\npresented in this work do not make use of hints text , but we believe this information may be\ninteresting for future investigations.\nCodebase. The codebase Ccontent is notstored in plaintext for every task instance. Rather, the\ntask instance contains a reference to the relevant codebase via the repo andbase commit field.\nBoth fields are available in the original PR\u2019s data. To make retrieval of the codebase Cfrom these\ntwo elements reproducible and reliable, we create mirrors of the original repository. Mirrors for\nthe repository constituting both the evaluation and fine tuning data are collected and open-sourced\nunder the SWE-bench GitHub organization. Because an original repository\u2019s code may be subject to\nchanges in its commit and edit history outside of the authors\u2019 control, we choose to create a mirror\nrepository to ensure that later modifications to the codebase do not potentially render a task instance\nunusable due to a corruption or removal of the associated base commit . Additionally, we create\na mirror instead of cloning and storing the latest version of a repository. This is because a mirror\nretains the original commit hashes, history, branches, and tags, serving as a faithful and complete\nhistory of the technical details of the original repository. A mirror does not retain stars, watchers,\nissues, or pull requests from the original repository.\nWe create a mirror from a repository after and within the same day when task instances were col-\nlected. The mirror retains the original repository\u2019s \u201c owner /name \u201d moniker, except that the \u201c/\u201d\ncharacter is converted to a \u201c \u201d to confirm to GitHub naming conventions. Given this infrastructure,\nretrieving a task instance\u2019s codebase is straightforward. First, the correct mirror can be cloned from\nthe SWE-bench organization using repo . Next, within the local copy of the mirror, checking out\nthebase commit will reset the repository to codebase C. To proceed to another task instance\nfrom the same repository, git version control is used to automatically remove any modifications\nassociated with the current task instance before checking out the next task instance\u2019s base commit.\nSolution, Test Patches. The solution \u03b4and tests Tare derived from the file changes data, or diff ,\nof a PR. As mentioned in Section 2.1, the original diff along with solution \u03b4and tests Tare\nrepresented as a .patch file, a format for efficiently specifying transformations to line-based text\nfiles. Generally speaking, a .patch is structured as a list of blocks, where each block consists of\na header and one or more hunks that collectively correspond to changes to a single file. The header\ncontains metadata specifying a file path and line numbers, while the actual modifications to the\ntarget file are encoded as multiple lines prefixed by \u201c+\u201d and \u201c-\u201d to indicate additions and removals.\nTo create the tests T, we first identifying every unique block within the patch, then pick out and\nconglomerate blocks with file paths that contain testing-related keywords (e.g. \u201ctests\u201d, \u201ctesting\u201d).\nThe remaining blocks are merged to form the solution \u03b4. We validate the robustness of the script\nwritten to parse correctly Tand\u03b4by applying both patches to the corresponding codebase Cand\nrunning the tests; we then check that the results reproduce the behavior of the base PR\u2019s diff data.\nThe solution \u03b4is saved as the patch field while the tests Tare saved as the test patch field.\nRemaining Fields. Thecreated atfield is a timestamp that specifies when the base PR was cre-\nated.", "start_char_idx": 0, "end_char_idx": 4636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03039334-257f-4bd3-a212-7f2040ccb085": {"__data__": {"id_": "03039334-257f-4bd3-a212-7f2040ccb085", "embedding": null, "metadata": {"page_label": "17", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c39ec53-9359-47a4-a215-8a631188dfdc", "node_type": "4", "metadata": {"page_label": "17", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5dfe888ffeb683c33ca5c4bfa4d64407db3ed4d16e3c3d39f1feea329e3d95fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e585f73-32bc-414a-b0d7-0943bfb67b47", "node_type": "1", "metadata": {"page_label": "17", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "812d8ebcf5425cb8af28bd6b12d2c2efa8a797aabbb3da3818080b1b1a12f53e", "class_name": "RelatedNodeInfo"}}, "text": "The header\ncontains metadata specifying a file path and line numbers, while the actual modifications to the\ntarget file are encoded as multiple lines prefixed by \u201c+\u201d and \u201c-\u201d to indicate additions and removals.\nTo create the tests T, we first identifying every unique block within the patch, then pick out and\nconglomerate blocks with file paths that contain testing-related keywords (e.g. \u201ctests\u201d, \u201ctesting\u201d).\nThe remaining blocks are merged to form the solution \u03b4. We validate the robustness of the script\nwritten to parse correctly Tand\u03b4by applying both patches to the corresponding codebase Cand\nrunning the tests; we then check that the results reproduce the behavior of the base PR\u2019s diff data.\nThe solution \u03b4is saved as the patch field while the tests Tare saved as the test patch field.\nRemaining Fields. Thecreated atfield is a timestamp that specifies when the base PR was cre-\nated. We retain the created atfield from the original data and use this field to perform temporal\nanalysis of model performance. The version field is a string that corresponds to the release ver-\nsion, with respect to the repo , during which the PR was released. Depending on availability and the\namount of effort required for each method, we create the version field by retrieving the informa-\ntion directly from the source code, building the repository locally and invoking code to display the\nversion to standard output, or comparing the created atfield with a timeline of release versions\nDocumentation for creating a mirror repository using GitHub\n17", "start_char_idx": 3744, "end_char_idx": 5286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee81dec9-9a33-4e60-9324-241aff9a8cd3": {"__data__": {"id_": "ee81dec9-9a33-4e60-9324-241aff9a8cd3", "embedding": null, "metadata": {"page_label": "18", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63e7e542-6cde-4e91-a0f4-abd22140a1d3", "node_type": "4", "metadata": {"page_label": "18", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "df880e3768af66df7557d2ad178d7364b8de8cdd6f333bd800131cf439bbd1cb", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nfrom a repository\u2019s webpage. We create executable contexts for every version of a repository, as\ndiscussed in greater detail in \u00a7 A.3.\nA.3 E XECUTION -BASED VALIDATION\nAfter filtering through all the PRs from a repository and converting those that satisfy the aforemen-\ntioned criteria into candidate task instances, the next step is to validate the usability of each task\ninstance via execution. This procedure is broken down into three steps. First, we create executable\ncontexts for each release version of a repository. Next, we check whether the solution \u03b4and tests T\ncan be applied, installed, and run successfully on top of codebase C. Finally, we examine each task\ninstance\u2019s execution log to verify a specific set of behaviors to ensure that the task is usable and fair\nfor model evaluation.\nExecutable Contexts. We choose to create executable contexts per release version after experi-\nmenting with various degrees of granularity with regards to what definition level to define virtual\nenvironments for. Defining task instance-specific contexts is most conducive to ensuring end-to-end\ninstallation success, but comes at the cost of laborious manual handcrafting. On the other hand, a\nrepository-specific context based on the latest version of a repository is typically too coarse of a\ndefinition that is not compatible with older versions\u2019 requirements. We find that release versions are\na good proxy for capturing the dependency requirements across a subset of task instances, striking a\nmanageable balance between installation success and manual effort. We manually create each exe-\ncutable context by examining the codebase of the latest task instance for each version. Based on the\nsource code and documentation typically found in the repository\u2019s README andCONTRIBUTING\nguides, we find out the Python version, necessary dependencies, and installation command.\nValidation Engine. The purpose of the validation engine is to verify candidate task instances.\nSpecifically, this step checks first, that the solution \u03b4and tests Tcan be applied to codebase C,\nand second, that the codebase can be properly installed and run within the corresponding virtual\nenvironment. To do this, we perform validation repository-by-repository, where for each repository\u2019s\nset of task instances, we perform the following procedure:\n1. Create executable contexts as conda envs. based on latest task instance per version .\n2. Group task instances by version .\n3. Iterate across each task instances group, where for each task instance, we perform the\nfollowing within the corresponding conda env.\n(a) Remove any file changes and checkout the task instance\u2019s base commit . This sets\nthe repository to codebase C.\n(b) Run the installation command to instantiate codebase C.\n(c) Apply the test patch Tto codebase C.\n(d) Run the testing script, determined from test patch T, to generate test result logs logpre.\n(e) Apply the solution \u03b4patch to the codebase Cwith tests T.\n(f) Run the testing script from part (d) again to generate test result logs logpost.\nThe testing command consists of the testing framework used by the repository (e.g. pytest ,tox)\nwith paths specified in Tappended. The testing command would run any and all tests that are speci-\nfied within the contents of each file path. If any of the steps (a)through (f)fails, the candidate task\ninstance is discarded from consideration. With moderate variation across repositories, we observe\nthat this step generally removes half of the candidate task instances.\nExamining Validation Logs. Last but not least, we check the logs logpreandlogpostcreated by the\nvalidation engine for specific properties. First, to guard against arbitrary naming choices, we check\nlogpreforImportError andAttributeError occurrences, which are potentially indicative\nof dependency naming related errors that would trivial and near-impossible to address correctly. To\nthis end, we remove all task instances with such errors in their logprefrom consideration. Next, we\ncompare the test results to check that the task instance is non-trivial, indicated by at least one or\nmore tests having a fail status before the solution \u03b4is applied, then a pass status after. To check\nthis, we first define several repository-specific parsers to convert logpreandlogpostinto mappings\nof test ti\u2208Tto a status s\u2208[fail,pass]. Given these two data structures, we then check that there\n18", "start_char_idx": 0, "end_char_idx": 4441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54ead7d6-5c39-4830-aec3-c01bedf19cc6": {"__data__": {"id_": "54ead7d6-5c39-4830-aec3-c01bedf19cc6", "embedding": null, "metadata": {"page_label": "19", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9846989c-e9f6-4203-9f43-ead78b93b056", "node_type": "4", "metadata": {"page_label": "19", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "128a334a9edff4181baa8ae9ce9d4b549774bde8213fe444568890ee6bc6c693", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nRepo Total PRs Crawled Post-Conversion Post-Validation (Final)\nastropy 9,469 1 ,016 95\ndjango 16,914 2 ,880 850\nflask 2,434 107 11\nmatplotlib 16,545 1 ,057 184\npylint 3,848 787 57\npytest 5,147 750 119\nrequests 2,344 84 44\nscikit-learn 15,159 1 ,169 229\nseaborn 1,004 203 22\nsphinx 4,931 645 187\nsympy 11,928 1 ,897 386\nxarray 3,416 812 110\nTotal 93,139 11 ,407 2 ,294\nTable 10: Statistics for how many candidate task instances were kept after the completion of a stage\nacross the construction and validation procedures.\nexists at least one tiwhere schanges from failtopass. If no such tests are found, the task instance\nis removed from consideration.\nIf a task instance fulfills these two criteria, then it is included in the evaluation dataset. Table 10\ndisplays a summary of how many task instances were removed from consideration across the con-\nstruction process and execution based validation steps. We save all finalized task instances to a\nsingle .json file that is open sourced and available for download.\nAlongside the task instances, we also create a corresponding folder containing the ground truth\ntest results. For each task instance, from their respective logpreandlogpost test-to-status map-\npings, we create a test results data structure where the keys are FAIL TOFAIL ,FAIL TOPASS ,\nPASS TOFAIL , andPASS TOPASS , and the values are lists of tests. By \u201ccaching\u201d these results,\nwe remove the need to re-run the solution \u03b4at evaluation time (although re-running is an available\noption). We use this data structure to verify task completion, as discussed in Section A.4.\nA.4 E VALUATION PROCEDURE\n Lang. ModelS WE-Llama\n HarnessApply  \u2192 R ep\nApply  \u2192 R ep\nRun T es t Scrip t\nL o g R esultsP a t chT es tsS WE-Bench T ask\n1 Input2 Genera t es3 Ev alua tion\n334 R esultsCodebaseA s tr op y /A s tr op y\n T es tstable/t es ts/t es t_ops.p \nt es t_ join_s truct_co\nt es t_v s tack_s truct_co\nt es t_ds tack_s truct_col\n P a t chdiff --git a/ as tr op y /utils/ ...\u2028\n--- a/ as tr op y /utils/ meta.p y\u2028\n+++ b / as tr op y /utils/ meta.p y\u2028\n@@ - 7 3,7 + 7 3,7 @@ def ...\u2028\n     ...\u2028\n-    r eturn arr .dtype.s tr\u2028\n+    r eturn arr .dtype.s tr if arr .\n dtype.names is None els e ...\n Pr oblem PTitle: v s tack'ing \ns tructur ed arra y tables \nf ails with cas ting err or\n R esults\ntable/t es ts/t es t_ops.p y\n t es t_ join_s truct_col\n t es t_v s tack_s truct_col\n t es t_ds tack_s truct_col\nFigure 8: Visualization of the evaluation pipeline at an individual task instance level. During evalua-\ntion, the Patch is model generated. A prediction .patch must be applied successfully and produce\nthe same results as the corresponding task instance\u2019s Dfor task completion.\nWe provide a visualization of the evaluation procedure in Figure 8. The evaluation procedure scores\nthe model\u2019s \u02c6\u03b4.patch generation with respect to the behavior of the solution \u03b4. At a finer-grained\nlevel, the evaluation procedure can be broken down into four separate steps, highlighted by the\nnumbered steps in Figure 8. First, the codebase and problem statement are visible and given to the\n19", "start_char_idx": 0, "end_char_idx": 3117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2493352c-6848-4ce7-b76f-1450631db509": {"__data__": {"id_": "2493352c-6848-4ce7-b76f-1450631db509", "embedding": null, "metadata": {"page_label": "20", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0625e51f-9931-491b-a8bb-38e09c44314d", "node_type": "4", "metadata": {"page_label": "20", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d0c2e9dc40aa7cf5ca7f50210ebc2b385c005232b490ae2710c00f815fd24097", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nLM; the LM then generates a .patch prediction \u02c6\u03b4. In the evaluation step, the following steps are\nperformed per prediction on the target task instance:\n1. Remove any file changes and checkout the task instance\u2019s base commit. This sets the\nrepository to codebase C.\n2. Activate the executable context corresponding to the task instance\u2019s version .\n3. Run installation command to instantiate codebase C.\n4. Apply test patch Tto codebase C.\n5. Apply prediction patch \u02c6\u03b4to codebase Cwith tests T.\n6. If the previous step fails, we attempt to fix prediction patch \u02c6\u03b4automatically and reapply it.\n7. Run the testing script, determined from test patch T, to generate test result logs log\u02c6\u03b4.\nSteps 1 through 4 reliably do not fail due to verification during the task instance validation process.\nIf applying the prediction patch (Step 5) fails, we attempt to repair the prediction patch file by\nremoving unnecessary context lines and recalculating the header values (Step 6). If the remaining\npatch fails again or running the test command (Step 7) fails, then the prediction is automatically\ngiven a score of 0. Assuming these steps succeed, the output log log\u02c6\u03b4can then be converted to\na test-to-status mapping, identical in structure to the via the appropriate, repository-specific parser\nintroduced in \u00a7 A.3.\nEvaluation Metrics Calculation. To determine task completion, we compare the test-to-status\nmapping parsed from log\u02c6\u03b4with the list of tests corresponding to the FAIL TOPASS and\nPASS TOPASS keys from the ground truth test results data structure. Determining task completion\nis straightforward; we check that all FAIL TOPASS andPASS TOPASS tests are found and have\napass status in the evaluation test-to-status mapping. If a test is missing or has a non- pass status, it\nis considered a failstatus. As defined and used in the main paper, a task is considered solved if all\ntests across FAIL TOPASS andPASS TOPASS pass.\nA.5 E VALUATION TESTSETCHARACTERIZATION\nWe include an expanded form of Table 1 that includes repository specific statistics in Table 11. Table\n12 presents a brief description of each repository extracted from the repository\u2019s documentation\nalong with the repository\u2019s associated open source license. The associated licenses all permit non-\ncommercial usage of the original library source code as long as the permissions in the original\nlicenses are upheld and retained. In addition to the original statistics presented in Table 1, we\nintroduce three new values. The \u03b4# Lines Added and \u03b4# Lines Removed together sum up to \u03b4Lines\nEdited. \u201cAdded\u201d refers to the number of new lines that are introduced, while \u201cRemoved\u201d are pre-\nexisting lines taken out by the solution. The |T|(Pass to Pass) statistic refers to the number of tests\nthat were passing before the solution \u03b4was applied during the validation pipeline. Unlike failtopass\ntests that are intended to characterize the problem statement Pand determine if a revision addresses\nthe issue, pass topass tests are included to ensure that the revision does not break or violate any\nexisting expected behavior. These tests are extracted during the validation log examination phase\nas discussed in \u00a7 A.3. We note that failtofailtests and pass tofailtests are not considered during\nevaluation, and those statistics are not reflected in the above table.\nTask Instance Issue Categories. To provide a better sense of the types of problems that\nSWE-bench task instances include, we perform simple analyses on the issues, identified by the\nissue numbers field, for each task instance. Per issue, we inspect metadata, specifically tags, to\ncharacterize the type of contribution put forth by the PR. Table 13 groups and shows several exam-\nples of the 2,289tags we found across all issues. While the absolute majority of issues are associated\nwith bug fixes, SWE-bench\u2019s task instances are associated with a diverse set of code changes with\npurposes beyond debugging and error correction.\nAttribute Distributions. In Figure 9, we present plots of the cumulative distribution function for\nattributes introduced in Table 1. From these plots, we see that the median SWE-bench task instance\nhas a problem description of 140words, and will take place within a codebase containing just shy of\n1900 files and 400K lines. The corresponding reference solution \u03b4will usually edit a single function\n20", "start_char_idx": 0, "end_char_idx": 4394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a8948c0-755f-4bdc-83c9-20aca4c3cb7e": {"__data__": {"id_": "5a8948c0-755f-4bdc-83c9-20aca4c3cb7e", "embedding": null, "metadata": {"page_label": "21", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9df3aaf0-a362-413c-90fb-5dffc961dacf", "node_type": "4", "metadata": {"page_label": "21", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b6299f15065d47fa608a4ddf139e2f02fdcd5236dec3adbd3849834c997eda99", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFigure 9: Cumulative Distribution Functions for different attributes of SWE-bench task instances.\n21", "start_char_idx": 0, "end_char_idx": 145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da300fd9-7c05-4819-87ec-063376be7192": {"__data__": {"id_": "da300fd9-7c05-4819-87ec-063376be7192", "embedding": null, "metadata": {"page_label": "22", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7528921-b73d-466c-99e0-ea1a798b6e4e", "node_type": "4", "metadata": {"page_label": "22", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c9c4acb697a687b8b2b47526d26307656b5575d906d6c360592b5dc9bda19fa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271fa919-b825-42a1-ae10-ae8a7bf2dbc9", "node_type": "1", "metadata": {}, "hash": "7c20e0575e962abe1ef11e242a910af5265ce4f03741536bfec98d85628c86f3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nastropy django flask matplotlib pylint pytest\nPLength (Characters) 2,742 1 ,307 1 ,185 2 ,381 2 ,011 2 ,948\nC# Files 1,811 6 ,356 225 4 ,395 2 ,426 497\nC# Lines 804k 407k 35k 646k 109k 111k\n\u03b4# Files Edited 1.5 1 .5 1 .6 1 .5 1 .8 1 .4\n\u03b4# Func. Edited 2.5 2 .0 2 .4 2 .2 1 .8 1 .7\n\u03b4# Lines Edited 36.0 18 .5 35 .4 58 .9 36 .0 24 .5\n\u03b4# Lines Added 25.0 12 .8 23 .7 35 .7 26 .6 18 .2\n\u03b4# Lines Removed 10.9 5 .7 11 .6 23 .2 9 .5 6 .4\n|T|(Fail to Pass) 21.7 8 .8 1 .4 2 .4 6 .8 4 .1\n|T|(Pass to Pass) 191.0 85 .9 32 .5 242 .4 47 .0 60 .7\n|T|(All) 212.8 94 .6 33 .9 244 .8 53 .7 64 .8\nrequests scikit-learn seaborn sphinx sympy xarray\nPLength (Characters) 1,654 2 ,239 1 ,667 1 ,888 1 ,213 3 ,515\nC# Files 119 1 ,224 273 1 ,483 1 ,666 260\nC# Lines 30k 361k 105k 423k 678k 137k\n\u03b4# Files Edited 1.64 1 .68 1 .77 1 .51 1 .9 2 .45\n\u03b4# Func. Edited 1.59 2 .24 1 .41 2 .72 3 .22 3 .16\n\u03b4# Lines Edited 25.5 44 .0 30 .1 30 .6 36 .3 124 .8\n\u03b4# Lines Added 19.2 32 .7 24 .9 22 .0 24 .2 95 .6\n\u03b4# Lines Removed 6.2 11 .3 5 .2 8 .6 12 .1 29 .2\n|T|(Fail to Pass) 7.6 7 .5 12 .9 2 .3 2 .2 58 .5\n|T|(Pass to Pass) 87.1 150 .7 86 .8 45 .1 74 .5 297 .5\n|T|(All) 94.7 158 .2 99 .7 47 .4 76 .8 356 .1\nTable 11: Average numbers characterizing different attributes of a SWE-bench task instance grouped\nby repository. In addition to the statistics presented in Table 1, we also introduce three new values:\n\u03b4# Lines Added, \u03b4# Lines Removed, and |T|(Pass to Pass).\nRepository Summary License\nastropy/astropy Astronomy and astrophysics core library BSD 3-Clause\ndjango/django Web framework for building web applications BSD 3-Clause\npallets/flask Lightweight framework for small web apps BSD 3-Clause\nmatplotlib/matplotlib Plotting library for creating visuals Custom\npylint-dev/pylint Static code analyser for Python 2 or 3 GPL 2.0\npytest-dev/pytest Testing framework for Python MIT\npsf/requests Simple, elegant library for writing HTTP requests Apache-2.0\nscikit-learn/scikit-learn Machine Learning in Python BSD 3-Clause\nmwaskom/seaborn Statistical data visualization in Python BSD 3-Clause\nsphinx-doc/sphinx Library for creating documentation Custom\nsympy/sympy Computer algebra system written in Python Custom\npydata/xarray N-D labeled arrays and datasets Apache-2.0\nTable 12: Summary and licenses for all GitHub repositories that task instances were extracted from.\nwithin a file, changing \u223c15lines, and has a single failtopass test to verify the correctness of the\nchange along with 51pass topass tests to check whether existing behavior is preserved.\nPatch Fix Rate.", "start_char_idx": 0, "end_char_idx": 2585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "271fa919-b825-42a1-ae10-ae8a7bf2dbc9": {"__data__": {"id_": "271fa919-b825-42a1-ae10-ae8a7bf2dbc9", "embedding": null, "metadata": {"page_label": "22", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7528921-b73d-466c-99e0-ea1a798b6e4e", "node_type": "4", "metadata": {"page_label": "22", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c9c4acb697a687b8b2b47526d26307656b5575d906d6c360592b5dc9bda19fa5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da300fd9-7c05-4819-87ec-063376be7192", "node_type": "1", "metadata": {"page_label": "22", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "243bcac1355e6a90bacae553c063dd4152581ca5cbbfd1cfa09c29175177840e", "class_name": "RelatedNodeInfo"}}, "text": "within a file, changing \u223c15lines, and has a single failtopass test to verify the correctness of the\nchange along with 51pass topass tests to check whether existing behavior is preserved.\nPatch Fix Rate. We present Table 14, which presents summary statistics of how many task instances\neach model generated patches for (out of 2294), how many of these patches applied successfully,\nand how many of the successfully applied patches required undergoing the patch fixing procedure\nintroduced in Appendix A.4. We find that fixed patches tend to make up a smaller percentage of the\nSWE-Llama patches that successfully applied, suggesting that SWE-Llama\u2019s fine tuning procedure\nhas a positive effect on generating well-formatted patches. For closed source models, fewer patches\napply successfully, and of the ones that do, a greater percentage require the post-generation fix,\nsuggesting that models still struggle with patch generation and structured outputs in general.\n22", "start_char_idx": 2383, "end_char_idx": 3350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90868e43-9681-45b6-ad81-92bf72a12554": {"__data__": {"id_": "90868e43-9681-45b6-ad81-92bf72a12554", "embedding": null, "metadata": {"page_label": "23", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a29522c-4f4a-4bf2-81e6-1d74ca488d43", "node_type": "4", "metadata": {"page_label": "23", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a63b64898a3458e10fa42f85ce0f66075b4d5acfe8926288a3e3e0322f596bab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d112a2f-6ed3-4c45-95ba-f10e26bd8409", "node_type": "1", "metadata": {}, "hash": "38c6b084bdca7ba45b7bf327695d6856e324b2836f985436bc213241a3afb729", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nCategory Count Examples\nBug 442 \u201cBug\u201d (179); \u201ctype:bug\u201d (114); \u201cbug\u201d (57); \u201ctype: bug\u201d (48);\n\u201cBug :beetle:\u201d (23); \u201cstatus: confirmed bug\u201d (20);;\nFeature 167 \u201ctype:enhancement\u201d (47); \u201cEnhancement\u201d (25); \u201cNew feature\u201d (24);\n\u201cFeature Request\u201d (22); \u201ctype: enhancement\u201d (19);\n\u201cEnhancement :star:\u201d (15); \u201cNew Feature\u201d (7); \u201cenhancement\u201d (6);\nRegression 39 \u201ctype: regression\u201d (14); \u201cRegression\u201d (14); \u201cregression\u201d (8);\nOther 1641 \u201chelp wanted\u201d (71); \u201cgood first issue\u201d (66); \u201cprinting\u201d (58);\n\u201cextensions:autodoc\u201d (58); \u201cEasy\u201d (57); \u201cEasy to Fix\u201d (54);\n\u201cdomains:py\u201d (27); \u201ccore\u201d (26); \u201csets\u201d (23); \u201cWrong Result\u201d (23);\n\u201cunits\u201d (22); \u201cGood first issue\u201d (21);\nTable 13: Categories of tags associated with issues from SWE-bench\u2019s task instances.\nModel Retrieval Setting Generations Applies Fixed Patch Fix %\nChatGPT-3.5 BM25 13k 2,270 604 363 60 .1%\nChatGPT-3.5 \u201cOracle\u201d 1,262 500 222 44 .4%\nChatGPT-3.5 \u201cOracle\u201d-collapsed 1,811 939 420 44 .73%\nClaude 2 BM25 13k 2,281 988 302 30 .57%\nClaude 2 \u201cOracle\u201d 2,138 1 ,441 360 24 .98%\nClaude 2 \u201cOracle\u201d-collapsed 2,242 1 ,564 465 29 .73%\nGPT-4 BM25 27k 573 85 59 69 .41%\nGPT-4 \u201cOracle\u201d 462 195 121 62 .05%\nGPT-4 \u201cOracle\u201d-collapsed 2,292 1 ,116 684 61 .29%\nSWE-Llama 13b BM25 13k 2,010 1 ,230 369 30 .0%\nSWE-Llama 13b \u201cOracle\u201d 2,125 1 ,532 378 24 .67%\nSWE-Llama 7b BM25 13k 2,139 1 ,187 340 28 .64%\nSWE-Llama 7b \u201cOracle\u201d 2,119 1 ,503 298 19 .83%\nTable 14: Statistics for how many patches for 2,294task instances were generated, applied suc-\ncessfully, and required a post-generation fix to apply successfully for each [model, retrieval setting]\ncombination during evaluation. The GPT-4 BM25 27k and \u201cOracle\u201d settings were ran on the 25%\nsubset. The GPT-4 \u201cOracle\u201d-collapsed setting was run on the full SWE-bench test set.\nA.6 D EVELOPMENT SETCHARACTERIZATION\nIn addition to the evaluation test set, we also provide a development set for evaluating models and\ntuning hyperparameters before running on the final test set. Following the style of tables and graphs\nfrom before, we present similar statistics to characterize the 225development task instances (slightly\nmore than 10% of the main evaluation set) collected from 6open source repositories with licenses\npermitting such usage. The development set was collected following the exact same set of method-\nologies and filters as the main evaluation set. In addition to the pre-existing steps, we also filter the\ndevelopment set to keep task instances that were created after January 1,2019 . Similar to Table 12,\nin Table 15, we briefly summarize the purpose and licenses of the 6selected repositories.\nFollowing Table 13, we also list the tags associated with the the development set tasks in Table 16,\nagain showcasing the diversity and coverage of task types beyond fixing bugs. Compared to the main\nevaluation tasks, we can also see tags (e.g., \u201cCrash :collision:\u201d, \u201cio\u201d) that refer to issues presenting\nproblems which are unique to the repositories in the development set.\nFollowing Table 1, we present the same set of repository-specific average statistics for the 6repos-\nitories in the development set in Table 17.", "start_char_idx": 0, "end_char_idx": 3151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d112a2f-6ed3-4c45-95ba-f10e26bd8409": {"__data__": {"id_": "5d112a2f-6ed3-4c45-95ba-f10e26bd8409", "embedding": null, "metadata": {"page_label": "23", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a29522c-4f4a-4bf2-81e6-1d74ca488d43", "node_type": "4", "metadata": {"page_label": "23", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a63b64898a3458e10fa42f85ce0f66075b4d5acfe8926288a3e3e0322f596bab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90868e43-9681-45b6-ad81-92bf72a12554", "node_type": "1", "metadata": {"page_label": "23", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0f12d221bc6ec0796b5c83aa6e4d820bd983306990754efd47a5998a8c067de3", "class_name": "RelatedNodeInfo"}}, "text": "The development set was collected following the exact same set of method-\nologies and filters as the main evaluation set. In addition to the pre-existing steps, we also filter the\ndevelopment set to keep task instances that were created after January 1,2019 . Similar to Table 12,\nin Table 15, we briefly summarize the purpose and licenses of the 6selected repositories.\nFollowing Table 13, we also list the tags associated with the the development set tasks in Table 16,\nagain showcasing the diversity and coverage of task types beyond fixing bugs. Compared to the main\nevaluation tasks, we can also see tags (e.g., \u201cCrash :collision:\u201d, \u201cio\u201d) that refer to issues presenting\nproblems which are unique to the repositories in the development set.\nFollowing Table 1, we present the same set of repository-specific average statistics for the 6repos-\nitories in the development set in Table 17. Across the entire development set, each task instance\nhas19.9average / 2median F2P tests. There are 171.3average / 79.0median P2P tests, and 191.2\naverage / 101.0median tests in total per task instance.\n23", "start_char_idx": 2261, "end_char_idx": 3357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2340776b-533e-4ffc-9040-e45adeacaab5": {"__data__": {"id_": "2340776b-533e-4ffc-9040-e45adeacaab5", "embedding": null, "metadata": {"page_label": "24", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da37ecfb-be3a-4872-8984-87093d4332c5", "node_type": "4", "metadata": {"page_label": "24", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "6c513b46abc855d3cabe7255799e65d8335d3d9de1d3106e666c15e97db4211e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nRepository Summary Count License\nmarshmallow-code/ Parse complex objects to/from Python data-types 9 MIT\nmarshmallow\npylint-dev/astroid Library for AST parsing, static analysis/inference 31 LGPL-2.1\npydicom/pydicom Read/modify/write DICOM files w/ Python 56 Custom\npvlib/pvlib-python Simulate photovoltaic energy systems performance 63 Custom\npyvista/pyvista 3D plotting, mesh analysis through interface 16 MIT\nsqlfluff/sqlfluff SQL linter, supports multiple dialects, templates 50 MIT\nTable 15: Summary and licenses for all GitHub repositories that development task instances were\nextracted from.\nCategory Count Examples\nBug 127 \u201cbug\u201d (111); \u201cBug :cockroach:\u201d (10); \u201crule bug\u201d (6);\nFeature 55 \u201cenhancement\u201d: 46; \u201cEnhancement :star:\u201d: 5; \u201cfeature-request\u201d: 2;\nRegression 4 \u201cRegression\u201d (4);\nOther 95 \u201capi\u201d: 11, \u201cdocumentation\u201d: 7, \u201chelp wanted\u201d: 6, \u201cconfig options\u201d: 5,\n\u201cio\u201d: 5, \u201cjinja\u201d: 4, \u201cgood first issue\u201d: 4, \u201cparser\u201d 3\nTable 16: Categories of tags associated with issues from SWE-bench\u2019s development task instances.\nB A DDITIONAL DETAILS ON TRAINING SWE-L LAMA\nB.1 T RAINING DETAILS\nOptimization. We finetune using LoRA (Hu et al., 2022) with r= 16 ,\u03b1= 16 , dropout = 0.05,\non the query, key, value, and output projection matrices of every attention sublayer. We train with\na learning rate of 6e\u22124and a batch size of 32sequences per gradient step for a maximum of\n4epochs. During training, we save checkpoints every 50steps, and after training, select the best\ncheckpoint based on the validation loss on a held-out 100instances. SWE-Llama 7b was initialized\nwith CodeLlama-Python 7b and trained in 20hours on 4NVIDIA A100s. SWE-Llama 13b was\ninitialized with CodeLlama-Python 13b and trained in 47hours on 8NVIDIA A100s. We used\nDeepSpeed Ulysses (Jacobs et al., 2023) and Flash Attention (Dao et al., 2022) to enable long\ncontext training.\nC A DDITIONAL RESULTS\nC.1 R ESULTS WITH \u201cORACLE \u201d RETRIEVAL\nUsing the \u201coracle\u201d retrieval method described in Section 4.1, we show the general performance\nresults in Table 18. Naturally, providing only the files edited by the reference solution\u2019s pull request,\nmodel performance improves compared to the noisier BM25 retrieval setting.\nC.2 E VALUATION TESTSET\nWe include a repository-by-repository breakdown of model performance in Table 19 that corre-\nsponds to Figure 4 in the main paper. As discussed, in the main paper, performance differs heavily\nacross repositories.\nC.3 GPT-4 E VALUATION SUBSET RESULTS\nIn this section, we present the statistics shown in Table 5 for the 25% random subset that GPT-4 was\ntested in Table 20. As the selection of the subset is random, we find that the % Resolved and %\nApply rates are consistent with the main results, and not significantly skewed towards being simpler\nor more difficult than the general evaluation set.\n24", "start_char_idx": 0, "end_char_idx": 2852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53651072-ed77-4deb-8333-ec2620554703": {"__data__": {"id_": "53651072-ed77-4deb-8333-ec2620554703", "embedding": null, "metadata": {"page_label": "25", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ab61080-3d7b-45a2-9eab-9deb73de4042", "node_type": "4", "metadata": {"page_label": "25", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "40d2205d16079b8c3a58fd7f8d4549fd75172a30f27e9602387658d739d24dd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e181833-f600-48ca-b018-52ee0db53259", "node_type": "1", "metadata": {}, "hash": "de9fed07d7d67f3de1e54a92b2bed678b5c034e43d97ae5ae7977c7ec7907f85", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nastroid marshmallow pvlib pydicom pyvista sqlfluff\nPLength (Characters) 2199 1619 1790 2076 1475 2639\nC# Files 252 82 294 455 866 2297\nC# Lines 60K 22K 459K170K 661K 205K\n\u03b4# Files Edited 2.51 1 .89 1 .83 1 .54 2 .1 3 .26\n\u03b4# Func. Edited 3.03 2 .11 2 .89 2 .23 3 .0 2 .71\n\u03b4# Lines Edited 83.1 36 .2 93 .3 42 .0 101 .0 102 .5\n\u03b4# Lines Added 52.8 24 .7 67 .0 29 .7 79 .4 63 .6\n\u03b4# Lines Removed 30.3 11 .6 26 .4 12 .3 21 .6 38 .9\n|T|(Fail to Pass) 23.2 53 .0 19 .1 24 .0 8 .8 14 .8\n|T|(Pass to Pass) 182.6 242 .9 107 .5 176 .1 96 .5 239 .7\n|T|(All) 205.8 295 .9 126 .6 200 .1 105 .3 254 .5\nTable 17: Average numbers characterizing different attributes of a SWE-bench task instance grouped\nby repository for repositories in the development dataset. The same statistics presented in Table 11\nare also shown here.\nBM25 Retrieval \u201cOracle\u201d Retrieval\nModel % Resolved % Apply % Resolved % Apply\nClaude 2 1.96 43.07 4.80 62.82\nChatGPT-3.5 0.17 26.33 0.52 21.80\nGPT-4\u22170.00 14.83 1.74 34.00\nSWE-Llama 7b 0.70 51.74 3.01 65.52\nSWE-Llama 13b 0.70 53.62 3.97 66.78\nTable 18: We compare models against each other using the BM25 and oracle retrieval settings as\ndescribed in Section 4. The main results table, Table 5, presents the results for the different models\nwhen using BM25 only.\u2217Due to budget constraints we evaluate GPT-4 on a 25% random subset of\nSWE-bench in the \u201cOracle\u201d and BM25 27K retriever settings only.\nC.4 E XTENDED TEMPORAL ANALYSIS\nIn this section, we present an extended temporal analysis of task instances solved by year that follows\nthe analysis shown in Table 7 of the evaluation section in the main paper. In Table 21, we present\nthe % Resolved statistic across models under the \u201cOracle\u201d retrieval setting for 6different temporal\npartitions that group tasks by the years in which the issues were created. It is evident from the\ntable that there is no consistent correlation between model performance and year, supporting our\nconclusion that despite having potentially seen older versions of code within its pre-training datasets,\nunderstanding and implementing in fixes in SWE-bench is a difficult task that requires understanding\nand cannot be accomplished feasibly or consistently via memoization of observed data.\nC.5 F2P, P2P R ATEANALYSIS\nIn the main paper results, we present the \u201c% Resolved\u201d statistic that indicates how many task in-\nstances were completely solved by the different models. In this section, we provide more fine-\ngrained insight into the gap of task instances where 1. The model\u2019s patch generation was applied\nsuccessfully and 2. The task instance was not resolved. Assuming a patch is applied successfully,\nwe define 6cases in Table 22 that fully capture the distribution of all possible outcomes based on\nthe pass/fail results of F2P and P2P tests. In addition to the \u201cResolved\u201d outcome that has been\nestablished, we introduce five new terms. The \u201cBreaking Resolved\u201d outcome refers to when the\ndesired behavior of the issue has been accomplished (all F2P tests pass), but not all prior behavior is\nmaintained (not all P2P tests pass).", "start_char_idx": 0, "end_char_idx": 3113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e181833-f600-48ca-b018-52ee0db53259": {"__data__": {"id_": "3e181833-f600-48ca-b018-52ee0db53259", "embedding": null, "metadata": {"page_label": "25", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ab61080-3d7b-45a2-9eab-9deb73de4042", "node_type": "4", "metadata": {"page_label": "25", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "40d2205d16079b8c3a58fd7f8d4549fd75172a30f27e9602387658d739d24dd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53651072-ed77-4deb-8333-ec2620554703", "node_type": "1", "metadata": {"page_label": "25", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "870af400d299b7eb70722794c6e1993ab2cfd8c0e4d2b08156b46998e0c20ec3", "class_name": "RelatedNodeInfo"}}, "text": "In this section, we provide more fine-\ngrained insight into the gap of task instances where 1. The model\u2019s patch generation was applied\nsuccessfully and 2. The task instance was not resolved. Assuming a patch is applied successfully,\nwe define 6cases in Table 22 that fully capture the distribution of all possible outcomes based on\nthe pass/fail results of F2P and P2P tests. In addition to the \u201cResolved\u201d outcome that has been\nestablished, we introduce five new terms. The \u201cBreaking Resolved\u201d outcome refers to when the\ndesired behavior of the issue has been accomplished (all F2P tests pass), but not all prior behavior is\nmaintained (not all P2P tests pass). \u201cPartially Resolved\u201d refers to when prior behavior of a codebase\nwas maintained (all P2P tests pass); however, the desired behavior is not fully accomplished (not all\nF2P tests pass). The \u201cWork in Progress\u201d case is when the desired behavior is not fully accomplished\n(not all F2P tests pass) and the prior behavior of the codebase is not maintained (not all P2P tests\npass). A \u201cNo-Op\u201d is when a code change does not have any effect on the original codebase; prior\n25", "start_char_idx": 2451, "end_char_idx": 3580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bccd09d-fae4-4a62-b413-40373f96d9ea": {"__data__": {"id_": "4bccd09d-fae4-4a62-b413-40373f96d9ea", "embedding": null, "metadata": {"page_label": "26", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aecdd0ca-fe22-4f21-9364-de299f1bb421", "node_type": "4", "metadata": {"page_label": "26", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d6fc6f19554c3255da809682934343f8c7c7372ba8b761ec982f35382bb975f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86c49bd4-c39f-4b8f-8f00-436dd2266263", "node_type": "1", "metadata": {}, "hash": "967773933eb6e8e2952b11a24ac9bb4a93b6e085fb5c7ce3bdf313789cbd120f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nRepo Claude 2 ChatGPT-3.5 GPT-4 SWE-Llama 13b SWE-Llama 7b\nastropy/astropy 3.23 0 .00 0 .00 1 .06 3 .16\ndjango/django 6.15 1 .32 2 .50 5 .19 4 .00\nmatplotlib/matplotlib 3.05 3 .33 0 .00 3 .12 1 .11\nmwaskom/seaborn 0.00 0 .00 0 .00 0 .00 0 .00\npallets/flask 0.00 0 .00 0 .00 9 .09 0 .00\npsf/requests 15.91 2 .33 8 .33 13 .64 18 .18\npydata/xarray 6.90 0 .00 0 .00 5 .81 3 .00\npylint-dev/pylint 1.75 0 .00 0 .00 1 .75 1 .75\npytest-dev/pytest 5.93 0 .00 0 .00 5 .04 4 .20\nscikit-learn/scikit-learn 5.41 0 .00 0 .00 3 .12 0 .88\nsphinx-doc/sphinx 5.65 1 .83 0 .00 2 .25 2 .69\nsympy/sympy 1.94 0 .00 0 .00 3 .01 1 .59\nTable 19: % Resolved for models per repository represented in SWE-bench.\nBM25 Retrieval \u201cOracle\u201d Retrieval\nModel % Resolved % Apply % Resolved % Apply\nClaude 2 2.27 \u21910.31 45.72\u21912.65 4.01\u21930.79 62.65\u21930.17\nChatGPT-3.5 0.17 \u22120.00 26.53\u21910.02 0.70\u21910.18 21.64\u21930.16\nGPT-4 0.00 \u22120.00 14.82\u22120.00 1.74\u22120.00 34.00\u22120.00\nSWE-Llama 7b 0.35 \u21910.35 49.04\u21932.70 1.92\u21931.09 63.70\u21931.82\nSWE-Llama 13b 0.70 \u22120.00 56.54\u21912.92 4.54\u21910.57 66.67\u21930.11\nTable 20: We compare models against each other using the BM25 and oracle retrieval settings as\ndescribed in Section 4 on a 25% random subset (574 instances) of SWE-bench in the \u201coracle\u201d and\nBM25 27K retriever settings only. This is the same subset that GPT-4 is evaluated on, as mentioned\nin Table 5. The difference relative to percentages in Table 5 and Table 18 is included as a subscript.\nbehavior is maintained (all P2P tests pass) but the issue remains completely unresolved ( 0F2P tests\npass). Finally, if the issue is unresolved ( 0F2P tests pass) and prior working behavior is reverted\n(some P2P tests fail), the codebase is left in a worse state, which we define to be a \u201cRegression\u201d.\nIn Table 23, we categorize patch generations that successfully applied according to these six cases.\nWe find that of non-\u201cResolved\u201d issues, the majority of patch generations proposed by the model do\nnot solve a single F2P test case from the corresponding task instance (\u201cNo-Op\u201d and \u201cRegression\u201d).\nWithin the subset of these cases, the majority ( 60% to70%) of cases are a No-Op, while the model\nbreaks existing behavior for the remainder of these situations.\nGenerally, the cases where model generations pass some, but not all tests (\u201cBreaking Resolved\u201d,\n\u201cPartially Resolved\u201d, \u201cWork in Progress\u201d) cumulatively represent a smaller subset of problems rel-\native to the other three categories. From manual inspection of several of these cases, it is clear that\nthe model demonstrates some understanding of the task requirements.", "start_char_idx": 0, "end_char_idx": 2595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86c49bd4-c39f-4b8f-8f00-436dd2266263": {"__data__": {"id_": "86c49bd4-c39f-4b8f-8f00-436dd2266263", "embedding": null, "metadata": {"page_label": "26", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aecdd0ca-fe22-4f21-9364-de299f1bb421", "node_type": "4", "metadata": {"page_label": "26", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d6fc6f19554c3255da809682934343f8c7c7372ba8b761ec982f35382bb975f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bccd09d-fae4-4a62-b413-40373f96d9ea", "node_type": "1", "metadata": {"page_label": "26", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c46fba80176f50f1a34873413f3585623e2f72323096eccaf62f4d436f6f9aba", "class_name": "RelatedNodeInfo"}}, "text": "In Table 23, we categorize patch generations that successfully applied according to these six cases.\nWe find that of non-\u201cResolved\u201d issues, the majority of patch generations proposed by the model do\nnot solve a single F2P test case from the corresponding task instance (\u201cNo-Op\u201d and \u201cRegression\u201d).\nWithin the subset of these cases, the majority ( 60% to70%) of cases are a No-Op, while the model\nbreaks existing behavior for the remainder of these situations.\nGenerally, the cases where model generations pass some, but not all tests (\u201cBreaking Resolved\u201d,\n\u201cPartially Resolved\u201d, \u201cWork in Progress\u201d) cumulatively represent a smaller subset of problems rel-\native to the other three categories. From manual inspection of several of these cases, it is clear that\nthe model demonstrates some understanding of the task requirements. However, due to the baseline\nmethods\u2019 limited view of the codebase that does not include information such as inter-file dependen-\ncies and functions\u2019 relationships, for many of these task instances often fail because a change that\ncorrectly resolves the immediate issue does not account for other modules that use and are affected\nby the changed entity. We include several case studies that directly highlight these shortcomings in\nSection F Overall, these results highlight not just the difficulty of SWE-bench, but also point to the\npotential value of providing feedback via an execution environment that would allow models to run\nfixes against existing tests, then decide whether to continue editing or submit the patch for review.\nC.6 P ATCH GENERATION EXTENDED ANALYSIS\nIn this section, we present a statistics to quantify various facets of patch generations following\nthe metrics laid out in Table 8. In Table 24, we recalculate these values for allpatch generations\nin the oracle retrieval setting for all models, regardless of whether or not the patch was applied\n26", "start_char_idx": 1770, "end_char_idx": 3670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6818b224-146d-4893-8a9a-51be06871611": {"__data__": {"id_": "6818b224-146d-4893-8a9a-51be06871611", "embedding": null, "metadata": {"page_label": "27", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a786d27-e161-4888-8a09-dbe8ac79cd75", "node_type": "4", "metadata": {"page_label": "27", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "99f08b7ca198425111f5cdc9f2cc98b5b9f701d756fc480d4b62905fd16d0fa1", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nYear Total 25% Claude 2 GPT-3.5 GPT-4\u2217SWE-Llama 13b SWE-Llama 7b\n2023 244 61 4.51 1.56 0.00 4.07 3.50\n2022 395 117 4.05 0.85 3.42 2.80 2.46\n2021 383 102 4.18 0.00 2.94 4.45 2.56\n2020 427 109 5.15 0.71 0.00 3.96 3.43\n2019 437 112 5.72 1.49 1.79 4.55 2.21\n2018 165 37 5.45 0.00 0.00 3.57 2.94\n<2018 89 36 4.49 0.00 2.78 3.37 1.09\nTable 21: We present an extended temporal analysis in this table, showing the % resolved for task\ninstances across models in the \u201cOracle\u201d retrieval setting, separated by different cutoff dates. The\nYear column refers to the subset of tasks that were created during the specified calendar year. In\ntheTotal column, we list the number of tasks that fall within the given year. The 25%column is\nthe same information for the subset that GPT-4 was evaluated on. The remaining model-specific\ncolumns contain the % resolved metric.\n# F2P Tests Pass\n# P2P Tests Pass All Partial None\nAll Resolved Partially Resolved No-Op\nPartial Breaking Resolved Work in Progress Regression\nNone Breaking Resolved Work in Progress Regression\nTable 22: We present the 6 possible outcomes for a patch generation that is applied successfully and\nthen executed. The outcomes are distinguished by the number of F2P and P2P tests that pass.\nsuccessfully. Across all metrics, we find that patch generations across models are much closer in\nsize to the characteristics of average gold edits. While some models still generate fewer lines relative\nto the corresponding Gold edit (e.g., Claude-2, ChatGPT-3.5, GPT-4), the SWE-Llama models edits\nare on average longer in most respects.. When considering both Table 8 and Table 24, it becomes\nclear that models struggle with generating longer output sequences to be correctly formatted patches.\nFurther inspection of such occurrences, as shown in our case studies in Section F, indicate that\nhallucinations, abiding to existing code style/structure, and referencing long range dependencies\ncorrectly are common errors that surface more frequently in longer generations.\nC.7 S OFTWARE ENGINEERING METRICS\nWe perform preliminary evaluations that explore using software engineering metrics to evaluate the\nefficiency and complexity of large code blocks integrated within a complex codebase. Unlike se-\nmantic similarity scoring functions for evaluating fluency and surface form likeness that are popular\nwith traditional NLP benchmarks and have been adopted for code generation, metrics such as Cyclo-\nmatic complexity McCabe (1976) and Halstead complexity measures Halstead (1977) are founded\nupon logical abstractions (e.g., Abstract Syntax Trees) and software principles to quantify the com-\nplexity, efficiency, and readability of code as a scalar value. The patch generations and SWE-bench\nevaluation logs are rich sources of information that software engineering metrics and static analyz-\ners can readily be applied to. Unlike small, code contest benchmarks where the insights of soft-\nware engineering metrics are not meaningful due to the minuscule scope of the target functionality,\nSWE-bench\u2019s task is complex enough that practitioners can use these tools to gain well-structured,\nrigorous, and wide-ranging feedback signals on the complexity of a patch generation\u2019s change and\nits effect on the rest of the codebase.\nWe include our exploratory work here that demonstrates how software engineering metrics can re-\nliably capture characteristics of code quality, and how comparing these statistics across two patches\ncan provide automatic observations about model capabilities. We use the Radon package, a library\nfor computing different software engineering metrics directly from source code.\nRadon Documentation, open-source codebase\n27", "start_char_idx": 0, "end_char_idx": 3732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b99001b-ac79-4314-96fa-4678826312f7": {"__data__": {"id_": "7b99001b-ac79-4314-96fa-4678826312f7", "embedding": null, "metadata": {"page_label": "28", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a078511-b5f0-4ff3-90db-2eb2cd1df1e8", "node_type": "4", "metadata": {"page_label": "28", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aa3c29b283bb3a976dea2a502118189dde29ec54daa8a88a5a0fbae141f4b88f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nModel Claude 2 ChatGPT-3.5 GPT-4\u2217SWE-Llama 7b SWE-Llama 13b\nApplied 1078 284 76 1257 1196\nResolved 110 12 10 69 91\nBreaking Resolved 26 2 3 17 10\nPartially Resolved 15 4 3 17 10\nWork in Progress 20 2 1 17 16\nNo-Op 471 174 30 716 672\nRegression 436 90 29 421 397\nTable 23: Categorization of model generations that applied successfully by the cases defined in\nTable 22. As mentioned, GPT-4 was evaluated on a 25% subset (574 instances) of SWE-bench.\nTable 24: Average edits of model generated patches in the oracle retrieval setting across all patches\n(including unsuccessfully applied patches). For the task instances specific to each model, we calcu-\nlate the same statistics across the gold patches.\nModel Total Lines Added Removed Functions Files\nClaude 2 27.2 6.6 3.3 1.2 1.1\nGold 61.6 17.8 8.6 2.6 1.4\nChatGPT-3.5 42.0 6.1 3.9 1.7 1.0\nGold 44.5 12.7 5.5 2.1 1.2\nGPT-4 22.4 4.4 1.8 0.8 0.9\nGold 50.3 14.0 6.5 2.3 1.3\nSWE-Llama 13b 68.9 9.5 4.3 2.5 1.6\nGold 61.5 17.8 8.6 2.6 1.4\nSWE-Llama 7b 78.9 10.1 7.6 2.5 1.5\nGold 65.1 18.8 9.0 2.7 1.5\nWe look specifically at successfully applied Claude 2 patch predictions in the \u201cOracle\u201d retrieval set-\nting for the psf/requests repository, which several models perform best at as reflected in Fig-\nure 4. Per prediction, we apply the patch to the codebase, then calculate the Cyclomatic complexity\nand Halstead complexity scores for the modified functions. Cyclomatic complexity quantifies the\ncontrol flow of a function, counting the number of independent execution paths through the source\ncode (McCabe, 1976). A higher Cyclomatic complexity score suggests a more complex function\nthat has higher likelihood of defects and usually suggests difficult maintainability. Halstead com-\nplexity counts the number of operators and operands in a program (Halstead, 1977). Per prediction,\nwe also perform the same set of steps for the corresponding gold patch.\nWe find that software engineering metrics provides automatic qualitative insights into model perfor-\nmance. Consider the following simple case study in Figure 10. While the model patch prediction\n(left) is fewer lines ( 6instead of 11) and modifies fewer files ( 1instead of 2) compared to the gold ref-\nerence solution (right), the model\u2019s edit places a conditional within a relatively complex and widely\nusedHTTPAdapter class. This introduces two new potential execution outcomes, raising the Cy-\nclomatic complexity of HTTPAdapter from3to5. In contrast, while longer, the reference solution\nimports intra-module dependencies, modifies a logically simpler function in getconnection ,\nand defines a new error type InvalidProxyURL to capture the novel bug described by the issue.\nD A DDITIONAL EXPERIMENTAL DETAILS\nD.1 R ETRIEVAL DETAILS\nSparse retrieval. During retrieval we make a slight augmentation to the documents by pre-pended\nfiles\u2019 contents with their file paths to better enable retrieval based on filenames that may be men-\ntioned directly in the issue.\n28", "start_char_idx": 0, "end_char_idx": 3011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e792912-93da-49d4-a338-dc8248c92cd4": {"__data__": {"id_": "9e792912-93da-49d4-a338-dc8248c92cd4", "embedding": null, "metadata": {"page_label": "29", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db27aafa-5ffe-437c-80fe-782cb013c41b", "node_type": "4", "metadata": {"page_label": "29", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3758845215e03be793afe141060bbff9733ddcacf3fc87c4689543ccf8e72862", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nProblem Statement : Misleading exception with invalid protocol in proxy variable. When the value\nofhttps proxy orHTTPS PROXY variable(s) accidentally miss one \u2019/\u2019 in the protocol, a trace-\nback is thrown to the user which doesn\u2019t pin point that the issue is with the proxy configuration...\nFigure 10: Comparison of the Claude 2 prediction (left) and reference solution (right) patches\nfor SWE-bench task instance psf requests-4356 . While the code generated by the\npatch is fewer lines of code and solves the problem correctly, the prediction patch intro-\nduces greater Cyclomatic complexity ( requests.adapters.py/HTTPAdapter : 3\u21925)\ncompared to the gold solution ( requests/adapters.py:get connection : 2\u21923,\nrequests/exceptions.py:InvalidHeader : 0\u21921). Changes that introduce new exe-\ncution paths are boxed in blue. Parts of the gold patch have been truncated for appearance.\nOracle retrieval. Oracle retrieval file paths are simply extracted directly from the reference solu-\ntion\u2019s patch file excluding test files.\nD.2 I NFERENCE SETTINGS\nSince generations are relatively expensive, we only generate a single patch file per instance. Follow-\ning precedent in code generation for evaluation in Pass@ 1(Chen et al., 2021; Rozi `ere et al., 2023),\nwe simply use greedy decoding for all models.\nD.3 P ROMPT TEMPLATE EXAMPLE\nModels are prompted with the following general template with slight variations depending on the\nmodel used.\nYou will be provided with a partial code base and an issue statement\nexplaining a problem to resolve.\n<issue>\n{ISSUE TEXT}\n</issue>\n<code>\n[start of README.md]\n{README.md text}\n[end of README.md]\n[start of file_1.py]\n{file_1.py text}\n[end of file_1.py]\n...\n</code>\nHere is an example of a patch file. It consists of changes to the code\nbase. It specifies the file names, the line numbers of each change,\n29", "start_char_idx": 0, "end_char_idx": 1883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5ab681b-0bb0-427b-8004-e4a8dafb00cf": {"__data__": {"id_": "a5ab681b-0bb0-427b-8004-e4a8dafb00cf", "embedding": null, "metadata": {"page_label": "30", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47f69e3e-d902-436e-8701-98897f601fe8", "node_type": "4", "metadata": {"page_label": "30", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f0326b9797360ca76d05dceee486a1baaa683893ea944dc3a1382c3f09c8fec2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nand the removed and added lines. A single patch file can contain\nchanges to multiple files.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\ndef euclidean(a, b):\n- while b:\n- a, b = b, a % b\n- return a\n+ if b == 0:\n+ return a\n+ return euclidean(b, a % b)\ndef bresenham(x0, y0, x1, y1):\npoints = []\ndx = abs(x1 - x0)\ndy = abs(y1 - y0)\n- sx = 1 if x0 < x1 else -1\n- sy = 1 if y0 < y1 else -1\n- err = dx - dy\n+ x, y = x0, y0\n+ sx = -1 if x0 > x1 else 1\n+ sy = -1 if y0 > y1 else 1\n- while True:\n- points.append((x0, y0))\n- if x0 == x1 and y0 == y1:\n- break\n- e2 = 2 *err\n- if e2 > -dy:\n+ if dx > dy:\n+ err = dx / 2.0\n+ while x != x1:\n+ points.append((x, y))\nerr -= dy\n- x0 += sx\n- if e2 < dx:\n- err += dx\n- y0 += sy\n+ if err < 0:\n+ y += sy\n+ err += dx\n+ x += sx\n+ else:\n+ err = dy / 2.0\n+ while y != y1:\n+ points.append((x, y))\n+ err -= dx\n+ if err < 0:\n+ x += sx\n+ err += dy\n+ y += sy\n+ points.append((x, y))\nreturn points\n</patch>\nI need you to solve the provded issue by generating a single patch file\nthat I can apply directly to this repository using git apply. Please\nrespond with a single patch file in the format shown above.\nRespond below:\n30", "start_char_idx": 0, "end_char_idx": 1198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76efa852-28d6-4caa-bb09-43345c43484c": {"__data__": {"id_": "76efa852-28d6-4caa-bb09-43345c43484c", "embedding": null, "metadata": {"page_label": "31", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d28dc3a-4b54-4ba9-af63-f6b372141d05", "node_type": "4", "metadata": {"page_label": "31", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3d8458330ec5154456cfe88f8021245608a1111ecd721f0c06cf2e0a4896af77", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nExperiments using slightly more or fewer lines of instructions or examples seemed to not affect\noverall performance substantially, except for the findings of experiments stated in Section 5.\nE S OCIETAL IMPACT\nAs reasoning on code has emerged as a foundational skill underlying many LM\u2019s capability, a po-\ntential future of machine-automated software engineering raises many important questions and has\nimportant potential ramifications with regards to AI Safety (Gros et al., 2023). It is important to ad-\ndress questions on how to ensure AI-generated code is faithful to human intents and what guardrails\nmight be in place when human objectives are misinterpreted by code agents that then carry out the\ntask. To observe such problems in a controlled setting and manifest their solutions, we hope SWE-\nbench might serve as a testbed for designing safe, robust measures towards aligned, verifiable, and\nsafe AI-driven software engineering.\nF I N-DEPTH ANALYSIS OF SWE-L LAMA GENERATIONS\nIn this section, we provide five additional qualitative analyses of generations from both Claude 2\nand SWE-Llama generations (Oracle retrieval setting) following the style of Section 5.1.\nClaude 2 qualitative studies can be found in Tables 25 and 26. Tables 27, 28, and 29 are task\ninstances that Claude 2 did not address correctly. SWE-Llama qualitative studies are covered across\nTables 30, 31, 32, 33, 34. For Tables 30, 31, and 32, we present task instances solved correctly\nby SWE-Llama 13b. In Table 33 and 34, we present two task instances where SWE-Llama 13b\ndoes not address the issue correctly, pointing out a subset of the reasoning and generation skills that\nmodels may not be adept at enough to accomplish the task at hand.\nThe observations we make across these sections corroborate with the points stated in the main paper,\nwhich is that models tend to struggle with multi-line and multi-file changes, are more adept when\nthe required fix is relatively short, and need help with understanding the codebase in an efficient\nmanner.\n31", "start_char_idx": 0, "end_char_idx": 2078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f924c019-fbc0-42be-9995-e5c026033e13": {"__data__": {"id_": "f924c019-fbc0-42be-9995-e5c026033e13", "embedding": null, "metadata": {"page_label": "32", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1a34d79-00b3-46a1-9166-473f2b292e8c", "node_type": "4", "metadata": {"page_label": "32", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "83008f90605a83e35cd9e70b7c8f077f590f432bb176dee70a815a8aa05c2795", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 25: In this example, Claude 2 correctly addresses an issue from scikit-learn/scikit-learn. How-\never, as demonstrated in the discussion, while the solution is correct, it demonstrates models\u2019 ten-\ndency to write primitive Python and not employ existing methods within the codebase.\nSetting\n\u2022 Model: Claude 2 with Oracle retrieval\n\u2022 Task Instance: scikit-learn scikit-learn-13328\n\u2013Link to Issue: https://github.com/scikit-learn/scikit-learn/pull/13314\n\u2013Link to PR: https://github.com/scikit-learn/scikit-learn/issues/13328\nIssue\nTypeError when supplying a boolean X to HuberRegressor fit\n#### Description\n\u2018TypeError\u2019 when fitting \u2018HuberRegressor\u2019 with boolean predictors.\n#### Steps/Code to Reproduce\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import HuberRegressor\n# Random data\nX, y, coef = make_regression(n_samples=200, n_features=2, noise=4.0, coef\n=True, random_state=0)\nX_bool = X > 0\nX_bool_as_float = np.asarray(X_bool, dtype= float )\n# Works\nhuber = HuberRegressor().fit(X, y)\n# Fails (!)\nhuber = HuberRegressor().fit(X_bool, y)\n# Also works\nhuber = HuberRegressor().fit(X_bool_as_float, y)\n#### Expected Results\nNo error is thrown when \u2018dtype\u2019 of \u2018X\u2019 is \u2018bool\u2019 (second line of code in the snipped above,\n.fit(X_bool, y) ). Boolean array is expected to be converted to \u2018float\u2019 by \u2018HuberRegressor.fit\u2019\nas it is done by, say \u2018LinearRegression\u2019.\n#### Actual Results\n\u2018TypeError\u2018 is thrown:\n(Remaining lines omitted)\nGold Patch\ndiff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.\npy\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -251,7 +251,8 @@ def fit(self, X, y, sample_weight=None):\nself : object\n\"\"\"\nX, y = check_X_y(\n- X, y, copy=False, accept_sparse=[\u2019csr\u2019], y_numeric=True)\n+ X, y, copy=False, accept_sparse=[\u2019csr\u2019], y_numeric=True,\n+ dtype=[np.float64, np.float32])\nif sample_weight is not None:\nsample_weight = np.array(sample_weight)\ncheck_consistent_length(y, sample_weight)\n32", "start_char_idx": 0, "end_char_idx": 2041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f4a9a77-6c7b-4007-bbd0-22571a327dbc": {"__data__": {"id_": "3f4a9a77-6c7b-4007-bbd0-22571a327dbc", "embedding": null, "metadata": {"page_label": "33", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d0efa5e7-736c-4f30-b3f8-a2e1e61a1817", "node_type": "4", "metadata": {"page_label": "33", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7cfd2c4e5ce139f1a39ce4b3af6f3cb07e12d2f0eeafbe87530cf4378e4b60af", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nModel Generated Patch\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -231,6 +231,9 @@ class HuberRegressor(LinearModel, RegressorMixin,\nBaseEstimator):\nself.tol = tol\n+def _validate_data(self, X, y=None):\n+ return X.astype(np.float64) ifX.dtype == np. bool else X\n+\ndef fit(self, X, y, sample_weight=None):\n\"\"\"Fit the model according to the given training data.\n@@ -250,7 +253,7 @@ class HuberRegressor(LinearModel, RegressorMixin,\nBaseEstimator):\n-------\nself : object\n\"\"\"\n- X, y = check_X_y(\n+ X, y = check_X_y(\nX, y, copy=False, accept_sparse=[\u2019csr\u2019], y_numeric=True)\nifsample_weight is not None:\nsample_weight = np.array(sample_weight)\n@@ -262,6 +265,9 @@ class HuberRegressor(LinearModel, RegressorMixin,\nBaseEstimator):\nraise ValueError(\n\"epsilon should begreater than orequal to1.0, got %f\"\n% self.epsilon)\n+\n+ X = self._validate_data(X)\nTests\nFail to Pass (1)\nsklearn/linear_model/tests/test_huber.py::test_huber_bool\nPass to Pass (9)\nsklearn/linear_model/tests/test_huber.py::test_huber_max_iter\nsklearn/linear_model/tests/test_huber.py::test_huber_gradient\nsklearn/linear_model/tests/test_huber.py::test_huber_sample_weights\nsklearn/linear_model/tests/test_huber.py::test_huber_sparse\nsklearn/linear_model/tests/test_huber.py::test_huber_scaling_invariant\nAdditional Pass to Pass Tests omitted...\nDiscussion. In this task instance, the issue is describing an issue regarding how types are cast.\nSpecifically, when When fitting the HuberRegressor model with boolean predictors, like\nX_bool , aTypeError occurs. This issue arises because the HuberRegressor does not auto-\nmatically convert boolean arrays to floats as some other models do, such as LinearRegression ,\ncausing a type mismatch error. For its fix, the model defines its own _validate_data function,\nwhich encapsulates the type casting logic, and applies it to X. While it passes the tests, this fix\ncould be more efficient and potentially introduces stylistic and organizational inconsistencies. As\ndemonstrated in the reference solution, there is a much simpler solution of passing in dtype argu-\nments that leverage the existing flags of the check_X_y function call to resolve this more cleanly.\nOther files\u2019 content was not included in the oracle retrieval setting due to the context window limi-\ntation.\n33", "start_char_idx": 0, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "010343b2-35f0-400b-96fd-9b8272d9028b": {"__data__": {"id_": "010343b2-35f0-400b-96fd-9b8272d9028b", "embedding": null, "metadata": {"page_label": "34", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ffc809a-9fa2-45bc-a00a-f46dd7a7f53a", "node_type": "4", "metadata": {"page_label": "34", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5f051a2f5f0a1250e5f9ebddd6ab7ddc90f776e98dd3321532829cb38d753afa", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 26: We provide another example where Claude-2 solves the issue correctly and develops a\nsolution similar to the gold patch. However, the gold patch solution is more cognizant of avoiding\nfuture potential issues that could be related to this code.\nSetting\n\u2022 Model: Claude 2 with Oracle retrieval\n\u2022 Task Instance: matplotlib matplotlib-24362\n\u2013Link to Issue: https://github.com/matplotlib/matplotlib/issues/24349\n\u2013Link to PR: https://github.com/matplotlib/matplotlib/pull/24362\nIssue\n[Bug]: sharex and sharey don\u2019t accept 0 and 1 as bool values\n### Bug summary\nWhen using \u20180\u2018 or \u20181\u2018 in place of \u2018False\u2018 or \u2018True\u2018 in \u2018sharex\u2018 or \u2018sharex\u2018 arguments of \u2018py-\nplot.subplots\u2018 an error is raised.\n### Code for reproduction\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(ncols=2,sharey=1)\n### Actual outcome\nWe get the following error :\nTraceback (most recent call last):\n(Earlier lines omitted)\nFile \"/ ***/matplotlib/lib/matplotlib/gridspec.py\", line 293, insubplots\n_api.check_in_list([\"all\", \"row\", \"col\", \"none\"],\nFile \"/ ***/matplotlib/lib/matplotlib/_api/__init__.py\", line 131, in\ncheck_in_list\nraise ValueError(msg)\nValueError: 1 is not a valid value for sharey; supported values are \u2019all\u2019\n, \u2019row\u2019, \u2019col\u2019, \u2019none\u2019\nUsing \u2018sharex\u2018 instead of \u2018sharey\u2018 produces the same error (albeit with the following warning:\nUserWarning: sharex argument to subplots() was an integer. Did you intend\nto use subplot() (without \u2019s\u2019)?\nbut this is expected and not part of the present issue)\n### Expected outcome\nI expected values 1 and 0 to be understood as bool.\nModel Generated Patch\ndiff --git a/lib/matplotlib/gridspec.py b/lib/matplotlib/gridspec.py\nindex 06dd3f19f6..32ee7c306e 100644\n--- a/lib/matplotlib/gridspec.py\n+++ b/lib/matplotlib/gridspec.py\n@@ -276,9 +276,9 @@ class GridSpecBase:\nraise ValueError(\"GridSpec.subplots() only works for GridSpecs\n\"\n\"created with aparent figure\")\n- if isinstance (sharex, bool ):\n+ if isinstance (sharex, bool )orsharex == 1 orsharex == 0:\nsharex = \"all\" ifsharex else \"none\"\n- if isinstance (sharey, bool ):\n+ if isinstance (sharey, bool )orsharey == 1 orsharey == 0:\nsharey = \"all\" ifsharey else \"none\"\n# This check was added because it is very easy to type\n# \u2018subplots(1, 2, 1)\u2018 when \u2018subplot(1, 2, 1)\u2018 was intended.\n34", "start_char_idx": 0, "end_char_idx": 2304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56eb98a5-c3b5-44eb-8eec-1119a9804f8b": {"__data__": {"id_": "56eb98a5-c3b5-44eb-8eec-1119a9804f8b", "embedding": null, "metadata": {"page_label": "35", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00c53246-49c5-480b-9396-3ef42bb4f090", "node_type": "4", "metadata": {"page_label": "35", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "33c078a2bb845305f882820c6d3fc03cba9d146a45cc025723ae8097f09ac756", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nGold Patch\ndiff --git a/lib/matplotlib/gridspec.py b/lib/matplotlib/gridspec.py\n--- a/lib/matplotlib/gridspec.py\n+++ b/lib/matplotlib/gridspec.py\n@@ -276,21 +276,12 @@ def subplots(self, *, sharex=False, sharey=False,\nsqueeze=True,\n- if isinstance (sharex, bool ):\n+ if not isinstance (sharex, str):\nsharex = \"all\" ifsharex else \"none\"\n- if isinstance (sharey, bool ):\n+ if not isinstance (sharey, str):\nsharey = \"all\" ifsharey else \"none\"\n- # This check was added because it is very easy to type\n- # \u2018subplots(1, 2, 1)\u2018 when \u2018subplot(1, 2, 1)\u2018 was intended.\n- # In most cases, no error will ever occur, but mysterious behavior\n- # will result because what was intended to be the subplot index is\n- # instead treated as a bool for sharex. This check should go away\n- # once sharex becomes kwonly.\n- if isinstance (sharex, Integral):\n- _api.warn_external(\n- \"sharex argument tosubplots() was aninteger. Did you \"\n- \"intend touse subplot() (without \u2019s\u2019)?\")\n- _api.check_in_list([\"all\", \"row\", \"col\", \"none\"],\n+\n+ _api.check_in_list([\"all\", \"row\", \"col\", \"none\", False, True],\nsharex=sharex, sharey=sharey)\nTests\nFail to Pass (1)\nlib/matplotlib/tests/test_subplots.py::test_shared\nPass to Pass (39)\nlib/matplotlib/tests/test_subplots.py::test_label_outer_span\nlib/matplotlib/tests/test_subplots.py::test_label_outer_non_gridspec\nlib/matplotlib/tests/test_subplots.py::test_shared_and_moved\nlib/matplotlib/tests/test_subplots.py::test_exceptions\nlib/matplotlib/tests/test_subplots.py::test_subplots_offsettext[png]\nAdditional Pass to Pass Tests omitted...\nDiscussion. This example asks for 1and0to be treated as boolean values for the sharex and\nsharey arguments of the pyplot.subplots function. The current code raises an error. In\nthe generated patch, the model adds 1and0as acceptable values. However, the gold patch adds\nthe acceptable values into a list [\"all\", \"row\", \"col\", \"none\", False, True] . It also\nremoves the Integral related warning tied to this issue. This clean up along with the clarity of\nthe check_in_list argument makes the code more concise and readable.\n35", "start_char_idx": 0, "end_char_idx": 2121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c81d2f9e-1d17-477c-9af8-e454710c3c53": {"__data__": {"id_": "c81d2f9e-1d17-477c-9af8-e454710c3c53", "embedding": null, "metadata": {"page_label": "36", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf5098dc-6d7a-4671-b3c8-f7f8e6972b63", "node_type": "4", "metadata": {"page_label": "36", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e22becfa7fc7b5348be80701a214f0cbea0c230c7a5417d2ed4f746ab8144e83", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 27: In this example, we show an issue from astropy/astropy that Claude 2 does not solve\ncorrectly. The error is primarily due to the patch generation\u2019s attempt to directly solve the issue.\nSetting\n\u2022 Model: Claude 2 with Oracle retrieval\n\u2022 Task Instance: astropy astropy-14365\n\u2013Link to Issue: https://github.com/astropy/astropy/issues/14363\n\u2013Link to PR: https://github.com/astropy/astropy/pull/14365\nIssue\nascii.qdp Table format assumes QDP commands are upper case\n### Description\nascii.qdp assumes that commands in a QDP file are upper case, for example, for errors they must be\n\u201cREAD SERR 1 2\u201d whereas QDP itself is not case sensitive and case use \u201cread serr 1 2\u201d. As many\nQDP files are created by hand, the expectation that all commands be all-caps should be removed.\n### Expected behavior\nThe following qdp file should read into a \u2018Table\u2018 with errors, rather than crashing.\nread serr 1 2\n1 0.5 1 0.5\n### How to Reproduce\nCreate a QDP file:\n>cat >test .qdp\nread serr 1 2\n1 0.5 1 0.5\n<EOF>\n> python\nPython 3.10.9 (main, Dec 7 2022, 02:03:23) [Clang 13.0.0 (clang\n-1300.0.29.30)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from astropy.table import Table\n>>> Table. read (\u2019test.qdp\u2019,format=\u2019ascii.qdp\u2019)\nWARNING: table_id not specified. Reading the first available table [\nastropy.io.ascii.qdp]\nTraceback (most recent call last):\n...\nraise ValueError(f\u2019Unrecognized QDP line: {line}\u2019)\nValueError: Unrecognized QDP line: read serr 1 2\nRunning \u201cqdp test.qdp\u201d works just fine.\nModel Generated Patch\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@ -60,7 +60,7 @@\nValueError: Unrecognized QDP line...\n\"\"\"\n_decimal_re = r\"[+-]?(\\d+(\\.\\d *)?|\\.\\d+)([eE][+-]?\\d+)?\"\n- _command_re = r\"READ [TS]ERR(\\s+[0-9]+)+\"\n+ _command_re = r\"read [ts]err(\\s+[0-9]+)+\"\nsep = delimiter\nif delimiter is None:\n36", "start_char_idx": 0, "end_char_idx": 1893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee888244-2d6a-4c42-9d75-2f0f25c19be8": {"__data__": {"id_": "ee888244-2d6a-4c42-9d75-2f0f25c19be8", "embedding": null, "metadata": {"page_label": "37", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d773a3f-33a0-4638-ac85-00b61f5e71f8", "node_type": "4", "metadata": {"page_label": "37", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f1d204dfc43dc953be90dad053e7db984a29eb653aada820ece9518654e9b4a3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nGold Patch\ndiff --git a/astropy/io/ascii/qdp.py b/astropy/io/ascii/qdp.py\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@ -68,7 +68,7 @@ def _line_type(line, delimiter=None):\n_new_re = rf\"NO({sep}NO)+\"\n_data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO\n|[-+]?nan)) *)\"\n_type_re = rf\"\u02c6\\s *((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<\ndata>{_data_re})?\\s *(\\!(?P<comment>. *))?\\s *$\"\n- _line_type_re = re. compile (_type_re)\n+ _line_type_re = re. compile (_type_re, re.IGNORECASE)\nline = line.strip()\nif not line:\nreturn \"comment\"\n@@ -306,7 +306,7 @@ def _get_tables_from_qdp_file(qdp_file,\ninput_colnames=None, delimiter=None):\nvalues = []\nfor vinline.split(delimiter):\n- ifv == \"NO\":\n+ ifv.upper() == \"NO\":\nvalues.append(np.ma.masked)\nelse :\n# Understand if number is int or float\nTests\nFail to Pass (1)\nastropy/io/ascii/tests/test_qdp.py::test_roundtrip[True]\nPass to Pass (8)\nastropy/io/ascii/tests/test_qdp.py::test_get_tables_from_qdp_file\nastropy/io/ascii/tests/test_qdp.py::test_roundtrip[False]\nastropy/io/ascii/tests/test_qdp.py::test_read_example\nastropy/io/ascii/tests/test_qdp.py::test_roundtrip_example\nastropy/io/ascii/tests/test_qdp.py::test_roundtrip_example_comma\nastropy/io/ascii/tests/test_qdp.py::test_read_write_simple\nastropy/io/ascii/tests/test_qdp.py::test_read_write_simple_specify_name\nastropy/io/ascii/tests/test_qdp.py::test_get_lines_from_qdp\nDiscussion. This issue requests a fix for handling QDP files; specifically, it asks for the expectation\nof the commands to be uppercase to be removed. In the model generated patch, the model locates\nthe READ and ERR commands mentioned in the issue description in a regex pattern and edits it\ndirectly. However, within the same file, it is clear that the regex pattern is used in additional patterns,\nsuch as _type_re , and so a fix to the \u201croot\u201d regex pattern may not be appropriate. In addition,\namidst multiple regex pattern variables, identifying the correct one to modify requires reasoning that\ncan potentially be helped by execution. This error in this solution is reflected in the fact that it not\nonly fails to pass the given unit tests, but also fails existing tests enforcing prior behavior.\n37", "start_char_idx": 0, "end_char_idx": 2256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e54d55a-5c24-445d-8b62-21e117c8b33e": {"__data__": {"id_": "1e54d55a-5c24-445d-8b62-21e117c8b33e", "embedding": null, "metadata": {"page_label": "38", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8538221a-6eaf-465d-8c19-1bcddb3035ef", "node_type": "4", "metadata": {"page_label": "38", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "37e441e8fef293787b97b1095dd8d8f4aa46e5f80f3295e3066474efef1e4977", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 28: For this issue from the mwaskom/seaborn repository, the problem statement includes\nhyperlinks to images. As discussed in Section 5, a minor subset of SWE-bench tasks include images\nin them, making image understanding a small but important component to resolving issues that is\nunexplored by the initial baselines.\nSetting\n\u2022 Model: Claude 2 with Oracle retrieval\n\u2022 Task Instance: mwaskom seaborn-3217\n\u2013Link to Issue: https://github.com/mwaskom/seaborn/issues/2907\n\u2013Link to PR: https://github.com/mwaskom/seaborn/pull/3217\nIssue\nWidth computation after histogram slightly wrong with log scale Note the slight overlap here:\n(\nso.Plot(tips, \"total_bill\")\n.add(so.Bars(alpha=.3, edgewidth=0), so.Hist(bins=4))\n.scale(x=\"log\")\n)\n![image]( https://user-images.githubusercontent.com/315810/\n178975852-d8fd830e-ae69-487d-be22-36531fca3f8f.png )\nIt becomes nearly imperceptible with more bins:\n(\nso.Plot(tips, \"total_bill\")\n.add(so.Bars(alpha=.3, edgewidth=0), so.Hist(bins=8))\n.scale(x=\"log\")\n)\n![image]( https://user-images.githubusercontent.com/315810/\n178976113-7026b3ae-0b87-48df-adc0-00e90d5aea94.png )\nThis is not about \u2018Bars\u2018; \u2018Bar\u2018 has it too:\n(\nso.Plot(tips, \"total_bill\")\n.add(so.Bar(alpha=.3, edgewidth=0, width=1), so.Hist(bins=4))\n.scale(x=\"log\")\n)\n![image]( https://user-images.githubusercontent.com/315810/\n178975910-484df65f-4ce6-482e-9992-5d02faf6b9ea.png )\nModel Generated Patch\n--- a/seaborn/_core/plot.py\n+++ b/seaborn/_core/plot.py\n@@ -1327,9 +1327,7 @@\nelif \"width\" indf:\nview_width = view_df[\"width\"]\nelse :\n- view_width = 0.8 # TODO what default?\n- spacing = scales[orient]._spacing(view_df.loc[view_idx,\norient])\n- width.loc[view_idx] = view_width *spacing\n+ width.loc[view_idx] = view_width\ndf[\"width\"] = width\nif\"baseline\" inmark._mappable_props:\n38", "start_char_idx": 0, "end_char_idx": 1822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1a84491-5209-44d4-96d4-95d57a6881c7": {"__data__": {"id_": "c1a84491-5209-44d4-96d4-95d57a6881c7", "embedding": null, "metadata": {"page_label": "39", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5c9c990-88d5-48d9-9f41-eb9c2c7143b8", "node_type": "4", "metadata": {"page_label": "39", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "41a3a88c3ba57c52c21916dd16b4fe4ff57c13b19ecd72a9d696deb1948e3d90", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nGold Patch\ndiff --git a/seaborn/_core/plot.py b/seaborn/_core/plot.py\n--- a/seaborn/_core/plot.py\n+++ b/seaborn/_core/plot.py\n@@ -1377,10 +1377,9 @@ def _unscale_coords(\n- drop_cols = [ *coord_cols, \"width\"] if\"width\" indfelse\ncoord_cols\nout_df = (\ndf\n- .drop(drop_cols, axis=1)\n+ .drop(coord_cols, axis=1)\n.reindex(df.columns, axis=1) # So unscaled columns retain their\nplace\n@@ -1396,12 +1395,6 @@ def _unscale_coords(\n- ifvar == orient and \"width\" inview_df:\n- width = view_df[\"width\"]\n- out_df.loc[values.index, \"width\"] = (\n- transform(values + width / 2) - transform(values -\nwidth / 2)\n- )\n-\nreturn out_df\ndiff --git a/seaborn/_marks/bar.py b/seaborn/_marks/bar.py\n--- a/seaborn/_marks/bar.py\n+++ b/seaborn/_marks/bar.py\n@@ -29,17 +29,23 @@ class BarBase(Mark):\n+ transform = scales[orient]._matplotlib_scale.get_transform()\n+ forward = transform.transform\n+ reverse = transform.inverted().transform\n+\n+ other = {\"x\": \"y\", \"y\": \"x\"}[orient]\n+\n+ pos = reverse(forward(data[orient]) - data[\"width\"] / 2)\n+ width = reverse(forward(data[orient]) + data[\"width\"] / 2) - pos\n+\n+ val = (data[other] - data[\"baseline\"]).to_numpy()\n+ base = data[\"baseline\"].to_numpy()\n+\n(Remaining lines omitted...)\nTests\nFail to Pass (1)\ntests/_marks/test_bar.py::TestBars::test_log_scale\nPass to Pass (8)\ntests/_marks/test_bar.py::TestBar::test_categorical_positions_vertical\ntests/_marks/test_bar.py::TestBar::test_categorical_positions_horizontal\ntests/_marks/test_bar.py::TestBar::test_numeric_positions_vertical\ntests/_marks/test_bar.py::TestBar::test_numeric_positions_horizontal\ntests/_marks/test_bar.py::TestBar::test_set_properties\nAdditional Pass to Pass Tests omitted...\nDiscussion. The baseline model\u2019s inability to solve this issue can be directly attributed to a lack of\nan image input. We include this example to demonstrate that debugging real software issues may in-\nvolve digesting images; this presents a whole separate body of understanding and reasoning-related\nchallenges. Beyond the SWE-bench task, additional problems in software engineering, such as inter-\npreting system design diagram, user interface specifications, and user feedback presents interesting\nvision-language challenges.\n39", "start_char_idx": 0, "end_char_idx": 2241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc8a4c5d-8f1d-475a-8c66-d799263e379f": {"__data__": {"id_": "dc8a4c5d-8f1d-475a-8c66-d799263e379f", "embedding": null, "metadata": {"page_label": "40", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e78bb85f-ebb4-405b-87cd-18a2f5dd63df", "node_type": "4", "metadata": {"page_label": "40", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3cd847179a9bee08323d340da1a773387d4152eeaad765541ee121bedd72db4e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 29: In this final example of a Claude 2 generation, the model must resolve an error related to\nresolving an error regarding cyclic dependencies. Claude 2\u2019s solution under-delivers on an otherwise\ncomplex problem.\nSetting\n\u2022 Model: Claude 2 with Oracle retrieval\n\u2022 Task Instance: sympy sympy-18211\n\u2013Link to Issue: https://github.com/pylint-dev/pylint/issues/3525\n\u2013Link to PR: https://github.com/pylint-dev/pylint/pull/4703\nIssue\nImports within TYPE CHECKING can induce \u201ccyclic-import\u201d\nSuppose you have two modules: a.py and b.py. \u2018a\u2018 imports \u2018b\u2018, but \u2018b\u2018 needs \u2018a\u2018 in order to do\ntype annotations, then this is usually done by importing \u2018a\u2018 within a \u2018TYPE CHECKING\u2018 block.\nUnfortunately, this causes pylint to report \u2018cyclic-import\u2018.\nPossibly related to https://github.com/PyCQA/pylint/issues/3285\nGold Patch\ndiff --git a/pylint/checkers/imports.py b/pylint/checkers/imports.py\n--- a/pylint/checkers/imports.py\n+++ b/pylint/checkers/imports.py\n@@ -813,7 +813,9 @@ def _get_imported_module(self, importnode, modname):\n-def _add_imported_module(self, node, importedmodname):\n+def _add_imported_module(\n+ self, node: Union[astroid.Import, astroid.ImportFrom],\nimportedmodname: str\n+ ) -> None:\n\"\"\"notify an imported module, used to analyze dependencies\"\"\"\nmodule_file = node.root(). file\ncontext_name = node.root().name\n@@ -826,6 +828,10 @@ def _add_imported_module(self, node, importedmodname\n):\nexcept ImportError:\npass\n+ in_type_checking_block = (\n+ isinstance (node.parent, astroid.If) and node.parent.\nis_typing_guard()\n+ )\n+\nifcontext_name == importedmodname:\nself.add_message(\"import-self\", node=node)\n@@ -845,7 +851,10 @@ def _add_imported_module(self, node, importedmodname\n):\n# update import graph\nself.import_graph[context_name].add(importedmodname)\n- if not self.linter.is_message_enabled(\"cyclic-import\", line=\nnode.lineno):\n+ if(\n+ not self.linter.is_message_enabled(\"cyclic-import\", line=\nnode.lineno)\n+ orin_type_checking_block\n+ ):\nself._excluded_edges[context_name].add(importedmodname)\n40", "start_char_idx": 0, "end_char_idx": 2053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2c8ad2d-8030-45ff-ae04-aac9b9a5cae6": {"__data__": {"id_": "e2c8ad2d-8030-45ff-ae04-aac9b9a5cae6", "embedding": null, "metadata": {"page_label": "41", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6629e62c-cc7b-4d80-8ea7-101f50d01dad", "node_type": "4", "metadata": {"page_label": "41", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "42a17c6b670336878f0233397730fc83aeea35d7fcf2a8063115143a24e973f0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nModel Generated Patch\ndiff --git a/pylint/checkers/imports.py b/pylint/checkers/imports.py\nindex 4f3eed2..2b74a8d 100644\n--- a/pylint/checkers/imports.py\n+++ b/pylint/checkers/imports.py\n@@ -49,6 +49,7 @@ import copy\nimport os\nimport sys\nfrom distutils import sysconfig\n+from typing import TYPE_CHECKING\nfrom typing import Dict, List, Union\nimport astroid\n@@ -61,6 +62,9 @@ from pylint.checkers.utils import (\nnode_ignores_exception,\n)\nfrom pylint.exceptions import EmptyReportError\n+ifTYPE_CHECKING:\n+ from pylint.lint import PyLinter\n+\nfrom pylint.graph import DotBackend, get_cycles\nfrom pylint.interfaces import IAstroidChecker\nfrom pylint.reporters.ureports.nodes import Paragraph, VerbatimText,\nVNode\nTests\nFail to Pass (1)\ntests/_marks/test_bar.py::TestBars::test_log_scale\nPass to Pass (11)\ntests/test_func.py::test_functionality[func_w0401_disabled_in_func.py]\ntests/test_func.py::test_functionality[func_i0022.py]\ntests/test_func.py::test_functionality[func_w0401.py]\ntests/test_func.py::test_functionality[func_w0801.py]\ntests/test_func.py::test_functionality[func_w0401_package]\nAdditional Pass to Pass Tests omitted...\nDiscussion. In this problem, the model is being asked to provide TYPE_CHECKING based handling\nfor resolving cyclic imports, which pylint currently throws an error for. The gold patch has a detailed\nsolution that involves conditional handling within the TYPE_CHECKING block, excludes the cyclic\nimport messages, and has generally more specific handling. The model generated patch employs a\n\u201cStack Overlow\u201d like solution; while creating a TYPE_CHECKING block is a popular solution for\nresolving imports, solving this issue for the codebase in particular requires a more nuanced solution\nthan a general answer that may be available online.\n41", "start_char_idx": 0, "end_char_idx": 1816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e92f9ed7-3778-4e28-939b-87f513caa73c": {"__data__": {"id_": "e92f9ed7-3778-4e28-939b-87f513caa73c", "embedding": null, "metadata": {"page_label": "42", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15f33460-1672-40f0-9c38-0bc561f4bc5c", "node_type": "4", "metadata": {"page_label": "42", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3bcf27b51c97afafe34f22b06998081c5af8628a809fd39ff69f31e3a4a34942", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 30: Example of a SWE-Llama 13b generation that correctly solves a SWE-bench task instance.\nIn this example, the SWE-Llama generation is exactly the same as the solution.\nSetting\n\u2022 Model: SWE-Llama 13B with Oracle retrieval\n\u2022 Task Instance: pydata xarray-5131\n\u2013Link to Issue: https://github.com/pydata/xarray/pull/5130\n\u2013Link to PR: https://github.com/pydata/xarray/pull/5131\nIssue\nWhen displaying a DatasetGroupBy in an interactive Python session, the first line of output contains\na trailing whitespace. The first example in the documentation demonstrate this:\n>>> import xarray as xr, numpy as np\n>>> ds = xr.Dataset(\n... {\"foo\": ((\"x\", \"y\"), np.random.rand(4, 3))},\n... coords={\"x\": [10, 20, 30, 40], \"letters\": (\"x\", list (\"abba\"))},\n... )\n>>> ds.groupby(\"letters\")\nDatasetGroupBy, grouped over \u2019letters\u2019\n2 groups with labels \u2019a\u2019, \u2019b\u2019.\nThere is a trailing whitespace in the first line of output which is \u201dDatasetGroupBy, grouped over\n\u2018letters\u2019 \u201d. This can be seen more clearly by converting the object to a string (note the whitespace\nbefore n):\n>>> str(ds.groupby(\"letters\"))\n\"DatasetGroupBy, grouped over \u2019letters\u2019 \\n2 groups with labels \u2019a\u2019, \u2019b\u2019.\"\nWhile this isn\u2019t a problem in itself, it causes an issue for us because we use flake8 in continuous\nintegration to verify that our code is correctly formatted... [20 more lines]\nGold Patch\ndiff --git a/xarray/core/groupby.py b/xarray/core/groupby.py\n--- a/xarray/core/groupby.py\n+++ b/xarray/core/groupby.py\n@@ -436,7 +436,7 @@ def __iter__(self):\nreturn zip (self._unique_coord.values, self._iter_grouped())\ndef __repr__(self):\n- return \"{}, grouped over {!r} \\n{!r} groups with labels {}.\". format (\n+ return \"{}, grouped over {!r}\\n{!r} groups with labels {}.\". format (\nself.__class__.__name__,\nModel Generated Patch\ndiff --git a/xarray/core/groupby.py b/xarray/core/groupby.py\n--- a/xarray/core/groupby.py\n+++ b/xarray/core/groupby.py\n@@ -436,7 +436,7 @@ def __iter__(self):\nreturn zip (self._unique_coord.values, self._iter_grouped())\ndef __repr__(self):\n- return \"{}, grouped over {!r} \\n{!r} groups with labels {}.\". format (\n+ return \"{}, grouped over {!r}\\n{!r} groups with labels {}.\". format (\nself.__class__.__name__,\n42", "start_char_idx": 0, "end_char_idx": 2238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "085f21eb-e129-4552-9fbe-9207eb073414": {"__data__": {"id_": "085f21eb-e129-4552-9fbe-9207eb073414", "embedding": null, "metadata": {"page_label": "43", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dee80d4d-37ba-428b-b5ad-70ead45bf44d", "node_type": "4", "metadata": {"page_label": "43", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5e4b020b43303c179cc9ede3a8efe48823dcf7daeb1ef015992e55f5010569cc", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTests\nFail to Pass (10)\nxarray/tests/test_groupby.py::test_groupby_repr[obj0-x]\nxarray/tests/test_groupby.py::test_groupby_repr[obj0-y]\nxarray/tests/test_groupby.py::test_groupby_repr[obj0-z]\nxarray/tests/test_groupby.py::test_groupby_repr[obj0-month]\nxarray/tests/test_groupby.py::test_groupby_repr[obj1-x]\nxarray/tests/test_groupby.py::test_groupby_repr[obj1-y]\nxarray/tests/test_groupby.py::test_groupby_repr[obj1-z]\nxarray/tests/test_groupby.py::test_groupby_repr[obj1-month]\nxarray/tests/test_groupby.py::test_groupby_repr_datetime[obj0]\nxarray/tests/test_groupby.py::test_groupby_repr_datetime[obj1]\nPass to Pass (16)\nxarray/tests/test_groupby.py::test_consolidate_slices\nxarray/tests/test_groupby.py::test_groupby_da_datetime\nxarray/tests/test_groupby.py::test_groupby_duplicate_coordinate_labels\nxarray/tests/test_groupby.py::test_groupby_input_mutation\nxarray/tests/test_groupby.py::test_groupby_map_shrink_groups[obj0]\nAdditional Pass to Pass Tests omitted...\nDiscussion. In this problem, the model is asked to correct a trailing whitespace issue, and the\nissue shows a demonstration of the error that arises during usage, but does not explicitly point\nout the function to fix in the model. In the oracle retrieval setting, the model is given the\nxarray/core/groupby.py file to edit directly, but most locate the line to edit. In this situ-\nation, the model successfully reasons that in the interactive Python setting, the underlying function\nthat is being invoked is actually the repr function. After locating the function, it must figure\nout where in the return statement the whitespace is located. At this point, the fix is quite evident,\nas there is an explicit space that the model removes. In this case, the model successfully passes all\ntests for verifying the correctness of this fix explicitly (10 failtopass tests) while also maintaining\nthe existing behavior of the file (16 pass topass tests).\n43", "start_char_idx": 0, "end_char_idx": 1963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6efd25f5-0e2e-4e41-9133-419240ae7b64": {"__data__": {"id_": "6efd25f5-0e2e-4e41-9133-419240ae7b64", "embedding": null, "metadata": {"page_label": "44", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5765a62e-fcdf-4246-b636-4f91b6cc1b68", "node_type": "4", "metadata": {"page_label": "44", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "19bf9d62b72d28aad5d9ff1c1073e464efa16e295f8a9598f7c48297899042b3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 31: Example of a SWE-Llama 13b generation that correctly solves a SWE-bench task instance.\nIn this example, the SWE-Llama generation is different from the gold patch.\nSetting\n\u2022 Model: SWE-Llama 13B with Oracle retrieval\n\u2022 Task Instance: psf requests-1724\n\u2013Link to Issue: https://github.com/psf/requests/issues/1723\n\u2013Link to PR: https://github.com/psf/requests/pull/1724\nIssue\nUnicode method names cause UnicodeDecodeError for some requests in Python 2.7.2 The following\nexample works fine:\nfiles = {u\u2019file\u2019: open (u\u2019/usr/bin/diff\u2019, u\u2019rb\u2019)}\nresponse = requests.request(method=\u2019POST\u2019, url=u\u2019http://httpbin.org/post\u2019\n,\nfiles=files)\nBut the following example (using \u2018method=u\u2019POST\u2019\u2018 instead of \u2018method=\u2019POST\u2019\u2018) produces a Uni-\ncodeDecodeError:\nfiles = {u\u2019file\u2019: open (u\u2019/usr/bin/diff\u2019, u\u2019rb\u2019)}\nresponse = requests.request(method=u\u2019POST\u2019, url=u\u2019http://httpbin.org/post\n\u2019,\nfiles=files)\nTraceback (most recent call last):\n[Full stack trace hidden]\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/...\nmsg +=message_body\nUnicodeDecodeError: \u2019ascii\u2019 codec can\u2019t decode byte 0xcf inposition 140:\nordinal not inrange(128)\nMy guess is that \u2018u\u2019POST\u2019\u2018 is infecting the header with unicode when it should be a string. This is\nbecause \u2018sessions.py:313\u2018 is simply:\nreq.method = method.upper()\n[Additional lines hidden...]\nGold Patch\ndiff --git a/requests/sessions.py b/requests/sessions.py\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -12,7 +12,7 @@\nfrom collections import Mapping\nfrom datetime import datetime\n-from .compat import cookielib, OrderedDict, [3 more omitted]\n+from .compat import cookielib, OrderedDict, [3 omitted,\n+ builtin_str\nfrom .cookies import cookiejar_from_dict, extract_cookies_to_jar,\nRequestsCookieJar\nfrom .models import Request, PreparedRequest\nfrom .hooks import default_hooks, dispatch_hook\n@@ -309,6 +309,9 @@ def request(self, method, url,\n:param cert: (optional) ifString, path to ssl client cert\nfile (.pem). If Tuple, (\u2019cert\u2019, \u2019key\u2019) pair.\n\"\"\"\n+\n+ method = builtin_str(method)\n44", "start_char_idx": 0, "end_char_idx": 2075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b94095fb-a0b3-4e45-970f-cccead0f6757": {"__data__": {"id_": "b94095fb-a0b3-4e45-970f-cccead0f6757", "embedding": null, "metadata": {"page_label": "45", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa6ab322-8b62-4268-a9b0-5841763f99e1", "node_type": "4", "metadata": {"page_label": "45", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "890018c8d9f56bf55a3ca1d9639cc1c2e3b3ee969121ab7bb8f8989bd45b77fe", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nModel Generated Patch\ndiff --git a/requests/sessions.py b/requests/sessions.py\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -254,7 +254,7 @@ def prepare_request(self, request):\np = PreparedRequest()\np.prepare(\n- method=request.method.upper(),\n+ method= str(request.method).upper(),\nurl=request.url,\nfiles=request.files,\ndata=request.data,\nTests\nFail to Pass (6)\ntest_requests.py::RequestsTestCase::test_DIGEST_AUTH_RETURNS_COOKIE\ntest_requests.py::RequestsTestCase::test_DIGEST_HTTP_200_OK_GET\ntest_requests.py::RequestsTestCase::test_different_encodings_dont_break\ntest_requests.py::RequestsTestCase::test_generic_cookiejar_works\ntest_requests.py::RequestsTestCase::test_uppercase_scheme_redirect\nPass to Pass (75)\ntest_requests.py::RequestsTestCase::test_DIGEST_AUTH_SETS_SESSION_COOKIES\ntest_requests.py::RequestsTestCase::test_DIGEST_STREAM\ntest_requests.py::RequestsTestCase::test_HTTP_200_OK_GET_ALTERNATIVE\ntest_requests.py::RequestsTestCase::test_HTTP_200_OK_GET_WITH_PARAMS\ntest_requests.py::RequestsTestCase::test_HTTP_200_OK_HEAD\nAdditional Pass to Pass Tests omitted...\nDiscussion. Unlike the problem presented in Table 30, instead of an interactive python input, the\nmodel is given an error trace from a function invocation, where the steps to recreate the prob-\nlem from a user\u2019s standpoint is offered. In addition, the issue also provides a suggestion dur-\ning the last line on a potential fix for the problem. Once again in the oracle setting, the model\ntakes advantage of the natural language suggestion and localizes the change to a line that is se-\nmantically quite similar to the suggestion ( req.method = method.upper() compared to\nmethod= str(request.method).upper() . The model correctly addresses the issue, as re-\nflected by all the passing tests. However, in this situation, we can see that the model is actually\nquite \u201clucky\u201d in its success. First, while the gold patch leverages built in functions that are used\nfor uniformity in handling string formatting, the model\u2019s decision to use more primitive Python to\ntackle this issue would likely not be well-received by a human reviewer due to its inconsistency with\nthe rest of the codebase. Further inspection of the builtin_str function, not shown in this\nfigure, also reveals that it handles additional string formatting cases beyond what was portrayed in\nthe issue. While the model solves the task this time, it is questionable whether this solution is robust\nto future issues.\n45", "start_char_idx": 0, "end_char_idx": 2516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "418638b6-4c00-4c2a-822a-fb5cfdf37540": {"__data__": {"id_": "418638b6-4c00-4c2a-822a-fb5cfdf37540", "embedding": null, "metadata": {"page_label": "46", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25c5489c-61fd-436a-be49-d9374ed3b412", "node_type": "4", "metadata": {"page_label": "46", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "50182c7f53007dd05ef5b64f7abde0628640d4fbac684d03e13389a23e31e1d9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 32: This is another example where SWE-Llama13b solves the task successfully. This example\nis interesting because the model develops a somewhat novel solution compared to the reference that\nis arguably more efficient and cleaner.\nSetting\n\u2022 Model: SWE-Llama 13B with Oracle retrieval\n\u2022 Task Instance: django django-11727\n\u2013Link to Issue: https://code.djangoproject.com/ticket/29376\n\u2013Link to PR: https://github.com/django/django/pull/11727\nIssue\nAllow hiding the \u201dSave and Add Another\u201d button with a show_save_and_add_another context\nvariable\nTo provide better adjustability, to introduce new context var - show_save_and_add_another .\nE.g. if I want to hide button \u201dSave and add another\u201d, I can just modify extra_context - write\nFalse to the variable. For other buttons - \u201dSave\u201d and \u201dSave and continue editing\u201d, this already\nworks exactly in this manner.\nGold Patch\ndiff --git a/django/contrib/admin/templatetags/admin_modify.py b/django\n/...\n--- a/django/contrib/admin/templatetags/admin_modify.py\n+++ b/django/contrib/admin/templatetags/admin_modify.py\n@@ -54,12 +54,20 @@ def submit_row(context):\nis_popup = context[\u2019is_popup\u2019]\nsave_as = context[\u2019save_as\u2019]\nshow_save = context.get(\u2019show_save\u2019, True)\n+ show_save_and_add_another =\ncontext.get(\u2019show_save_and_add_another\u2019, True)\n(7 lines omitted...)\nor(has_add_permission and add)\norhas_editable_inline_admin_formsets\n+ can_save_and_add_another = (\n+ has_add_permission and\n+ not is_popup and\n+ ( not save_as oradd) and\n+ can_save and\n+ show_save_and_add_another\n+ )\ncan_save_and_continue = not is_popup and can_save and\nhas_view_permission\nand show_save_and_continue\ncan_change = has_change_permission or\nhas_editable_inline_admin_formsets\nctx = Context(context)\n@@ -70,10 +78,7 @@ def submit_row(context):\nchange and context.get(\u2019show_delete\u2019, True)\n),\n\u2019show_save_as_new\u2019: not is_popup and has_change_permission\nand change and save_as,\n- \u2019show_save_and_add_another\u2019: (\n- has_add_permission and not is_popup and\n- ( not save_as oradd) and can_save\n- ),\n+ \u2019show_save_and_add_another\u2019: can_save_and_add_another,\n\u2019show_save_and_continue\u2019: can_save_and_continue,\n\u2019show_save\u2019: show_save and can_save,\n46", "start_char_idx": 0, "end_char_idx": 2197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86ab90b9-fc34-4269-b36d-1aad4eeeb386": {"__data__": {"id_": "86ab90b9-fc34-4269-b36d-1aad4eeeb386", "embedding": null, "metadata": {"page_label": "47", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bba78ebc-a9bf-4312-a3d9-269b18e25020", "node_type": "4", "metadata": {"page_label": "47", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ad841b315d40bcf9cc400839e385907a7a4fe12289285289d5ff1efa81ebda88", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nModel Generated Patch\ndiff --git a/django/contrib/admin/templatetags/admin_modify.py b/django\n/...\n--- a/django/contrib/admin/templatetags/admin_modify.py\n+++ b/django/contrib/admin/templatetags/admin_modify.py\n@@ -72,7 +72,7 @@ def submit_row(context):\n\u2019show_save_as_new\u2019: not is_popup and has_change_permission\nand change and save_as,\n\u2019show_save_and_add_another\u2019: (\nhas_add_permission and not is_popup and\n- ( not save_as oradd) and can_save\n+ ( not save_as oradd) and can_save and context\n.get(\u2019show_save_and_add_another\u2019, True)\n),\n\u2019show_save_and_continue\u2019: can_save_and_continue,\n\u2019show_save\u2019: show_save and can_save,\nTests\nFail to Pass (2)\ntest_override_show_save_and_add_another\ntest_submit_row (admin_views.test_templatetags.Admin...)\nPass to Pass (3)\ntest_choice_links (admin_views.test_templatetags.DateHierarchyTests)\ntest_override_change_form_template_tags\ntest_override_change_list_template_tags\nDiscussion. For this task instance that comes from the django/django repository, the model is asked\nto introduce a context variable that would allow a user to hide the \u201cSave and Add Another\u201d button\nvia a context variable, similar to how it is done for two other existing buttons. The task is a bit\nmore difficult compared to the prior two settings because no explicit stack trace or programmatic\ndemonstration of the issue is offered. In this relatively under-specified setting that does not provide\nsuggestion with regards to localizing the function correctly, the model successfully reasons that it\nshould adjust the existing show_save_and_add_another key/value pair. When comparing the\ngold patch solution, it can be argued that the model generated patch produces a much more efficient\nsolution in terms of lines edited, as it makes the smallest edit necessary to incorporate context\nas a flag for setting the show_save_and_add_another hidden status. However, similar to the\ndiscussion in Table 31, stylistically, the gold patch edits are much more consistent with the codebase,\nand additional changes that are not explicitly discussed in the issue are also made to adhere to what\nhas been done in the codebase (i.e. the addition of a can_save_and_add_another field\n. This task is an example of a potentially exciting direction where via human guidance or better\nunderstanding of a codebase in general, models would then adjust their generations to not just make\nthefunctionally correct changes, but also the stylistically right ones, too.\n47", "start_char_idx": 0, "end_char_idx": 2497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64c1be50-1b0a-47e2-9bd4-b070e02d9637": {"__data__": {"id_": "64c1be50-1b0a-47e2-9bd4-b070e02d9637", "embedding": null, "metadata": {"page_label": "48", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c927528-dca5-4559-bf82-53e132977f9d", "node_type": "4", "metadata": {"page_label": "48", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4a98a3721dcd64aa870c707721bb0dea6bbb49a2509492021f4c77b2d423e0e4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 33: This is an example where SWE-Llama13b writes an incorrect solution, but maintains prior\nbehavior successfully. The difference in the model\u2019s generation and the gold patch here demonstrates\nhow models tend to under-generate the necessary fixes.\nSetting\n\u2022 Model: SWE-Llama 13B with Oracle retrieval\n\u2022 Task Instance: sphinx-doc sphinx-11445\n\u2013Link to Issue: https://github.com/sphinx-doc/sphinx/issues/11437\n\u2013Link to PR: https://github.com/sphinx-doc/sphinx/pull/11445\nIssue\nUsing rst_prolog removes top level headings containing a domain directive\n### Describe the bug\nIfrst_prolog is set, then any documents that contain a domain directive as the first head-\ning (eg :mod: ) do not render the heading correctly or include the heading in the toctree. In\nthe example below, if the heading of docs/mypackage.rst were mypackage2 instead of\n:mod:mypackage2 then the heading displays correctly. Similarly, if you do not set rst_prolog\nthen the heading will display correctly. This appears to have been broken for some time because I\ncan reproduce it in v4.0.0 of Sphinx\n### How to Reproduce\n$ sphinx-quickstart --no-sep --project mypackage --author me -v 0.1.0\n--release 0.1.0 --language en docs\n$echo -e \u2019Welcome\\n=======\\n\\n.. toctree::\\n\\n mypackage\\n\u2019\n> docs/index.rst\n$echo -e \u2019:mod:\u2018mypackage2\u2018\\n=================\\n\\nContent\\n\\n\nSubheading\\n----------\\n\u2019 > docs/mypackage.rst\n$echo -e \u2019rst_prolog =\"\"\"\\n.. |psf| replace:: Python Software\nFoundation\\n\"\"\"\\n\u2019 >> docs/conf.py\n$ sphinx-build -b html . _build\n$ grep \u2019mypackage2\u2019 docs/_build/index.html\nModel Generated Patch\ndiff --git a/sphinx/util/rst.py b/sphinx/util/rst.py\n--- a/sphinx/util/rst.py\n+++ b/sphinx/util/rst.py\n@@ -25,7 +25,7 @@\nlogger = logging.getLogger(__name__)\n-docinfo_re = re. compile (\u2019:\\\\w+:. *?\u2019)\n+docinfo_re = re. compile (\u2019:\\\\w+:. *?\\n\u2019)\nsymbols_re = re. compile (r\u2019([!-\\-/:-@\\[-\u2018{-\u02dc])\u2019)\nSECTIONING_CHARS = [\u2019=\u2019, \u2019-\u2019, \u2019\u02dc\u2019]\nTests\nFail to Pass (2)\ntests/test_util_rst.py::test_prepend_prolog_with_roles_with_newline\ntests/test_util_rst.py::test_prepend_prolog_with_roles_without_newline\nPass to Pass (8)\ntests/test_util_rst.py::test_prepend_prolog\ntests/test_util_rst.py::test_prepend_prolog_with_CR\ntests/test_util_rst.py::test_prepend_prolog_without_CR\nAdditional Pass to Pass Tests omitted...\n48", "start_char_idx": 0, "end_char_idx": 2323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d054ebab-121d-43d2-8bc9-6e4b28e49f10": {"__data__": {"id_": "d054ebab-121d-43d2-8bc9-6e4b28e49f10", "embedding": null, "metadata": {"page_label": "49", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26a88d0d-72df-4257-a412-51b92921486a", "node_type": "4", "metadata": {"page_label": "49", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1643d8d1194edbfbe4b1880eaca978b0cc24d25fcd74f46730692d3e8d2addb0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nGold Patch\ndiff --git a/sphinx/util/rst.py b/sphinx/util/rst.py\n--- a/sphinx/util/rst.py\n+++ b/sphinx/util/rst.py\n@@ -10,22 +10,17 @@\nfrom docutils.parsers.rst import roles\nfrom docutils.parsers.rst.languages import en as english\n+from docutils.parsers.rst.states import Body\nfrom docutils.statemachine import StringList\nfrom docutils.utils import Reporter\n-from jinja2 import Environment\n+from jinja2 import Environment, pass_environment\nfrom sphinx.locale import __\nfrom sphinx.util import docutils, logging\n-try:\n- from jinja2.utils import pass_environment\n-except ImportError:\n- from jinja2 import environmentfilter as pass_environment\n-\n-\nlogger = logging.getLogger(__name__)\n-docinfo_re = re. compile (\u2019:\\\\w+:. *?\u2019)\n+FIELD_NAME_RE = re. compile (Body.patterns[\u2019field_marker\u2019])\nsymbols_re = re. compile (r\u2019([!-\\-/:-@\\[-\u2018{-\u02dc])\u2019) # symbols without dot(0\nx2e)\nSECTIONING_CHARS = [\u2019=\u2019, \u2019-\u2019, \u2019\u02dc\u2019]\n@@ -80,7 +75,7 @@ def prepend_prolog(content: StringList, prolog: str) ->\nNone:\nifprolog:\npos = 0\nfor line incontent:\n- ifdocinfo_re.match(line):\n+ ifFIELD_NAME_RE.match(line):\npos += 1\nelse :\nbreak\n@@ -91,6 +86,7 @@ def prepend_prolog(content: StringList, prolog: str) ->\nNone:\npos += 1\n# insert prolog (after docinfo if exists)\n+ lineno = 0\nfor lineno, line in enumerate (prolog.splitlines()):\ncontent.insert(pos + lineno, line, \u2019<rst_prolog>\u2019, lineno)\nDiscussion. For this task instance from the sphinx-doc/sphinx repository, a model is asked\nto write logic to fix a case where the title is incorrectly being rendered. Simply understanding the\njargon being used and mapping such words to logic within the codebase is a significant challenge\nfaced by the model. The model is given a command line call that can help with this, but grounding\nthe terminology presented in the issues within the codebase is essential. From comparing the gold\npatch and model generated patch, it is clear that the model does not come close to solving the task.\nThe model does generally identify that fixing the regex pattern is the correct action, as this is what\nthe gold patch does, too. However, where the model and oracle retrieval setting collectively fall short\nis mainly due to the significant use of additional modules from both the codebase itself and third\nparty libraries. This example highlights the importance and potential for training language models\nand designing inference procedures that allow for the automated discovery of such information.\n49", "start_char_idx": 0, "end_char_idx": 2485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62fe18bd-0f14-46e7-8949-1baa4c90880f": {"__data__": {"id_": "62fe18bd-0f14-46e7-8949-1baa4c90880f", "embedding": null, "metadata": {"page_label": "50", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2985ad3-ffc4-4d25-9afd-db2268e34fd7", "node_type": "4", "metadata": {"page_label": "50", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bf2446a2d9e19e8f73618d71f5dc9e74572ea1048d1edfa7218968b3d0d8c625", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 34: In this final example, SWE-Llama 13b not only does not solve the task, but also corrupts\nexisting behavior in the model. This example demonstrates the need for models to understand the\ncodebase beyond the scope of the required edits.\nSetting\n\u2022 Model: SWE-Llama 13B with Oracle retrieval\n\u2022 Task Instance: scikit-learn scikit-learn-13241\n\u2013Link to Issue: https://github.com/scikit-learn/scikit-learn/issues/8798\n\u2013Link to PR: https://github.com/scikit-learn/scikit-learn/pull/13241\nIssue\nDifferences among the results of KernelPCA with rbf kernel\n#### Description\nWhen I run KernelPCA for dimension reduction for the same datasets, the results are different in\nsigns.\n#### Steps/Code to Reproduce\nJust to reduce the dimension to 7 with rbf kernel:\npca = KernelPCA(n_components=7, kernel=\u2019rbf\u2019, copy_X=False, n_jobs=-1)\npca.fit_transform(X)\n#### Expected Results\nThe same result.\n#### Actual Results\nThe results are the same except for their signs:\n[[-0.44457 -0.18155 -0.10873 0.13548 -0.14371 -0.05746 0.18124]]\n[[ 0.44457 0.18155 0.10873 -0.13548 -0.14371 -0.05746 -0.18124]]\n[[-0.44457 -0.18155 0.10873 0.13548 0.14371 0.057469 0.18124]]\nGold Patch\ndiff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decompos...\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -8,6 +8,7 @@\nfrom scipy.sparse.linalg import eigsh\nfrom ..utils import check_random_state\n+from ..utils.extmath import svd_flip\nfrom ..utils.validation import check_is_fitted, check_array\nfrom ..exceptions import NotFittedError\nfrom ..base import BaseEstimator, TransformerMixin,\n_UnstableOn32BitMixin\n@@ -210,6 +211,10 @@ def _fit_transform(self, K):\nmaxiter=self.max_iter,\nv0=v0)\n+ # flip eigenvectors\u2019 sign to enforce deterministic output\n+ self.alphas_, _ = svd_flip(self.alphas_,\n+ np.empty_like(self.alphas_).T)\n+\n# sort eigenvectors in descending order\nindices = self.lambdas_.argsort()[::-1]\nself.lambdas_ = self.lambdas_[indices]\n50", "start_char_idx": 0, "end_char_idx": 2002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1711f4c4-eb63-4321-802d-5538678ed38b": {"__data__": {"id_": "1711f4c4-eb63-4321-802d-5538678ed38b", "embedding": null, "metadata": {"page_label": "51", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76794624-b8cd-4fdd-a907-0620c2ad634f", "node_type": "4", "metadata": {"page_label": "51", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "511f7449fac00049e03ea0f936a7ba608e7d9ad0055c4d0a8bce330a23b9613b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nModel Generated Patch\ndiff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decompos...\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -220,7 +220,7 @@ def _fit_transform(self, K):\nself.alphas_ = self.alphas_[:, self.lambdas_ > 0]\nself.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n- return K\n+ return K / self.lambdas_\ndef _fit_inverse_transform(self, X_transformed, X):\nif hasattr (X, \"tocsr\"):\nTests\nFail to Pass (1)\nsklearn/tests/test_kernel_pca.py::test_kernel_pca_deterministic_output\nPass to Pass (54)\nsklearn/tests/test_kernel_pca.py::test_kernel_pca\nsklearn/tests/test_kernel_pca.py::test_kernel_pca_consistent_transform\nsklearn/tests/test_kernel_pca.py::test_kernel_pca_deterministic_output\nsklearn/tests/test_kernel_pca.py::test_kernel_pca_sparse\nsklearn/tests/test_kernel_pca.py::test_kernel_pca_linear_kernel\nsklearn/tests/test_kernel_pca.py::test_kernel_pca_n_components\nsklearn/tests/test_kernel_pca.py::test_remove_zero_eig\nsklearn/tests/test_kernel_pca.py::test_kernel_pca_precomputed\nsklearn/tests/test_kernel_pca.py::test_gridsearch_pipeline\nsklearn/tests/test_kernel_pca.py::test_gridsearch_pipeline_precomputed\nsklearn/tests/test_pca.py::test_pca_dtype_preservation[randomized]\nsklearn/tests/test_pca.py::test_pca_dtype_preservation[auto]\nsklearn/tests/test_pca.py::test_pca_deterministic_output\nAdditional Pass to Pass Tests omitted...\nDiscussion. In this example from the scikit-learn/scikit-learn library, a model is given\nan observation that the parities of the output from a call to the fit_transform function are\nflipped. The model is also given lines of Python code and its standard output that recreates the issue.\nThe gold patch imports and uses the svd_flip function to solve this issue within a different\nline of the _fit_transform function. What\u2019s different about the model\u2019s failure for this task\nbeyond the points discussed for the Table 33 example is that, in addition to understanding third party\ndependencies that its edits rely on, it is also important for a model to understand what other parts of\nthe codebase in turn depend on the function it is changing. This example presents a different facet\nas to why processing long contexts extend beyond the local edit scope is a difficult but worthwhile\nchallenge.\n51", "start_char_idx": 0, "end_char_idx": 2339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bb5da42-40e2-4735-90cc-44e4fbd399a6": {"__data__": {"id_": "6bb5da42-40e2-4735-90cc-44e4fbd399a6", "embedding": null, "metadata": {"page_label": "1", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1b31a7a-ebb2-477d-ad0a-d0a09799ea61", "node_type": "4", "metadata": {"page_label": "1", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3f3aa15431f507b15b947bea24972a4d4c78090f85c278195b617a7a58b9325e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nVALUES: A F RAMEWORK FOR SYSTEMATIC\nVALIDATION OF UNCERTAINTY ESTIMATION IN\nSEMANTIC SEGMENTATION\nKim-Celine Kahl1,2*, Carsten T. L \u00a8uth1,2*, Maximilian Zenk3,\nKlaus Maier-Hein2,3, Paul F. Jaeger1,2\n1German Cancer Research Center (DKFZ) Heidelberg, Interactive Machine Learning Group, Germany\n2Helmholtz Imaging, German Cancer Research Center (DKFZ), Heidelberg, Germany\n3German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany\n{k.kahl, carsten.lueth }@dkfz-heidelberg.de\nABSTRACT\nUncertainty estimation is an essential and heavily-studied component for the re-\nliable application of semantic segmentation methods. While various studies ex-\nist claiming methodological advances on the one hand, and successful applica-\ntion on the other hand, the field is currently hampered by a gap between theory\nand practice leaving fundamental questions unanswered: Can data-related and\nmodel-related uncertainty really be separated in practice? Which components\nof an uncertainty method are essential for real-world performance? Which un-\ncertainty method works well for which application? In this work, we link this\nresearch gap to a lack of systematic and comprehensive evaluation of uncer-\ntainty methods. Specifically, we identify three key pitfalls in current literature\nand present an evaluation framework that bridges the research gap by providing\n1) a controlled environment for studying data ambiguities as well as distribution\nshifts, 2) systematic ablations of relevant method components, and 3) test-beds\nfor the five predominant uncertainty applications: OoD-detection, active learn-\ning, failure detection, calibration, and ambiguity modeling. Empirical results\non simulated as well as real-world data demonstrate how the proposed frame-\nwork is able to answer the predominant questions in the field revealing for in-\nstance that 1) separation of uncertainty types works on simulated data but does\nnot necessarily translate to real-world data, 2) aggregation of scores is a crucial\nbut currently neglected component of uncertainty methods, 3) While ensembles\nare performing most robustly across the different downstream tasks and settings,\ntest-time augmentation often constitutes a light-weight alternative. Code is at:\nhttps://github.com/IML-DKFZ/values\n1 I NTRODUCTION\nIn order to reliably deploy image segmentation systems in real-world applications, there is a critical\nneed to estimate and quantify the uncertainty associated with their predictions. Despite numerous\nstudies on uncertainty methods for segmentation in recent years, their effective utilization is cur-\nrently hindered by a significant gap between the theoretical development and their application in\nrelevant downstream tasks. One aspect of this disparity is the fact that uncertainty methods are often\nstated to model a specific type of uncertainty, i.e., either the data-related, aleatoric uncertainty (AU)\nor the model-related, epistemic uncertainty (EU). However, explicit evaluation of the claimed behav-\nior is not the focal point of these studies. As a prominent example, two highly-cited studies built on\nthe claim that test-time data augmentations (TTA) improve a model\u2019s ability to capture AU, without\ntheoretical or empirical evidence for this hypothesis (Wang et al. (2019); Ayhan & Berens (2018)).\n*These authors contributed equally to this work\n1", "start_char_idx": 0, "end_char_idx": 3418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c13c30a4-925f-403b-b71a-602e02f50803": {"__data__": {"id_": "c13c30a4-925f-403b-b71a-602e02f50803", "embedding": null, "metadata": {"page_label": "2", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fab0a071-c1ac-4cc9-b04d-072e0e5abe4f", "node_type": "4", "metadata": {"page_label": "2", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "91cf1e29ed97a857926b107af955b2abb320e5cd309f387c8c3524c697e4bfd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46bc2a51-03d5-487d-b4cd-4c6ef1dd0bbc", "node_type": "1", "metadata": {}, "hash": "9de487949392d2f65a69bd4fb7e68baa1c8c8d822002764a50009120b283532f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n0.8\nDownstream Tasks (R3)\nPrediction Model (C1)Uncertainty Measure (C2)Aggregation \nStrategy (C3)\nInput ImageReference Seg.Aggregated Uncertainty \nScoreExplicit Uncertainty References (R1)\nSeg. Backbone (C0)PAUL:Uncertainty Method (R2)\nAmbiguity ModelingCalibrationActive LearningFailure DetectionOoD-Detection??Ambiguities \n(multiple raters, AU)Distribution Shifts (EU)Predicted\nUncertainty\nHeatmapPredicted Seg.\n(optionally multiple samples)\n0.8\nDownstream Tasks (R3)\nPrediction Model (C1)Uncertainty Measure (C2)Aggregation Strategy (C3)\nInput ImageReference Seg.Aggregated Uncertainty \nScoreExplicit Uncertainty References (R1)\nSeg. Backbone (C0)Uncertainty Method (R2)\nAmbiguity ModelingCalibrationActive LearningFailure DetectionOoD-Detection??Ambiguities \n(multiple raters, AU)Distribution Shifts (EU)Predicted\nUncertainty\nHeatmapPredicted Seg.\n(optionally multiple samples)\n0.8\nDownstream Tasks (R3)\nPrediction Model (C1)Uncertainty Measure (C2)Aggregation Strategy (C3)\nInput ImageReference Seg.Aggregated Uncertainty \nScoreExplicit Uncertainty References (R1)\nSeg. Backbone (C0)Uncertainty Method (R2)\nAmbiguity ModelingCalibrationActive LearningFailure DetectionOoD-Detection??Ambiguities \n(multiple raters, AU)Distribution Shifts (EU)Predicted\nUncertainty\nHeatmapPredicted Segmentation\n(optionally multiple samples)\n(e.g. U-Net architecture)(e.g.  softmax, \ntest-time dropout)\n(e.g. predictive entropy, \nmutual information)(e.g., mean, patch-based, theshold-based)\nFigure 1: Framework for systematic validation of uncertainty methods in segmentation . With\nour framework, we aim to overcome pitfalls in the current validation of uncertainty methods for\nsemantic segmentation by satisfying the three requirements (R1-R3) for a systematic validation: We\nexplicitly control for aleatoric and epistemic uncertainty in the data and references (R1). We define\nand validate four individual components C0-C3 of uncertainty methods (R2): First, one or multiple\nsegmentation outputs are generated by the segmentation backbone (C0) and the prediction model\n(C1). Next, an uncertainty measure is applied (C2) producing an uncertainty heatmap, which can be\naggregated using an aggregation strategy (C3). Finally, the real-world capabilities of methods need\nto be validated on various downstream tasks (R3).\nHowever, a follow-up study proposed the opposite, i.e., TTA to model EU, without validating this\nstatement either (Hu et al. (2019)). Thus, there is a clear need to 1) validate the stated behavior of\nmethods regarding feasibility of separation and 2) provide evidence for the necessity of separation\nby checking whether applications actually benefit. Another aspect of the current research gap is the\nunderexplored study of all practically relevant components of an uncertainty method. For instance,\nan adequate aggregation of uncertainty estimates from pixel level to image level is highly relevant\nto performance in many downstream tasks but often overlooked, leading to tasks like failure detec-\ntion being purely validated on pixel-level instead of image-level (Zhang et al. (2022); Mehta et al.\n(2020)) or simplistic aggregation strategies being employed (Gonzalez et al. (2021); Czolbe et al.\n(2021)). Finally, the current gap between theory and application is nurtured by the fact that pro-\nposed methods are rarely validated on a broad set of relevant downstream tasks, making it difficult\nand expensive for practitioners to identify the best uncertainty method for their problem.\nThis work bridges the gap between theoretical advancements in uncertainty estimation and its real-\nworld application in segmentation systems by presenting a framework for standardized and system-\natic validation (see Figure 1). The framework features 1) a controlled environment for studying data\nambiguities as well as distribution shifts, 2) systematic ablations of relevant method components,\nand 3) test-beds for the five predominant uncertainty applications: Out-of-Distribution-detection\n(OoD-D ), active learning ( AL), failure detection ( FD), calibration ( CALIB ), and ambiguity mod-\neling ( AM).", "start_char_idx": 0, "end_char_idx": 4146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46bc2a51-03d5-487d-b4cd-4c6ef1dd0bbc": {"__data__": {"id_": "46bc2a51-03d5-487d-b4cd-4c6ef1dd0bbc", "embedding": null, "metadata": {"page_label": "2", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fab0a071-c1ac-4cc9-b04d-072e0e5abe4f", "node_type": "4", "metadata": {"page_label": "2", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "91cf1e29ed97a857926b107af955b2abb320e5cd309f387c8c3524c697e4bfd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c13c30a4-925f-403b-b71a-602e02f50803", "node_type": "1", "metadata": {"page_label": "2", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b603f6cab24af5b3714238d75e51ce3e8f6c5eac609b73a481cdae1c2eb95f36", "class_name": "RelatedNodeInfo"}}, "text": "(2021); Czolbe et al.\n(2021)). Finally, the current gap between theory and application is nurtured by the fact that pro-\nposed methods are rarely validated on a broad set of relevant downstream tasks, making it difficult\nand expensive for practitioners to identify the best uncertainty method for their problem.\nThis work bridges the gap between theoretical advancements in uncertainty estimation and its real-\nworld application in segmentation systems by presenting a framework for standardized and system-\natic validation (see Figure 1). The framework features 1) a controlled environment for studying data\nambiguities as well as distribution shifts, 2) systematic ablations of relevant method components,\nand 3) test-beds for the five predominant uncertainty applications: Out-of-Distribution-detection\n(OoD-D ), active learning ( AL), failure detection ( FD), calibration ( CALIB ), and ambiguity mod-\neling ( AM). We demonstrate the effectiveness of our proposed framework based on an exemplary\nempirical study that sheds light on the unanswered questions and current inconsistencies in the field\nand allows us to compile a list of hands-on recommendations.\n2 U NCERTAINTY ESTIMATION IN SEMANTIC SEGMENTATION\nTo effectively discuss pitfalls and challenges in the field of uncertainty estimation for segmentation,\nwe begin by establishing a common language by defining components of an uncertainty method.\nC0 - Segmentation Backbone. The segmentation backbone is the fundamental building block for\nuncertainty estimation, depicting the method\u2019s architecture, e.g., a U-Net architecture (Ronneberger\net al. (2015)). As well-established architectures exist, C0 is often fixed in uncertainty studies.\nC1 - Prediction Model. The prediction model (PM) operates based on the segmentation backbone\nand produces the final predicted class scores for segmentation. Depending on the PM, a single set\n(\u201ddeterministic\u201d) or multiple sets (\u201dsampling-based\u201d) of scores per input image can be generated.\nThe PM may include dedicated training and inference paradigms, like ensemble training or test-time\ndropout (TTD). Examples of PMs include deterministic models like softmax, Bayesian approaches,\nand probabilistic models like stochastic segmentation networks (SSNs) (Monteiro et al. (2020)).\n2", "start_char_idx": 3228, "end_char_idx": 5510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34d0a42d-c31b-4fc5-9ef9-986fa7a67967": {"__data__": {"id_": "34d0a42d-c31b-4fc5-9ef9-986fa7a67967", "embedding": null, "metadata": {"page_label": "3", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35c748ce-0f7f-487a-94cc-c304e78cb68e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e2d4bd9ae97b78ea4074ffb3784e97553447ab378a5a3e23cb2a29dd11d7241c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dbaee3f-d706-44d9-9d9f-cc962b61915f", "node_type": "1", "metadata": {}, "hash": "083e87f34493f34b6f304cb031276387ff5f746efbec8d8b929c1deb76e223ee", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nC2 - Uncertainty Measure. The uncertainty measure involves computing an uncertainty score per\npixel based on predicted class scores, which can be represented as an uncertainty heatmap. Examples\nof uncertainty measures include expected entropy and mutual information.\nC3 - Aggregation Strategy. The aggregation strategy is a unique component of uncertainty esti-\nmation for semantic segmentation that is not needed in tasks such as image classification. Here, the\npixel-level uncertainty heatmap is aggregated to a single scalar value at the desired level of granu-\nlarity depending on the downstream task (e.g., patch-level or image-level). A simple example of an\naggregation strategy is to compute the sum or mean over the pixel-level uncertainties.\n2.1 M EASURING UNCERTAINTIES\nIn literature, typically, two types of uncertainty are distinguished: aleatoric uncertainty (AU) and\nepistemic uncertainty (EU) (Kendall & Gal (2017)). AU relates to inherent ambiguities in the im-\nage, such as those caused by spatial occlusions. On the other hand, EU relates to the model itself\nand arises from a lack of knowledge, which can be mitigated by incorporating additional relevant\nknowledge, such as images, into the training data. The combination of AU and EU is referred to\nas predictive uncertainty (PU). The most prominent approach to capture these uncertainties was\nintroduced by (Kendall & Gal (2017)), viewing it from the perspective of a Bayesian classifier,\nwhich receives an input xand outputs the probabilities for classes Y:p(Y|x) =E\u03c9\u223c\u2126[p(Y|x, \u03c9)],\nwhere the model parameters \u2126follow p(\u03c9|D)given the training data D. This Bayesian framework\n(Mukhoti et al. (2021)) assumes the predictive entropy (PE) to represent the PU which is the sum\nof the mutual information (MI) representing the EU and the expected entropy (EE) representing the\nAU:\nH(Y|x)|{z}\nPU=MI(Y,\u2126|x)|{z}\nEU+E\u03c9\u223c\u2126[H(Y|\u03c9, x)]| {z }\nAU (for i.i.d. x)(1)\nHere Hstands for Shannon\u2019s entropy (Shannon (1948)). Besides this, alternative functions like\ndensity estimators, such as the Mahalanobis distance to the i.i.d. (independent and identically dis-\ntributed) training distribution, can be used to approximate uncertainty (Gonzalez et al. (2021)).\n3 P ITFALLS AND SOLUTIONS FOR A SYSTEMATIC VALIDATION OF\nUNCERTAINTY METHODS\nOur goal is to bridge the gap between theory and practical application of uncertainty methods in\nsegmentation. To this end, we formulate three requirements (R1-R3) for evaluation protocols aiming\nto deepen the understanding of how uncertainty methods behave in application and thus allow a safe\nand reliable deployment of segmentation systems. For each requirement, we also make the described\ngap explicit by stating the pitfalls of current validation practices in the field.\nR1: Evaluate uncertainty methods claiming to separate AU and EU by means of explicit ref-\nerences and metrics. Theoretical studies often make claims about a specific uncertainty method\ncapturing either EU or AU. Validating this claimed behavior requires 1) for AU a test set with refer-\nences from multiple raters that reflect the ambiguities in the data and a metric that explicitly assesses\nthe capturing of these ambiguities such as the normalized cross-correlation (NCC) (Hu et al. (2019)),\nand 2) for EU a test set featuring samples with explicit distribution shift (i.e. induced EU) and a met-\nric that explicitly assesses whether an EU-measure can separate these cases, such as the Area Under\nthe Receiver Operating Characteristic Curve (AUROC). As described in the pitfall below, the cur-\nrent state of research lacks a systematic validation of uncertainty modeling, leaving fundamental\nquestions unanswered: Can AU and EU be separated in practice? To what extent can different appli-\ncations benefit from a potential separation? We design a specific study to answer the open questions\nregarding the separation of EU and AU in simulated and real-world settings.\nPitfalls of current practice: Several AU studies feature test sets with only a single rater (Wang et al.\n(2019); Whitbread & Jenkinson (2022); Kendall & Gal (2017)) whilst in EU-studies no distribution\nshifts are used for evaluation (Mukhoti & Gal (2018); Mobiny et al.", "start_char_idx": 0, "end_char_idx": 4249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dbaee3f-d706-44d9-9d9f-cc962b61915f": {"__data__": {"id_": "3dbaee3f-d706-44d9-9d9f-cc962b61915f", "embedding": null, "metadata": {"page_label": "3", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35c748ce-0f7f-487a-94cc-c304e78cb68e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e2d4bd9ae97b78ea4074ffb3784e97553447ab378a5a3e23cb2a29dd11d7241c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34d0a42d-c31b-4fc5-9ef9-986fa7a67967", "node_type": "1", "metadata": {"page_label": "3", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ee3238fa0e26cdc8a7b5a193c8f80d75b3bcb622e3ce0e0a7279d5fc0e9e6f44", "class_name": "RelatedNodeInfo"}}, "text": "induced EU) and a met-\nric that explicitly assesses whether an EU-measure can separate these cases, such as the Area Under\nthe Receiver Operating Characteristic Curve (AUROC). As described in the pitfall below, the cur-\nrent state of research lacks a systematic validation of uncertainty modeling, leaving fundamental\nquestions unanswered: Can AU and EU be separated in practice? To what extent can different appli-\ncations benefit from a potential separation? We design a specific study to answer the open questions\nregarding the separation of EU and AU in simulated and real-world settings.\nPitfalls of current practice: Several AU studies feature test sets with only a single rater (Wang et al.\n(2019); Whitbread & Jenkinson (2022); Kendall & Gal (2017)) whilst in EU-studies no distribution\nshifts are used for evaluation (Mukhoti & Gal (2018); Mobiny et al. (2021); Whitbread & Jenkinson\n(2022)). Further, current studies commonly do not report the required metrics but either segmen-\ntation performance (Zhang et al. (2022)), CALIB (Wang et al. (2019); Postels et al. (2019)), FD\n(Zhang et al. (2022); Mukhoti et al. (2021); Mobiny et al. (2021)), or are based on visual inspection\n(Mukhoti et al. (2021); Wang et al. (2019); Whitbread & Jenkinson (2022); Mobiny et al. (2021)). In\n3", "start_char_idx": 3387, "end_char_idx": 4676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70334453-1c6a-4f22-b184-a111fc664994": {"__data__": {"id_": "70334453-1c6a-4f22-b184-a111fc664994", "embedding": null, "metadata": {"page_label": "4", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b56ffd43-7fb5-4423-aac7-b1ed99005058", "node_type": "4", "metadata": {"page_label": "4", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ddad45a84a4d993353f7b557977d2fef2dcb46db11be4c288bd0bf13d3fcc8df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ee89903-3efd-476f-8d84-187d3403d68f", "node_type": "1", "metadata": {}, "hash": "faf5918c5723ad0d193408df05c9cdcd4f68e79eacbbca9139655b0e7a005679", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nthe prominent study by (Kendall & Gal (2017)), an explicit validation of EU on a distribution shift\nis performed; however, only comparing raw EU-scores over data sets instead of assessing the sep-\naration power with AUROC and using predictive entropy as an EU-measure, thereby contradicting\nEquation 1. As a consequence of these pitfalls, confusion and contradictions arise, such as the fact\nthat different studies claim TTA to either specifically capture EU (Hu et al. (2019)) or AU (Ayhan &\nBerens (2018); Wang et al. (2019)) without providing quantitative evidence for their claim.\nR2: Evaluate uncertainty methods with regard to all components of an uncertainty method.\nIn order to assess the capabilities of an uncertainty method, it is crucial to trace back improvements\nto its individual components C0-C3 (see Sec. 2) and, in the case of a proposed variation of one\ncomponent, study how this interacts with the others. Only such rigorous analysis allows identifying\nscientific progress and fosters a deeper understanding of uncertainty estimation in segmentation.\nPitfalls of current practice: A common pattern in current literature is to focus on a potential\nimprovement in one component without attending to the others, such as in the form of a single sim-\nplified setting. For instance, Gonzalez et al. (2021) study a specific uncertainty measure (C2) that\ndoes not require aggregation while applying a simple \u201dmean aggregation\u201d to all baselines, which can\nbe heavily affected by the number of foreground pixels. This leaves it unclear whether the reported\nimprovement comes from the proposed C2 or, in fact, from the subpar aggregation strategy of base-\nlines (C3). Similarly, Czolbe et al. (2021) use a \u201dsum aggregation\u201d for AL, which might result in\nquerying larger objects. Another common pattern is that studies report only pixel-level downstream\ntasks and neglect image-level tasks that would require aggregation. Examples are studies reporting\nonly CALIB (Wang et al. (2019); Gustafsson et al. (2020); Hu et al. (2019); Postels et al. (2021)) or\npixel-level FD (Zhang et al. (2022); Mehta et al. (2020)). However, the task of FD aims to identify\nand defer faulty subjects or inputs for e.g. human analysis, which questions a plausible application\nfor deferring individual pixels. In contrast, Jungo et al. (2020) follow R2 by studying and ablating\nindividual components of uncertainty methods. Despite this exception, we argue a general reflection\nby the community on validation practice in this context is required to overcome this pitfall at scale.\nR3: Evaluate uncertainty methods on all relevant downstream tasks. Next to theoretical studies\nand claims of separating uncertainty types, it is important to state that uncertainty estimation is no\nself-purpose. Instead, it needs to come with a clearly stated purpose, which has to be validated on\nreal-life applications. In order for practitioners to decide whether an existing uncertainty method is\nadequate for their specific task, it is crucial that proposed methods are generally validated on a broad\nspectrum of downstream tasks such as OoD-D, AL, FD, CALIB, and AM.\nPitfalls of current practice: In current literature, most studies validate uncertainty methods on\na single downstream task such as OoD-D (Lambert et al. (2022); Holder & Shafique (2021)), FD\n(Zhang et al. (2022); Mukhoti et al. (2021); Mobiny et al. (2021)), AL (Mackowiak et al. (2018);\nColling et al. (2020); Xie et al. (2020)), CALIB (Wang et al. (2019); Gustafsson et al. (2020); Hu\net al. (2019); Postels et al. (2021); Mehrtash et al. (2020)), or AM (Kohl et al. (2018); Monteiro\net al. (2020)). Also, some task formulations are limited in scope, such as FD purely on i.i.d. test\ndata not considering failure sources from potential distribution shifts, a pitfall that has been studied\nrecently for classification tasks Jaeger et al. (2023).", "start_char_idx": 0, "end_char_idx": 3932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ee89903-3efd-476f-8d84-187d3403d68f": {"__data__": {"id_": "2ee89903-3efd-476f-8d84-187d3403d68f", "embedding": null, "metadata": {"page_label": "4", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b56ffd43-7fb5-4423-aac7-b1ed99005058", "node_type": "4", "metadata": {"page_label": "4", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ddad45a84a4d993353f7b557977d2fef2dcb46db11be4c288bd0bf13d3fcc8df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70334453-1c6a-4f22-b184-a111fc664994", "node_type": "1", "metadata": {"page_label": "4", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1854b472ed677b2f764bffdb019d94b9fddcec6d83f9ae07b27a902b325c1abb", "class_name": "RelatedNodeInfo"}}, "text": "(2022); Mukhoti et al. (2021); Mobiny et al. (2021)), AL (Mackowiak et al. (2018);\nColling et al. (2020); Xie et al. (2020)), CALIB (Wang et al. (2019); Gustafsson et al. (2020); Hu\net al. (2019); Postels et al. (2021); Mehrtash et al. (2020)), or AM (Kohl et al. (2018); Monteiro\net al. (2020)). Also, some task formulations are limited in scope, such as FD purely on i.i.d. test\ndata not considering failure sources from potential distribution shifts, a pitfall that has been studied\nrecently for classification tasks Jaeger et al. (2023). However, the more general pitfall in this context\nis that the underlying concepts of a proposed uncertainty method are typically not bound to a single\napplication, but studying their general usability on a broad set of downstream tasks is relevant to\nthe community. Thus, the current practice of sparse task validation poses a major challenge to\npractitioners who seek to choose the best method for their particular problem.\n4 E MPIRICAL STUDY\n4.1 S TUDY DESIGN\nUncertainty separation study. In this comprehensive separation study, our primary focus lies in\ninvestigating the ability of uncertainty measures to effectively separate AU and EU, a claim com-\nmonly made in theoretical works (Kendall & Gal (2017)). With the understanding that uncertainty\nmeasures are often associated with specific uncertainty types (see Equation 1), we test for different\nprediction models (C1) to see whether the corresponding uncertainty measures (C2) successfully\ncapture their theoretically claimed uncertainty types. We express the task of separating AU and EU\nby formulating four questions:\n4", "start_char_idx": 3391, "end_char_idx": 5013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2153e1b-3877-49c8-b864-d81259977824": {"__data__": {"id_": "a2153e1b-3877-49c8-b864-d81259977824", "embedding": null, "metadata": {"page_label": "5", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b470c5e4-e2fa-408b-acc0-7610283b9f46", "node_type": "4", "metadata": {"page_label": "5", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4273e071325678cf1b675ac1c31f6eade62285d020246916dc44bef4eda5c05e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01e27d7e-880b-415b-b5e9-db02c081ea98", "node_type": "1", "metadata": {}, "hash": "52466ac86080ef3f6105669b4f745d8c2bbcb8017502a84b95eb8a9ae79e23ff", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nQ1Do AU-measures capture AU? Q2Do EU-measures capture AU?\nQ3Do EU-measures capture EU? Q4Do AU-measures capture EU?\nQ1 + Q2: For assessing the capability of uncertainty methods in capturing AU, we employ the\nnormalized cross-correlation (NCC) as a quantitative measure between the predicted uncertainty\nmap and the reference uncertainty map based on the disagreement of multiple raters (for details see\nAppendix A). Additionally, we perform qualitative inspections of the uncertainty maps. Based on\nthe theory, successful separation of uncertainties would imply AU-measures to exhibit high NCC\nand high qualitative fidelity (Q1 = \u201dyes\u201d) and vice versa for EU-measures (Q2 = \u201dno\u201d).\nQ3 + Q4: To evaluate the capability of uncertainty methods in capturing EU, we measure the perfor-\nmance of separating cases of an induced distribution shift (associated with EU) from the i.i.d. cases\nby means of the AUROC ranking metric on image level. Since the expected spatial manifestation\nof epistemic uncertainty in the image is not known, we do not perform qualitative inspection for\nthis task. Based on the theory, successful separation of uncertainties would imply EU-measures to\nexhibit high AUROC (Q3 = \u201dyes\u201d) and AU-measures to exhibit low AUROC (Q4 = \u201dno\u201d).\nWe conduct this separation study on different datasets, including a toy dataset, the LIDC-IDRI\n(LIDC) dataset with two metadata shifts, and the GTA5/Cityscapes (GTA5/CS) dataset. Sec. 4.2\nprovides detailed information on the specific data set settings.\nEvaluation on downstream tasks. Through this study, we aim to comprehensively understand the\nperformance and capabilities of various uncertainty methods in practical settings. The study is per-\nformed on the LIDC dataset, again with two metadata shifts, and the GTA5/CS dataset. Concretely,\nwe evaluate the following downstream tasks (see Appendix A for task definitions and metric details):\n1) OoD-D , where the goal is to identify images that exhibit distribution shifts from the training data,\nand measured by means of the AUROC. 2) FD , where the focus is on identifying images on which\nthe overall segmentation is unsatisfactory based on the Dice score as measured by the AURC (Jaeger\net al. (2023)). To further provide a ranking of uncertainty methods independent of the segmentation\nperformance, we adapt the E-AURC (Geifman et al. (2019)) for the segmentation task. 3) AL , where\nevaluation is performed at the image level, aiming to select the most informative and representative\nimages from an unlabeled pool to improve the model\u2019s performance. To measure the performance\nof uncertainty methods in querying informative samples, we calculate the relative improvement of\nthe Dice score between a first training (starting budget) and second training (including the queried\nsamples) and subtract the performance increase of random querying. 4) CALIB , where we measure\nthe Average Calibration Error (ACE) (Neumann et al. (2018); Jungo et al. (2020)) in combination\nwith Platt scaling following Jaeger et al. (2023) to assess the model\u2019s reliability of uncertainty es-\ntimates at pixel level. 5) AM , where we assess uncertainty methods\u2019 ability to model and quantify\nlabel ambiguity at the pixel level. This includes measuring the NCC (Hu et al. (2019)) as well as\nthe Generalized Energy Difference (GED) (Kohl et al. (2018)) between segmentation outputs and\nreference segmentations to evaluate sample diversity.\n4.2 U TILIZED DATASETS\nThe following section provides an overview of all datasets with the most important information,\nespecially how we meet R1 by inducing both AU and EU. For more details on datasets, we refer to\nAppendix B. Both real-world datasets are split into i.i.d. and OoD sets. The initial models are only\ntrained on the i.i.d. sets and evaluated on i.i.d. and OoD sets for the respective downstream tasks.\nToy dataset We generate a 3D toy dataset comprising spheres and cubes as target structures for\nsegmentation. To simulate AU, we add Gaussian blur to the border of the spheres and simulate\nthree raters to provide different segmentation styles at the border.", "start_char_idx": 0, "end_char_idx": 4143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01e27d7e-880b-415b-b5e9-db02c081ea98": {"__data__": {"id_": "01e27d7e-880b-415b-b5e9-db02c081ea98", "embedding": null, "metadata": {"page_label": "5", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b470c5e4-e2fa-408b-acc0-7610283b9f46", "node_type": "4", "metadata": {"page_label": "5", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4273e071325678cf1b675ac1c31f6eade62285d020246916dc44bef4eda5c05e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2153e1b-3877-49c8-b864-d81259977824", "node_type": "1", "metadata": {"page_label": "5", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "422b02cc477285c1c27f68cd0de035dfaa4647fa9c2db205a406de7a06a09ccd", "class_name": "RelatedNodeInfo"}}, "text": "(2019)) as well as\nthe Generalized Energy Difference (GED) (Kohl et al. (2018)) between segmentation outputs and\nreference segmentations to evaluate sample diversity.\n4.2 U TILIZED DATASETS\nThe following section provides an overview of all datasets with the most important information,\nespecially how we meet R1 by inducing both AU and EU. For more details on datasets, we refer to\nAppendix B. Both real-world datasets are split into i.i.d. and OoD sets. The initial models are only\ntrained on the i.i.d. sets and evaluated on i.i.d. and OoD sets for the respective downstream tasks.\nToy dataset We generate a 3D toy dataset comprising spheres and cubes as target structures for\nsegmentation. To simulate AU, we add Gaussian blur to the border of the spheres and simulate\nthree raters to provide different segmentation styles at the border. EU is simulated by introducing\ndistribution shifts in the geometric objects, such as changes in color, shape, and position, along with\nbackground noise to prevent shortcut learning (Geirhos et al. (2020)). As the toy dataset is designed\nto answer the questions posed in the separation study (see Sec. 4.1), we created the following training\nand testing scenarios:\n1. Q1+Q2: Training models on data with induced AU; testing on i.i.d. data also containing AU\n2. Q3 + Q4: Training models on data without ambiguity; testing on i.i.d. data and shifted data\n3. Q4: Training models on data with AU; testing on (a) i.i.d. data and shifted data without AU and\n(b) i.i.d. data with AU (blur) and shifted data without AU (blur) (see Sec. B.1.1 for details)\n5", "start_char_idx": 3303, "end_char_idx": 4891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2425432-1cfa-4edd-9995-d7fa1ceb8742": {"__data__": {"id_": "e2425432-1cfa-4edd-9995-d7fa1ceb8742", "embedding": null, "metadata": {"page_label": "6", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "765a50f0-f35a-4c48-84b1-4b78e745372a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "25a0c0c9c156047c935b936c9ab1a09a904f0f719e0b6383a09559b6a66e2ce0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ce24e59-defd-4e20-8208-478c10b06c05", "node_type": "1", "metadata": {}, "hash": "b3f6c1d2ee16bb8f555403494be8f84a53bafa0e475a8df78c9c3a21cd3b542f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nLIDC-IDRI (LIDC) To study uncertainty methods in a real-world setting, we use the LIDC-IDRI\ndataset (Armato III et al. (2011)), with the task to segment long nodules in 64x64x64 crops from 3D\nCT volumes, similarly to Kohl et al. (2018). We only include nodules that have been annotated by\nfour different raters serving as an AU reference. We further induce EU by designing two distribution\nshifts based on two metadata features, which are textured (i.i.d) vs. non-textured (OoD) (texture\nshift/ LIDC TEX) and benign (i.i.d) vs. malignant (OoD) nodules (malignancy shift/ LIDC MAL).\nGTA5/Cityscapes (GTA5/CS). We use the GTA5 (Richter et al. (2016)) and Cityscapes (Cordts\net al. (2016)) datasets jointly, with both datasets being comprised of the same classes. To induce\nAU, we employ a similar strategy as Kohl et al. (2018): We randomly flip some classes (\u201csidewalk\u201d,\n\u201cperson\u201d, \u201ccar\u201d, \u201cvegetation\u201d and \u201croad\u201d) with a probability of1\n3from <class>to<class 2 >. We\nsimulate EU with the shift from GTA5 (i.i.d.) to Cityscapes (OoD).\n4.3 S TUDIED UNCERTAINTY METHODS\nC0: Segmentation Backbone. For the toy and the LIDC datasets, we use the 3D U-Net architecture\nas the segmentation backbone (Ronneberger et al. (2015)), as this is a simple and well-established\narchitecture in medical image segmentation. For the GTA5/CS dataset, we use the HRNet (Wang\net al. (2020)), which has been shown to achieve state-of-the-art results on the Cityscapes dataset. We\nkeep C0 fixed but it can be varied for future experiments. For implementation details, see Sec. C.1.\nC1: Prediction Model We study five different prediction models: A plain deterministic softmax\nmodel, a model using MC-Dropout at test-time (TTD), and an ensemble of 5 models both viewed\nfrom a Bayesian perspective, a softmax model using data augmentations at test-time (TTA) and a\nStochastic Segmentation Network (SSN). The SSN is a specific type of model that directly learns\nto predict AU by producing multiple plausible segmentations for a given input by using a random\nvariable that represents the variability of segmentations. For implementation details, see Sec. C.2.\nC2: Uncertainty Measure We validate various uncertainty measures stated to capture either the\nPU, EU, or AU. For the deterministic model, we study the maximum softmax response (MSR) as a\nmeasure of PU by calculating 1\u2212MSR. For the Bayesian models, we study the predictive entropy\nas a measure for PU, MI (Y, \u03c9|x)as a measure for EU, and the expected entropy as a measure for\nAU (see Sec. 2.1). For the TTA model, introducing the random augmentation variable T, we assume\nthat the predictive entropy is a measure for PU, MI (Y, T|x)as a measure for EU, and the expected\nentropy as a measure for AU (see Appendix E). For the SSN, which uses a variable to model the\nvariability of the label maps, we assume that the predictive entropy measures PU, the expected\nentropy measures EU, and MI (Y, Z|x)with the variable Zis a measure for AU (see Appendix D).\nC3: Aggregation Strategy We validate three different aggregation strategies, taking the pixel-level\nuncertainties as input and returning one score per image: 1) Image level aggregation , as used in\nCzolbe et al. (2021); Gonzalez et al. (2021); Jungo et al. (2020), where uncertainty scores for all\npixels are summed per image. We find that for segmentation maps with one single object in the fore-\nground, the uncertainty score directly correlates with the size of the target object (see Appendix F).\nThus, we only use this aggregation strategy on the GTA5/CS dataset.", "start_char_idx": 0, "end_char_idx": 3593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ce24e59-defd-4e20-8208-478c10b06c05": {"__data__": {"id_": "1ce24e59-defd-4e20-8208-478c10b06c05", "embedding": null, "metadata": {"page_label": "6", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "765a50f0-f35a-4c48-84b1-4b78e745372a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "25a0c0c9c156047c935b936c9ab1a09a904f0f719e0b6383a09559b6a66e2ce0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2425432-1cfa-4edd-9995-d7fa1ceb8742", "node_type": "1", "metadata": {"page_label": "6", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ab88fe1d5eb2ab030c807714f878da57782d38467f09dc766a03f81ce51ee8e1", "class_name": "RelatedNodeInfo"}}, "text": "For the SSN, which uses a variable to model the\nvariability of the label maps, we assume that the predictive entropy measures PU, the expected\nentropy measures EU, and MI (Y, Z|x)with the variable Zis a measure for AU (see Appendix D).\nC3: Aggregation Strategy We validate three different aggregation strategies, taking the pixel-level\nuncertainties as input and returning one score per image: 1) Image level aggregation , as used in\nCzolbe et al. (2021); Gonzalez et al. (2021); Jungo et al. (2020), where uncertainty scores for all\npixels are summed per image. We find that for segmentation maps with one single object in the fore-\nground, the uncertainty score directly correlates with the size of the target object (see Appendix F).\nThus, we only use this aggregation strategy on the GTA5/CS dataset. 2) Patch level aggregation ,\nwhich uses a sliding window of size 10D(Dis the dimensionality of the image) to sum the uncer-\ntainties inside the window, and then selects the patch with the highest uncertainty as the image-level\nuncertainty score. 3) Threshold level aggregation considers only uncertainty scores above a thresh-\nold\u03bb(see Appendix F for how this threshold is determined) as uncertain, and calculates the mean of\nthose scores. Notably, as selecting the threshold depends on the foreground object size, this strategy\nis not applicable to the GTA5/CS dataset.\n4.4 R ESULTS OF THE SEPARATION STUDY\nThe general findings of the uncertainty separation study are summarized in Figure 2a, while under-\nlying results are shown in Figure 2b for the toy dataset and Figure 3 (see gray-shaded \u201dQ\u201d indicators\non respective panels) for LIDC and GTA5/CS. More detailed descriptions and results as well as a\nqualitative analysis of uncertainty maps are provided in Appendix G.\nModeling aleatoric uncertainty (Q1 + Q2) While AU-measures clearly captured AU much better\nthan EU-measures for the toy dataset, this behavior is inconsistent on the real-world datasets. On\nthe LIDC datasets with AU stemming from rater ambiguity, which mostly occurs at the border of\n6", "start_char_idx": 2789, "end_char_idx": 4853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd0140ba-e784-464f-a3e8-325ae055f634": {"__data__": {"id_": "fd0140ba-e784-464f-a3e8-325ae055f634", "embedding": null, "metadata": {"page_label": "7", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "169a72c6-f20e-45eb-805b-7529751a4caa", "node_type": "4", "metadata": {"page_label": "7", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "834966d59855ea0b14db62e851512487754de2813527d24f9f239659384912ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3aae84e-2c2b-4d55-b71c-e0869acd272b", "node_type": "1", "metadata": {}, "hash": "43d594d6b0cbf4a88e68850302888a44ddf718bf28debb5373f8fee09b71931b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nQuestionQual./Quant.Toy DatasetLIDCGTA5/CSQ1: Do AU-measures capture AU?QuantitativeyespartiallypartiallyQualitativeyesoften (i.i.d.)partiallyQ2: Do EU-measures capture AU?Quantitativnooften yesnoQualitativenooften yesnoQ3: Do EU-measures capture EU?QuantitativyesyesyesQ4: Do AU-measures capture EU?Quantitativdepends on AU in data partiallymostly not Unc. MeasureAU-MeasureEU-MeasurePU-Measure(a)(b)\nIncreasing AU in i.i.d. data\nQ1+2Q3+4\nFigure 2: a)General findings of the separation study. Green/red denotes agreement/disagreement\nwith theoretical claims, orange represents partial agreement. b)Underlying quantitative results on\nthe toy data set. Results show C2 and are aggregated over C1 and C3. Results for LIDC and\nGTA5/CS are displayed in Figure 3 (see gray-shaded \u201dQ\u201d indicators). Details are in Appendix G\n.\nstructures, the benefit of separating AU-measures from EU-measures is not evident when examining\nthe NCC scores. For GTA5/CS, where induced label ambiguities span entire spatial structures, the\nAU-measures generally capture AU better than EU-measures. However, the absolute NCC scores\nfrom the AU-measures vary greatly across prediction models. We attribute this to SSNs capturing\nthe widespread label ambiguities, while other models overemphasize the border regions.\nModeling epistemic uncertainty (Q3 + Q4) While EU-measures capture EU better than AU- and\nPU-measures on all datasets, the benefit of this separation varied greatly depending on the AU in the\nrespective training and test data. More specifically, when more AU is present in the i.i.d. training\nand test data, the benefit of EU- over AU- and PU-measures increases as the ambiguity modeling\nin the i.i.d. setting is separated from the EU-measure. This connection can also be observed on\nGTA5/CS, where the captured AU induced by spatially widespread ambiguities translates to a higher\nEU-measure performance compared to LIDC.\nGeneral Insights. Although both, AU- and EU-measures, mostly do behave as expected, the extent\nof achieved separation depends on the data set properties such as the presence of ambiguities in\ni.i.d and/or OoD cases. As theoretically motivated in Appendix E, we observe that TTA is in fact\nmost suited for modeling EU, resolving a controversial debate in current literature. We base this on\nthe behavior of our derived EU-measure for TTA being very similar to ensembles and TTD, often\neven outperforming TTD. The comparable performance to ensembles renders TTA often a cheap\nalternative for estimating EU. For SSNs, our proposed EU- and AU-measures perform as intended\non the toy dataset and GTA5/CS. Whereas on LIDC, the ambiguity, which is mostly present in the\nborder regions, seems to be captured by EU-measures.\n4.5 R ESULTS OF THE EVALUATION ON DOWNSTREAM TASKS\nIn this section, we address five fundamental questions essential for practitioners when selecting an\nuncertainty method. The first three concern the components of the uncertainty method: What is the\nbest (1) uncertainty type, (2) prediction model, and (3) aggregation? The latter two focus on the\nrobustness of trends: How robust are settings (4) across datasets and (5) distribution shifts?\nDetailed results can be found in Appendix H. To facilitate parsing the details and to systematically\nanswer the questions from above across downstream tasks, we isolate the performance of each un-\ncertainty type, prediction model, and aggregation method while averaging over the remaining com-\nponents. Finally, we visualize the performance of the analyzed component with respect to the mean\nperformance on the downstream task for each dataset. The results are shown in Figure 3. We exclude\nuncertainty types that are not suitable for specific downstream tasks during the averaging process\nfor prediction models and aggregation methods, indicated with crosses in the color of the specific\nuncertainty measure. Furthermore, we report the standard deviation across the averaged dimensions.\nIt\u2019s essential to recognize that these standard deviations are expected to be relatively high and this\nindicates a meaningful influence of individual components (C1-C3) on the final performance .\nOoD-D.", "start_char_idx": 0, "end_char_idx": 4205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3aae84e-2c2b-4d55-b71c-e0869acd272b": {"__data__": {"id_": "a3aae84e-2c2b-4d55-b71c-e0869acd272b", "embedding": null, "metadata": {"page_label": "7", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "169a72c6-f20e-45eb-805b-7529751a4caa", "node_type": "4", "metadata": {"page_label": "7", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "834966d59855ea0b14db62e851512487754de2813527d24f9f239659384912ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd0140ba-e784-464f-a3e8-325ae055f634", "node_type": "1", "metadata": {"page_label": "7", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "16081a9aafd19815b4a2842ce9d5459f3d0f7550966f4a6f1b4fb0324cafb93f", "class_name": "RelatedNodeInfo"}}, "text": "Detailed results can be found in Appendix H. To facilitate parsing the details and to systematically\nanswer the questions from above across downstream tasks, we isolate the performance of each un-\ncertainty type, prediction model, and aggregation method while averaging over the remaining com-\nponents. Finally, we visualize the performance of the analyzed component with respect to the mean\nperformance on the downstream task for each dataset. The results are shown in Figure 3. We exclude\nuncertainty types that are not suitable for specific downstream tasks during the averaging process\nfor prediction models and aggregation methods, indicated with crosses in the color of the specific\nuncertainty measure. Furthermore, we report the standard deviation across the averaged dimensions.\nIt\u2019s essential to recognize that these standard deviations are expected to be relatively high and this\nindicates a meaningful influence of individual components (C1-C3) on the final performance .\nOoD-D. As expected from theory, EU-measures consistently achieve an AUROC above average,\nmostly outperforming PU-measures as well. Among the prediction models, ensembles are the only\nmodel that consistently performs above average across datasets, followed by TTA, which quite ro-\n7", "start_char_idx": 3215, "end_char_idx": 4480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41244adf-b664-42fb-8461-a091d86489e5": {"__data__": {"id_": "41244adf-b664-42fb-8461-a091d86489e5", "embedding": null, "metadata": {"page_label": "8", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72fa8667-c192-4edf-bd23-0de79807fa4e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "356b0493cbfebc8a3bed99f97da8a9eca61bc2c03f732f842ec02c6f48269589", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nCALIB i.i.d.CALIB OoDOoD DetAL\nFD i.i.d.\nCALIB i.i.d.\nAM i.i.d.Uncertainty MeasurePrediction ModelAggregation\nFD OoD\nCALIB OoD\nAM OoDOoD DetAL\nFD i.i.d.\nAM i.i.d.FD OoD\nAM OoDOoD DetAL\nFD i.i.d.FD OoD\nCarsten TestUncertainty MeasureAU MeasureEU MeasurePU MeasureAggregationPatch LevelThresholdImage LevelPrediction ModelSoftmaxDropoutEnsembleTTASSN\n= This uncertainty type is excluded in aggregation\nQ1+2Q1+2Q3+4Q3+4\nQ1+2=Quantitative results for Q1 & Q2 of the \nseparation study (Sec. 4.4, Fig. 2(a))Q3+4=Quantitative results for Q3 & Q4 of the \nseparation study (Sec. 4.4, Fig. 2(a))\nFigure 3: Aggregated results showing deviations from the mean performance to assess general\ntrends for each component (C1-C3) across settings for each dataset. Due to the averaging across\ndifferent components (not seeds), high standard deviations are expected. Uncertainty measures\nnot suited for a specific downstream task are excluded in the average, indicated with crosses in the\ncolor of the specific uncertainty measure. Detailed results are shown in Appendix H. Metrics: OoD\n(AUROC), AL (% improvement over random), FD (AURC), CALIB (ACE), AM (NCC).\nbustly performs on or above average. For the GTA5/CS dataset, SSNs outperform other models, as\nthey are able to capture the spatially widespread label ambiguities explicitly and thus isolate the EU.\nChoosing the optimal aggregation method for this task seems heavily dependent on dataset proper-\nties but at the same time very crucial for the performance. This can be seen in the case of LIDC\nMAL, where differences between aggregation methods exceed standard deviations, emphasizing the\nsubstantial influence, regardless of modifications to other components of the uncertainty method.\nActive Learning. Falling in line with the theoretical perspective, EU-measures generally out-\nperform PU-measures, except on the LIDC MAL task. For prediction models, no model consis-\ntently performs above average across all datasets. TTD exhibits strong performance on the LIDC\ndatasets, while SSNs excel on the GTA5/CS dataset. Ensembles are consistently above or close\nto average performance. The choice of aggregation method exhibits dataset-dependent variability.\nOn the LIDC datasets, patch-level aggregation outperforms threshold aggregation, whereas for the\nGTA5/CS dataset, image-level aggregation yields the best results. Overall, surpassing the \u201drandom\u201d\nAL baseline appears challenging with marginal improvements on LIDC MAL and GTA5/CS, this\nobservation is in line with recent studies (Mittal et al. (2019; 2023); L \u00a8uth et al. (2023)).\n8", "start_char_idx": 0, "end_char_idx": 2623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6c1492f-7a77-4eb9-80b9-3a7dbf7ced63": {"__data__": {"id_": "a6c1492f-7a77-4eb9-80b9-3a7dbf7ced63", "embedding": null, "metadata": {"page_label": "9", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2efd9480-141a-4edb-a4f8-e407d6af6ab9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1f6aaf3d2c2ef7be9ae674f154c6b32e8ff412004048f452985a52da624c48c0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFailure Detection. Following the theory, PU-measures consistently perform above average on i.i.d\ndata, as failures can arise due to both AU and EU. However, EU proves superior on the LIDC\ndatasets, while AU excels on the GTA5/CS dataset. This trend may be attributed to the larger areas\nof induced AU in the GTA5/CS dataset, which pose a challenging failure source and increase the\nimportance of AU modeling. Concerning prediction models, ensembles are almost consistently\noutperforming others, closely followed by TTA. The choice of aggregation method yields mixed\nresults on LIDC TEX, similar to other downstream tasks. Specifically, on i.i.d. data, threshold\naggregation is more effective, while patch-level aggregation is better on OoD data. On the other\ndatasets, the trends from the other downstream tasks are confirmed.\nCalibration. As pixels can be misclassified due to both AU and EU, it is in line with theoretical ex-\npectations that PU-measures are consistently the best choice across datasets. For prediction models,\nTTD performs at least nearly on average or above average compared to other models. Addition-\nally, SSNs exhibit strong performance, particularly on the LIDC datasets. This performance trend\nremains largely consistent between i.i.d. and OoD data.\nAmbiguity Modeling. In line with theoretical expectations, AU consistently emerges as the most\neffective uncertainty type across all datasets. Expectably, this trend is most visible for the large\nspatial ambiguities induced in the GTA5/CS dataset. When assessing prediction models, SSNs\noutperform other models, both in i.i.d. and OoD scenarios across all datasets. Notably, SSNs are the\nonly compared model that is specifically designed for AM. One somewhat unexpected observation\nis the strong NCC performance of the Deterministic model on the GTA5/CS dataset.\nConsistency across datasets and shifts. Assessing the consistency of best-performing uncertainty\nmethods across datasets, we observe that besides expected results, like EU excelling in OoD detec-\ntion or SSNs excelling in AM, trends often differ between datasets. This can be especially seen for\nchoosing an appropriate aggregation. Given the low cost of evaluating this post-hoc component, we\nrecommend benchmarking different aggregations. Between the i.i.d. and OoD data, the observed\npatterns seem more stable, while, as expected, the performance on the OoD data is generally lower.\n5 C ONCLUSION AND TAKE-AWAYS\nGeneral insights & Recommendations. Our empirical study generates the following insights based\non the systematic implementation of requirements R1-R3:\nR1)When testing the feasibility of separating uncertainty in AU and EU (Sec. 4.4) we found that it\nworks in toy settings but does not necessarily translate to real-world data. In examining the actual\nbenefits of separation (Sec. 4.5) we discovered that these are heavily dependent on the downstream\ntask and the dataset properties. Therefore, neither the feasibility nor the benefit of separation should\nbe taken for granted when presenting a new uncertainty method; instead, convincing evidence should\nbe required for both. Our study shows that such rigorous testing resolves prior contradictions in the\nliterature, e.g. by disproving the assumptions made in Ayhan & Berens (2018) and Wang et al.\n(2019), revealing that TTA is in fact most suited for modeling EU rather than AU.\nR2)The explicit validation of individual components C0-C3 shows that in practice it is essential\nto select optimal components individually based on the dataset properties. One prominent insight\nis the importance of the aggregation strategy (C3) which can be subject to unwanted correlations\nand is often oversimplified or neglected in previous work. We show that the choice of C3 is further\ninterdependent on the choices of C1 and C2 and only a joint consideration of all components allows\nfinding the best method configuration for a given task.\nR3)Our study enables practitioners to make informed choices for all relevant components on their\nspecific task. It also identifies red flags such as the fact that SSNs, while excelling in AM, fall short in\nFD. Further, the study identifies ensembles as the generally most robust method across downstream\ntasks, while TTA often represents an adequate and light-weight alternative.\nImpact. Practitioners can use ValUES to make informed design decisions for uncertainty methods\non their problems and methodological developments can be rigorously validated using ValUES,\nfostering a systematic knowledge base in the field.\n9", "start_char_idx": 0, "end_char_idx": 4596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40b36c25-2ce5-4b3b-ac15-41680f568264": {"__data__": {"id_": "40b36c25-2ce5-4b3b-ac15-41680f568264", "embedding": null, "metadata": {"page_label": "10", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19d95850-8684-4093-ac2d-c7d4db626f66", "node_type": "4", "metadata": {"page_label": "10", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "84462b03007dfa544a9936d1b2a3f93ce9e27194272289141a64519b893e83c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca856611-5a4a-42e4-af87-5c4341730378", "node_type": "1", "metadata": {}, "hash": "6494ecb0cbe6e7f7cd8a10158d6020756a9b95d4e72ef275f76abf09ca7d0225", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nThis work was funded by Helmholtz Imaging (HI), a platform of the Helmholtz Incubator on In-\nformation and Data Science. This work is supported by the Helmholtz Association Initiative and\nNetworking Fund under the Helmholtz AI platform grant (ALEGRA (ZT-I-PF-5-121)).\nREFERENCES\nSamuel G. Armato III, Geoffrey McLennan, Luc Bidaut, Michael F. McNitt-Gray, Charles R. Meyer,\nAnthony P. Reeves, Binsheng Zhao, Denise R. Aberle, Claudia I. Henschke, Eric A. Hoffman,\nElla A. Kazerooni, Heber MacMahon, Edwin J. R. van Beek, David Yankelevitz, Alberto M.\nBiancardi, Peyton H. Bland, Matthew S. Brown, Roger M. Engelmann, Gary E. Laderach, Daniel\nMax, Richard C. Pais, David P.-Y . Qing, Rachael Y . Roberts, Amanda R. Smith, Adam Starkey,\nPoonam Batra, Philip Caligiuri, Ali Farooqi, Gregory W. Gladish, C. Matilda Jude, Reginald F.\nMunden, Iva Petkovska, Leslie E. Quint, Lawrence H. Schwartz, Baskaran Sundaram, Lori E.\nDodd, Charles Fenimore, David Gur, Nicholas Petrick, John Freymann, Justin Kirby, Brian\nHughes, Alessi Vande Casteele, Sangeeta Gupte, Maha Sallam, Michael D. Heath, Michael H.\nKuhn, Ekta Dharaiya, Richard Burns, David S. Fryd, Marcos Salganicoff, Vikram Anand, Uri\nShreter, Stephen Vastagh, Barbara Y . Croft, and Laurence P. Clarke. The Lung Image Database\nConsortium (LIDC) and Image Database Resource Initiative (IDRI): A Completed Reference\nDatabase of Lung Nodules on CT Scans. Medical Physics , 38(2):915\u2013931, 2011. ISSN 2473-\n4209. doi: 10.1118/1.3528204.\nMurat Seckin Ayhan and Philipp Berens. Test-time data augmentation for estimation of het-\neroscedastic aleatoric uncertainty in deep neural networks. In Medical Imaging with Deep Learn-\ning, 2018.\nChristian F Baumgartner, Kerem C Tezcan, Krishna Chaitanya, Andreas M H \u00a8otker, Urs J\nMuehlematter, Khoschy Schawkat, Anton S Becker, Olivio Donati, and Ender Konukoglu. Phiseg:\nCapturing uncertainty in medical image segmentation. In Medical Image Computing and Com-\nputer Assisted Intervention\u2013MICCAI 2019: 22nd International Conference, Shenzhen, China,\nOctober 13\u201317, 2019, Proceedings, Part II 22 , pp. 119\u2013127. Springer, 2019.\nPascal Colling, Lutz Roese-Koerner, Hanno Gottschalk, and Matthias Rottmann. Metabox+: A\nnew region based active learning method for semantic segmentation using priority maps. arXiv\npreprint arXiv:2010.01884 , 2020.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2016.\nSteffen Czolbe, Kasra Arnavaz, Oswin Krause, and Aasa Feragen. Is segmentation uncertainty\nuseful? In Information Processing in Medical Imaging: 27th International Conference, IPMI\n2021, Virtual Event, June 28\u2013June 30, 2021, Proceedings 27 , pp. 715\u2013726. Springer, 2021.\nYonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep\nneural classifiers. In International Conference on Learning Representations , 2019.", "start_char_idx": 0, "end_char_idx": 3128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca856611-5a4a-42e4-af87-5c4341730378": {"__data__": {"id_": "ca856611-5a4a-42e4-af87-5c4341730378", "embedding": null, "metadata": {"page_label": "10", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19d95850-8684-4093-ac2d-c7d4db626f66", "node_type": "4", "metadata": {"page_label": "10", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "84462b03007dfa544a9936d1b2a3f93ce9e27194272289141a64519b893e83c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40b36c25-2ce5-4b3b-ac15-41680f568264", "node_type": "1", "metadata": {"page_label": "10", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3a8b755b22c81f8ff724803fc5191be82c81691142cace3e591f604a5d98e393", "class_name": "RelatedNodeInfo"}}, "text": "The cityscapes dataset for semantic\nurban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2016.\nSteffen Czolbe, Kasra Arnavaz, Oswin Krause, and Aasa Feragen. Is segmentation uncertainty\nuseful? In Information Processing in Medical Imaging: 27th International Conference, IPMI\n2021, Virtual Event, June 28\u2013June 30, 2021, Proceedings 27 , pp. 715\u2013726. Springer, 2021.\nYonatan Geifman, Guy Uziel, and Ran El-Yaniv. Bias-reduced uncertainty estimation for deep\nneural classifiers. In International Conference on Learning Representations , 2019.\nRobert Geirhos, J \u00a8orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature\nMachine Intelligence , 2(11):665\u2013673, 2020.\nCamila Gonzalez, Karol Gotkowski, Andreas Bucher, Ricarda Fischbach, Isabel Kaltenborn, and\nAnirban Mukhopadhyay. Detecting When Pre-trained nnU-Net Models Fail Silently for Covid-\n19 Lung Lesion Segmentation. In Medical Image Computing and Computer Assisted Intervention\nMICCAI 2021 , pp. 304\u2013314, Cham, 2021. Springer International Publishing. ISBN 978-3-030-\n87234-2. doi: 10.1007/978-3-030-87234-2 29.\nFredrik K Gustafsson, Martin Danelljan, and Thomas B Schon. Evaluating scalable bayesian deep\nlearning methods for robust computer vision. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops , pp. 318\u2013319, 2020.\n10", "start_char_idx": 2530, "end_char_idx": 4016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b18fabbb-b720-4165-9734-d6c183767ac5": {"__data__": {"id_": "b18fabbb-b720-4165-9734-d6c183767ac5", "embedding": null, "metadata": {"page_label": "11", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d69cf9d1-e3c3-4a64-8666-a9929c425f97", "node_type": "4", "metadata": {"page_label": "11", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b6a781fdd93c2ff132fbcffe7fb718b6d7a286a386125e0a42f9f8378cb035ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6835e32b-f647-4142-a00f-c0a4cb605e1a", "node_type": "1", "metadata": {}, "hash": "2d64bd4aebe700d312f2ce40a01b5753c9127466ce30bbce2c2b407418ca776c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMatthew C. Hancock and Jerry F. Magnan. Lung nodule malignancy classification using only\nradiologist-quantified image features as inputs to statistical learning algorithms: Probing the Lung\nImage Database Consortium dataset with two statistical learning methods. Journal of Medical\nImaging , 3(4):044504, 2016. ISSN 2329-4302. doi: 10.1117/1.JMI.3.4.044504.\nChristopher J. Holder and Muhammad Shafique. Efficient Uncertainty Estimation in Seman-\ntic Segmentation via Distillation. In 2021 IEEE/CVF International Conference on Computer\nVision Workshops (ICCVW) , pp. 3080\u20133087. IEEE, 2021. ISBN 978-1-66540-191-3. doi:\n10.1109/ICCVW54120.2021.00343.\nShi Hu, Daniel Worrall, Stefan Knegt, Bas Veeling, Henkjan Huisman, and Max Welling. Super-\nvised uncertainty quantification for segmentation with multiple annotations. In Medical Image\nComputing and Computer Assisted Intervention\u2013MICCAI 2019: 22nd International Conference,\nShenzhen, China, October 13\u201317, 2019, Proceedings, Part II 22 , pp. 137\u2013145. Springer, 2019.\nPaul F Jaeger, Carsten Tim L \u00a8uth, Lukas Klein, and Till J Bungert. A call to reflect on evaluation\npractices for failure detection in image classification. In The Eleventh International Conference\non Learning Representations , 2023.\nJohan Ludwig William Valdemar Jensen. Sur les fonctions convexes et les in \u00b4egalit \u00b4es entre les valeurs\nmoyennes. Acta Mathematica , 30(1):175 \u2013 193, 1906. doi: 10.1007/BF02418571.\nAlain Jungo, Fabian Balsiger, and Mauricio Reyes. Analyzing the quality and challenges of uncer-\ntainty estimations for brain tumor segmentation. Frontiers in neuroscience , 14:282, 2020.\nAlex Kendall and Yarin Gal. What Uncertainties Do We Need in Bayesian Deep Learning for\nComputer Vision? Advances in neural information processing systems , 30, 2017.\nSimon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R. Ledsam,\nKlaus Maier-Hein, S. M. Ali Eslami, Danilo Jimenez Rezende, and Olaf Ronneberger. A Prob-\nabilistic U-Net for Segmentation of Ambiguous Images. In Advances in Neural Information\nProcessing Systems , volume 31. Curran Associates, Inc., 2018.\nBenjamin Lambert, Florence Forbes, Senan Doyle, Alan Tucholka, and Michel Dojat. Improving\nUncertainty-based Out-of-Distribution Detection for Medical Image Segmentation. arXiv preprint\narXiv:2211.05421 , 2022.\nCarsten Tim L \u00a8uth, Till J. Bungert, Lukas Klein, and Paul F Jaeger. Navigating the pitfalls of active\nlearning evaluation: A systematic framework for meaningful performance assessment. In Thirty-\nSeventh Conference on Neural Information Processing Systems , volume 36, 2023.\nRadek Mackowiak, Philip Lenz, Omair Ghori, Ferran Diego, Oliver Lange, and Carsten Rother.\nCEREALS - Cost-Effective REgion-based Active Learning for Semantic Segmentation. British\nMachine Vision Conference 2018 (BMVC) , 2018.\nAlireza Mehrtash, William M Wells, Clare M Tempany, Purang Abolmaesumi, and Tina Kapur.\nConfidence calibration and predictive uncertainty estimation for deep medical image segmenta-\ntion. IEEE transactions on medical imaging , 39(12):3868\u20133878, 2020.\nRaghav Mehta, Angelos Filos, Yarin Gal, and Tal Arbel. Uncertainty Evaluation Metric for Brain\nTumour Segmentation. arXiv preprint arXiv:2005.14262 , 2020.\nSudhanshu Mittal, Maxim Tatarchenko, \u00a8Ozg\u00a8un C \u00b8 ic \u00b8ek, and Thomas Brox. Parting with Illusions\nabout Deep Active Learning.", "start_char_idx": 0, "end_char_idx": 3407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6835e32b-f647-4142-a00f-c0a4cb605e1a": {"__data__": {"id_": "6835e32b-f647-4142-a00f-c0a4cb605e1a", "embedding": null, "metadata": {"page_label": "11", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d69cf9d1-e3c3-4a64-8666-a9929c425f97", "node_type": "4", "metadata": {"page_label": "11", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b6a781fdd93c2ff132fbcffe7fb718b6d7a286a386125e0a42f9f8378cb035ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b18fabbb-b720-4165-9734-d6c183767ac5", "node_type": "1", "metadata": {"page_label": "11", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1a0d92841b26f76f9124d0f25fec27fdc19a6d2120aa4a73a716578028ed8011", "class_name": "RelatedNodeInfo"}}, "text": "British\nMachine Vision Conference 2018 (BMVC) , 2018.\nAlireza Mehrtash, William M Wells, Clare M Tempany, Purang Abolmaesumi, and Tina Kapur.\nConfidence calibration and predictive uncertainty estimation for deep medical image segmenta-\ntion. IEEE transactions on medical imaging , 39(12):3868\u20133878, 2020.\nRaghav Mehta, Angelos Filos, Yarin Gal, and Tal Arbel. Uncertainty Evaluation Metric for Brain\nTumour Segmentation. arXiv preprint arXiv:2005.14262 , 2020.\nSudhanshu Mittal, Maxim Tatarchenko, \u00a8Ozg\u00a8un C \u00b8 ic \u00b8ek, and Thomas Brox. Parting with Illusions\nabout Deep Active Learning. arXiv preprint arXiv:1912.05361 , 2019.\nSudhanshu Mittal, J. Niemeijer, J. Sch \u00a8afer, and Thomas Brox. Best practices in active learning for\nsemantic segmentation. In German Conference on Pattern Recognition (GCPR) , 2023.\nAryan Mobiny, Pengyu Yuan, Supratik K Moulik, Naveen Garg, Carol C Wu, and Hien Van Nguyen.\nDropconnect is effective in modeling uncertainty of bayesian deep networks. Scientific reports ,\n11(1):1\u201314, 2021.\n11", "start_char_idx": 2822, "end_char_idx": 3840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdbaa196-f7b1-4c32-81bd-6677e2055287": {"__data__": {"id_": "bdbaa196-f7b1-4c32-81bd-6677e2055287", "embedding": null, "metadata": {"page_label": "12", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55251ba9-1e8e-447d-81fc-9ddf6f66fd9c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "919f524429140351f4f0b4c8e7423cb8a87725db881da46837ee280714a0eb32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37f09b6e-8f7a-45bb-92f8-cd651dc6955d", "node_type": "1", "metadata": {}, "hash": "e8d342cff28720cca947a2fc132dacefee434a65ba67eb657a4994a4744bc93c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMiguel Monteiro, Loic Le Folgoc, Daniel Coelho de Castro, Nick Pawlowski, Bernardo Marques,\nKonstantinos Kamnitsas, Mark van der Wilk, and Ben Glocker. Stochastic Segmentation Net-\nworks: Modelling Spatially Correlated Aleatoric Uncertainty. In Advances in Neural Information\nProcessing Systems , volume 33, pp. 12756\u201312767. Curran Associates, Inc., 2020.\nJishnu Mukhoti and Yarin Gal. Evaluating bayesian deep learning methods for semantic segmenta-\ntion. arXiv preprint arXiv:1811.12709 , 2018.\nJishnu Mukhoti, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic uncer-\ntainty for semantic segmentation. arXiv preprint arXiv:2111.00079 , 2021.\nWill Nash, Liang Zheng, and Nick Birbilis. Deep learning corrosion detection with confidence. npj\nMaterials degradation , 6(1):26, 2022. ISSN 2397-2106. doi: 10.1038/s41529-022-00232-6.\nLukas Neumann, Andrew Zisserman, and Andrea Vedaldi. Relaxed Softmax: Efficient Confidence\nAuto-Calibration for Safe Pedestrian Detection. 2018.\nJanis Postels, Francesco Ferroni, Huseyin Coskun, Nassir Navab, and Federico Tombari. Sampling-\nfree epistemic uncertainty estimation using approximated variance propagation. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pp. 2931\u20132940, 2019.\nJanis Postels, Mattia Segu, Tao Sun, Luc Van Gool, Fisher Yu, and Federico Tombari. On the\npracticality of deterministic epistemic uncertainty. arXiv preprint arXiv:2107.00649 , 2021.\nStephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground\ntruth from computer games. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.),\nEuropean Conference on Computer Vision (ECCV) , volume 9906 of LNCS , pp. 102\u2013118. Springer\nInternational Publishing, 2016.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomed-\nical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention \u2013\nMICCAI 2015 , pp. 234\u2013241. Springer International Publishing, 2015. ISBN 978-3-319-24574-4.\ndoi: 10.1007/978-3-319-24574-4 28.\nClaude Elwood Shannon. A Mathematical Theory of Communication. The Bell system technical\njournal , 27(3):379\u2013423, 1948.\nGuotai Wang, Wenqi Li, Michael Aertsen, Jan Deprest, S \u00b4ebastien Ourselin, and Tom Vercauteren.\nAleatoric uncertainty estimation with test-time augmentation for medical image segmentation\nwith convolutional neural networks. Neurocomputing , 338:34\u201345, 2019.\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu,\nYadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution\nrepresentation learning for visual recognition. IEEE transactions on pattern analysis and ma-\nchine intelligence , 43(10):3349\u20133364, 2020.\nLuke Whitbread and Mark Jenkinson. Uncertainty Categories in Medical Image Segmentation: A\nStudy of Source-Related Diversity. In International Workshop on Uncertainty for Safe Utilization\nof Machine Learning in Medical Imaging , pp. 26\u201335. Springer, 2022.\nShuai Xie, Zunlei Feng, Ying Chen, Songtao Sun, Chao Ma, and Mingli Song. Deal: Difficulty-\naware active learning for semantic segmentation. In Proceedings of the Asian Conference on\nComputer Vision , 2020.\nGe Zhang, Hao Dang, and Yulong Xu.", "start_char_idx": 0, "end_char_idx": 3304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37f09b6e-8f7a-45bb-92f8-cd651dc6955d": {"__data__": {"id_": "37f09b6e-8f7a-45bb-92f8-cd651dc6955d", "embedding": null, "metadata": {"page_label": "12", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55251ba9-1e8e-447d-81fc-9ddf6f66fd9c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "919f524429140351f4f0b4c8e7423cb8a87725db881da46837ee280714a0eb32", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdbaa196-f7b1-4c32-81bd-6677e2055287", "node_type": "1", "metadata": {"page_label": "12", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d605fdf0cbcaa800aa078a8b0ac86de0729c0d4618f5b35ccc3894e6694108bf", "class_name": "RelatedNodeInfo"}}, "text": "Deep high-resolution\nrepresentation learning for visual recognition. IEEE transactions on pattern analysis and ma-\nchine intelligence , 43(10):3349\u20133364, 2020.\nLuke Whitbread and Mark Jenkinson. Uncertainty Categories in Medical Image Segmentation: A\nStudy of Source-Related Diversity. In International Workshop on Uncertainty for Safe Utilization\nof Machine Learning in Medical Imaging , pp. 26\u201335. Springer, 2022.\nShuai Xie, Zunlei Feng, Ying Chen, Songtao Sun, Chao Ma, and Mingli Song. Deal: Difficulty-\naware active learning for semantic segmentation. In Proceedings of the Asian Conference on\nComputer Vision , 2020.\nGe Zhang, Hao Dang, and Yulong Xu. Epistemic and aleatoric uncertainties reduction with rotation\nvariation for medical image segmentation with ConvNets. SN Applied Sciences , 4(2):56, February\n2022. ISSN 2523-3963, 2523-3971. doi: 10.1007/s42452-022-04936-x.\n12", "start_char_idx": 2647, "end_char_idx": 3531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8b5b9e1-65a9-4a75-9402-1bb2eb76759b": {"__data__": {"id_": "f8b5b9e1-65a9-4a75-9402-1bb2eb76759b", "embedding": null, "metadata": {"page_label": "13", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "127ce84c-719d-489e-a1cc-ec0023b7fb41", "node_type": "4", "metadata": {"page_label": "13", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "99b233fc7fcb90a4e0b834168c1ca05e56664724d52a8699a362163a8297a07b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA D OWNSTREAM TASKS & M ETRICS\nA.1 S EGMENTATION PERFORMANCE ASSESSMENT\nDice To measure the segmentation performance of the segmentation backbone and prediction mod-\nels, we used the Dice score which is defined as:\nDice(\u02c6y, y\u2217) =2|y\u2217\u2229\u02c6y|\n|y\u2217|+|\u02c6y|=2TP\n2TP+FP+FN(2)\nAs we have multiple segmentation predictions \u02c6yfor most prediction models and multiple reference\nsegmentations y\u2217, we decided to take the average Dice between each of the Nreference segmenta-\ntions and the mean prediction \u00afy:\nDice =1\nNNX\ni=1Dice(\u00afy, y\u2217\ni) (3)\nA.2 O UT OF DISTRIBUTION DETECTION\nIn our evaluation, we concentrate on image-level OoD-D to facilitate human assessment, as humans\ntypically evaluate complete images rather than individual pixels. Notably, if any part of an image is\nidentified as OoD, it has the potential to impact all predictions, rendering them unreliable.\nArea Under the Receiver Operating Characteristics Curve (AUROC) We calculate the Area\nUnder the Receiver Operating Characteristics Curve (AUROC) to determine a method\u2019s capability\nof detecting OoD cases. Thus, we assign each image a label with 1equal to an image being OoD\nand0equal to an image being i.i.d. We then use the sklearn library for determining the ROC curve1\nwith the ground truth input ( 0or1) and the uncertainty scores as target values. We then determine\nthe AUC also using sklearn2.\nA.3 F AILURE DETECTION\nIn our evaluation, we concentrate on image-level FD to facilitate human assessment, as humans\ntypically evaluate complete images rather than individual pixels. To this end, we make use of our\nperformance assessment metric on image-level (Dice) to define a continuous failure label for our\nutilized FD metrics.\nOur motivation behind this approach is that FD based on the Dice is more informative with regard\nto the performance of the model than on the pixel level, as deciding whether a single image should\nbe assessed by a human in place of an automated decision-making process requires to have an\nassessment of the quality of the segmentation for an entire image than for single pixels.\nWe compute our FD metrics twice: first using the i.i.d. test data and then the OoD test data. This\napproach enables us to assess how effectively failures are detected within the i.i.d. data and, subse-\nquently, how well the uncertainty method detects failures when exposed to OoD data.\nArea under the Risk-Coverage-Curve (AURC) The Area under the Risk-Coverage-Curve\n(AURC) is a metric used in selective classification. The goal hereby is to successfully identify\nfailures by having a low risk, i.e. a good classifier performance but also achieve high coverage ,\ni.e. select as few cases as possible for manual correction. For calculating the Area under the Risk-\nCoverage-Curve, we use the implementation following Jaeger et al. (2023). To adapt it for a semantic\nsegmentation predictor fand evaluation dataset D={(xi, yi)}N\ni=1, we define the confidence scor-\ning function (CSF) g(xi)as the negative uncertainty score. Furthermore, we choose the inverted\n1https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_\ncurve.html\n2https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.\nhtml\n13", "start_char_idx": 0, "end_char_idx": 3227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9e16bfa-6ff1-412b-a219-26e41094b863": {"__data__": {"id_": "c9e16bfa-6ff1-412b-a219-26e41094b863", "embedding": null, "metadata": {"page_label": "14", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b5c7e98-453c-4cd5-a436-8804597000b7", "node_type": "4", "metadata": {"page_label": "14", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d43551c7269ca45db43a699f3c06835f0d52ef10642e6bb4d8b9c673624fd4d6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nDice score as the risk lassociated with a prediction:\nl(x, y, f ) = 1\u2212Dice(f(x), y) (4)\nThe risk-coverage curve is obtained by introducing a confidence threshold \u03c4, which leads to the\nselective risk\nRisk(\u03c4|f, g, D ) =PN\ni=1l(xi, yi, f)\u00b7I(g(xi)\u2265\u03c4)\nPN\ni=1I(g(xi)\u2265\u03c4)(5)\nand coverage, defined in Jaeger et al. (2023) as the ratio of cases remaining after selection:\nCoverage (\u03c4|g, D) =PN\ni=1I(g(xi)\u2265\u03c4)\nN(6)\nThe AURC based on a threshold list {\u03c4}T\nt=1withTvalues of a CSF that are sorted ascending can\nthen be calculated as Jaeger et al. (2023):\nAURC (f, g, D ) =TX\nt=1(Coverage (\u03c4t)\u2212Coverage (\u03c4(t\u22121)))\u00b7(Risk(\u03c4t) +Risk(\u03c4t\u22121))/2(7)\nwhere we omitted the conditioning on f, g, D on the RHS for clarity.\nExcess-AURC (E-AURC) Further, as also analyzed in Jaeger et al. (2023) and originally pro-\nposed in Geifman et al. (2019), we use the excess AURC (E-AURC) as an evaluation metric that is\nindependent of the segmentation model\u2019s performance:\nE-AURC =AURC (f, g, D )\u2212AURC (f, g\u2217, D) (8)\nwhere the second term corresponds to the optimal AURC. The optimal CSF g\u2217can be formally\nobtained, for example, by using an oracle CSF that returns a confidence equal to the negative risk\nof a particular prediction, g\u2217(x) =\u2212l(x, y, f ). Practically, it ranks the predictions perfectly by\ntheir risk (in our case ascending Dice score). Although we are aware of the fact that evaluating a\nCSF without considering the performance of the model itself harms the meaningful comparison of\nuncertainty methods (see Jaeger et al. (2023)), we use this as an additional debugging metric which\nis feasible in our case as the there are no significant outliers in terms of segmentation performance\nas seen in the Dice score of Table 5.\nA.4 A CTIVE LEARNING\nIn our evaluation, we concentrate on AL performing queries on image-level to facilitate human\nassessment, as humans typically evaluate complete images rather than individual pixels. The general\nconcept here is that we have a model that is already performing well on an i.i.d. dataset with saturated\nperformance for a specific task which should be adapted to a shifted (OoD) dataset with the same\ntask. Therefore we only measure the performance increase on the OoD test set.\nActive Learning Improvement (AL improvement) To assess the AL improvement of the uncer-\ntainty methods, we measure the relative performance change between two cycles t1andt2on the\nOoD test set:\nC=Dice t2\u2212Dice t1\nDice t1(9)\nAs we do not want to consider effects of random querying in our evaluation, we subtract the perfor-\nmance change that is reached with random querying from the performance change of the uncertainty\nmethod, leading to the following final performance change:\nCfinal=Cmethod\u2212Crandom (10)\nA.5 C ALIBRATION\nOur evaluation of the CALIB follows standard protocol is performed with pixel-level ground truth\nand aggregated to single images requiring therefore no aggregation.\nWe compute our CALIB metrics twice: first using the i.i.d. test data and then the OoD test data.\nThis approach enables us to assess how well the uncertainty measure is calibrated on i.i.d. data and,\nsubsequently, how well the uncertainty measure is calibrated when exposed to inputs from OoD\ndata.\n14", "start_char_idx": 0, "end_char_idx": 3230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b632007-29f6-4c4a-ab9b-df7fb2fc7f2f": {"__data__": {"id_": "6b632007-29f6-4c4a-ab9b-df7fb2fc7f2f", "embedding": null, "metadata": {"page_label": "15", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9fcba23-6d54-4844-aada-75d373dfc994", "node_type": "4", "metadata": {"page_label": "15", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "58bb135515deabfb3fc77a1f76d23a4a1feceb3d6ad48cc414ce7f7677dbfc31", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAverage Calibration Error (ACE) The Average Calibration Error (ACE) is introduced in Neu-\nmann et al. (2018) and used for segmentation in Jungo et al. (2020). In contrast to the Expected\nCalibration Error (ECE), which is used e.g. in Gustafsson et al. (2020); Jungo et al. (2020), every\nbin in the calibration histogram is weighted equally, leading to the following formulation:\nACE =1\nMMX\nm|cm\u2212Accm| (11)\nHere, Mis the number of non-empty bins, cmis the average confidence in bin m, and Acc mthe\nrespective average accuracy. We apply Platt scaling in order to get confidence scores between 0and\n1. We chose this metric in comparison to the ECE as it avoids overweighting the background pixels\nwhich are predominant in our case.\nA.6 A MBIGUITY MODELING\nOur evaluation of AM is separated into two main parts: first, whether an uncertainty measure can\nsuccessfully indicate AU in the correct regions, and second, whether a prediction model is able to\nproduce multiple realistic predictions.\nThe evaluation is performed using pixel-level ground truth based on single images requiring there-\nfore no aggregation.\nWe compute our AM metrics twice: first using only the i.i.d. test data and on the OoD test data.\nThis approach enables us to assess how good the uncertainty measures model AU on i.i.d. data and,\nsubsequently, how good the uncertainty measures model AU on the OoD data.\nNormalized Cross-Correlation (NCC) We calculate the normalized cross-correlation (NCC) fol-\nlowing Hu et al. (2019):\n1\nnp\u03c3a\u03c3bnpX\ni=1(ai\u2212\u00b5a)\u00b7(bi\u2212\u00b5b) (12)\nHere, ais the reference uncertainty map, bis the predicted uncertainty map, npis the total number\nof pixels in the uncertainty maps, and \u00b5and\u03c3are mean and standard deviation of the uncertainty\nmaps. The reference uncertainty map is calculated with the pixel variance of a pixel yiforN\ndifferent segmentation raters {y1\ni, ..., yN\ni}:\nVp(D)[yi] =1\nNNX\nj=1(yj\ni\u2212\u00afyi)2(13)\nwhere \u00afyiis the mean over the segmentation raters \u00afyi=1\nNPN\nj=1yj\ni.\nGeneralized Energy Distance (GED) To better assess the capability of the uncertainty methods\nto model multiple raters, we use the generalized energy distance (GED), which has been used in\nvarious other works focusing on AM (Kohl et al. (2018); Monteiro et al. (2020); Hu et al. (2019)):\nD2\nGED(p,\u02c6p) = 2Ey\u223cp,\u02c6y\u223c\u02c6p[d(y,\u02c6y)]\u2212Ey,y\u2032\u223cp[d(y, y\u2032)]\u2212E\u02c6y,\u02c6y\u2032\u223c\u02c6p[d(\u02c6y,\u02c6y\u2032)] (14)\nHere, d(y, y\u2032)is the distance between two reference segmentations, and d(\u02c6y,\u02c6y\u2032)is the distance\nbetween two predicted segmentation variants. pand\u02c6pare the respective reference and predicted\ndistributions for the segmentations masks. The distance has to satisfy that it increases for more\ndissimilar masks and further d(x, y) = 0 forx=y. As we use the Dice as our main evaluation\nmetric, we chose to use d(x, y) = 1\u2212Dice(x, y)as distance.\n15", "start_char_idx": 0, "end_char_idx": 2823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd1b9809-1187-4b37-b695-226b8753187f": {"__data__": {"id_": "cd1b9809-1187-4b37-b695-226b8753187f", "embedding": null, "metadata": {"page_label": "16", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73f6ac98-d20a-43b7-8ac7-7b35280b7bb3", "node_type": "4", "metadata": {"page_label": "16", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "05dfaa32ab129308b56bca2672d07aa5b5c6b274fc550e403155ec2dbf45a921", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nB D ATASETS\nB.1 T OY DATASET SETUP\nB.1.1 D ATASET SCENARIOS\nAs mentioned in Sec. 4.2, we generate three different training and four different testing scenarios\nfor the toy dataset. An overview of the different scenarios, including the number of training and\ntesting cases in each scenario, is shown in Table 1. Each of those settings is targeted at answering\na specific question in our separation study, described in Sec. 4.1. In setting 1, we induce AU, thus\naiming to answer Q1 and Q2 of the separation study. Setting 2 focuses on EU, and thus aims to\nanswer Q3 and Q4. However, since AU is not induced in setting 2, we hypothesize that the behavior\nof AU-measures should not be well-predictable, limiting the ability to clearly answer Q4. Therefore,\nwe design setting 3, and provide testing scenarios (a) and (b) where AU is induced during training\nand (b) where AU is also present in the i.i.d testing data. These aim at understanding the behavior\nof our uncertainty measures to detect EU with varying degrees of AU present.\nTable 1: Number of training and testing cases for the toy dataset. For each scenario, the number\nof training cases and the number of testing cases is specified. Further, the number of cases with\nambiguity / blur is specified in brackets and the number of i.i.d and OoD cases in the testset.\nScenario Description # Train (# blur)# Test\n# i.i.d (# blur) # OoD\n1Training models on data with induced\nAU; testing on i.i.d. data also contain-\ning AU200 (200)20\n20 (20) 0\n2Training models on data without ambi-\nguity; testing on i.i.d. data and shifted\ndata200 (0)42\n21 (0) 21\n3aTraining models on data with and with-\nout blur/ambiguity; testing on i.i.d.\ndata and shifted data without blur200 (100)42\n21 (0) 21\n3bTraining models on data with and with-\nout blur/ambiguity; testing on i.i.d.\ndata and shifted data without blur and\ni.i.d data with blur200 (100)63\n42 (21) 21\nB.1.2 D ATA WITH INDUCED ALEATORIC UNCERTAINTY\nFigure 4 shows the data scenario that is created with induced aleatoric uncertainty. The input (Fig-\nure 4a) shows a sphere that has Gaussian blur to the outside. Due to the blur to the outside, the exact\nborder of the sphere is ambiguous. This ambiguity is modeled by three different reference raters\n(Figure 4b - Figure 4d). Thereby the segmentation of rater 1 (Figure 4b), that segments the smallest\nsphere, is 10% the size of the segmentation of rater 3 (Figure 4d). Rater 2 (Figure 4c) lies exactly\nbetween those two raters, so the size of its segmentation is 55% the size of rater 3.\nThe test set (Figure 4e) is created in the same manner as the training set. The expected uncertainty\nis shown in Figure 4f with the respective legend in Figure 4g. No epistemic uncertainty should be\npresent in the data when the model converged after training because the test set is created identically\nto the training set. Instead, only aleatoric uncertainty should be present. This aleatoric uncertainty\nis expected to be in the ambiguous area of the border of the sphere.\nB.1.3 D ATA WITH INDUCED EPISTEMIC UNCERTAINTY\nFigure 5 shows the data scenario that was created with induced epistemic uncertainty. The input\nobject in the training data (Figure 5a) is a sphere, like in the dataset with aleatoric uncertainty.\n16", "start_char_idx": 0, "end_char_idx": 3298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44872cca-5aeb-4d05-bd39-e2d8fa15dfc1": {"__data__": {"id_": "44872cca-5aeb-4d05-bd39-e2d8fa15dfc1", "embedding": null, "metadata": {"page_label": "17", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46f34564-ba2e-4b64-bdd4-5e80b316fb0b", "node_type": "4", "metadata": {"page_label": "17", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3e60bbe954057ed150c2ff5a06ab1f5d485c770bf043be96b393066023066ec5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Training Data\n (b) Rater 1\n (c) Rater 2\n (d) Rater 3\n(e) Test Data\n (f) Expected Uncertainty\n (g) Legend\nFigure 4: Aleatoric data scenario. (a) shows the input images in the training set, which are am-\nbiguous due to Gaussian blur to the outside. (b) - (d) show three different reference ratings that are\ngenerated for the input images. (e) shows test images and (f) the expected uncertainty maps. The\nuncertainty regions are explained in (g).\nHowever, for the epistemic data scenario, this sphere has no blur to the outside, to define a clear\nsegmentation boundary for the ground truth segmentation (Figure 5b). As the segmentation problem\nwould be too simple if the background was just black, random noise was added to the background\nfor this case. The test set for this dataset is shown in Figure 5c. It consists of objects of different\nshapes and colors that were not present in the training data. Some of the objects are still spheres but\nwith varying gray values. Furthermore, there are cubes in the test set and spheres that are partially\noutside the image while the spheres in the training set were always fully inside the image.\nAs all segmentations are unique, no aleatoric uncertainty should be present in the data. However,\nwhere exactly to expect epistemic uncertainty is not that clear. In some cases, the network might\ngeneralize, depending on which features were mainly learned during the training (Geirhos et al.\n(2020)). For the given toy example it is unknown which training solution the network learned. If\nit learned to recognize the shape of the object, new shapes should yield a higher uncertainty in\nthe prediction, as shown in Figure 5d. On the other hand, if the network learned the intensity, the\nepistemic uncertainty might look more like in Figure 5e. It might also be the case that the network\nlearned another decision rule which might result in a different epistemic uncertainty.\nB.2 LIDC-IDRI DATASET SETUP\nB.2.1 D ATASET PREPROCESSING\nFor preprocessing the dataset, we use the pylidc library (Hancock & Magnan (2016)). With this\nlibrary, all nodules with size \u22653 mm can be queried and clustered, such that each nodule gets\nassigned up to four raters. We ignore cases that are so close together that they cannot be grouped\nto one nodule automatically. Further, we calculate a consensus mask which is the union of all raters\nand ignore cases that have a consensus mask larger than 64voxels in one direction. We crop patches\nof size 64\u00d764\u00d764with the nodule in the center and all images are resampled to a resolution of\n1\u00d71\u00d71 mm . Also, we only consider nodules in our following analysis that have four reference\nsegmentation masks, which are overall 901nodules.\n17", "start_char_idx": 0, "end_char_idx": 2743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d21c534-1c21-4588-8486-e1414ba6fb6d": {"__data__": {"id_": "2d21c534-1c21-4588-8486-e1414ba6fb6d", "embedding": null, "metadata": {"page_label": "18", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe92000c-6b6c-4e23-a8e2-48fc992ed62a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d97f16c5414e43f9e223c1d5c51871b710dbcb51c08df20fcbf0d5a6684bcfd8", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(a) Training Data\n (b) Ground Truth\n (c) Test Data\n(d) Expected Uncertainty - Net-\nwork Learned Shape\n(e) Expected Uncertainty - Net-\nwork Learned Intensity\n(f) Legend\nFigure 5: Epistemic data scenario. (a) shows the input images in the training set. (b) shows the\nground truth segmentation. (c) shows test images that differ in various aspects from the training\ndata. (d) and (e) show possible uncertainty maps, depending on what the network learned. The\nuncertainty regions are explained in (f).\nB.2.2 M ETADATA DISTRIBUTION SHIFT ANALYSIS\nOverall, the dataset contains nine different features described in the metadata: subtlety ,internal\nstructure ,calcification ,sphericity ,margin ,lobulation ,spiculation ,texture andmalignancy . All of\nthese features contain 4-6 different possible categories. Each segmentation rater assigned one of\nthe categories to the metadata features. For inducing distribution shifts, we binarize each metadata\nfeature into two classes (i.i.d. and OoD) instead of the original categories. To not have too much\nentanglement with aleatoric uncertainty in the distribution shift analysis, we leave out the features\nsubtlety andmargin because if a nodule is subtle, it might be likely that it is not labeled by all\nraters and if the margin is not sharp, there might be a high variability at the border of the nodule.\nFurther, we do not consider the feature internal structure , as it has only one OoD case which makes\nit unsuitable for a comparison between i.i.d. and OoD cases.\nNext, we construct a train/test split to analyze the performance difference of a deterministic U-Net\nmodel on the i.i.d. test set and the OoD test set. The way this split is constructed is shown in\nFigure 6. We first remove all nodules that do not have a majority vote for being i.i.d. or OoD, i.e.\nwhen two raters voted for the nodule being i.i.d. and two voted for the nodule being OoD. Next,\nall patients are identified that have at least one OoD nodule. The OoD nodules of these patients are\nadded to the OoD test set and the i.i.d. nodules of these patients are added to the i.i.d. test set. From\nthe remaining patients that only have i.i.d. nodules, most of the nodules are taken in the i.i.d. training\nset and some nodules are added to the i.i.d. test set, such that the overall ratio of i.i.d. nodules in the\ntraining set and the i.i.d. test set is 80%/20%. The split which cases to include in the training set\nand the i.i.d. test set is decided by the patient identifier. With the described approach for creating\nthe splits, it is ensured that no patient has nodules in the training- and the test set at the same time.\nTo measure the performance drop between the i.i.d. and the OoD test set, 5folds are trained for\nevery metadata split with varying seeds between the folds. The mean Dice between the prediction\nand one random rater and the standard deviation are calculated on the i.i.d. and the OoD test set to\nmeasure the performance. The results are shown in Table 2.\nAfter determining the performance drop on each feature, the two features with the highest perfor-\nmance drops are selected to examine in further experiments. These are the texture shift and the\nmalignancy shift. It can be seen from the results that there is a substantial drop between the i.i.d.\nand OoD performance which confirms the approach for inducing epistemic uncertainty.\n18", "start_char_idx": 0, "end_char_idx": 3414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65575090-8554-4378-8906-8b6ef9985d0c": {"__data__": {"id_": "65575090-8554-4378-8906-8b6ef9985d0c", "embedding": null, "metadata": {"page_label": "19", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "290d33d1-2795-4809-ae6b-ed770523a435", "node_type": "4", "metadata": {"page_label": "19", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3a7b647be017e981fc8f159dfd2ae22fd75d4d0cb9f9cdcee4ffdc2eb2ec5d56", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAll Patients\nPatient has\nOoD Nodule?\nPatients with i.i.d.\nNodules onlyPatients with at least\none OoD Nodule\nOoD Test Set i.i.d. Test Set i.i.d. Training Setyes no\nOoD\nNodulesi.i.d\nNodules\nFill to 20%\nof all i.i.d.\nNodules\n80% i.i.d.\nNodules20% i.i.d.\nNodulesNodule has Majority V oting \nfor i.i.d. / OoD?\nyes\n\"No Majority\"\nNodules no\nSplit by Patient ID\nFigure 6: Splits for the LIDC-IDRI shift analysis. Only nodules are considered that have a majority\nvote for either being i.i.d. or OoD. Furthermore, the splits are created considering the patient ID, so\nthat no nodules of the same patient are in the training set and the test set at the same time. In the end,\nan i.i.d. training set, an i.i.d. test set, and an OoD test set are created to analyze the shifts between\nthe features.\nB.2.3 S ETUP FOR EVALUATION ON DOWNSTREAM TASKS\nTo evaluate the performance on the various downstream tasks, the lung nodules are divided into\nthree sets: i.i.d. training set, i.i.d. and OoD test set, and i.i.d. and OoD unlabeled pool. The size\nof these sets is shown in Table 3. Initially, we only train the model on the i.i.d. training set and\nassume that its performance on i.i.d. data reaches saturation. After this training, we evaluate the\nTable 2: Results for the LIDC-IDRI shift analysis. For each feature, the Dice score on the i.i.d. test\nset, the Dice score on the OoD test set and the performance drop between i.i.d. and OoD test set\nare shown. Mean and standard deviation are reported for training with 5folds, each with a different\nseed.\nFeature i.i.d. / OoD Dice i.i.d. Dice OoD Performance Drop (%)\nCalcification Absent / Present 0.804\u00b10.0022 0.7669\u00b10.0133 4.6112\u00b11.7699\nSphericity Round / Linear 0.7934\u00b10.0042 0.7474\u00b10.0106 5.7905\u00b11.191\nLobulation No Lobulation / Lobula-\ntion0.7887\u00b10.0042 0.7649\u00b10.0046 3.0163\u00b10.6264\nSpiculation No Spiculation / Spicula-\ntion0.7958\u00b10.0022 0.7458\u00b10.0068 6.2865\u00b10.9447\nTexture Solid & Part Solid / Non-\nsolid0.81\u00b10.0012 0.6081\u00b10.0124 24.9244\u00b11.431\nMalignancy Non-malignant /\nMalignant0.7789\u00b10.0051 0.6677\u00b10.0645 14.3093\u00b17.9522\n19", "start_char_idx": 0, "end_char_idx": 2109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0818c7c5-6ea4-4fdc-bca2-acebe99e22f2": {"__data__": {"id_": "0818c7c5-6ea4-4fdc-bca2-acebe99e22f2", "embedding": null, "metadata": {"page_label": "20", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c85e02ff-d6f9-48d0-beb1-6fa5e01c785b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "65826f5060ba2907d5818a69cf2f5841812b50c16653579acaf76638b7ac96ec", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nperformance of the uncertainty methods on FD, CALIB, and AM. Then, we select samples from the\nunlabeled pool based on uncertainty rankings. Based on this uncertainty ranking on the unlabeled\npool, we determine the performance of a method for detecting OoD samples and add the highest\n50% of uncertain samples to the training pool, aiming for improved performance on the OoD test\nset. With this modified training set, we train another iteration and afterward again measure the test\nset performance.\nTable 3: Size of the different sets in the LIDC dataset for the evaluation on the various downstream\ntasks.\nSplit Train ValTest Unlabeled Pool\ni.i.d OoD i.i.d OoD\nTexture 513 129 167 20 42 20\nMalignancy 200 51 105 93 184 92\nB.3 GTA5/C ITYSCAPES DATASET SETUP\nAs a further dataset, we use a combination of the GTA5 dataset (Richter et al. (2016)) and the\nCityscapes dataset (Cordts et al. (2016)). As mentioned in Sec. 4.2, both datasets contain the same\nclasses, and thus we use the GTA5 dataset as i.i.d. data and the Cityscapes dataset as OoD data.\nFrom the Cityscapes dataset, we use the training set as an unlabeled pool for the AL downstream\ntasks and the validation set as test set. The scheme for splitting the datasets into training, test set,\nand unlabeled pool is thereby shown in Figure 7, with the concrete number of images per split.\nConcretely, we randomly select the same amount of images from the GTA5 dataset to be in the\nunlabeled pool and further create a 75/25training/testing split for the GTA5 dataset.\n2490 4 3475GTA5 Cityscapes\n21929Train / Test Images\n2975Unlabeled Pool\n16447Train Images\n5482Test Images2975Unlabeled Pool\n500Test Images\n5950i.i.d. & OoD\nUnlabeled Pool\ni.i.d. Training set\n19422\ni.i.d. & OoD\nTraining set (AL)5997\ni.i.d. & OoD\nTest set 75% 25%\nFigure 7: Splits for the GTA5/CS dataset. From the Cityscapes dataset, the training set is used as\nunlabeled pool and the validation set is used as test set.\nThe Cityscapes dataset contains up to 30 classes, but only 19 of them are used for validation. As\nonly those 19 classes are contained in the GTA5 dataset, we restrict our analysis to these classes.\nFurther, as mentioned in Sec. 4.2, we perform random class switches for the classes \u201csidewalk\u201d,\n\u201cperson\u201d, \u201ccar\u201d, \u201cvegetation\u201d and \u201croad\u201d with a probability of1\n3from <class>to<class 2 >. This\napproach is also applied by Kohl et al. (2018).\n20", "start_char_idx": 0, "end_char_idx": 2427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ffba7fa-8efb-4e07-9797-44509cd7458e": {"__data__": {"id_": "6ffba7fa-8efb-4e07-9797-44509cd7458e", "embedding": null, "metadata": {"page_label": "21", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51b18c4a-6a26-4173-a5ce-5e4555842d51", "node_type": "4", "metadata": {"page_label": "21", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "46dc2081e7d38497786e20283d6babc72a2a3fe88de9be98b33de6da4e72b494", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAll images are first cropped to a size of 1024\u00d71912 and then rescaled to 25% of the size, resulting\nin an image size of 256\u00d7478.\nC M ODEL IMPLEMENTATION DETAILS\nIn this section, we describe the implementation details of the model backbones and prediction mod-\nels. It is important to note that we did not do extensive hyperparameter searches for all the hy-\nperparameters that we state here but rather used standard values if they worked reasonably and if\nnot, we performed sweeps over the hyperparameters to find appropriate settings. We are aware that\nespecially in the implementation of the prediction models, many hyperparameters are involved, and\nexploring more hyperparameter settings might be an interesting direction for future experiments. All\nmodels are trained for 150epochs.\nC.1 S EGMENTATION BACKBONES\nU-Net For the toy dataset and the LIDC datasets, we use a 3D U-Net architecture as segmentation\nbackbone. We thereby use an initial filter size of 8for the toy dataset and 16for the LIDC datasets\nand four encoder and four decoder blocks. As loss function, we use a combination of the Dice loss\nand the cross-entropy loss except for the SSNs as prediction model. For the SSNs, we use the loss\nfunction as specified in Monteiro et al. (2020). The Adam optimizer is used with a learning rate of\n3e\u22124and a weight decay of 1e\u22125. The batch size is set to 8. As augmentations, we apply random\nflipping and Gaussian noise.\nHRNet For the GTA5/CS dataset, we use the HRNet as segmentation backbone, pretrained on\nImageNet. As loss function, we use the cross-entropy loss, again, except for the SSNs. For all\nprediction models except SSNs, SGD is used as optimizer with a learning rate of 0.01, weight decay\nof5e\u22124, and momentum of 0.9. For the SSNs, RMSprop is used as optimizer with a learning rate of\n1e\u22124, weight decay of 5e\u22124, and momentum of 0.6. The batch size is set to 6. As augmentations,\nwe use random horizontal flipping, rotations, random scaling, random cropping, and Gaussian noise.\nC.2 P REDICTION MODELS\nTest-time dropout (TTD) For the U-Net, we add dropout after each convolutional block with a\nprobability of p= 0.5. For the HRNet, we add dropout at the end of each branch, following Nash\net al. (2022). Again, the probability is set to p= 0.5. During inference, we perform 10MC-Dropout\nforward passes for each input.\nEnsemble For the ensemble models, we do not change anything about the models and training\nschemes themselves but train 5models with different seeds. During inference, we pass each input\nimage through all 5models.\nTest-time data augmentations (TTA) For the TTA models, we apply the same augmentations\nas used in training for the 3D U-Net. Thereby, we apply all possible combinations of flipping and\nGaussian noise, which result in 16forward passes per input image ( 8possible flipping directions,\neach with and without noise). For the HRNet, we also apply all possible combinations of random\nhorizontal flipping and Gaussian noise, resulting in 4forward passes per input image ( 2flipping\npossibilities, each with and without noise).\nStochastic Segmentation Networks (SSNs) For the stochastic segmentation networks, we do 10\nforward passes per input image. For the toy dataset and the LIDC datasets, we use a rank of 5and\nfor the GTA5/CS dataset, we use a rank of 10. As the training behaved more stable when pretraining\nthe mean first, we perform 5pretraining epochs where we only train the mean before we also train\nthe covariance matrix.\n21", "start_char_idx": 0, "end_char_idx": 3528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "111c05e6-44f0-44cf-a6af-6cea987aac7f": {"__data__": {"id_": "111c05e6-44f0-44cf-a6af-6cea987aac7f", "embedding": null, "metadata": {"page_label": "22", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44f75753-6a5f-4f89-bf50-200e23cf3644", "node_type": "4", "metadata": {"page_label": "22", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ee122525db48fd729673c7402c2ab325591bbad337d83abafcaa978064abf427", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nD U NCERTAINTY MEASURES FOR PROBABILISTIC VARIABILITY VARIABLE\nPREDICTION MODELS\nFor a probabilistic prediction model p(Y|x) =Ez\u223cp(z)[p(Y|x, z)]which predicts the class variable\nYgiven a sample xwith an additional variable Zfollowing p(z)which is supposed to capture the\nvariability of the raters/labels (variability variable), we hypothesize that AU und EU can be estimated\nin a similar fashion as it is done for Bayesian models following\nH(Y|x)|{z}\nPU=MI(Y, Z)|{z}\nAU (for i.i.d. x)+Ez\u223cZ[H(Y|z, x)]|{z }\nEU. (15)\nExamples of these methods are the SSNs (Monteiro et al. (2020)), the probabilistic U-Net (Kohl\net al. (2018)) or PHiSeg (Baumgartner et al. (2019)) where the prediction model is trained explicitly\nto learn the variability of the raters.\nA more detailed motivation is given in the following two paragraphs and a reason for our observed\nfailure mode is described in the third paragraph.\nAleatoric uncertainty. Multiple plausible predictions for a sample due to ambiguity or other fac-\ntors are commonly attributed as AU (Monteiro et al. (2020); Kendall & Gal (2017)) and therefore\nleads to the assumption that the variability variable Zessentially captures the learned AU of the\nprediction model. Therefore the mutual information between the class label Yand the variability\nvariable Zgiven a sample xdescribes how much information about the AU could be gained by\nobtaining the class label y.\nMI(Y, Z|x) =H(Y|x)\u2212Ez\u223cZ[H(Y|x, z)] (16)\nKnowing the optimal variability variable Zwould essentially lead to alleviating the uncertainty.\nTherefore we hypothesize that this uncertainty measure models AU.\nEpistemic uncertainty. Following the notion that there is no reason for a variability variable pre-\ndiction model ever to be unsure about its prediction on i.i.d. data if it is still dependent on the\nvariability variable p(Y|x, z)3Therefore the uncertainty of the classifier H(Y|x)which can not\nbe attributed to the variability variable Zshould be novel and previously unseen (by the prediction\nmodel). Following this line of reasoning, we hypothesize that the expected entropy of the variability\nvariable models EU.\nEz\u223cZ[H(Y|x, z)] =H(Y|x)\u2212MI(Y, Z|x) (17)\nFailure mode. For the SSNs, we observe in our experiments that the model while still dependent\non the variability variable is often uncertain in border regions between two classes but generally does\nnot extend to large regions of the image4. This offers an explanation why for the experiments on\nthe LIDC-IDRI dataset, where for most samples the disagreement between raters is present purely\nin the border regions of the nodule, MI (Y, Z|x)has the lowest NCC scores (Q1 + Q2).\nE U NCERTAINTY MEASURES FOR TEST-TIMEAUGMENTATION MODELS\nGiven a model using a set label preserving data augmentations during inference which are defined\non the input space Tand used the form of a random variable T(support (T) =T) from which\nsamples are drawn from t\u223cT. The inference using test-time augmentations can can be described\nasp(Y|x) =Et\u223cT[p(Y|t, x)] = Et\u223cT[p(Y|t(x))]. During training the model is optimized\non the training set Dwith a training objective (usually the cross-entropy loss (CE-Loss)) to be\ninvariant against augmentations in D. Given an optimal model for which the training objective is\nminimal (e.g. CE-Loss=0), the outputs of the model on the training set Dare fully invariant to all\ntransformations p(Y|x, t1) =p(Y|x, t2)\u2200t1, t2\u2208 T, x\u2208 D.\n3This is essentially designed into the training of the variability variable prediction models. E.g. for the\nSSNs, this is done so by using the logsumexp of the logarithmic loss Monteiro et al. (2020).\n4We hypothesize that this behavior arises due to p(z)modeling a Gaussian distribution in logit space.\n22", "start_char_idx": 0, "end_char_idx": 3760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b45f3c7a-0296-48ef-8104-e174fbde26eb": {"__data__": {"id_": "b45f3c7a-0296-48ef-8104-e174fbde26eb", "embedding": null, "metadata": {"page_label": "23", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b65e1569-a524-4fd5-9d6d-6f16c45e7a2b", "node_type": "4", "metadata": {"page_label": "23", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bac37de03a6962f81edee879bfcf70ffee8b7e4e11fb6a7393e858bbdb2f5b0e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFor this model, we hypothesize that AU und EU can be estimated in a similar fashion as it is done\nfor Bayesian models following\nH(Y|x)|{z}\nPU=MI(Y, T)|{z}\nEU+Et\u223cT[H(Y|t, x)]|{z }\nAU (for i.i.d. x). (18)\nA more detailed motivation is given in the following two paragraphs.\nAleatoric uncertainty. As our model is perfectly trained on the training set, the model is able\nto detect previously seen uncertainty over the augmentations similar to a Bayesian model can do\nso with each set of parameters Mukhoti et al. (2021). Therefore, the expected entropy over the\naugmentations should give information about the amount of AU in the prediction of a datapoint.\nEt\u223cT[H(Y|x, t)] (19)\nEpistemic uncertainty. As our model is invariant to augmentations on the training set it also\nfollows that MI (Y, T|x) = 0\u2200x\u2208 D Jensen (1906). If the mutual information between the aug-\nmentation variable and predicted label is greater than zero (MI (Y, T|\u02c6x)>0) for a datapoint \u02c6x /\u2208 D,\nthen this indicates that this datapoint deviates in some form from D. Further, if \u02c6xwould be added\nto the training set and the model retrained, the model would have learned to be invariant against\nthe augmentations for this datapoint. Following this argumentation, this term is therefore reducible\nby adding previously unseen datapoints. Based on this, we hypothesize that the mutual information\nbetween the augmentation variable and the predicted label models EU.\nMI(Y, T|x) =H(Y|x)\u2212Et\u223cT[H(Y|x, t)] (20)\nImplications. Based on these derivations it seems that TTA actually allows the model to estimate\nEU, rather than improving the estimation of AU. This falls in line with the hypothesis made by Hu\net al. (2019) and directly opposes the claims of two prominent papers claiming it models AU (Wang\net al. (2019); Ayhan & Berens (2018)).\nF D ETAILS ON THE AGGREGATION STRATEGIES\nF.1 A BLATION STUDY : CORRELATION OF IMAGE LEVEL AGGREGATION AND OBJECT SIZE\nTo confirm the hypothesis about the correlation between the object size and the amount of uncer-\ntainty, we generated plots to see the connection between those two variables for the LIDC datasets.\nOne of the generated plots is shown in Figure 8. This plot is for a TTD model on the LIDC TEX\ndataset. In the top row, the aggregated amount of uncertainty compared to the mean size of the\npredicted segmentation is shown for the epistemic, the aleatoric, and the predictive uncertainty. In\nthe bottom row, the summed uncertainty is divided by the object size. To confirm that the size of\nthe predicted segmentation corresponds to the ground truth segmentation size, the two variables are\nplotted on the right-hand side. It can be seen, that a positive correlation between the aggregation\nsum and the object size is given in the top row, but if the aggregation mean is taken in the bottom\nrow, this correlation is not present. This means that the summed uncertainty only correlates with\nthe size of the objects and does not represent the objects\u2019 uncertainty independent of the size.\nF.2 S ELECTION OF THRESHOLD FOR THRESHOLD LEVEL AGGREGATION\nFor the threshold level aggregation, we need to determine a threshold where the pixels that are above\nthis are considered as \u201duncertain\u201d. Intuitively, most uncertainty is likely to be at the border of the\nobject and thus correlates with the object size. Therefore, the threshold is calculated with respect to\nthe object sizes in the validation set in the following way: First, the mean foreground ratio \u03b1over\nall predicted segmentations in the validation set is determined:\n\u03b1=#voxels foreground pred\n#voxels(21)\n23", "start_char_idx": 0, "end_char_idx": 3617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c69f20ce-6e76-416d-a04c-9c6f934a0620": {"__data__": {"id_": "c69f20ce-6e76-416d-a04c-9c6f934a0620", "embedding": null, "metadata": {"page_label": "24", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0df19d52-f2b4-433d-8dba-194eaac0bcd3", "node_type": "4", "metadata": {"page_label": "24", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "186ef598d862092025717c4ed175ac38c7b3aaa7d874421225558a96c84ab7a8", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFigure 8: Correlation between object size and uncertainty for image level aggregation. In the top\nrow, all pixels in the uncertainty maps are added up and this aggregation sum is plotted with respect\nto the mean size of the predicted segmentations. In the bottom row, the aggregation sum is addition-\nally divided by the predicted object size, resulting in the aggregation mean. On the right-hand side,\nthe mean prediction size is plotted with respect to the mean reference segmentation size to see that\nthe size of the predictions roughly corresponds to the reference segmentation sizes of the objects.\nWith this foreground ratio, the quantile value qis calculated with q= 1\u2212\u03b1. This quantile value\nis applied on the predicted uncertainty maps of the validation set uval, to determine a pixel value\nof pixels that lie in that quantile Q. This pixel value then serves as a threshold for later predicted\nimages:\nthreshold =Q(q, u val) (22)\nWith this method, one threshold per uncertainty modeling method is determined.\nG D ETAILED RESULTS OF THE SEPARATION STUDY\nG.1 D ETAILED ANALYSIS\nQ1 & Q2\nToy dataset. In the toy dataset analysis, AU uncertainty measures have generally higher NCC values\ncompared to EU uncertainty measures, indicating a successful separation of AU and the highlighting\nof relevant areas (Q1) which is also supported by the qualitative analysis with high uncertainty\nsignals in areas with rater disagreement (see Sec. G.3). Meanwhile, the EU-measures perform worse\nthan PU and AU measures indicating that EU-measures do not measure AU. An exception to this\nfinding are SSNs, where NCC scores are higher for EU-measures compared to other prediction\nmodels. This discrepancy may be attributed to the presence of AU at the border regions which is not\nexplained by the variability variable.\nLIDC datasets. On the LIDC datasets, EU-measures performance is similar to that of AU-measures.\nTherefore the approaches seem to model EU in the areas attributed to AU (Q2).\nIn fact, for SSNs, the AU-measure even shows a lower NCC than the EU-measure, which could be\nattributed to the SSNs rating the border regions with high EU, which are the regions of disagreement.\nThe qualitative analysis shows that a slightly better indication of AU by AU-measures becomes ap-\nparent when there is meaningful inter-rater variability beyond small border regions (see Figure 10).\nInterestingly, this effect is only noticeable for i.i.d. nodules. For example, the same nodule that\n24", "start_char_idx": 0, "end_char_idx": 2522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f300f12-bf91-480f-a794-216e5fa9113c": {"__data__": {"id_": "5f300f12-bf91-480f-a794-216e5fa9113c", "embedding": null, "metadata": {"page_label": "25", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d744064-fca6-4808-a94f-5d0f4b60f93c", "node_type": "4", "metadata": {"page_label": "25", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1bef806dc7b12d368c3f9a2595e2b4f27ed6087ae58d726a7a8f327f7826716d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nshows a good indication for AU in the i.i.d. test set on the LIDC TEX dataset (Figure 10) shows a\npoor indication of AU in the OoD test set on the LIDC MAL dataset (Figure 13).\nGTA5/Cityscapes dataset. For the GTA5/CS dataset, the NCC scores are generally lower compared\nto the other datasets. However, for AU-measures, the NCC scores are at least positively correlated,\nwhile the EU-measures are mostly even negatively correlated with the AU, showing that they really\ndo not model AU. The only prediction that reaches a high AU with its respective AU-measure are\nSSNs. This qualitative difference can also be seen in Figure 14: While most prediction models\nshow the highest AU at the borders of the object, SSNs AU-measure highlight the whole ambiguous\narea.\nQ3 & Q4\nToy dataset. In Setting 2, where only EU is present in the data, there is no significant difference in\nAUROC between AU-measures and EU-measures. It could be assumed that in the absence of learn-\ning AU in the training data, every uncertainty measure can be interpreted as EU-measure. However,\nas soon as AU is introduced into the training data in settings 3a and 3b, EU-measures become a\nbetter separator between i.i.d and OoD data. In setting 3b, where AU is present in both the training\nand test data, the separation of EU becomes beneficial. To address Q3 and Q4 on the toy dataset,\nit can be seen that the AUROC retrieved with EU-measures is almost always better than random,\nconfirming Q3. The answer to Q4 depends on the amount of AU present in the training and test\ndata.\nLIDC datasets. On the LIDC datasets, it is evident that the separation between AU and EU brings\nparticular benefits for TTD, Ensembles, and TTA. Specifically, on LIDC TEX, EU-measures prove\nto be a more effective separator between i.i.d. and OoD data. Overall, it can be seen that whenever\nthe separation between PU and EU is advantageous, AU as an OoD-detector performs worse than\nrandom. Another hypothesis that arises is that the separation of EU appears to be most beneficial\nin settings where the OoD-detection performance is not yet saturated, such as in the case of LIDC\nTEX. To summarize the answer for Q3 and Q4 for the LIDC dataset, the AUROC for EU-measures\nis always better than random, confirming Q3. However, Q4 can only be partially confirmed in the\nsense that AU is not a good measure whenever separating EU from PU is beneficial.\nGTA5/Cityscapes dataset. For the GTA5/CS dataset, most EU-measures significantly outperform the\nrespective AU-measure by means of the AUROC. The only exception is TTD, where the EU-measure\neven performs worse than the AU-measure. Besides that, the AU-measures are even below random\nperformance for the patch-level aggregation, while for the image-level aggregation, they perform\nslightly better than random. This indicates in summary with regards to Q3, that EU-measures,\nexcept for TTD, capture EU, while for Q4, it can be at least mostly confirmed that AU-measures do\nnot consistently outperform random selection of OoD cases.\nG.2 Q UANTITATIVE RESULTS\nThe detailed quantitative results for the separation study, presented in Sec. 4.4, can be found in\nTable 4. Table 4a provides insights on answering Q1 and Q2, while Table 4b addresses Q3 and Q4.\n25", "start_char_idx": 0, "end_char_idx": 3292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd7c38b6-41fe-4639-86ec-40fcf698edfc": {"__data__": {"id_": "fd7c38b6-41fe-4639-86ec-40fcf698edfc", "embedding": null, "metadata": {"page_label": "26", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41487428-7b98-4c4e-b2aa-db8956dee02e", "node_type": "4", "metadata": {"page_label": "26", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0385c83df8936c5278594a56ef522e0f8abb7f0fb5df8ba80550e432a435faef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3300171a-ef73-48e9-966d-8608aebeeed6", "node_type": "1", "metadata": {}, "hash": "08934f075efcf9cef7343d319eb7da1d77d80cbb83019cbf3ee3600235669f83", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 4: Quantitative results for the separation study. In order to answer Q1 and Q2 from the\nseparation study, the NCC scores are calculated between the uncertainty maps and the variance of\nthe reference segmentations, shown in Table 4a. To answer Q3 and Q4, the AUROC scores are\ncalculated and reported in Table 4b. Mean results are shown over 3 runs with different seeds for all\nrelevant dataset settings to answer the respective questions. Abbreviations: PM: Prediction model,\nUM: Uncertainty measure, UT: Modeled uncertainty Type (according to theory), AGG: Aggregation\nstrategy.\nToy 1LIDC\nTEXLIDC\nMAL GTA5/CS\nTestset PM UM UT\ni.i.dDeterm. MSR PU 0.68 0.32 0.28 0.51\nTTDPE PU 0.80 0.51 0.48 0.27\nEE AU 0.86 0.52 0.48 0.28\nMI EU 0.47 0.46 0.45 -0.23\nEnsemblePE PU 0.83 0.48 0.43 0.24\nEE AU 0.84 0.49 0.44 0.27\nMI EU 0.51 0.39 0.36 -0.23\nTTAPE PU 0.82 0.46 0.41 0.25\nEE AU 0.82 0.48 0.42 0.26\nMI EU 0.54 0.38 0.35 -0.16\nSSNPE PU 0.96 0.63 0.61 0.56\nMI AU 0.96 0.59 0.55 0.70\nEE EU 0.80 0.64 0.62 0.05\nOoDDeterm. MSR PU - 0.20 0.20 0.47\nTTDPE PU - 0.37 0.36 0.26\nEE AU - 0.37 0.39 0.26\nMI EU - 0.33 0.31 -0.13\nEnsemblePE PU - 0.35 0.33 0.25\nEE AU - 0.36 0.35 0.30\nMI EU - 0.30 0.27 -0.06\nTTAPE PU - 0.32 0.30 0.28\nEE AU - 0.33 0.33 0.30\nMI EU - 0.27 0.25 -0.04\nSSNPE PU - 0.51 0.47 0.37\nMI AU - 0.47 0.44 0.52\nEE EU - 0.52 0.47 0.03\n(a) NCC scores\nToy 2 Toy 3a Toy 3bLIDC\nTEXLIDC\nMAL GTA5/CS\nPM UM UT AGG\nDeterm.", "start_char_idx": 0, "end_char_idx": 1459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3300171a-ef73-48e9-966d-8608aebeeed6": {"__data__": {"id_": "3300171a-ef73-48e9-966d-8608aebeeed6", "embedding": null, "metadata": {"page_label": "26", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41487428-7b98-4c4e-b2aa-db8956dee02e", "node_type": "4", "metadata": {"page_label": "26", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0385c83df8936c5278594a56ef522e0f8abb7f0fb5df8ba80550e432a435faef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd7c38b6-41fe-4639-86ec-40fcf698edfc", "node_type": "1", "metadata": {"page_label": "26", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7192a54763e64041b2aef322c77948b9575df0ab4f77b5166a7d5faa1416d023", "class_name": "RelatedNodeInfo"}}, "text": "MSR PUPatch 0.84 0.78 0.41 0.46 0.86 0.33\nThresh 0.73 0.45 0.40 0.52 0.59 -\nImage - - - - - 0.70\nTTDPE PUPatch 0.83 0.68 0.37 0.46 0.90 0.37\nThresh 0.48 0.50 0.38 0.61 0.74 -\nImage - - - - - 0.68\nEE AUPatch 0.74 0.69 0.36 0.43 0.90 0.37\nThresh 0.53 0.53 0.29 0.40 0.88 -\nImage - - - - - 0.68\nMI EUPatch 0.83 0.61 0.73 0.52 0.88 0.46\nThresh 0.54 0.43 0.71 0.65 0.60 -\nImage - - - - - 0.51\nEnsemblePE PUPatch 0.95 0.94 0.50 0.55 0.91 0.33\nThresh 0.90 0.73 0.69 0.66 0.72 -\nImage - - - - - 0.72\nEE AUPatch 0.94 0.83 0.44 0.49 0.89 0.29\nThresh 0.78 0.19 0.12 0.53 0.53 -\nImage - - - - - 0.67\nMI EUPatch 0.95 0.95 0.85 0.65 0.89 0.91\nThresh 0.91 0.77 0.87 0.72 0.75 -\nImage - - - - - 0.90\nTTAPE PUPatch 0.95 0.91 0.48 0.51 0.88 0.32\nThresh 0.93 0.66 0.55 0.60 0.67 -\nImage - - - - - 0.70\nEE AUPatch 0.95 0.83 0.44 0.46 0.87 0.29\nThresh 0.89 0.27 0.16 0.49 0.53 -\nImage - - - - - 0.67\nMI EUPatch 0.95 0.94 0.92 0.59 0.86 0.93\nThresh 0.93 0.71 0.84 0.67 0.70 -\nImage - - - - - 0.94\nSSNPE PUPatch 0.87 0.76 0.38 0.54 0.84 0.78\nThresh 0.74 0.65 0.34 0.51 0.72 -\nImage - - - - - 0.82\nMI AUPatch 0.68 0.63 0.32 0.54 0.72 0.53\nThresh 0.67 0.51 0.25 0.49 0.57 -\nImage - - - - - 0.55\nEE EUPatch 0.87 0.90 0.43 0.54 0.85 0.78\nThresh 0.74 0.68 0.78 0.50 0.68 -\nImage - - - - - 0.86\n(b) AUROC scores\n26", "start_char_idx": 1460, "end_char_idx": 2745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb3446c2-5d33-4251-b31d-ae34f1bc7002": {"__data__": {"id_": "eb3446c2-5d33-4251-b31d-ae34f1bc7002", "embedding": null, "metadata": {"page_label": "27", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74d5a7d6-5102-4cd9-a7bc-b2f946a04eca", "node_type": "4", "metadata": {"page_label": "27", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a2fda93715758d8c7a57a7e6c6b5c2a433c5b6b15e4721ceec02e41ca5858313", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nG.3 Q UALITATIVE RESULTS\nIn the following sections, samples are shown for the qualitative analysis to answer Q1 and Q2 from\nthe separation study (see Sec. 4.4).\nG.3.1 Q UALITATIVE RESULTS FOR THE TOY DATASET\nFigure 9: Qualitative results for separating aleatoric and epistemic uncertainty for the toy dataset.\nThe reference segmentations are shown as overlay over the input image. Further, the interrater\nvariability based on the pixel variance is shown. The uncertainty scores per pixel are normalized\nbetween 0and0.5for the deterministic model and between 0and0.7for the other prediction models,\nreflecting the possible range of uncertainty values.\n27", "start_char_idx": 0, "end_char_idx": 698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec12159e-dc79-4a62-870a-cc72bca972f7": {"__data__": {"id_": "ec12159e-dc79-4a62-870a-cc72bca972f7", "embedding": null, "metadata": {"page_label": "28", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "081dc218-d569-4b78-afcc-03d5ae64639b", "node_type": "4", "metadata": {"page_label": "28", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0a097741ebbb66e83e8cccce21877cd784eccac804faa908d296b5409aecb250", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nG.3.2 Q UALITATIVE RESULTS FOR THE LIDC-IDRI DATASETS\nTexture shift i.i.d. example\nFigure 10: Qualitative results for separating aleatoric and epistemic uncertainty for the LIDC TEX\ndataset. A case that is part of the i.i.d. test set is shown. The reference segmentations are shown as\noverlay over the input image. Further, the interrater variability based on the pixel variance is shown.\nThe uncertainty scores per pixel are normalized between 0and0.5for the deterministic model and\nbetween 0and0.7for the other prediction models, reflecting the possible range of uncertainty values.\n28", "start_char_idx": 0, "end_char_idx": 632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e518d1a9-ccdf-41f2-b3aa-bd36b0be071c": {"__data__": {"id_": "e518d1a9-ccdf-41f2-b3aa-bd36b0be071c", "embedding": null, "metadata": {"page_label": "29", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b1fe07d-f576-430e-a5a5-5609bac620d8", "node_type": "4", "metadata": {"page_label": "29", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "229f0c30979f226cc88a2bc78f501c190cb61c733662595b12b2d8ee8a6627c5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTexture shift OoD example\nFigure 11: Qualitative results for separating aleatoric and epistemic uncertainty for the LIDC TEX\ndataset. A case that is part of the OoD test set is shown. The reference segmentations are shown as\noverlay over the input image. Further, the interrater variability based on the pixel variance is shown.\nThe uncertainty scores per pixel are normalized between 0and0.5for the deterministic model and\nbetween 0and0.7for the other prediction models, reflecting the possible range of uncertainty values.\n29", "start_char_idx": 0, "end_char_idx": 572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bd5149a-c217-41cc-a14f-66074114e956": {"__data__": {"id_": "9bd5149a-c217-41cc-a14f-66074114e956", "embedding": null, "metadata": {"page_label": "30", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f0be2ca-a6e1-48d1-8a09-803b0ac6aadc", "node_type": "4", "metadata": {"page_label": "30", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "06be0ec0eaf6870398e53b70ba5af46c71e5ab8adf6280926618d17ec01fd6fe", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMalignancy shift i.i.d. example\nFigure 12: Qualitative results for separating aleatoric and epistemic uncertainty for the LIDC MAL\ndataset. A case that is part of the i.i.d. test set is shown. The reference segmentations are shown as\noverlay over the input image. Further, the interrater variability based on the pixel variance is shown.\nThe uncertainty scores per pixel are normalized between 0and0.5for the deterministic model and\nbetween 0and0.7for the other prediction models, reflecting the possible range of uncertainty values.\n30", "start_char_idx": 0, "end_char_idx": 581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "806188d5-d066-4b7b-9627-2a65735ec556": {"__data__": {"id_": "806188d5-d066-4b7b-9627-2a65735ec556", "embedding": null, "metadata": {"page_label": "31", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d86632e-4098-49ef-9bce-5941abfaa41b", "node_type": "4", "metadata": {"page_label": "31", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c2b87c4cdbe72b5224f92fb8cd0a8e85cc84198aeca16a9c9ec698d63042264b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMalignancy shift OoD example\nFigure 13: Qualitative results for separating aleatoric and epistemic uncertainty for the LIDC MAL\ndataset. A case that is part of the OoD test set is shown. The reference segmentations are shown as\noverlay over the input image. Further, the interrater variability based on the pixel variance is shown.\nThe uncertainty scores per pixel are normalized between 0and0.5for the deterministic model and\nbetween 0and0.7for the other prediction models, reflecting the possible range of uncertainty values.\n31", "start_char_idx": 0, "end_char_idx": 575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69d33f2e-cac0-4591-b5f8-5f7675c7c8c1": {"__data__": {"id_": "69d33f2e-cac0-4591-b5f8-5f7675c7c8c1", "embedding": null, "metadata": {"page_label": "32", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31bab90b-6577-4f76-becd-d41b6ef67ca6", "node_type": "4", "metadata": {"page_label": "32", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0126f78e5ddb3cde527d3aa3b2e7392bd517964b05da36948e3abeab8d326503", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nG.3.3 Q UALITATIVE RESULTS FOR THE GTA 5 / C ITYSCAPES DATASET\nFigure 14: Qualitative results for separating aleatoric and epistemic uncertainty for the GTA 5 /\nCityscapes dataset. The reference segmentations are shown as overlay over the input image. Further,\nthe interrater variability based on the pixel variance is shown. The uncertainty scores per pixel are\nnormalized per image.\n32", "start_char_idx": 0, "end_char_idx": 432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45c99848-eece-4f2d-a205-ca5264b05725": {"__data__": {"id_": "45c99848-eece-4f2d-a205-ca5264b05725", "embedding": null, "metadata": {"page_label": "33", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37402106-d5de-44b2-ac3a-42a2c3335742", "node_type": "4", "metadata": {"page_label": "33", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fbb8f6db6b2111fe654a90d6a21cc684d7ab7a9a28044c6d54114f470ec485ee", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nH D ETAILED RESULTS OF THE EVALUATION ON DOWNSTREAM TASKS\nThe following tables show the detailed results on the downstream tasks as described in Sec. 4.5. For\nthe LIDC datasets, the results are shown in Table 5, while for the GTA5/CS dataset, the results are\nshown in Table 6.\n33", "start_char_idx": 0, "end_char_idx": 324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "550f8478-b7f7-42a0-9ed8-be8b03dba448": {"__data__": {"id_": "550f8478-b7f7-42a0-9ed8-be8b03dba448", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a3fc04c-4c55-4a83-8c8c-09c3ddd93e2c", "node_type": "1", "metadata": {}, "hash": "fb1b81bd98c51a7ff68485a2f6b5a0f9061bb592859bd388dbd4ea54166a7645", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024 Table 5: Evaluation of downstream tasks on the LIDC datasets. The table shows the segmentation performance by means of\nthe Dice score and evaluation metrics for 5 different downstream tasks, where \u2191depict higher scores are better and \u2193lower scores\nare better. All scores are multiplyed by 102. The color heatmap is normalized per column and per shift, brighter columns imply\nbetter scores. For AL, the second cycle was only executed with EU and PU, indicated by empty grey entries for AU. Reported\nresults show the mean and standard deviation over 3 different seeds. Abbreviations: PM: Prediction model, UM: Uncertainty\nmeasure, UT: Modeled uncertainty Type (according to theory), AGG: Aggregation strategy.\nSeg. Performance OoD-D Failure Detection AL Calibration Ambiguity Modeling\nShift PM UM UT AGGDice\ni.i.d.\u2191Dice\nOoD\u2191AUROC\n\u2191AURC\ni.i.d.\u2193AURC\nOoD\u2193EAURC\ni.i.d.\u2193EAURC\nOoD\u2193AL\nImprov.\nOoD\u2191ACE\ni.i.d.\u2193ACE\nOoD\u2193NCC\ni.i.d.\u2191NCC\nOoD\u2191GED\ni.i.d.\u2193GED\nOoD\u2193Texture-ShiftDeterm. MSR PUPatch 81.42\u00b10.28 64.05\u00b10.1 46.43\u00b12.28 19.18\u00b10.24 41.27\u00b11.19 6.13\u00b10.09 18.64\u00b11.27 -0.78\u00b11.93 29.92\u00b10.7 32.58\u00b10.99 31.55\u00b10.26 20.09\u00b11.57 22.18\u00b10.53 52.06\u00b10.18\nThresh 81.42\u00b10.28 64.05\u00b10.1 52.02\u00b111.21 17.7\u00b11.02 41.82\u00b11.54 4.65\u00b10.82 19.19\u00b11.18 -2.57\u00b13.49 29.92\u00b10.7 32.58\u00b10.99 31.55\u00b10.26 20.09\u00b11.57 22.18\u00b10.53 52.06\u00b10.18TTDPE PUPatch 81.1\u00b10.14 63.9\u00b11.43 46.35\u00b11.77 19.97\u00b10.09 42.52\u00b12.12 6.52\u00b10.16 19.05\u00b11.18 1.06\u00b11.0 19.47\u00b10.67 21.98\u00b11.04 51.17\u00b10.13 36.58\u00b11.73 16.83\u00b10.26 40.84\u00b12.01\nThresh 81.1\u00b10.14 63.9\u00b11.43 60.95\u00b12.3 17.35\u00b10.51 37.97\u00b11.82 3.9\u00b10.42 14.5\u00b11.02 0.9\u00b10.76 19.47\u00b10.67 21.98\u00b11.04 51.17\u00b10.13 36.58\u00b11.73 16.83\u00b10.26 40.84\u00b12.01\nEE AUPatch 81.1\u00b10.14 63.9\u00b11.43 43.41\u00b11.38 20.25\u00b10.04 41.65\u00b13.65 6.8\u00b10.13 18.19\u00b12.78 26.95\u00b10.27 26.86\u00b11.66 51.74\u00b10.26 37.21\u00b12.13 16.83\u00b10.26 40.84\u00b12.01\nThresh 81.1\u00b10.14 63.9\u00b11.43 40.4\u00b11.68 20.03\u00b10.25 42.24\u00b12.22 6.57\u00b10.17 18.77\u00b11.28 26.95\u00b10.27 26.86\u00b11.66 51.74\u00b10.26 37.21\u00b12.13 16.83\u00b10.26 40.84\u00b12.01\nMI EUPatch 81.", "start_char_idx": 0, "end_char_idx": 1957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a3fc04c-4c55-4a83-8c8c-09c3ddd93e2c": {"__data__": {"id_": "4a3fc04c-4c55-4a83-8c8c-09c3ddd93e2c", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "550f8478-b7f7-42a0-9ed8-be8b03dba448", "node_type": "1", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "3f23d6ab08377753fc53e48792ee3edffcbd7039bd3bbcf995a7e69d02a88b3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e915169e-4c4e-43ae-8baa-fd82556feb0b", "node_type": "1", "metadata": {}, "hash": "ffb4d0b70714cba57f1ab7220b842ba763f17159b79f6ff2c8ad0020bdf3ca92", "class_name": "RelatedNodeInfo"}}, "text": "41\u00b11.38 20.25\u00b10.04 41.65\u00b13.65 6.8\u00b10.13 18.19\u00b12.78 26.95\u00b10.27 26.86\u00b11.66 51.74\u00b10.26 37.21\u00b12.13 16.83\u00b10.26 40.84\u00b12.01\nThresh 81.1\u00b10.14 63.9\u00b11.43 40.4\u00b11.68 20.03\u00b10.25 42.24\u00b12.22 6.57\u00b10.17 18.77\u00b11.28 26.95\u00b10.27 26.86\u00b11.66 51.74\u00b10.26 37.21\u00b12.13 16.83\u00b10.26 40.84\u00b12.01\nMI EUPatch 81.1\u00b10.14 63.9\u00b11.43 51.55\u00b10.9 19.51\u00b10.12 42.15\u00b11.92 6.06\u00b10.09 18.69\u00b11.01 2.05\u00b12.57 25.42\u00b10.12 27.82\u00b11.35 46.45\u00b10.35 33.33\u00b11.41 16.83\u00b10.26 40.84\u00b12.01\nThresh 81.1\u00b10.14 63.9\u00b11.43 65.2\u00b11.38 16.86\u00b10.28 36.3\u00b12.34 3.41\u00b10.18 12.83\u00b11.78 3.24\u00b11.92 25.42\u00b10.12 27.82\u00b11.35 46.45\u00b10.35 33.33\u00b11.41 16.83\u00b10.26 40.84\u00b12.01EnsemblePE PUPatch 82.34\u00b10.18 64.36\u00b10.85 54.72\u00b12.55 18.24\u00b10.29 37.06\u00b14.43 5.76\u00b10.2 15.05\u00b13.67 1.02\u00b13.04 19.96\u00b10.91 24.7\u00b10.54 47.82\u00b10.55 34.62\u00b11.03 16.05\u00b10.35 40.37\u00b11.09\nThresh 82.34\u00b10.18 64.36\u00b10.85 66.47\u00b15.2 16.07\u00b10.39 38.38\u00b12.84 3.59\u00b10.37 16.37\u00b12.06 0.56\u00b12.53 19.96\u00b10.91 24.7\u00b10.54 47.82\u00b10.55 34.62\u00b11.03 16.05\u00b10.35 40.37\u00b11.09\nEE AUPatch 82.34\u00b10.18 64.36\u00b10.85 48.57\u00b10.74 18.53\u00b10.26 37.49\u00b15.1 6.05\u00b10.17 15.47\u00b14.41 27.21\u00b10.28 29.18\u00b11.92 49.31\u00b10.46 35.52\u00b10.82 16.05\u00b10.35 40.37\u00b11.09\nThresh 82.34\u00b10.18 64.36\u00b10.85 52.66\u00b14.31 16.7\u00b10.35 41.57\u00b12.44 4.22\u00b10.31 19.55\u00b11.97 27.21\u00b10.28 29.18\u00b11.92 49.31\u00b10.46 35.52\u00b10.82 16.05\u00b10.35 40.37\u00b11.09\nMI EUPatch 82.34\u00b10.18 64.36\u00b10.85 65.4\u00b13.71 17.55\u00b10.29 38.15\u00b15.64 5.06\u00b10.22 16.13\u00b14.92 0.96\u00b13.48 28.71\u00b10.38 31.35\u00b11.58 39.05\u00b10.8 30.1\u00b10.84 16.05\u00b10.35 40.37\u00b11.", "start_char_idx": 1681, "end_char_idx": 3040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e915169e-4c4e-43ae-8baa-fd82556feb0b": {"__data__": {"id_": "e915169e-4c4e-43ae-8baa-fd82556feb0b", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a3fc04c-4c55-4a83-8c8c-09c3ddd93e2c", "node_type": "1", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5ebcbe5987cb722ebcad3c784104e1aa5b5bc8819f7fed2aff8d9a16c0abb561", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6148072-b16a-4f17-95c8-178842c9b272", "node_type": "1", "metadata": {}, "hash": "457027133a6b9b193f4caf5f722509dc55c49d17c0934ee46fe4f86b320a078d", "class_name": "RelatedNodeInfo"}}, "text": "31 16.7\u00b10.35 41.57\u00b12.44 4.22\u00b10.31 19.55\u00b11.97 27.21\u00b10.28 29.18\u00b11.92 49.31\u00b10.46 35.52\u00b10.82 16.05\u00b10.35 40.37\u00b11.09\nMI EUPatch 82.34\u00b10.18 64.36\u00b10.85 65.4\u00b13.71 17.55\u00b10.29 38.15\u00b15.64 5.06\u00b10.22 16.13\u00b14.92 0.96\u00b13.48 28.71\u00b10.38 31.35\u00b11.58 39.05\u00b10.8 30.1\u00b10.84 16.05\u00b10.35 40.37\u00b11.09\nThresh 82.34\u00b10.18 64.36\u00b10.85 72.1\u00b15.05 15.58\u00b10.32 36.76\u00b12.92 3.1\u00b10.3 14.75\u00b12.13 -0.75\u00b11.84 28.71\u00b10.38 31.35\u00b11.58 39.05\u00b10.8 30.1\u00b10.84 16.05\u00b10.35 40.37\u00b11.09TTAPE PUPatch 81.83\u00b10.3 65.08\u00b10.45 50.95\u00b11.4 18.57\u00b10.25 36.28\u00b11.96 5.76\u00b10.12 14.38\u00b11.92 0.45\u00b11.97 22.78\u00b11.01 26.07\u00b12.37 46.04\u00b10.31 31.51\u00b10.22 17.37\u00b10.6 41.49\u00b10.9\nThresh 81.83\u00b10.3 65.08\u00b10.45 59.8\u00b110.85 16.77\u00b10.52 39.21\u00b10.91 3.97\u00b10.38 17.31\u00b10.78 -0.25\u00b11.44 22.78\u00b11.01 26.07\u00b12.37 46.04\u00b10.31 31.51\u00b10.22 17.37\u00b10.6 41.49\u00b10.9\nEE AUPatch 81.83\u00b10.3 65.08\u00b10.45 46.23\u00b11.89 18.96\u00b10.09 39.39\u00b10.5 6.15\u00b10.13 17.49\u00b10.62 26.45\u00b10.15 28.32\u00b11.87 47.57\u00b10.47 33.48\u00b10.33 17.37\u00b10.6 41.49\u00b10.9\nThresh 81.83\u00b10.3 65.08\u00b10.45 48.93\u00b111.85 17.6\u00b10.77 40.74\u00b11.09 4.79\u00b10.65 18.84\u00b10.96 26.45\u00b10.15 28.32\u00b11.87 47.57\u00b10.47 33.48\u00b10.33 17.37\u00b10.6 41.49\u00b10.9\nMI EUPatch 81.83\u00b10.3 65.08\u00b10.45 59.09\u00b12.68 17.52\u00b10.43 33.53\u00b13.36 4.71\u00b10.28 11.63\u00b13.29 1.52\u00b10.43 28.95\u00b10.59 30.61\u00b10.39 38.08\u00b10.46 26.68\u00b10.36 17.37\u00b10.6 41.49\u00b10.9\nThresh 81.83\u00b10.3 65.08\u00b10.45 67.38\u00b17.09 16.31\u00b10.57 36.14\u00b12.59 3.51\u00b10.42 14.23\u00b12.47 0.3\u00b12.32 28.95\u00b10.59 30.61\u00b10.39 38.08\u00b10.46 26.68\u00b10.36 17.37\u00b10.6 41.49\u00b10.", "start_char_idx": 2772, "end_char_idx": 4124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6148072-b16a-4f17-95c8-178842c9b272": {"__data__": {"id_": "a6148072-b16a-4f17-95c8-178842c9b272", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e915169e-4c4e-43ae-8baa-fd82556feb0b", "node_type": "1", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f12742d7939d56a891c5743cc41fa22aed0b1bc87e5a91495eac36eb437543eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b014450-7c46-4201-bccc-8fe19366e485", "node_type": "1", "metadata": {}, "hash": "20ebec33576a99da1c37a0c3dd848f4dc087e420f4dea51888c11308f3af62c8", "class_name": "RelatedNodeInfo"}}, "text": "52\u00b10.43 33.53\u00b13.36 4.71\u00b10.28 11.63\u00b13.29 1.52\u00b10.43 28.95\u00b10.59 30.61\u00b10.39 38.08\u00b10.46 26.68\u00b10.36 17.37\u00b10.6 41.49\u00b10.9\nThresh 81.83\u00b10.3 65.08\u00b10.45 67.38\u00b17.09 16.31\u00b10.57 36.14\u00b12.59 3.51\u00b10.42 14.23\u00b12.47 0.3\u00b12.32 28.95\u00b10.59 30.61\u00b10.39 38.08\u00b10.46 26.68\u00b10.36 17.37\u00b10.6 41.49\u00b10.9SSNPE PUPatch 81.28\u00b10.16 59.51\u00b11.79 53.57\u00b13.02 19.6\u00b10.42 40.79\u00b16.31 6.41\u00b10.39 16.2\u00b15.62 -5.11\u00b18.76 9.68\u00b10.35 17.87\u00b12.1 63.49\u00b10.41 51.22\u00b10.11 13.57\u00b10.3 35.69\u00b12.43\nThresh 81.28\u00b10.16 59.51\u00b11.79 50.52\u00b15.24 18.47\u00b10.04 49.89\u00b14.56 5.28\u00b10.02 25.3\u00b13.72 -7.24\u00b18.79 9.68\u00b10.35 17.87\u00b12.1 63.49\u00b10.41 51.22\u00b10.11 13.57\u00b10.3 35.69\u00b12.43\nMI AUPatch 81.28\u00b10.16 59.51\u00b11.79 54.48\u00b16.47 19.31\u00b10.76 37.86\u00b12.83 6.11\u00b10.73 13.26\u00b12.12 27.71\u00b10.29 27.98\u00b11.09 59.2\u00b10.63 47.33\u00b10.54 13.57\u00b10.3 35.69\u00b12.43\nThresh 81.28\u00b10.16 59.51\u00b11.79 49.4\u00b16.31 18.49\u00b10.6 36.24\u00b13.38 5.3\u00b10.56 11.65\u00b12.09 27.71\u00b10.29 27.98\u00b11.09 59.2\u00b10.63 47.33\u00b10.54 13.57\u00b10.3 35.69\u00b12.43\nEE EUPatch 81.28\u00b10.16 59.51\u00b11.79 54.13\u00b11.43 19.54\u00b10.33 41.57\u00b16.22 6.34\u00b10.3 16.98\u00b15.69 -0.73\u00b19.19 17.16\u00b10.31 22.56\u00b10.4 63.61\u00b10.47 51.8\u00b10.23 13.57\u00b10.3 35.69\u00b12.43\nThresh 81.28\u00b10.16 59.51\u00b11.79 50.44\u00b16.67 18.61\u00b10.28 50.56\u00b13.84 5.42\u00b10.33 25.97\u00b13.06 -5.7\u00b110.94 17.16\u00b10.31 22.56\u00b10.4 63.61\u00b10.47 51.8\u00b10.23 13.57\u00b10.3 35.69\u00b12.43Malignancy-ShiftDeterm. MSR PUPatch 78.97\u00b10.29 65.03\u00b13.36 86.44\u00b12.33 19.79\u00b10.16 36.1\u00b19.33 4.77\u00b10.27 17.87\u00b18.27 0.79\u00b18.0 32.17\u00b10.96 28.41\u00b11.45 28.39\u00b11.24 20.15\u00b11.", "start_char_idx": 3857, "end_char_idx": 5215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b014450-7c46-4201-bccc-8fe19366e485": {"__data__": {"id_": "2b014450-7c46-4201-bccc-8fe19366e485", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6148072-b16a-4f17-95c8-178842c9b272", "node_type": "1", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f34fd2e3669432cde1adbb21945fd55b2ab8b7d19c0a75e58cb06e089047d608", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fd1218a-1024-4718-b57d-f6521a0ec07d", "node_type": "1", "metadata": {}, "hash": "879dc7b0c682fc6aa9a1cba86a486b33b8005115dd7ae97e368a5ccb54991e7e", "class_name": "RelatedNodeInfo"}}, "text": "67 18.61\u00b10.28 50.56\u00b13.84 5.42\u00b10.33 25.97\u00b13.06 -5.7\u00b110.94 17.16\u00b10.31 22.56\u00b10.4 63.61\u00b10.47 51.8\u00b10.23 13.57\u00b10.3 35.69\u00b12.43Malignancy-ShiftDeterm. MSR PUPatch 78.97\u00b10.29 65.03\u00b13.36 86.44\u00b12.33 19.79\u00b10.16 36.1\u00b19.33 4.77\u00b10.27 17.87\u00b18.27 0.79\u00b18.0 32.17\u00b10.96 28.41\u00b11.45 28.39\u00b11.24 20.15\u00b11.78 26.42\u00b10.63 54.57\u00b15.45\nThresh 78.97\u00b10.29 65.03\u00b13.36 58.94\u00b14.42 21.86\u00b10.19 48.62\u00b18.06 6.84\u00b10.38 30.39\u00b16.94 -1.65\u00b17.86 32.17\u00b10.96 28.41\u00b11.45 28.39\u00b11.24 20.15\u00b11.78 26.42\u00b10.63 54.57\u00b15.45TTDPE PUPatch 78.6\u00b10.41 67.84\u00b11.62 90.13\u00b10.07 20.6\u00b10.31 22.93\u00b13.61 4.85\u00b10.34 5.39\u00b14.15 2.27\u00b14.26 21.09\u00b10.63 19.13\u00b10.82 47.88\u00b10.27 35.6\u00b11.53 19.88\u00b10.74 39.09\u00b12.89\nThresh 78.6\u00b10.41 67.84\u00b11.62 73.98\u00b17.04 22.08\u00b10.69 25.97\u00b13.92 6.34\u00b10.67 8.43\u00b14.47 2.68\u00b14.48 21.09\u00b10.63 19.13\u00b10.82 47.88\u00b10.27 35.6\u00b11.53 19.88\u00b10.74 39.09\u00b12.89\nEE AUPatch 78.6\u00b10.41 67.84\u00b11.62 90.5\u00b10.56 20.55\u00b10.38 27.35\u00b15.72 4.81\u00b10.43 9.81\u00b16.14 28.34\u00b10.17 24.24\u00b10.63 48.22\u00b10.48 38.51\u00b11.22 19.88\u00b10.74 39.09\u00b12.89\nThresh 78.6\u00b10.41 67.84\u00b11.62 87.62\u00b11.42 22.29\u00b10.75 39.51\u00b11.95 6.54\u00b10.71 21.97\u00b12.45 28.34\u00b10.17 24.24\u00b10.63 48.22\u00b10.48 38.51\u00b11.22 19.88\u00b10.74 39.09\u00b12.89\nMI EUPatch 78.6\u00b10.41 67.84\u00b11.62 88.12\u00b11.29 20.6\u00b10.34 21.96\u00b12.13 4.85\u00b10.36 4.43\u00b12.68 1.84\u00b14.39 26.6\u00b10.38 24.43\u00b10.37 44.55\u00b10.08 31.08\u00b10.89 19.88\u00b10.74 39.09\u00b12.89\nThresh 78.6\u00b10.41 67.84\u00b11.62 59.61\u00b114.74 21.62\u00b10.58 24.2\u00b12.91 5.87\u00b10.58 6.66\u00b13.4 1.55\u00b14.4 26.6\u00b10.38 24.43\u00b10.37 44.55\u00b10.08 31.", "start_char_idx": 4935, "end_char_idx": 6298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fd1218a-1024-4718-b57d-f6521a0ec07d": {"__data__": {"id_": "6fd1218a-1024-4718-b57d-f6521a0ec07d", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b014450-7c46-4201-bccc-8fe19366e485", "node_type": "1", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "eb736e73fdfe4e909974826163631e6ddecb3c324e0a529cd07995c2f2de0e41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30cccc45-b025-430e-984f-08eca27b6aed", "node_type": "1", "metadata": {}, "hash": "1914603376bf68735e0ad813c6cc8147c24a720b705711ef882e4cfec177502b", "class_name": "RelatedNodeInfo"}}, "text": "41 67.84\u00b11.62 88.12\u00b11.29 20.6\u00b10.34 21.96\u00b12.13 4.85\u00b10.36 4.43\u00b12.68 1.84\u00b14.39 26.6\u00b10.38 24.43\u00b10.37 44.55\u00b10.08 31.08\u00b10.89 19.88\u00b10.74 39.09\u00b12.89\nThresh 78.6\u00b10.41 67.84\u00b11.62 59.61\u00b114.74 21.62\u00b10.58 24.2\u00b12.91 5.87\u00b10.58 6.66\u00b13.4 1.55\u00b14.4 26.6\u00b10.38 24.43\u00b10.37 44.55\u00b10.08 31.08\u00b10.89 19.88\u00b10.74 39.09\u00b12.89EnsemblePE PUPatch 79.36\u00b10.13 65.34\u00b10.65 91.31\u00b10.14 19.12\u00b10.1 24.77\u00b12.49 4.41\u00b10.06 6.98\u00b12.71 2.15\u00b12.77 22.93\u00b10.44 21.13\u00b10.25 42.85\u00b10.15 32.62\u00b10.58 20.51\u00b10.28 36.79\u00b11.42\nThresh 79.36\u00b10.13 65.34\u00b10.65 72.07\u00b10.52 19.75\u00b10.17 28.75\u00b12.94 5.04\u00b10.18 10.97\u00b13.16 0.85\u00b12.98 22.93\u00b10.44 21.13\u00b10.25 42.85\u00b10.15 32.62\u00b10.58 20.51\u00b10.28 36.79\u00b11.42\nEE AUPatch 79.36\u00b10.13 65.34\u00b10.65 89.39\u00b10.45 19.56\u00b10.14 37.06\u00b12.1 4.85\u00b10.11 19.28\u00b12.33 29.48\u00b10.47 26.4\u00b10.83 44.23\u00b10.39 35.38\u00b10.73 20.51\u00b10.28 36.79\u00b11.42\nThresh 79.36\u00b10.13 65.34\u00b10.65 53.44\u00b12.59 22.37\u00b10.29 52.64\u00b11.34 7.66\u00b10.32 34.85\u00b11.41 29.48\u00b10.47 26.4\u00b10.83 44.23\u00b10.39 35.38\u00b10.73 20.51\u00b10.28 36.79\u00b11.42\nMI EUPatch 79.36\u00b10.13 65.34\u00b10.65 89.42\u00b10.73 18.75\u00b10.01 24.43\u00b11.94 4.04\u00b10.04 6.65\u00b12.17 1.94\u00b12.62 30.72\u00b10.07 27.43\u00b10.31 36.3\u00b10.17 26.97\u00b10.66 20.51\u00b10.28 36.79\u00b11.42\nThresh 79.36\u00b10.13 65.34\u00b10.65 75.02\u00b10.96 18.95\u00b10.36 26.68\u00b12.47 4.24\u00b10.39 8.9\u00b12.67 0.92\u00b12.93 30.72\u00b10.07 27.43\u00b10.31 36.3\u00b10.17 26.97\u00b10.66 20.51\u00b10.28 36.79\u00b11.42TTAPE PUPatch 79.08\u00b10.12 65.2\u00b13.68 88.41\u00b11.02 19.24\u00b10.14 28.94\u00b17.15 4.39\u00b10.23 11.07\u00b16.19 0.17\u00b18.74 25.13\u00b10.34 21.54\u00b10.25 40.89\u00b10.", "start_char_idx": 6033, "end_char_idx": 7399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30cccc45-b025-430e-984f-08eca27b6aed": {"__data__": {"id_": "30cccc45-b025-430e-984f-08eca27b6aed", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fd1218a-1024-4718-b57d-f6521a0ec07d", "node_type": "1", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fa413707499ca077f4245bdb3603ac3f8e1984ebb9a294cd37a7e50a073772b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bbdc785-3fa7-4868-8d42-98e455bdfe10", "node_type": "1", "metadata": {}, "hash": "10a02af34e81897c4f9ddef728ac740ecf7265726bd8534fcb814a4ffdade9e4", "class_name": "RelatedNodeInfo"}}, "text": "13 65.34\u00b10.65 75.02\u00b10.96 18.95\u00b10.36 26.68\u00b12.47 4.24\u00b10.39 8.9\u00b12.67 0.92\u00b12.93 30.72\u00b10.07 27.43\u00b10.31 36.3\u00b10.17 26.97\u00b10.66 20.51\u00b10.28 36.79\u00b11.42TTAPE PUPatch 79.08\u00b10.12 65.2\u00b13.68 88.41\u00b11.02 19.24\u00b10.14 28.94\u00b17.15 4.39\u00b10.23 11.07\u00b16.19 0.17\u00b18.74 25.13\u00b10.34 21.54\u00b10.25 40.89\u00b10.61 30.39\u00b12.45 21.85\u00b10.26 42.45\u00b14.5\nThresh 79.08\u00b10.12 65.2\u00b13.68 66.79\u00b12.58 20.34\u00b10.74 35.83\u00b17.94 5.5\u00b10.42 17.97\u00b17.13 0.92\u00b18.45 25.13\u00b10.34 21.54\u00b10.25 40.89\u00b10.61 30.39\u00b12.45 21.85\u00b10.26 42.45\u00b14.5\nEE AUPatch 79.08\u00b10.12 65.2\u00b13.68 86.63\u00b11.94 19.64\u00b10.11 35.61\u00b19.85 4.8\u00b10.39 17.75\u00b18.61 29.23\u00b10.79 25.37\u00b10.71 42.46\u00b10.47 32.91\u00b12.11 21.85\u00b10.26 42.45\u00b14.5\nThresh 79.08\u00b10.12 65.2\u00b13.68 52.66\u00b15.18 22.31\u00b10.31 51.72\u00b18.34 7.46\u00b10.23 33.86\u00b17.08 29.23\u00b10.79 25.37\u00b10.71 42.46\u00b10.47 32.91\u00b12.11 21.85\u00b10.26 42.45\u00b14.5\nMI EUPatch 79.08\u00b10.12 65.2\u00b13.68 86.32\u00b12.14 18.75\u00b10.19 28.06\u00b17.07 3.9\u00b10.32 10.19\u00b16.25 0.87\u00b18.57 30.86\u00b10.3 26.51\u00b10.73 34.99\u00b11.08 25.15\u00b12.15 21.85\u00b10.26 42.45\u00b14.5\nThresh 79.08\u00b10.12 65.2\u00b13.68 70.06\u00b13.16 19.26\u00b10.76 31.25\u00b17.73 4.42\u00b10.38 13.39\u00b16.89 0.49\u00b19.23 30.86\u00b10.3 26.51\u00b10.73 34.99\u00b11.08 25.15\u00b12.15 21.85\u00b10.26 42.45\u00b14.5SSNPE PUPatch 79.05\u00b10.24 67.79\u00b11.75 83.59\u00b10.6 20.42\u00b10.68 23.09\u00b11.64 5.57\u00b10.44 5.7\u00b11.4 1.53\u00b15.39 11.62\u00b10.24 13.59\u00b10.32 61.14\u00b11.11 47.48\u00b10.03 16.79\u00b10.3 36.2\u00b12.02\nThresh 79.05\u00b10.24 67.79\u00b11.75 71.97\u00b11.26 20.74\u00b10.15 37.39\u00b15.41 5.88\u00b10.36 20.0\u00b15.0 0.62\u00b14.99 11.62\u00b10.24 13.59\u00b10.32 61.14\u00b11.", "start_char_idx": 7130, "end_char_idx": 8482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bbdc785-3fa7-4868-8d42-98e455bdfe10": {"__data__": {"id_": "3bbdc785-3fa7-4868-8d42-98e455bdfe10", "embedding": null, "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35adcf12-65ca-4b3b-8256-37b0e2acd4c0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2dca6588d2a1c2d081b491aeca298d8a85d7c06466c99f846dee5a2e3979b3fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30cccc45-b025-430e-984f-08eca27b6aed", "node_type": "1", "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "35571840d48e2b9b574b56fe03f5a68bbdd3fd0d1873fb4d0fdd20ba1ab82301", "class_name": "RelatedNodeInfo"}}, "text": "05\u00b10.24 67.79\u00b11.75 83.59\u00b10.6 20.42\u00b10.68 23.09\u00b11.64 5.57\u00b10.44 5.7\u00b11.4 1.53\u00b15.39 11.62\u00b10.24 13.59\u00b10.32 61.14\u00b11.11 47.48\u00b10.03 16.79\u00b10.3 36.2\u00b12.02\nThresh 79.05\u00b10.24 67.79\u00b11.75 71.97\u00b11.26 20.74\u00b10.15 37.39\u00b15.41 5.88\u00b10.36 20.0\u00b15.0 0.62\u00b14.99 11.62\u00b10.24 13.59\u00b10.32 61.14\u00b11.11 47.48\u00b10.03 16.79\u00b10.3 36.2\u00b12.02\nMI AUPatch 79.05\u00b10.24 67.79\u00b11.75 71.68\u00b13.1 20.23\u00b10.98 26.71\u00b12.76 5.38\u00b10.79 9.32\u00b12.43 30.31\u00b10.67 25.93\u00b10.37 55.42\u00b11.71 44.16\u00b10.26 16.79\u00b10.3 36.2\u00b12.02\nThresh 79.05\u00b10.24 67.79\u00b11.75 56.56\u00b18.8 20.63\u00b11.3 31.75\u00b13.27 5.77\u00b11.15 14.36\u00b13.14 30.31\u00b10.67 25.93\u00b10.37 55.42\u00b11.71 44.16\u00b10.26 16.79\u00b10.3 36.2\u00b12.02\nEE EUPatch 79.05\u00b10.24 67.79\u00b11.75 85.3\u00b11.15 20.48\u00b10.49 23.74\u00b11.79 5.62\u00b10.28 6.35\u00b11.63 1.23\u00b15.06 17.67\u00b10.62 19.66\u00b10.28 61.82\u00b10.68 47.3\u00b10.08 16.79\u00b10.3 36.2\u00b12.02\nThresh 79.05\u00b10.24 67.79\u00b11.75 67.74\u00b14.7 20.56\u00b10.41 37.92\u00b15.94 5.71\u00b10.69 20.53\u00b15.53 0.48\u00b15.55 17.67\u00b10.62 19.66\u00b10.28 61.82\u00b10.68 47.3\u00b10.08 16.79\u00b10.3 36.2\u00b12.02\n34", "start_char_idx": 8218, "end_char_idx": 9125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67e7574a-b270-4ee7-9456-786b5e28f468": {"__data__": {"id_": "67e7574a-b270-4ee7-9456-786b5e28f468", "embedding": null, "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec", "node_type": "4", "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8ac9e77dec5f433baeff312dbe70b11481831058415cffd2b6481baa5422ec0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdf24518-2885-4c5a-8179-83a0cfa9112b", "node_type": "1", "metadata": {}, "hash": "9f93f98eab7e1203b78b0e9b24ac0380c52c855b06d6e2a6ddaee1b8e662cdda", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 6: Evaluation of downstream tasks on the GTA 5 / Cityscapes dataset. The table shows the segmentation performance\nby means of the Dice score and evaluation metrics for 5 different downstream tasks, where \u2191depict higher scores are better and \u2193\nlower scores are better. All scores are multiplyed by 102. The color heatmap is normalized per column, brighter columns imply\nbetter scores. For AL, the second cycle was only executed with EU and PU, indicated by empty grey entries for AU. Reported\nresults show the mean and standard deviation over 3 different seeds. Abbreviations: PM: Prediction model, UM: Uncertainty\nmeasure, UT: Modeled uncertainty Type (according to theory), AGG: Aggregation strategy.\nSeg. Performance OoD-D Failure Detection AL Calibration Ambiguity Modeling\nPM UM UT AGGDice\ni.i.d.\u2191Dice\nOoD\u2191AUROC\n\u2191AURC\ni.i.d.\u2193AURC\nOoD\u2193EAURC\ni.i.d.\u2193EAURC\nOoD\u2193AL\nImprov.\nOoD\u2191ACE\ni.i.d.\u2193ACE\nOoD\u2193NCC\ni.i.d.\u2191NCC\nOoD\u2191GED\ni.i.d.\u2193GED\nOoD\u2193\nDeterm. MSR PUPatch 71.73\u00b10.13 58.37\u00b10.51 33.43\u00b11.16 26.52\u00b10.23 39.24\u00b10.75 7.28\u00b10.15 8.15\u00b10.72 -0.5\u00b11.78 13.09\u00b10.15 17.21\u00b10.31 50.61\u00b10.43 46.81\u00b10.61 36.09\u00b10.26 58.87\u00b10.83\nImage 71.73\u00b10.13 58.37\u00b10.51 69.99\u00b10.58 24.2\u00b10.17 36.85\u00b10.33 4.96\u00b10.04 5.76\u00b10.24 1.44\u00b11.73 13.09\u00b10.15 17.21\u00b10.31 50.61\u00b10.43 46.81\u00b10.61 36.09\u00b10.26 58.87\u00b10.83TTDPE PUPatch 71.63\u00b10.14 58.62\u00b10.91 36.67\u00b11.42 26.77\u00b10.2 39.46\u00b10.57 7.43\u00b10.06 8.43\u00b10.35 -0.29\u00b12.72 15.14\u00b10.11 15.07\u00b10.07 26.68\u00b10.29 25.57\u00b13.5 34.03\u00b10.28 56.08\u00b11.61\nImage 71.63\u00b10.14 58.62\u00b10.91 67.8\u00b10.77 24.51\u00b10.21 36.14\u00b10.3 5.16\u00b10.06 5.12\u00b10.09 0.94\u00b12.81 15.14\u00b10.11 15.07\u00b10.07 26.68\u00b10.29 25.57\u00b13.5 34.03\u00b10.28 56.08\u00b11.61\nEE AUPatch 71.63\u00b10.14 58.62\u00b10.91 36.83\u00b11.35 26.76\u00b10.21 39.45\u00b10.61 7.41\u00b10.07 8.43\u00b10.38 15.08\u00b10.11 15.18\u00b10.08 27.67\u00b10.29 26.39\u00b13.46 34.03\u00b10.28 56.08\u00b11.61\nImage 71.63\u00b10.14 58.62\u00b10.91 68.03\u00b10.75 24.5\u00b10.21 36.15\u00b10.3 5.15\u00b10.06 5.13\u00b10.09 15.08\u00b10.11 15.18\u00b10.08 27.67\u00b10.29 26.39\u00b13.46 34.03\u00b10.28 56.08\u00b11.61\nMI EUPatch 71.63\u00b10.", "start_char_idx": 0, "end_char_idx": 1945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdf24518-2885-4c5a-8179-83a0cfa9112b": {"__data__": {"id_": "fdf24518-2885-4c5a-8179-83a0cfa9112b", "embedding": null, "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec", "node_type": "4", "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8ac9e77dec5f433baeff312dbe70b11481831058415cffd2b6481baa5422ec0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67e7574a-b270-4ee7-9456-786b5e28f468", "node_type": "1", "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "b612691c1b78b67e65f2c5ae8a735e943471d9e0fd9e2b3857dcc984d3d13aaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0083e35-acf4-43d7-a656-3b29f2bb7f21", "node_type": "1", "metadata": {}, "hash": "859961a2964dd6758c867c48bf39b04066a491f0e057bcb168cae836df1c885b", "class_name": "RelatedNodeInfo"}}, "text": "35 26.76\u00b10.21 39.45\u00b10.61 7.41\u00b10.07 8.43\u00b10.38 15.08\u00b10.11 15.18\u00b10.08 27.67\u00b10.29 26.39\u00b13.46 34.03\u00b10.28 56.08\u00b11.61\nImage 71.63\u00b10.14 58.62\u00b10.91 68.03\u00b10.75 24.5\u00b10.21 36.15\u00b10.3 5.15\u00b10.06 5.13\u00b10.09 15.08\u00b10.11 15.18\u00b10.08 27.67\u00b10.29 26.39\u00b13.46 34.03\u00b10.28 56.08\u00b11.61\nMI EUPatch 71.63\u00b10.14 58.62\u00b10.91 46.3\u00b14.04 26.88\u00b10.11 39.18\u00b10.6 7.54\u00b10.05 8.15\u00b10.36 0.03\u00b12.86 17.56\u00b10.13 17.82\u00b10.5 -23.17\u00b10.19 -12.85\u00b12.61 34.03\u00b10.28 56.08\u00b11.61\nImage 71.63\u00b10.14 58.62\u00b10.91 51.08\u00b12.53 26.05\u00b10.24 36.61\u00b10.38 6.71\u00b10.09 5.58\u00b10.16 0.45\u00b12.82 17.56\u00b10.13 17.82\u00b10.5 -23.17\u00b10.19 -12.85\u00b12.61 34.03\u00b10.28 56.08\u00b11.61EnsemblePE PUPatch 71.92\u00b10.14 59.36\u00b10.46 32.72\u00b10.57 26.4\u00b10.13 38.33\u00b10.63 7.42\u00b10.07 7.98\u00b10.56 -0.65\u00b11.67 15.64\u00b10.12 15.61\u00b10.22 23.77\u00b10.11 25.37\u00b10.98 31.92\u00b10.22 50.12\u00b10.36\nImage 71.92\u00b10.14 59.36\u00b10.46 72.06\u00b10.8 24.02\u00b10.12 35.51\u00b10.83 5.05\u00b10.01 5.16\u00b10.17 1.17\u00b11.73 15.64\u00b10.12 15.61\u00b10.22 23.77\u00b10.11 25.37\u00b10.98 31.92\u00b10.22 50.12\u00b10.36\nEE AUPatch 71.92\u00b10.14 59.36\u00b10.46 28.69\u00b10.35 26.5\u00b10.15 38.79\u00b10.42 7.53\u00b10.08 8.44\u00b10.41 15.57\u00b10.11 15.31\u00b10.21 26.74\u00b10.14 29.59\u00b10.9 31.92\u00b10.22 50.12\u00b10.36\nImage 71.92\u00b10.14 59.36\u00b10.46 66.81\u00b10.94 24.05\u00b10.11 35.54\u00b10.78 5.08\u00b10.01 5.19\u00b10.22 15.57\u00b10.11 15.31\u00b10.21 26.74\u00b10.14 29.59\u00b10.9 31.92\u00b10.22 50.12\u00b10.36\nMI EUPatch 71.92\u00b10.14 59.36\u00b10.46 90.63\u00b10.29 26.69\u00b10.04 38.57\u00b11.15 7.72\u00b10.13 8.22\u00b10.61 1.77\u00b11.79 21.33\u00b10.06 17.43\u00b10.28 -22.55\u00b10.08 -6.06\u00b10.79 31.92\u00b10.22 50.12\u00b10.36\nImage 71.92\u00b10.", "start_char_idx": 1670, "end_char_idx": 3043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0083e35-acf4-43d7-a656-3b29f2bb7f21": {"__data__": {"id_": "f0083e35-acf4-43d7-a656-3b29f2bb7f21", "embedding": null, "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec", "node_type": "4", "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8ac9e77dec5f433baeff312dbe70b11481831058415cffd2b6481baa5422ec0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdf24518-2885-4c5a-8179-83a0cfa9112b", "node_type": "1", "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "dd5c862f1edb13fedcff15e26411adcf49149f7735b814c13d2e69945fad0094", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91ecc5fe-dbba-4360-b3f3-911840d92d0a", "node_type": "1", "metadata": {}, "hash": "3ecffcefd88d27ebe969633eb45ec03873c51ff1f5216012d823bdddfc0785be", "class_name": "RelatedNodeInfo"}}, "text": "11 35.54\u00b10.78 5.08\u00b10.01 5.19\u00b10.22 15.57\u00b10.11 15.31\u00b10.21 26.74\u00b10.14 29.59\u00b10.9 31.92\u00b10.22 50.12\u00b10.36\nMI EUPatch 71.92\u00b10.14 59.36\u00b10.46 90.63\u00b10.29 26.69\u00b10.04 38.57\u00b11.15 7.72\u00b10.13 8.22\u00b10.61 1.77\u00b11.79 21.33\u00b10.06 17.43\u00b10.28 -22.55\u00b10.08 -6.06\u00b10.79 31.92\u00b10.22 50.12\u00b10.36\nImage 71.92\u00b10.14 59.36\u00b10.46 90.05\u00b10.29 25.68\u00b10.09 36.51\u00b11.29 6.71\u00b10.08 6.16\u00b10.63 1.85\u00b11.8 21.33\u00b10.06 17.43\u00b10.28 -22.55\u00b10.08 -6.06\u00b10.79 31.92\u00b10.22 50.12\u00b10.36TTAPE PUPatch 71.82\u00b10.13 58.39\u00b10.47 31.66\u00b10.71 26.54\u00b10.26 39.44\u00b10.94 7.38\u00b10.15 8.39\u00b11.07 -0.61\u00b11.71 15.37\u00b10.14 15.42\u00b10.31 24.84\u00b10.23 28.14\u00b11.28 33.52\u00b10.26 54.06\u00b10.72\nImage 71.82\u00b10.13 58.39\u00b10.47 69.81\u00b10.32 24.27\u00b10.18 36.44\u00b10.16 5.11\u00b10.05 5.39\u00b10.05 1.44\u00b11.78 15.37\u00b10.14 15.42\u00b10.31 24.84\u00b10.23 28.14\u00b11.28 33.52\u00b10.26 54.06\u00b10.72\nEE AUPatch 71.82\u00b10.13 58.39\u00b10.47 28.85\u00b10.8 26.55\u00b10.27 39.39\u00b10.98 7.39\u00b10.17 8.35\u00b11.12 15.3\u00b10.14 15.32\u00b10.33 26.29\u00b10.24 30.47\u00b11.19 33.52\u00b10.26 54.06\u00b10.72\nImage 71.82\u00b10.13 58.39\u00b10.47 66.61\u00b10.31 24.28\u00b10.18 36.42\u00b10.16 5.12\u00b10.05 5.37\u00b10.05 15.3\u00b10.14 15.32\u00b10.33 26.29\u00b10.24 30.47\u00b11.19 33.52\u00b10.26 54.06\u00b10.72\nMI EUPatch 71.82\u00b10.13 58.39\u00b10.47 92.75\u00b10.11 27.1\u00b10.09 39.65\u00b10.47 7.95\u00b10.08 8.61\u00b10.41 2.12\u00b11.77 22.04\u00b10.39 19.1\u00b10.32 -16.25\u00b10.14 -4.45\u00b11.2 33.52\u00b10.26 54.06\u00b10.72\nImage 71.82\u00b10.13 58.39\u00b10.47 94.32\u00b10.33 26.12\u00b10.05 37.57\u00b10.41 6.96\u00b10.08 6.53\u00b10.54 2.46\u00b11.77 22.04\u00b10.39 19.1\u00b10.32 -16.25\u00b10.14 -4.45\u00b11.2 33.52\u00b10.26 54.06\u00b10.72SSNPE PUPatch 65.49\u00b10.", "start_char_idx": 2767, "end_char_idx": 4141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91ecc5fe-dbba-4360-b3f3-911840d92d0a": {"__data__": {"id_": "91ecc5fe-dbba-4360-b3f3-911840d92d0a", "embedding": null, "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b5b985f-d9a6-4348-b481-d8b5751fbaec", "node_type": "4", "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8ac9e77dec5f433baeff312dbe70b11481831058415cffd2b6481baa5422ec0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0083e35-acf4-43d7-a656-3b29f2bb7f21", "node_type": "1", "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c714302767d1d9115b4629dfa129c1392656b9da375f1d445d3eb3c336f916fc", "class_name": "RelatedNodeInfo"}}, "text": "47 7.95\u00b10.08 8.61\u00b10.41 2.12\u00b11.77 22.04\u00b10.39 19.1\u00b10.32 -16.25\u00b10.14 -4.45\u00b11.2 33.52\u00b10.26 54.06\u00b10.72\nImage 71.82\u00b10.13 58.39\u00b10.47 94.32\u00b10.33 26.12\u00b10.05 37.57\u00b10.41 6.96\u00b10.08 6.53\u00b10.54 2.46\u00b11.77 22.04\u00b10.39 19.1\u00b10.32 -16.25\u00b10.14 -4.45\u00b11.2 33.52\u00b10.26 54.06\u00b10.72SSNPE PUPatch 65.49\u00b10.13 51.9\u00b10.44 77.95\u00b11.28 33.85\u00b10.35 46.53\u00b10.89 10.14\u00b10.29 9.91\u00b10.55 2.01\u00b12.21 16.1\u00b10.13 17.84\u00b10.41 55.97\u00b10.09 37.26\u00b10.95 25.6\u00b10.03 45.26\u00b11.1\nImage 65.49\u00b10.13 51.9\u00b10.44 82.0\u00b11.1 30.07\u00b10.05 43.05\u00b10.95 6.36\u00b10.14 6.43\u00b10.41 3.21\u00b12.25 16.1\u00b10.13 17.84\u00b10.41 55.97\u00b10.09 37.26\u00b10.95 25.6\u00b10.03 45.26\u00b11.1\nMI AUPatch 65.49\u00b10.13 51.9\u00b10.44 53.14\u00b13.37 32.57\u00b10.31 46.28\u00b11.29 8.86\u00b10.4 9.65\u00b11.0 21.6\u00b10.14 25.8\u00b10.42 70.34\u00b10.36 51.8\u00b10.44 25.6\u00b10.03 45.26\u00b11.1\nImage 65.49\u00b10.13 51.9\u00b10.44 55.21\u00b11.37 31.0\u00b10.09 46.07\u00b10.67 7.29\u00b10.13 9.44\u00b10.51 21.6\u00b10.14 25.8\u00b10.42 70.34\u00b10.36 51.8\u00b10.44 25.6\u00b10.03 45.26\u00b11.1\nEE EUPatch 65.49\u00b10.13 51.9\u00b10.44 78.12\u00b11.74 33.87\u00b10.42 46.86\u00b10.86 10.16\u00b10.37 10.23\u00b10.39 2.44\u00b12.17 10.49\u00b10.31 11.12\u00b10.45 4.57\u00b11.07 2.95\u00b10.9 25.6\u00b10.03 45.26\u00b11.1\nImage 65.49\u00b10.13 51.9\u00b10.44 86.2\u00b11.08 32.58\u00b10.43 43.62\u00b11.1 8.87\u00b10.41 7.0\u00b10.56 2.37\u00b11.91 10.49\u00b10.31 11.12\u00b10.45 4.57\u00b11.07 2.95\u00b10.9 25.6\u00b10.03 45.26\u00b11.1\n35", "start_char_idx": 3866, "end_char_idx": 5024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b95ed1c-5c5b-4c9d-a133-46ef888b63d0": {"__data__": {"id_": "5b95ed1c-5c5b-4c9d-a133-46ef888b63d0", "embedding": null, "metadata": {"page_label": "1", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "964ccec2-45cb-479b-994f-2efb4ddd721f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ee1b46fb0a0d5c35b4dff79c7891c87d4c5018c40f8f119a48848ffd7053e69a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nMETA CONTINUAL LEARNING REVISITED : IMPLIC -\nITLY ENHANCING ONLINE HESSIAN APPROXIMATION\nVIAVARIANCE REDUCTION\nYichen Wu1,2\u2217 \u2217, Long-Kai Huang2\u2020, Renzhen Wang3, Deyu Meng3,4, Ying Wei1,5\u2020\n1City University of Hong Kong,2Tencent AI Lab,3Xi\u2019an Jiaotong University,\n4Pazhou Laboratory (Huangpu),5Nanyang Technological University\nABSTRACT\nRegularization-based methods have so far been among the de facto choices for\ncontinual learning. Recent theoretical studies have revealed that these methods\nall boil down to relying on the Hessian matrix approximation of model weights.\nHowever, these methods suffer from suboptimal trade-offs between knowledge\ntransfer and forgetting due to fixed and unchanging Hessian estimations during\ntraining. Another seemingly parallel strand of Meta-Continual Learning (Meta-\nCL) algorithms enforces alignment between gradients of previous tasks and that\nof the current task. In this work we revisit Meta-CL and for the first time bridge it\nwith regularization-based methods. Concretely, Meta-CL implicitly approximates\nHessian in an online manner, which enjoys the benefits of timely adaptation but\nmeantime suffers from high variance induced by random memory buffer sampling.\nWe are thus highly motivated to combine the best of both worlds, through the pro-\nposal of Variance Reduced Meta-CL (VR-MCL) to achieve both timely and ac-\ncurate Hessian approximation. Through comprehensive experiments across three\ndatasets and various settings, we consistently observe that VR-MCL outperforms\nother SOTA methods, which further validates the effectiveness of VR-MCL1.\n1 I NTRODUCTION\nContinual learning (CL) is a promising approach to constructing learning systems that can contin-\nuously process information streams, adapt to changing environments, and acquire new knowledge\nwhile consolidating and retaining previously learned knowledge (Parisi et al., 2019). In contrast\nto traditional supervised learning, which trains on independent and identically distributed (i.i.d)\nsamples, continual learning involves models trained on non-stationary data distributions, where dif-\nferent classification tasks are presented sequentially. The violation of the i.i.d. assumption in CL can\nlead to catastrophic forgetting, causing a significant drop in test performance on previously learned\ntasks (French, 1999; McClelland et al., 1995; McCloskey & Cohen, 1989).\nIn recent years, numerous methods in the field of CL have been proposed with the aim of tackling\nthe catastrophic forgetting problem. Regularization-based methods (Kirkpatrick et al., 2017; Ritter\net al., 2018; Husz \u00b4ar, 2017; Zenke et al., 2017), being an important branch of these methods, aim to\ndiscern and retain the important parameter weights associated with prior tasks so as to sustain their\nperformance. Specifically, they assess the importance of different parameter weights through the\nsecond-order Hessian of previous tasks computed at the end of each task training. The differences\namong these regularization-based methods primarily reside in their distinct approaches to approxi-\nmating the Hessian. For example, EWC (Kirkpatrick et al., 2017) and Online-EWC (Husz \u00b4ar, 2017)\nemploy the diagonal Fisher information matrix, whereas Kronecker factored Laplace approximation\n(KFLA) (Ritter et al., 2018) utilizes a more accurate Kronecker factorization approximation, con-\nsidering off-diagonal elements. Similarly, Intelligent Synapses (IS) (Zenke et al., 2017) adopts an\napproximate diagonal matrix, but its elements take into account the importance of a task throughout\nthe entire training trajectory. Despite these methods striving to improve the Hessian approximation\n\u2217Part of the work was done when the author interned in Tencent AI Lab (wuyichen.am97@gmail.com).\n\u2020Corresponding author: Long-Kai Huang (hlongkai@gmail.com) and Ying Wei (ying.wei@ntu.edu.sg)\n1Code is available at https://github.com/WuYichen-97/Meta-CL-Revised\n1", "start_char_idx": 0, "end_char_idx": 3967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf9a4a53-65b0-4c02-bcba-cfad0aa8dbcd": {"__data__": {"id_": "cf9a4a53-65b0-4c02-bcba-cfad0aa8dbcd", "embedding": null, "metadata": {"page_label": "2", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fbda692-108f-459c-908b-7a1e6e7c315b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0c03e96afee360788fc1a8834fa5b2d167f12854fa3c1d2ad8e92fe6e30a79fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2f8856d-0c04-494e-a2ad-fe6fd9593674", "node_type": "1", "metadata": {}, "hash": "8251c64ea06e1eb84e3a576c7f69d64653a38577514a207375d739a58726acba", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nto preserve important weights for previous tasks, their estimated Hessian matrices of previous tasks\nremain unchanged and fail to update during subsequent training. As the model parameters gradually\ndeviate from the point at which the Hessian was initially computed, the unaltered estimated Hessian\nprogressively loses accuracy due to the increasing truncation error of the Taylor expansion.\nDifferent from regularization-based methods that explicitly approximate the second-order Hessian at\nthe end of training for each task, as will be detailed in Sec. 3, Meta-Continual Learning (Meta-CL)\nmethods utilize the second-order Hessian information implicitly through the hypergradient obtained\nvia bi-level optimization (Riemer et al., 2019; Gupta et al., 2020; V on Oswald et al., 2021). The\ncomputation of hypergradient involves a batch of samples drawn from the memory buffer, which\nstores examples from previous tasks, enabling Meta-CL to leverage up-to-date second-order infor-\nmation from previous tasks. However, it is noteworthy that the utilization of samples drawn from\nthe memory buffer may not offer a complete representation of previous tasks, and may even lack\ndata pertaining to certain tasks. This can result in the presence of erroneous information in the Hes-\nsian estimation. As a consequence, there may be considerable variation in the implicitly estimated\nHessian, ultimately leading to a decline in performance for previous tasks.\nTo leverage the benefits of Meta-CL, specifically the timely updating of second-order Hessian infor-\nmation, while mitigating the negative impact of variation, we propose a momentum-based Variance-\nReduced Meta-CL(VR-MCL ). Through our theoretical analysis, we demonstrate that the integra-\ntion of the momentum-based variance-reduced technique into Meta-CL is equivalent to imposing a\npenalty on its online estimated Hessian. This penalty effectively suppresses excessive model up-\ndates, thereby better preserving crucial weights for previous tasks and their performance. Our main\ncontributions are summarized as follows:\n\u25e6We introduce a new perspective on Meta-CL by characterizing it as a technique that approximates\nthe Hessian matrix, thereby establishing a connection between Meta-CL and regularized methods.\n\u25e6To address the issue of high variance in Meta-CL, we propose Variance-Reduced Meta-CL (VR-\nMCL), which incorporates hypergradient variance reduction technique into Meta-CL.\n\u25e6Theoretically, we demonstrate that our proposed momentum-based hypergradient variance reduc-\ntion technique is equivalent to the inclusion of an effective penalty term within the implicitly\nestimated Hessian. Additionally, we provide the regret bound of VR-MCL to further explain its\nsuperior performance in the context of continual learning.\n\u25e6We conduct comprehensive experiments covering three datasets and multiple continual learning\nsettings, providing empirical evidence for the effectiveness of the proposed algorithm.\n2 T HEUNIFIED FRAMEWORK OF REGULARIZATION -BASED CL M ETHODS\nIn this section, we will first derive the unified iterative updating framework used by regularization-\nbased algorithms following the analysis of Yin et al. (2020) and then show the impact of Hessian\napproximation accuracy on the performance of these algorithms.\nAnalysis of Regularization-based methods. Continual learning considers a sequence of Ntasks\n[T1,T2,\u00b7\u00b7\u00b7,TN], where the i-th task consists of Nisamples, i.e., Ti= (Xi,Yi) ={(xi\nn, yi\nn)}Ni\nn=0.\nHere,Tjrepresents the current training task, T[1:j]refers to all jtasks the model have seen so\nfar, and Li(\u03b8)denotes the empirical risk of the parameter \u03b8onTi. The objective of CL is to learn a\nmodel with parameters \u03b8that minimizes the empirical risk of all seen tasks T[1:j], i.e.,1\nj(Pj\ni=1Li(\u03b8)).\nHowever, in the context of regularization-based CL, we have no access to the samples of previously\nlearned tasks T[1:j\u22121]. The model parameters \u03b8cannot be directly optimized to minimize the cor-\nresponding empirical risks, i.e.,Pj\u22121\ni=1Li(\u03b8). Therefore, regularized CL methods choose to approx-\nimate the empirical risk of previous tasks during training on Tj(Yin et al., 2020).", "start_char_idx": 0, "end_char_idx": 4205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2f8856d-0c04-494e-a2ad-fe6fd9593674": {"__data__": {"id_": "e2f8856d-0c04-494e-a2ad-fe6fd9593674", "embedding": null, "metadata": {"page_label": "2", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fbda692-108f-459c-908b-7a1e6e7c315b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0c03e96afee360788fc1a8834fa5b2d167f12854fa3c1d2ad8e92fe6e30a79fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf9a4a53-65b0-4c02-bcba-cfad0aa8dbcd", "node_type": "1", "metadata": {"page_label": "2", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7edce2428da10e7647b7a3695d797cbba81f08d98d5b633db9c12f17d3d28831", "class_name": "RelatedNodeInfo"}}, "text": "Here,Tjrepresents the current training task, T[1:j]refers to all jtasks the model have seen so\nfar, and Li(\u03b8)denotes the empirical risk of the parameter \u03b8onTi. The objective of CL is to learn a\nmodel with parameters \u03b8that minimizes the empirical risk of all seen tasks T[1:j], i.e.,1\nj(Pj\ni=1Li(\u03b8)).\nHowever, in the context of regularization-based CL, we have no access to the samples of previously\nlearned tasks T[1:j\u22121]. The model parameters \u03b8cannot be directly optimized to minimize the cor-\nresponding empirical risks, i.e.,Pj\u22121\ni=1Li(\u03b8). Therefore, regularized CL methods choose to approx-\nimate the empirical risk of previous tasks during training on Tj(Yin et al., 2020). Concretely, in\nthe simplest case where j=2, when training on T2, the objective of regularization-based methods is\nto minimize1\n2(Lprox\n1+L2), where Lprox\n1(\u03b8)is an approximation of L1(\u03b8)derived through a Taylor\nexpansion at \u02c6\u03b81asLprox\n1=L1(\u02c6\u03b81) + (\u03b8\u2212\u02c6\u03b81)\u22a4\u2207L1(\u02c6\u03b81) +1\n2(\u03b8\u2212\u02c6\u03b81)\u22a4\u22072L1(\u02c6\u03b81)(\u03b8\u2212\u02c6\u03b81). Here, \u02c6\u03b81\nrepresents the model parameters at the end training of T1. For a more general setting where j >2,\nwe can approximate the empirical loss of the previous (j\u22121)tasks by,\nLprox\nj\u22121(\u03b8) =j\u22121X\ni=1Li(\u02c6\u03b8i)|{z}\n(a)+ (\u03b8\u2212\u02c6\u03b8i)\u22a4\u2207\u03b8Li(\u02c6\u03b8i)| {z }\n(b)+1\n2(\u03b8\u2212\u02c6\u03b8i)\u22a4Hi(\u03b8\u2212\u02c6\u03b8i)\n| {z }\n(c),(1)\n2", "start_char_idx": 3527, "end_char_idx": 4781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd536435-76ca-450c-9c80-2db2199fffc3": {"__data__": {"id_": "cd536435-76ca-450c-9c80-2db2199fffc3", "embedding": null, "metadata": {"page_label": "3", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a87b7d5-cc94-47d2-bbc1-fb14e2720577", "node_type": "4", "metadata": {"page_label": "3", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1acebc44597a61098e657b86236d27b0ecb7907b0c76fa814c2fc1056333194b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db00f74b-808f-4e94-aaea-2d2c35730878", "node_type": "1", "metadata": {}, "hash": "a2951410c92505dce537d72c0564ced1f68bf04fbb442fa7dc2cede7a88d73c5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 1: Summary of continual learning methods under the unified iterative update rule with various\ninstantiations of Hessian approximation. Here FandFDare the full and diagonal Fisher information\nmatrices, respectively. HDdenotes diagonal approximation of the Hessian matrix, and HMand\nHVRare the online Hessian estimated on previous tasks and its regularized version, respectively.\nMethodHessian ApproximationIterative Update RuleOff-diagonal elements Online update Penalty\nEWC / On-EWC \u2717 \u2717 \u2717 \u03b8:=\u03b8\u2212\u03b1(Pj\u22121\ni=1Fi\nD)\u22121\u2207\u03b8Lj(\u03b8)\nKFLA \u2713 \u2717 \u2717 \u03b8:=\u03b8\u2212\u03b1(Pj\u22121\ni=1Fi)\u22121\u2207\u03b8Lj(\u03b8)\nIS \u2717 \u2713 \u2717 \u03b8:=\u03b8\u2212\u03b1(Pj\u22121\ni=1Hi\nD)\u22121\u2207\u03b8Lj(\u03b8)\nLa-MAML \u2713 \u2713 \u2717 \u03b8:=\u03b8\u2212\u03b1(Hj\nM)\u22121\u2207\u03b8Lj(\u03b8)\nVR-MCL \u2713 \u2713 \u2713 \u03b8:=\u03b8\u2212\u03b1(Hj\nVR)\u22121\u2207\u03b8Lj(\u03b8)\nwhere \u02c6\u03b8iandHi=\u22072\n\u03b8Li(\u02c6\u03b8i)denote the model parameter at the end of training Tiand the Hessian\nmatrix, respectively. For the concrete analysis of Lprox\nj\u22121(\u03b8)in Eqn. (1), the first term (a) is not related\nto\u03b8and thus can be discarded. Furthermore, since the training process at the end of each task is\nusually converged, the gradient \u2207Li(\u02c6\u03b8i)is near zero and the term (b) can be ignored in practice.\nTherefore, the empirical loss approximation Lprox\nj\u22121(\u03b8)is almost equivalent to approximating the Hes-\nsian matrix in item (c). Based on this approximation and the analysis, we can derive the iterative\nupdate rule of regularized methods in Proposition 1. (See Appendix A.1 for proof.)\nProposition 1. In regularization-based continual learning, if the model parameter \u03b8is searched\nwithin the neighborhood set \u222aj\u22121\ni=1NiwithNi={\u03b8:d(\u03b8,\u02c6\u03b8i)< \u03b4i}, then the iterative update rule of \u03b8\napproximately is,\n\u03b8:\u2248\u03b8\u2212\u03b1(H1+H2+\u00b7\u00b7\u00b7+Hj\u22121)\u22121\u2207\u03b8Lj(\u03b8). (2)\nThe Effect of Hessian in Regularization-based CL. LetH=Pj\u22121\ni=1Hidenote the Hessian part in\nEqn. (2). This update rule implies that the Hessian Hplays a crucial role in the optimization process.\nSpecifically, by multiplying the inverse of Hwith the gradient \u2207\u03b8Lj(\u03b8), the gradient is suppressed\nalong the large curvature directions, i.e., the directions along the eigenvectors corresponding to large\neigenvalues of H, and amplified along the small curvature directions. In regularization-based CL\nalgorithms, the large curvature directions of the Hessian align with the important weights storing the\nknowledge of the previous tasks, which suppresses the update scale to reduce forgetting, while the\nsmall curvature directions facilitate updating the model to fit the current task.\nRegularization-based CL in a Unified Framework. Given the critical role of the Hessian in model\nupdates, various regularization-based methods primarily focus on enhancing their approximation of\nthe Hessian. By integrating Proposition 1, we encapsulate existing regularization-based CL methods\nwithin a unified iterative update framework as summarized them in Table 1. Concretely, EWC (Kirk-\npatrick et al., 2017) and On-EWC (Husz \u00b4ar, 2017) utilize the diagonal Fisher information matrix Fi\nD\nto approximate the Hiin Eqn. (2). In contrast, Intelligent Synapses (IS) (Zenke et al., 2017) em-\nploys a diagonal matrix Hi\nD, with elements signifying the importance of a task throughout the entire\ntraining trajectory. To enhance the approximation of the Hessian, Kronecker factored Laplace ap-\nproximation (KFLA) (Ritter et al., 2018) opts to use the Kronecker factorization approximation Fi,\ntaking into account off-diagonal elements, to approximate Hi.", "start_char_idx": 0, "end_char_idx": 3383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db00f74b-808f-4e94-aaea-2d2c35730878": {"__data__": {"id_": "db00f74b-808f-4e94-aaea-2d2c35730878", "embedding": null, "metadata": {"page_label": "3", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a87b7d5-cc94-47d2-bbc1-fb14e2720577", "node_type": "4", "metadata": {"page_label": "3", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "1acebc44597a61098e657b86236d27b0ecb7907b0c76fa814c2fc1056333194b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd536435-76ca-450c-9c80-2db2199fffc3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d85ab0db43af395400b3ca32dce113469a8de0f0994da4244c58c5529b2f4d8a", "class_name": "RelatedNodeInfo"}}, "text": "By integrating Proposition 1, we encapsulate existing regularization-based CL methods\nwithin a unified iterative update framework as summarized them in Table 1. Concretely, EWC (Kirk-\npatrick et al., 2017) and On-EWC (Husz \u00b4ar, 2017) utilize the diagonal Fisher information matrix Fi\nD\nto approximate the Hiin Eqn. (2). In contrast, Intelligent Synapses (IS) (Zenke et al., 2017) em-\nploys a diagonal matrix Hi\nD, with elements signifying the importance of a task throughout the entire\ntraining trajectory. To enhance the approximation of the Hessian, Kronecker factored Laplace ap-\nproximation (KFLA) (Ritter et al., 2018) opts to use the Kronecker factorization approximation Fi,\ntaking into account off-diagonal elements, to approximate Hi.\n3 R EVISITING META-CL FROM A HESSIAN APPROXIMATION PERSPECTIVE\nIn this section, we revisit the Meta-CL from a Hessian approximation perspective. Specifically,\nwhile regularization-based methods explicitly approximate the second-order Hessian, Meta-CL uti-\nlizes second-order information through the computation of hypergradient. Consequently, it can also\nbe scrutinized within the iterative update framework presented in Eqn. (2).\nMeta-Continual Learning (Meta-CL). As one of the earliest works of Meta-CL, Riemer et al.\n(2019) propose a meta-learning formulation for minimizing the empirical risk of all seen tasks T[1:j].\nIn this approach, the data in T[1:j\u22121]are partially accessible via a memory buffer M, which stores\n3", "start_char_idx": 2640, "end_char_idx": 4108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f42d2590-74aa-4743-bf27-3da25faeba26": {"__data__": {"id_": "f42d2590-74aa-4743-bf27-3da25faeba26", "embedding": null, "metadata": {"page_label": "4", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe550835-8566-4978-9bf7-dda40e2b3bb3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "934b068181bed4714dce1ead36486fff47fa486bb9ed212412f66bebbc1baf37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c74e65e-8133-4256-96c8-cb380760aa08", "node_type": "1", "metadata": {}, "hash": "9f01c5c3e66187af227069b69c90650553327e492d1f88358fd992f45de5f63c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n\u2026\u2026Inner Loop Outer Loop Inputs Outer Gradient Weights\n\ud835\udcaf\ud835\udcaf(1)\ud835\udc57\ud835\udc57\ud835\udcaf\ud835\udcaf(2)\ud835\udc57\ud835\udc57\ud835\udcaf\ud835\udcaf(\ud835\udc3e\ud835\udc3e)\ud835\udc57\ud835\udc57\n\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f+1 \ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f(1)\ud835\udf16\ud835\udf16\ud835\udc4f\ud835\udc4f\u223c\u2133\u222a\ud835\udcaf\ud835\udcaf\ud835\udc57\ud835\udc57\n\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f(\ud835\udc3e\ud835\udc3e) \ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4fg\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\ud835\udf16\ud835\udf16\ud835\udc4f\ud835\udc4f\ud835\udf03\ud835\udf03(\ud835\udc3e\ud835\udc3e)=\ud835\udc48\ud835\udc48\ud835\udc3e\ud835\udc3e(\ud835\udf03\ud835\udf03,\ud835\udcaf\ud835\udcaf\ud835\udc57\ud835\udc57) min\ud835\udf03\ud835\udf03\u21121:\ud835\udc57\ud835\udc57(\ud835\udf03\ud835\udf03(\ud835\udc3e\ud835\udc3e))\nFigure 1: Iterative update process of Meta-CL for the b-th iteration. The notation Tj\n(k)means samples\ndrawn from Tj,Mrefers to the memory buffer and g\u03f5b\n\u03b8bis the update gradient.\ndata from previous tasks. Concretely, the optimization problem is formulated as,\nmin\u03b8L[1:j](\u03b8(K))s.t. \u03b8 (K)=UK(\u03b8;Tj),\nwhere UK(\u03b8;Tj)=Kinner stepsz}|{\nU\u25e6\u00b7\u00b7\u00b7\u25e6 U\u25e6U(\u03b8(0);Tj\n(0)), U (\u03b8(k);Tj\n(k)) = [\u03b8\u2212\u03b2\u2207\u03b8Lj\n(k)(\u03b8)]\f\f\n\u03b8=\u03b8(k\u22121),(3)\ntheL[1:j](\u03b8(K))characterizes the empirical risk of \u03b8(K)on all jtasksT[1:j]which in practice is\nestimated on Tjand the data in memory buffer M,\u03b8(k)denotes the parameters of the k-thinner\nstep of learning for task Tj, and U(\u00b7)means one-step SGD update in the inner loop optimization.\nWe illustrate the iterative update process for the model parameters \u03b8for the b-th iteration, denoted\nby\u03b8b, in Fig. 1. In the inner loop, the model is initialized with \u03b8b(0)=\u03b8band iteratively updated\nviaKsteps of SGD updates, where Tj\n(k)is the data randomly sampled from Tjin the k-th step.\nFollowing the acquisition of \u03b8b(K), in the outer loop, we randomly sample data \u03f5bfromTjand the\nmemory buffer M. Then we compute the loss L[1:j](\u03b8b(K))and obtain the hypergradient w.r.t. \u03b8bas\ng\u03f5b\n\u03b8b:=\u2202L(\u03b8b(K);\u03f5b)/\u2202\u03b8b. We finally update \u03b8busing the hypergradient. It should be noted that dif-\nferent Meta-CL methods typically employ different approximations of the update gradient g\u03f5b\n\u03b8b. For\ninstance, MER (Riemer et al., 2019) utilizes a first-order approximation, while La-MAML (Gupta\net al., 2020), following MAML (Finn et al., 2017), directly computes the hypergradient.\nRevisiting from the Hessian approximation perspective. To solve the bi-level optimization prob-\nlem as shown in Eqn. (3), we need to compute the hypergradient of L[1:j](\u03b8(K))w.r.t. \u03b8, i.e.,\n\u2202L[1:j](\u03b8(K))\n\u2202\u03b8=\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)\u2202\u03b8(K)\n\u03b8,\nwhere the Taylor approximation of the first term around \u03b8is,\n\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)=\u2207\u03b8(K)L[1:j](\u03b8(K))\u2248 \u2207 \u03b8(K)L[1:j](\u03b8)+Hj\nM(\u03b8(K)\u2212\u03b8) + (\u03b8(K)\u2212\u03b8)T\u2297T\u2297(\u03b8(K)\u2212\u03b8).", "start_char_idx": 0, "end_char_idx": 2149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c74e65e-8133-4256-96c8-cb380760aa08": {"__data__": {"id_": "9c74e65e-8133-4256-96c8-cb380760aa08", "embedding": null, "metadata": {"page_label": "4", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe550835-8566-4978-9bf7-dda40e2b3bb3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "934b068181bed4714dce1ead36486fff47fa486bb9ed212412f66bebbc1baf37", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42d2590-74aa-4743-bf27-3da25faeba26", "node_type": "1", "metadata": {"page_label": "4", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "53c35d06b8172ab4c7cf56e96e4dcff200e584ec03b15866c9109980c9494efa", "class_name": "RelatedNodeInfo"}}, "text": "Revisiting from the Hessian approximation perspective. To solve the bi-level optimization prob-\nlem as shown in Eqn. (3), we need to compute the hypergradient of L[1:j](\u03b8(K))w.r.t. \u03b8, i.e.,\n\u2202L[1:j](\u03b8(K))\n\u2202\u03b8=\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)\u2202\u03b8(K)\n\u03b8,\nwhere the Taylor approximation of the first term around \u03b8is,\n\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)=\u2207\u03b8(K)L[1:j](\u03b8(K))\u2248 \u2207 \u03b8(K)L[1:j](\u03b8)+Hj\nM(\u03b8(K)\u2212\u03b8) + (\u03b8(K)\u2212\u03b8)T\u2297T\u2297(\u03b8(K)\u2212\u03b8).\nNote that Hj\nM=\u22072\n\u03b8(K)L[1:j](\u03b8)andTdenote the Hessian matrix and the third-order symmetric\ntensor, respectively, and \u2297represents the Kronecker product. Through the above approximation and\nassuming a single inner step in optimizing Tj(i.e.,K=1) for simplicity, we have Proposition 2.\nProposition 2. For Meta-CL with single inner step adaption, suppose that \u03b8(K)is located in the\n\u03f5-neighborhood N(\u03b8\u2217, \u03f5)of the optimal model parameter \u03b8\u2217= argmin\u03b8L[1:j](\u03b8(K)),Lis\u00b5-smooth,\nand\u03b2 <p\n\u03b4/|\u2207\u03b8Lj(\u03b8)\u2212(\u2207\u03b8Lj(\u03b8))2|where \u03b4is a small number. Let \u03b1=\u03b22, then the iterative\nupdate rule approximately is,\n\u03b8:\u2248\u03b8\u2212\u03b1(Hj\nM)\u22121\u2207\u03b8Lj(\u03b8).\nWe defer the proof and analysis with K-step ( K > 1) adaption to Appendix A.2. Proposition 2\ndemonstrates that the use of hypergradient by Meta-CL is equivalent to implicitly employing the\nHessian ( Hj\nM) and adheres to the unified iterative update rule of regularization-based techniques.\nBy comparing Proposition 2 and Proposition 1, we can deduce that Meta-CL utilizes the samples\ndrawn from Mto implicitly compute Hessian Hj\nMso as to approximate (H1+H2+...+Hj\u22121)\nin Eqn (2). In contrast to regularized methods that estimate and fix each individual Hi, Meta-CL\nimplicitly computes one Hessian Hj\nMenabling it to have the up-to-date second-order information.\nDespite the advantages of Meta-CL in utilizing up-to-date Hessian information via M, it can also\nintroduce erroneous information. For example, the samples drawn from Mmay not adequately\n4", "start_char_idx": 1762, "end_char_idx": 3606, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5b246e6-0989-450f-8c13-c66b7f5d5f11": {"__data__": {"id_": "d5b246e6-0989-450f-8c13-c66b7f5d5f11", "embedding": null, "metadata": {"page_label": "5", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cbc66741-005e-4e68-9e5f-d4861015a063", "node_type": "4", "metadata": {"page_label": "5", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4067bb9ede87e0f0d5edf053386a337156b5f860fe2d66e83801ce8ade3cb26b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e74c06dd-c210-4c89-9545-c968ad577785", "node_type": "1", "metadata": {}, "hash": "d1ecb55fc89e54798d8f583f39d0db75ca52ca4635c9257349734593536dd9a2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nrepresent previous tasks or may lack instances from specific tasks. This inadequacy may lead to the\nestimated Hessian mistakenly having a small curvature direction along important weights of these\nprevious tasks, which in turn fails to suppress updates along this direction. Consequently, this could\nresult in the loss of previous knowledge from those tasks and lead to forgetting.\n4 V ARIANCE REDUCTION ON META-CL\nTo tackle the issue of high variance in Meta-CL caused by the random sampling strategy in the mem-\nory buffer, we propose Variance-Reduced Meta-CL (VR-MCL) that effectively reduces the variance\nin the hypergradient of Meta-CL. Our theoretical analysis demonstrates that this variance reduction\nin the hypergradient is equivalent to the addition of regularization to the implicitly estimated Hes-\nsian matrix, resulting in a decrease in the variance of the Hessian matrix and an improvement in its\nestimation accuracy. Moreover, we also provide a theoretical guarantee of the regret bound.\n4.1 V ARIANCE -REDUCED META-CL (VR-MCL)\nVR-MCL aims to reduce the variance of its implicitly estimated Hessian by diminishing the variance\nof its hypergradient. While there are various variance reduction approaches, such as SAG (Gower\net al., 2020), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) for convex opti-\nmization as well as the non-convex variants (Allen-Zhu & Hazan, 2016; Nguyen et al., 2017; Fang\net al., 2018), directly applying these methods to online continual learning is challenging due to the\nrequirement of computing the full-batch gradient for all samples in a task. However, in the online\ncontinual learning setting, samples from a task are received in mini-batches, and the full sample set\nremains inaccessible, which poses the challenge to the implementation of these techniques.\n\u2026\u2026\u2026\u2026\ud835\udcaf\ud835\udcaf(1)\ud835\udc57\ud835\udc57\nInner Loop\nOuter Loop\nInputs\nOuter GradientWeights\ud835\udcaf\ud835\udcaf(2)\ud835\udc57\ud835\udc57\ud835\udcaf\ud835\udcaf(\ud835\udc3e\ud835\udc3e)\ud835\udc57\ud835\udc57\n\ud835\udcaf\ud835\udcaf(1)\ud835\udc57\ud835\udc57\ud835\udcaf\ud835\udcaf(2)\ud835\udc57\ud835\udc57\ud835\udcaf\ud835\udcaf(\ud835\udc3e\ud835\udc3e)\ud835\udc57\ud835\udc57\n\ud835\udf16\ud835\udf16\ud835\udc4f\ud835\udc4f\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\u22121(1)\n\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\n\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f+1\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\u22121\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\u22121(\ud835\udc3e\ud835\udc3e)\n\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f(1) \ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f(\ud835\udc3e\ud835\udc3e)\n\ufffdg\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\ud835\udf16\ud835\udf16\ud835\udc4f\ud835\udc4f=g\ud835\udf03\ud835\udf03\ud835\udf16\ud835\udf16\ud835\udc4f\ud835\udc4f+\ud835\udc5f\ud835\udc5f(\ufffdg\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\u22121\ud835\udf16\ud835\udf16\ud835\udc4f\ud835\udc4f\u22121\u2212g\ud835\udf03\ud835\udf03\ud835\udc4f\ud835\udc4f\u22121\ud835\udf16\ud835\udf16\ud835\udc4f\ud835\udc4f)\nFigure 2: The iterative update process of VR-\nMCL for the b-th iteration, where \u02c6g\u03f5b\n\u03b8bis the\nfinal update gradient to obtain the \u03b8b+1.On this account, we propose a momentum-based\nvariance-reduced Meta-Continual Learning (Meta-\nCL) method, inspired by previous works Cutkosky\n& Orabona (2019); Khanduri et al. (2021). Instead\nof computing the full-batch gradient which is im-\npractical for online CL, our method, VR-MCL, re-\ntains the parameters from the previous iterative step.\nTheb-th iteration of VR-MCL is illustrated in Fig.", "start_char_idx": 0, "end_char_idx": 2608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e74c06dd-c210-4c89-9545-c968ad577785": {"__data__": {"id_": "e74c06dd-c210-4c89-9545-c968ad577785", "embedding": null, "metadata": {"page_label": "5", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cbc66741-005e-4e68-9e5f-d4861015a063", "node_type": "4", "metadata": {"page_label": "5", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4067bb9ede87e0f0d5edf053386a337156b5f860fe2d66e83801ce8ade3cb26b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5b246e6-0989-450f-8c13-c66b7f5d5f11", "node_type": "1", "metadata": {"page_label": "5", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5c6871a5498c6e96ee898bda12edfc81e967b1d2e5481cd78b667ad7c66931b5", "class_name": "RelatedNodeInfo"}}, "text": "(2021). Instead\nof computing the full-batch gradient which is im-\npractical for online CL, our method, VR-MCL, re-\ntains the parameters from the previous iterative step.\nTheb-th iteration of VR-MCL is illustrated in Fig. 2,\nwhere \u02c6g\u03f5b\u22121\n\u03b8b\u22121represents the momentum component\nandg\u03f5b\n\u03b8b\u22121andg\u03f5b\n\u03b8bdenote the hypergradients on pre-\nvious \u03b8b\u22121and\u03b8b, respectively, computed using data\n\u03f5b. In VR-MCL, the final update gradient with re-\nduced variance is shown in Eqn. (4), where rrep-\nresents the momentum ratio. For clarity, we present\nthe pseudo-codes of the algorithm in Appendix E.\n\u02c6g\u03f5b\n\u03b8b=g\u03f5b\n\u03b8b+r(\u02c6g\u03f5b\u22121\n\u03b8b\u22121\u2212g\u03f5b\n\u03b8b\u22121). (4)\nTo comprehend why Eqn. (4) delivers a smaller gradient variance, we initially examine \u2206b:=\u02c6g\u03f5b\n\u03b8b\u2212\nG\u03b8b. This term measures the error incurred when we use \u02c6g\u03f5b\n\u03b8bas the gradient instead of the true,\nbut unknown full-batch gradient direction G\u03b8b. Through demonstrating that the term E[\u2225\u2206b\u22252]\ndecreases over time, we can show the effectiveness of the VR-MCL in variance reduction Cutkosky\n& Orabona (2019). Substituting Eqn. (4) into \u2206b, we can obtain the following expression,\n\u2206b=r\u2206b\u22121+ (1\u2212r)(g\u03f5b\n\u03b8b\u2212G\u03b8b) +r(g\u03f5b\n\u03b8b\u2212g\u03f5b\n\u03b8b\u22121\u2212(G\u03b8b\u2212G\u03b8b\u22121)). (5)\nThe second term (1\u2212r)(g\u03f5b\n\u03b8b\u2212G\u03b8b)can be modulated by adjusting the ratio of r, and the third term\ng\u03f5b\n\u03b8b\u2212g\u03f5b\n\u03b8b\u22121\u2212(G\u03b8b\u2212G\u03b8b\u22121)is expected in the order of O(\u2225\u03b8b\u2212\u03b8b\u22121\u2225) =O(\u03b1\u2225\u02c6g\u03f5b\u22121\n\u03b8b\u22121\u2225)given the \u00b5-\nsmooth loss function L. Thus, we have \u2225\u2206b\u2225=r\u2225\u2206b\u22121\u2225+Swhere Sis a number that influenced by\nrand the learning rate \u03b1. As\u2225\u2206b\u2225keeps decreasing until it reaches S/(1\u2212r), choosing appropriate\nvalues of rand\u03b1that render S/(1\u2212r)as small as possible ensures variance reduction as we expect.\nRemark. As previously analyzed, most variance reduction methods, such as SAG (Gower et al.,\n2020), SVRG (Johnson & Zhang, 2013) and their nonconvex version, are not applicable in online\nCL settings. Therefore, we design the momentum-based variance reduction technique specifically\nfor Meta-CL. We also discuss other potentially viable variance reduction methods in Sec. 5.\n5", "start_char_idx": 2388, "end_char_idx": 4405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2610876-7f81-49f4-a38f-6159563de431": {"__data__": {"id_": "f2610876-7f81-49f4-a38f-6159563de431", "embedding": null, "metadata": {"page_label": "6", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b75af7-ba77-46cb-8dcc-4a14b1b4a579", "node_type": "4", "metadata": {"page_label": "6", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8d66db3a5ec29316eab752df7046cfed2cf459dbce684a383205c3fe4912c437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f04ca86c-77d5-4068-821e-e839b3ec2a6a", "node_type": "1", "metadata": {}, "hash": "7cb4003832e56b83cc598061ae905cd0094e1d865d4289b9a13c548b3a10c1f4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n4.2 T HEORETICAL ANALYSIS OF VR-MCL.\nIn this subsection, we will demonstrate that the reduction of hypergradient variance by VR-MCL is\nequivalent to imposing a penalty on its implicitly estimated Hessian. As a result, it effectively miti-\ngates the adverse effects of significant Hessian variation, as detailed in Proposition 3. Additionally,\nwe also provide the regret bound of VR-MCL in Theorem 1 to further explain its effectiveness.\nProposition 3. Assume that the batch size for inner step adaptation is sufficiently large and the b-th\nstep trains on Tj. LetHj\nMb=\u22072\n\u03b8b(K)L(\u03b8b;\u03f5b) = [h1\nb,h2\nb,\u00b7\u00b7\u00b7,hD\nb]denote the Hessian at \u03b8bcalculated\non\u03f5b, where hd\nbis the d-th column vector of Hj\nMb. Similarly, bHj\nMb\u22121= [\u02c6h1\nb\u22121,\u00b7\u00b7\u00b7,\u02c6hD\nb\u22121]denotes the\nHessian for the momentum term at the (b\u22121)-th iteration. If (PD\nd=1\u03bbdhd\nb)(PD\nd=1\u03bbd\u02c6hd\nb\u22121)\u22650holds\nfor any \u03bbd\u0338=0,d={1,2,\u00b7\u00b7\u00b7, D}, then we have the following iterative update rule for VR-MCL,\n\u03b8:\u2248\u03b8\u2212\u03b1\u02c6g\u03f5b\n\u03b8b=\u03b8\u2212\u03b1(Hj\nVR)\u22121\u2207\u03b8Lj(\u03b8),\nwhere (Hj\nVR)\u22121=(Hj\nMb)\u22121+r((bHj\nMb\u22121)\u22121\u2212(Hj\nMb\u22121)\u22121)withHj\nMb\u22121denoting the Hessian at \u03b8b\u22121calcu-\nlated on \u03f5b. The eigenvalues of (Hj\nVR)\u22121are approximately given by v\u22121\nb+r((\u02c6vb\u22121)\u22121\u2212(vb\u22121)\u22121),\nwhere vb,\u02c6vb\u22121, and vb\u22121denote the eigenvalues of Hj\nMb,bHj\nMb\u22121, andHj\nMb\u22121, respectively.\nPlease kindly refer to Appendix A.3 for the detailed proof. Proposition 3 develops the relationship\nbetween the Hessian Hj\nVRused in the update rule of VR-MCL and that used in the update rule of\nMeta-CL (i.e., Hj\nMb). Similar to the analysis of the effect of Hessian in Secs. 2 and 3, we reach the\nfollowing conclusions.\n\u25e6For those wrongly estimated low-curvature directions by Meta-CL, i.e., \u02c6vb\u22121\u226bvb,vb\u22121(some\npotential crucial weights are erroneously viewed as unimportant), we have v\u22121\nb+r((\u02c6vb\u22121)\u22121\u2212\n(vb\u22121)\u22121)\u2248r(\u02c6vb\u22121)\u22121+ (1\u2212r)v\u22121\nb\u2264v\u22121\nb. This signifies that VR-MCL can prevent excessive updates\ntriggered by the wrongly estimated low-curvature direction of the Hessian, thereby retaining the\ncritical weights for previous tasks and improving the accuracy of its iterative updating formula.\n\u25e6For the high-curvature directions with large eigenvalues, i.e., \u02c6vb\u22121\u2248vb\u2248vb\u22121, we have v\u22121\nb+\nr((\u02c6vb\u22121)\u22121\u2212(vb\u22121)\u22121)\u2248v\u22121\nb. This suggests that VR-MCL can effectively ensure cautious\nupdates along these high-curvature directions of Meta-CL.\nTo sum up, Meta-CL is susceptible to inaccurate estimations of Hj\nMbdue to the random samples\ndrawn from M. In contrast, the proposed VR-MCL incorporates a regularization term on Hj\nMb,\nwhich effectively limits the model from making large updates and improves the precision of the\niterative update. As a result, VR-MCL can effectively preserve the important parameters for previous\ntasks, thereby maintaining their performance.", "start_char_idx": 0, "end_char_idx": 2770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f04ca86c-77d5-4068-821e-e839b3ec2a6a": {"__data__": {"id_": "f04ca86c-77d5-4068-821e-e839b3ec2a6a", "embedding": null, "metadata": {"page_label": "6", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6b75af7-ba77-46cb-8dcc-4a14b1b4a579", "node_type": "4", "metadata": {"page_label": "6", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8d66db3a5ec29316eab752df7046cfed2cf459dbce684a383205c3fe4912c437", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2610876-7f81-49f4-a38f-6159563de431", "node_type": "1", "metadata": {"page_label": "6", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "da2ad08a270546e984e214a6a53c1a04245bef4ec541875e08f8dbb13fa4354d", "class_name": "RelatedNodeInfo"}}, "text": "\u25e6For the high-curvature directions with large eigenvalues, i.e., \u02c6vb\u22121\u2248vb\u2248vb\u22121, we have v\u22121\nb+\nr((\u02c6vb\u22121)\u22121\u2212(vb\u22121)\u22121)\u2248v\u22121\nb. This suggests that VR-MCL can effectively ensure cautious\nupdates along these high-curvature directions of Meta-CL.\nTo sum up, Meta-CL is susceptible to inaccurate estimations of Hj\nMbdue to the random samples\ndrawn from M. In contrast, the proposed VR-MCL incorporates a regularization term on Hj\nMb,\nwhich effectively limits the model from making large updates and improves the precision of the\niterative update. As a result, VR-MCL can effectively preserve the important parameters for previous\ntasks, thereby maintaining their performance.\nTheorem 1 (Regret Bound of VR-MCL) .The regret in online continual learning follows CRj=\n\u02dcF(\u03b8)\u2212F(\u03b8\u2217), where \u02dcF(\u03b8) =Pj\ni=1Li(\u02c6\u03b8j),F(\u03b8\u2217) = min \u03b8Pj\ni=1Li(\u03b8).Assuming Fwith\u03c6-\nLipschitz hessian is \u00b5-strongly convex, G-Lipschitz, \u03b7-smooth and grounded on the four assumptions\nin Appendix A.4, \u03c3and M are two large constants in Assumption 3 and 4 respectively, and Tdenotes\nthe training iteration. Then with probability at least 1\u2212\u03b4for any \u03b4\u2208(0,1),we can get,\nCRj\u2264(logT+ 1)( F(\u03b81)\u2212F(\u03b8\u2217)) +LD2(logT+ 1)2\n2\n+LD2(logT+ 1)2\n2+\u0000\n16LD2+ 16\u03c3D+ 4M\u0001p\n2Tlog(8T/\u03b4) =\u02dcO(\u221a\nT).\nTheorem 1 illustrates that, under mild assumptions in a stochastic setting, VR-MCL attains a nearly\noptimal \u02dcO(\u221a\nT)regret bound w.h.p. for the online optimization problem. This theoretically demon-\nstrates the effectiveness of the proposed VR-MCL algorithm. Detailed proof see Appendix A.4.\n5 E XPERIMENTS\nDatasets and Settings. To verify the effectiveness of the proposed VR-MCL, we conduct compre-\nhensive experiments on commonly used datasets Seq-CIFAR10, as well as the longer task sequences\n6", "start_char_idx": 2103, "end_char_idx": 3825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b95984b-a08c-4bda-878d-da8bae1d48d1": {"__data__": {"id_": "6b95984b-a08c-4bda-878d-da8bae1d48d1", "embedding": null, "metadata": {"page_label": "7", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7368dbb-4b16-4821-a93b-f086b2ac0bd3", "node_type": "4", "metadata": {"page_label": "7", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fb175442101f7a83e4dc4c8d599f53ad49c09a51c6fa9f52a94d1cf511e75d50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fa8765b-2fa5-4aea-bf2d-00fa9621de19", "node_type": "1", "metadata": {}, "hash": "2ed511141910b6264c972459bf7ce93dad954284815b9978452c9518895fcba7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 2: Performance of Seq-CIFAR10 and longer task sequences Seq-CIFAR100, Seq-\nTinyImageNet with 95% confidence interval on reduced ResNet-18. The memory buffer size is\nset as 1000 (i.e., |M|=1000). All reported numbers are the average of 5 runs. Shaded areas are our\nmethods, and \u2018\u2013\u2019 indicates the result is omitted due to high instability.\nSeq-CIFAR10 Seq-CIFAR100 Seq-TinyImageNet\nMethod AAA Acc AAA Acc AAA Acc\nSGD 34.85 \u00b11.71 16.96 \u00b10.60 11.63 \u00b10.38 5.27\u00b10.28 8.99\u00b10.39 3.86\u00b10.20\nLWF 35.31 \u00b10.98 18.84 \u00b10.10 11.98 \u00b10.40 5.63\u00b10.37 9.21\u00b10.37 4.01\u00b10.29\nA-GEM 38.66 \u00b10.79 18.46 \u00b10.17 13.15 \u00b10.23 6.02\u00b10.17 9.81\u00b10.31 4.07\u00b10.18\nGEM 38.67 \u00b10.77 18.49 \u00b10.15 15.18 \u00b10.38 8.30\u00b10.58 10.99 \u00b10.32 5.15\u00b10.31\nER 55.53 \u00b12.58 43.83 \u00b14.84 23.19 \u00b10.38 16.07 \u00b10.88 19.45 \u00b10.40 11.13 \u00b10.39\nDER 45.85 \u00b11.62 29.87 \u00b12.95 13.35 \u00b10.36 6.12\u00b10.18 10.35 \u00b10.33 4.08\u00b10.13\nDER++ 64.22 \u00b10.70 52.29 \u00b11.86 19.88 \u00b10.43 11.79 \u00b10.65 14.75 \u00b10.15 8.26\u00b10.34\nCLSER 63.02 \u00b11.54 52.80 \u00b11.66 25.46 \u00b10.57 17.88 \u00b10.69 19.43 \u00b10.37 11.09 \u00b10.24\nOCM 66.14 \u00b10.95 53.39 \u00b11.00 22.54 \u00b10.79 14.40 \u00b10.82 10.45 \u00b10.65 4.53\u00b10.54\nER-OBC 65.82 \u00b10.91 54.85 \u00b12.16 25.54 \u00b10.25 17.21 \u00b10.92 20.11 \u00b10.31 11.51 \u00b10.18\nOn-EWC 38.44 \u00b10.50 17.12 \u00b10.51 11.81 \u00b10.42 5.88\u00b10.31 9.38\u00b10.17 3.65\u00b10.22\nIS 37.33 \u00b10.23 17.39 \u00b10.19 12.32 \u00b10.22 5.20\u00b10.18 8.73\u00b10.25 3.33\u00b10.28\nMER 50.99 \u00b10.65 36.92 \u00b12.42 \u2013 \u2013 \u2013 \u2013\nLa-MAML 42.98 \u00b11.60 33.43 \u00b11.21 12.55 \u00b10.39 11.78 \u00b10.65 11.10 \u00b10.70 6.74\u00b10.36\nVR-MCL 66.97 \u00b11.58 56.48 \u00b11.79 27.01 \u00b10.48 19.49 \u00b10.69 21.26 \u00b10.53 13.27 \u00b10.39\nSeq-CIFAR100 and Seq-TinyImageNet (Buzzega et al., 2020). Specifically, the Seq-CIFAR10\ndataset comprises 5 tasks, with each task containing 2 classes. In contrast, Seq-CIFAR100 consists\nof 10 tasks, each with 10 classes, while Seq-TinyImageNet includes 20 tasks, each encompassing\n10 classes. In this paper, we focus on the following settings of continual learning:\n\u2022Online CL: the data is fed to the model in small batches and trained in a single pass.", "start_char_idx": 0, "end_char_idx": 1989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fa8765b-2fa5-4aea-bf2d-00fa9621de19": {"__data__": {"id_": "9fa8765b-2fa5-4aea-bf2d-00fa9621de19", "embedding": null, "metadata": {"page_label": "7", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7368dbb-4b16-4821-a93b-f086b2ac0bd3", "node_type": "4", "metadata": {"page_label": "7", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fb175442101f7a83e4dc4c8d599f53ad49c09a51c6fa9f52a94d1cf511e75d50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b95984b-a08c-4bda-878d-da8bae1d48d1", "node_type": "1", "metadata": {"page_label": "7", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f8f61e5ba00a133b362fcd8d87516f3d71d5a7c6f7506e40d2a5ec8b08403672", "class_name": "RelatedNodeInfo"}}, "text": "Specifically, the Seq-CIFAR10\ndataset comprises 5 tasks, with each task containing 2 classes. In contrast, Seq-CIFAR100 consists\nof 10 tasks, each with 10 classes, while Seq-TinyImageNet includes 20 tasks, each encompassing\n10 classes. In this paper, we focus on the following settings of continual learning:\n\u2022Online CL: the data is fed to the model in small batches and trained in a single pass.\n\u2022Class Incremental: the model cannot get the oracle task index during testing.\nOur evaluation includes the metrics and experimental settings following the previous works on on-\nlineCL with a single-head (Caccia et al., 2021; Ji et al., 2020; Shim et al., 2021). We choose the\nfinal Averaged accuracy (Acc) across all tasks after sequential training on each task as the main\nmetric for comparing approaches. Moreover, under online CL, we use the Averaged Anytime Ac-\ncuracy (AAA) Caccia et al. (2021) to evaluate the model through the stream tasks. Let AAjdenote\nthe test average accuracy after training on Tj, then the evaluation metrics of AAA andAcc are:\nAAA =1\nNPN\nj=1(AAj), Acc =AAN.\nBaselines and Training Details. To more effectively validate the efficacy of VR-MCL, which\nimproves the Hessian component and thus better utilizes second-order information, we select key\nregularization-based CL methods (Husz \u00b4ar, 2017; Zenke et al., 2017) and Meta-CL methods (Riemer\net al., 2019; Gupta et al., 2020). We also opt for other representative SOTA CL methods (Chaudhry\net al., 2018; Lopez-Paz & Ranzato, 2017; Rolnick et al., 2019; Buzzega et al., 2020; Arani et al.,\n2021; Guo et al., 2022; Chrysakis & Moens, 2023) as baselines to further highlight the supe-\nrior performance of VR-MCL. For detailed information on each baseline method, please refer to\nAppendix C. For fair comparison among different CL methods, we train all the models using\nthe Stochastic Gradient Descent (SGD) optimizer under the online continual learning. Follow-\ning (Chrysakis & Moens, 2023; Lopez-Paz & Ranzato, 2017) and (Gupta et al., 2020) we utilize\ntwo backbones: reduced ResNet-18 and a smaller network named PcCNN which has only 3 conv-\nlayers. For the reduced ResNet18, we set both the batch size and the replay batch size (i.e., the batch\nsize sampled from the memory buffer M) as 32. For the smaller network PcCNN, we set both the\nbatch size and the replay bath size as 10. The momentum ratio rand the learning rate \u03b1of VR-MCL\nare both set as 0.25 for all experiments. For other training details please check Appendix C.\nTo assess the effectiveness of VR-MCL, we perform a comprehensive set of experiments and con-\nduct ablation studies to address the following six key questions:\nQuestion 1: How does VR-MCL perform on online CL benchmarks? We report results with\na 95%confidence interval on Seq-CIFAR10 and the longer task sequences Seq-CIFAR100, Seq-\nTinyImageNet in Table 2. The methods in the bottom row are the methods utilizing second-order\ninformation while the top row are the methods of other representative CL methods. It is evident that\nthe proposed VR-MCL substantially enhances the performance of CL methods employing second-\n7", "start_char_idx": 1593, "end_char_idx": 4719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c63eb826-e8a0-484f-88e2-9b26d5587079": {"__data__": {"id_": "c63eb826-e8a0-484f-88e2-9b26d5587079", "embedding": null, "metadata": {"page_label": "8", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85991672-7277-4561-809c-642434fefec2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2f5c66a2f23ef022e6da4f9221c2c9acf654935f1bbd0d1846a9692338f0374e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c02d0a5d-544e-4159-a655-f149336a4cbf", "node_type": "1", "metadata": {}, "hash": "24557e002d7b52be83d98848645a36c38f788f7c11c9756de203f1682b7ce81d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 3: Performance of Seq-CIFAR100 with 95% confidence interval (CI) on different memory\nbuffer sizes |M|. The backbone is reduced ResNet-18, and all numbers are the average of 5 runs.\nShaded areas are our methods, and \u2018\u2013\u2019 indicates the result is omitted due to high instability.\n|M|=200 |M|=600 |M|=1000\nMethod AAA Acc AAA Acc AAA Acc\nSGD 11.63 \u00b10.38 5.60\u00b10.28 11.63 \u00b10.38 5.60\u00b10.28 11.63 \u00b10.38 5.60\u00b10.28\nLWF 12.01 \u00b10.42 5.84\u00b10.21 12.12 \u00b10.41 5.97\u00b10.33 12.01 \u00b10.43 5.77\u00b10.32\nA-GEM 13.45 \u00b10.29 6.25\u00b10.25 13.36 \u00b10.21 6.00\u00b10.16 13.15 \u00b10.36 6.12\u00b10.18\nGEM 15.41 \u00b10.21 7.92\u00b10.35 14.92 \u00b10.24 7.89\u00b10.31 15.18 \u00b10.38 8.30\u00b10.58\nER 18.52 \u00b10.63 10.15 \u00b10.42 21.73 \u00b10.25 13.63 \u00b10.60 23.19 \u00b10.38 16.07 \u00b10.88\nDER 13.44 \u00b10.31 6.12\u00b10.12 13.57 \u00b10.27 6.23\u00b10.12 13.35 \u00b10.36 6.12\u00b10.18\nDER++ 17.91 \u00b10.47 9.59\u00b10.42 19.13 \u00b10.27 10.45 \u00b10.30 19.88 \u00b10.43 11.79 \u00b10.65\nCLSER 20.56 \u00b10.35 11.35 \u00b10.36 23.63 \u00b10.33 16.55 \u00b10.35 25.46 \u00b10.57 17.88 \u00b10.69\nOCM \u2013 \u2013 18.73 \u00b10.10 8.40\u00b10.37 22.54 \u00b10.79 14.40 \u00b10.82\nER-OBC 19.53 \u00b10.28 10.38 \u00b10.37 23.96 \u00b10.40 15.83 \u00b10.41 25.54 \u00b10.25 17.21 \u00b10.92\nOn-EWC 11.81 \u00b10.42 5.88\u00b10.31 11.81 \u00b10.42 5.88\u00b10.31 11.81 \u00b10.42 5.88\u00b10.31\nIS 12.32 \u00b10.22 5.20\u00b10.18 12.32 \u00b10.22 5.20\u00b10.18 12.32 \u00b10.22 5.20\u00b10.18\nLa-MAML 8.48 \u00b10.34 6.95\u00b10.27 10.07 \u00b11.10 9.05\u00b10.58 12.55 \u00b10.39 11.78 \u00b10.65\nVR-MCL 22.41 \u00b10.59 13.27 \u00b10.40 25.97 \u00b10.47 17.15 \u00b10.59 27.01 \u00b10.48 19.49 \u00b10.69\norder information, and also outperforms other representative SOTA CL methods by nearly 2%. It\nis noteworthy that the performance of VR-MCL is superior to both MER (Riemer et al., 2019)\nand La-MAML (Gupta et al., 2020), and significantly better than IS (Zenke et al., 2017) and On-\nEWC (Husz \u00b4ar, 2017). This observation aligns with our analysis in Table 1.\nQuestion 2: How does the performance gain from VR-MCL vary with buffer size |M|?In\nlarge-scale CL, it is impractical to store a large number of examples from previous tasks due to\nstorage constraints. Therefore, an ideal CL method should still perform well even with a modest\nmemory buffer.", "start_char_idx": 0, "end_char_idx": 2040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c02d0a5d-544e-4159-a655-f149336a4cbf": {"__data__": {"id_": "c02d0a5d-544e-4159-a655-f149336a4cbf", "embedding": null, "metadata": {"page_label": "8", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85991672-7277-4561-809c-642434fefec2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "2f5c66a2f23ef022e6da4f9221c2c9acf654935f1bbd0d1846a9692338f0374e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c63eb826-e8a0-484f-88e2-9b26d5587079", "node_type": "1", "metadata": {"page_label": "8", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bdf86c736d85db006313c235e55958b10cd89fc975222339dcde66753f70a665", "class_name": "RelatedNodeInfo"}}, "text": "It\nis noteworthy that the performance of VR-MCL is superior to both MER (Riemer et al., 2019)\nand La-MAML (Gupta et al., 2020), and significantly better than IS (Zenke et al., 2017) and On-\nEWC (Husz \u00b4ar, 2017). This observation aligns with our analysis in Table 1.\nQuestion 2: How does the performance gain from VR-MCL vary with buffer size |M|?In\nlarge-scale CL, it is impractical to store a large number of examples from previous tasks due to\nstorage constraints. Therefore, an ideal CL method should still perform well even with a modest\nmemory buffer. To investigate if VR-MCL is still effective with a smaller buffer size, we conduct\nexperiments on Seq-CIFAR100 with different |M| as shown in Table 3. The results show that the\nperformance of most methods improves with the increase of memory buffer size |M|. And our\nVR-MCL consistently outperforms other baselines, suggesting that VR-MCL is effective even with\nmodest buffer sizes. Note that GEM (Lopez-Paz & Ranzato, 2017) and A-GEM (Chaudhry et al.,\n2018) are insensitive to the buffer size complying with the results in Shim et al. (2021).\nQuestion 3: How effective is VR-MCL in dealing with increasingly non-stationary Settings\n(i.e., imbalanced CL)? In the imbalanced CL setting, where the number of samples varies across\ntasks, an imbalance issue arises in the samples stored in M. This introduces a high variance during\nthe calculation process of the hyper-gradient, thereby negatively affecting the impact on the implicit\nHessian. To assess the effectiveness of VR-MCL in addressing these challenges, we choose differ-\nent imbalanced settings and explore the performance of various methods under this setup in Table 4.\nSpecifically, we choose three imbalanced settings. The Normal means the total number of samples\nper streaming task from high to low, and the Reversed is the opposite. The Random setting, on the\nother hand, represents a situation where there is no specific pattern in the number of samples per\nstreaming task. From Table 4, we can see that the performance of most models degrades in this\nchallenging setup. However, VR-MCL significantly outperforms other baselines and even surpasses\nmethods such as CBRS (Chrysakis & Moens, 2020) and Coresets (Borsos et al., 2020), which are\nspecifically designed for imbalanced CL. This suggests that the VR-MCL has excellent performance\nunder the challenged imbalanced CL setting. The 95% CI for Table 4, as well as additional experi-\nmental results for different datasets and various imbalance ratios, can be found in Appendix D.4.\nQuestion 4: Whether the proposed method VR-MCL is still effective using different back-\nbones? To verify whether the improvement of VR-MCL is related to the backbone network, we\nchange the reduced ResNet-18 as a shallow network PcCNN following Gupta et al. (2020). The\noutcomes are provided in Table 5. It can be observed that VR-MCL shows a nearly 3% improve-\nment over the other best method in terms of both the AAA andAccmetrics. This indicates that the\nproposed VR-MCL remains effective across different backbone architectures.\nQuestion 5: How does VR-MCL compare to other variance reduction methods? As dis-\ncussed in Sec. 4.1, most existing popular variance reduction methods like SAG (Gower et al., 2020),\nSAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) cannot be directly applied since\nthey need to compute the full-batch gradient which is not accessible in online CL. Therefore, we\n8", "start_char_idx": 1484, "end_char_idx": 4944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae23b968-f18b-41f1-a4c5-408c77268c0e": {"__data__": {"id_": "ae23b968-f18b-41f1-a4c5-408c77268c0e", "embedding": null, "metadata": {"page_label": "9", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f951dd04-b13e-4a67-8333-0828e8420835", "node_type": "4", "metadata": {"page_label": "9", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c6a886aa0dc984f195fedd10f2de369a28a9f1efb86adb09dccb1a0a17a20f25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b818340f-eac2-4270-9f60-4bb0d6f1346f", "node_type": "1", "metadata": {}, "hash": "10b16224bc053da42494e2fdbbf42669499b3b32c5f3a0eaca4598a2b03c521e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\ncompare VR-MCL with other techniques that promise to reduce the variance: 1) VR-MCL1, which\nsimply increases the replay batch size ; 2) VR-MCL2, which incorporates SGD with the naive mo-\nmentum term. The outcomes shown in Table 6 indicate that: 1) Other variance reduction techniques\nare also effective, supporting our analysis in Sec. 4; 2) Our proposed momentum-based VR-MCL\nsignificantly outperforms other variance techniques, further demonstrating its effectiveness.\nTable 4: Performance on the imbalanced Seq-CIFAR10\n(|M|=1000) with imbalance ratio \u03b3=2. The Reversed\nmeans the total number of samples per streaming task\nfrom low to high. Full table see Appendix D.6.\n\u03b3=2 (Normal )\u03b3=2 (Reversed )\u03b3=2 (Random )\nMethods AAA Acc AAA Acc AAA Acc\nSGD 36.64 16.46 35.57 17.54 34.62 17.33\nA-GEM 38.50 17.64 36.72 17.43 37.79 17.85\nGEM 41.29 17.90 38.46 18.37 39.63 18.00\nER 52.50 33.10 46.58 28.49 43.78 31.10\nDER 46.95 16.82 40.62 18.63 41.44 18.78\nDER++ 62.02 44.14 58.22 39.21 60.25 42.83\nCLS-ER 61.37 47.75 54.92 40.51 56.60 47.48\nOn-EWC 38.85 16.92 37.41 17.79 37.26 14.35\nCBRS 59.07 43.81 57.57 44.20 58.16 44.66\nCoresets 61.11 45.37 58.12 45.80 58.56 45.63\nLa-MAML 36.64 29.17 32.08 31.17 42.52 31.24\nVR-MCL 65.06 49.82 61.16 51.36 61.91 50.74Table 5: Performance of Seq-CIFAR10\nwith 95% confidence interval on the\nsmall network PcCNN with |M|=200.\nThe results are the average of 5 runs.\nMethod AAA Acc\nSGD 37.84 \u00b10.03 17.58 \u00b10.20\nA-GEM 41.84 \u00b10.44 18.50 \u00b10.38\nGEM 45.07 \u00b10.52 22.52 \u00b11.02\nER 54.64 \u00b10.91 32.46 \u00b11.58\nDER 58.06 \u00b10.18 38.21 \u00b10.29\nDER++ 56.07 \u00b10.32 34.28 \u00b10.79\nCLS-ER 58.03 \u00b10.75 39.38 \u00b11.44\nER-OBC 56.75 \u00b10.68 38.10 \u00b11.28\nOn-EWC 38.72 \u00b10.58 17.05 \u00b10.08\nMER 55.50 \u00b10.38 32.01 \u00b11.22\nLa-MAML 46.36 \u00b10.98 29.45 \u00b10.51\nVR-MCL 60.88 \u00b10.22 43.41 \u00b10.30\nTable 6: Performance with 95% CI using various variance reduction techniques, where |M|= 1000 .", "start_char_idx": 0, "end_char_idx": 1902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b818340f-eac2-4270-9f60-4bb0d6f1346f": {"__data__": {"id_": "b818340f-eac2-4270-9f60-4bb0d6f1346f", "embedding": null, "metadata": {"page_label": "9", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f951dd04-b13e-4a67-8333-0828e8420835", "node_type": "4", "metadata": {"page_label": "9", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c6a886aa0dc984f195fedd10f2de369a28a9f1efb86adb09dccb1a0a17a20f25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae23b968-f18b-41f1-a4c5-408c77268c0e", "node_type": "1", "metadata": {"page_label": "9", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "84e822610c8a2f4c44b304b4a72201479ab153156e899315901174f278212c62", "class_name": "RelatedNodeInfo"}}, "text": "Seq-CIFAR10 Seq-CIFAR100 Seq-TinyImageNet\nMethod AAA Acc AAA Acc AAA Acc\nLa-MAML 42.98 \u00b11.60 33.43 \u00b11.21 12.55 \u00b10.39 11.78 \u00b10.65 11.10 \u00b10.70 6.74\u00b10.36\nVR-MCL166.74 \u00b12.49 55.02 \u00b13.70 26.15 \u00b10.53 19.02 \u00b10.45 20.39 \u00b11.17 12.09 \u00b11.30\nVR-MCL262.87 \u00b11.70 53.20 \u00b11.52 20.15 \u00b11.52 16.05 \u00b12.04 15.96 \u00b10.51 9.82\u00b10.45\nVR-MCL(ours) 66.97 \u00b11.58 56.48 \u00b11.79 27.01 \u00b10.48 19.49 \u00b10.69 21.26 \u00b10.53 13.27 \u00b10.39\n0 500 1000 1500 2000\n/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni000000560.000.250.500.751.001.25/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000003/uni00000039/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000046/uni00000048\n/uni00000030/uni00000048/uni00000057/uni00000044/uni00000010/uni00000026/uni0000002f\n/uni00000039/uni00000035/uni00000010/uni00000030/uni00000026/uni0000002f\nFigure 3: The relative variance of Meta-CL\nand the proposed VR-MCL.Question 6: Whether the proposed method VR-\nMCL can effectively reduce the variance of gra-\ndients? To explore this problem, following (Yang &\nKwok, 2022), we choose the relative variance metric\n(i.e.,E\u2225g\u03b8t\u2212E(g\u03b8t)\u22252\n\u2225E(g\u03b8t)\u22252), which can eliminate the influ-\nence of the gradient value on the variance. The out-\ncomes are shown in Fig. 3, where the Meta-CL refers\nto La-MAML (Gupta et al., 2020). It can be seen that\nthe proposed VR-MCL indeed has less variance dur-\ning training, complying with the analysis in Eqn. (5).\nOther Results. Due to space constraints, additional comprehensive experiments have been pro-\nvided in Appendix D, including other evaluation metrics (i.e., forgetting measure), hyperparameter\nselection (i.e., the selection of the momentum ratio rand learning rate \u03b1), and training time analysis.\n6 C ONCLUSION\nIn this paper, we revisited Meta-CL and first bridged it with regularization-based methods from the\nHessian matrix approximation perspective. Specifically, Meta-CL, through the use of hypergradient,\nimplicitly approximates the Hessian in an online manner. While this approach benefits from timely\nadaptation, it also grapples with high variance resulting from random memory buffer sampling.\nBuilding on this understanding, we proposed the VR-MCL method, which effectively reduces the\nvariance of the hypergradient and exhibits superior performance under the online continual learning\nsetting. We provide theoretical proof that VR-MCL imposes a regularization term on the implicitly\nestimated Hessian matrix. This prevents updates from moving excessively in the wrongly estimated\nlow-curvature directions and thus has a more accurate iterative update rule shown in Table 1. More-\nover, to enhance comprehension, we also provide the regret bound of our VR-MCL.\n9", "start_char_idx": 1903, "end_char_idx": 4790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f4ba791-5a5a-4728-a735-8208fa00ebbd": {"__data__": {"id_": "7f4ba791-5a5a-4728-a735-8208fa00ebbd", "embedding": null, "metadata": {"page_label": "10", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "097d0a30-4c1d-4e9a-8e0a-78fa3a40bdaa", "node_type": "4", "metadata": {"page_label": "10", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "64f5ce0c56bb79c6614c168a1e06ebac23a35430ccd2673b4c4e26065f2ed9b7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nACKNOWLEDGEMENT\nWe thank all the anonymous reviewers for their constructive suggestions on improving this paper.\nThis research was sponsored by the National Key R&D Program of China (2020YFA0713900), the\nRMGS 9229073, the China NSFC projects under contract 62272375, 12226004 and 62306233, the\nYoung Elite Scientists Sponsorship Program by CAST 2023QNRC001.\nREFERENCES\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection\nfor online continual learning. Advances in Neural Information Processing Systems , 32, 2019.\nZeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In\nInternational Conference on Machine Learning , pp. 699\u2013707. PMLR, 2016.\nElahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general contin-\nual learning method based on complementary learning system. In International Conference on\nLearning Representations , 2021.\nZal\u00b4an Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual\nlearning and streaming. Advances in Neural Information Processing Systems , 2020.\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark expe-\nrience for general continual learning: a strong, simple baseline. Advances in Neural Information\nProcessing Systems , 33:15920\u201315930, 2020.\nLucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky.\nNew insights on reducing abrupt representation change in online continual learning. International\nConference on Learning Representations , 2021.\nArslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient\nlifelong learning with a-gem. In International Conference on Learning Representations , 2018.\nAristotelis Chrysakis and Marie-Francine Moens. Online continual learning from imbalanced data.\nInInternational Conference on Machine Learning , pp. 1952\u20131961. PMLR, 2020.\nAristotelis Chrysakis and Marie-Francine Moens. Online bias correction for task-free continual\nlearning. In International Conference on Learning Representations , 2023.\nAshok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.\nAdvances in Neural Information Processing Systems , 32, 2019.\nAaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method\nwith support for non-strongly convex composite objectives. Advances in Neural Information\nProcessing Systems , 27, 2014.\nCong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex\noptimization via stochastic path-integrated differential estimator. Advances in Neural Information\nProcessing Systems , 31, 2018.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In International conference on machine learning , pp. 1126\u20131135. PMLR, 2017.\nChelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In\nInternational Conference on Machine Learning , pp. 1920\u20131930. PMLR, 2019.\nRobert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences ,\n3(4):128\u2013135, 1999.\nRobert M Gower, Mark Schmidt, Francis Bach, and Peter Richt \u00b4arik. Variance-reduced methods for\nmachine learning. Proceedings of the IEEE , 108(11):1968\u20131983, 2020.\n10", "start_char_idx": 0, "end_char_idx": 3398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dbc07b1-2a4b-4964-a714-2b868d248041": {"__data__": {"id_": "6dbc07b1-2a4b-4964-a714-2b868d248041", "embedding": null, "metadata": {"page_label": "11", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1246465d-6e2d-4544-b407-84a9a1652ebb", "node_type": "4", "metadata": {"page_label": "11", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9b48f7c279b351dcb9a8eb6be99e45178b7d3e60a34e25bb410213e020a05d3b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nYanan Gu, Xu Yang, Kun Wei, and Cheng Deng. Not just selection, but exploration: Online class-\nincremental continual learning via dual view consistency. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pp. 7442\u20137451, 2022.\nYiduo Guo, Bing Liu, and Dongyan Zhao. Online continual learning through mutual information\nmaximization. In International Conference on Machine Learning , 2022.\nGunshi Gupta, Karmesh Yadav, and Liam Paull. Look-ahead meta learning for continual learning.\nAdvances in Neural Information Processing Systems , 33:11588\u201311598, 2020.\nFerenc Husz \u00b4ar. On quadratic penalties in elastic weight consolidation. arXiv preprint\narXiv:1712.03847 , 2017.\nKhurram Javed and Martha White. Meta-learning representations for continual learning. Advances\nin Neural Information Processing Systems , 32, 2019.\nXu Ji, Joao Henriques, Tinne Tuytelaars, and Andrea Vedaldi. Automatic recall machines: Internal\nreplay, continual learning and the brain. arXiv preprint arXiv:2006.12323 , 2020.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance\nreduction. Advances in Neural Information Processing Systems , 26, 2013.\nRobert W Keener. Theoretical statistics: Topics for a core course . Springer, 2010.\nPrashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.\nA near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances in\nNeural Information Processing Systems , 34:30271\u201330283, 2021.\nChris Dongjoo Kim, Jinseo Jeong, and Gunhee Kim. Imbalanced continual learning with partition-\ning reservoir sampling. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XIII 16 , pp. 411\u2013428. Springer, 2020.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-\ning catastrophic forgetting in neural networks. Proceedings of the national academy of sciences ,\n114(13):3521\u20133526, 2017.\nHyunseo Koh, Minhyuk Seo, Jihwan Bang, Hwanjun Song, Deokki Hong, Seulki Park, Jung-Woo\nHa, and Jonghyun Choi. Online boundary-free continual learning by scheduled data prior. In The\nEleventh International Conference on Learning Representations .\nZhengfeng Lai, Chao Wang, Sen-ching Cheung, and Chen-Nee Chuah. Sar: Self-adaptive refine-\nment on pseudo labels for multiclass-imbalanced semi-supervised learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4091\u20134100, 2022a.\nZhengfeng Lai, Chao Wang, Henrry Gunawan, Sen-Ching S Cheung, and Chen-Nee Chuah.\nSmoothed adaptive weighting for imbalanced semi-supervised learning: Improve reliability\nagainst unknown distribution data. In International Conference on Machine Learning , pp. 11828\u2013\n11843. PMLR, 2022b.\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis\nand machine intelligence , 40(12):2935\u20132947, 2017.\nDavid Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning.\nAdvances in Neural Information Processing Systems , 30, 2017.\nJames L McClelland, Bruce L McNaughton, and Randall C O\u2019Reilly. Why there are complementary\nlearning systems in the hippocampus and neocortex: insights from the successes and failures of\nconnectionist models of learning and memory. Psychological review , 102(3):419, 1995.\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learning and motivation , volume 24, pp. 109\u2013165.\nElsevier, 1989.\n11", "start_char_idx": 0, "end_char_idx": 3739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "180de058-3c24-4192-9eaa-ff4f3b44a64a": {"__data__": {"id_": "180de058-3c24-4192-9eaa-ff4f3b44a64a", "embedding": null, "metadata": {"page_label": "12", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75729f0b-27fe-4751-ad50-dd5a1a24779e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "58290c879ae871030af969685b9d34d00f1be07c1ebad24a648d370cb4759cd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1f883d3-3e01-4189-a9cb-c3d819efabce", "node_type": "1", "metadata": {}, "hash": "49ec4e2a102cf9218580466fef3fc723e3b9629af4446916f4dade36c6bc6a8f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:\nFrom convex minimization to submodular maximization. The Journal of Machine Learning Re-\nsearch , 21(1):4232\u20134280, 2020.\nLam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak \u00b4a\u02c7c. Sarah: A novel method for machine\nlearning problems using stochastic recursive gradient. In International Conference on Machine\nLearning , pp. 2613\u20132621. PMLR, 2017.\nGerman I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual\nlifelong learning with neural networks: A review. Neural Networks , 113:54\u201371, 2019.\nIosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of\nProbability , pp. 1679\u20131706, 1994.\nJathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Mubarak Shah.\nitaml: An incremental task-agnostic meta-learning approach. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 13588\u201313597, 2020.\nJathushan Rajasegaran, Chelsea Finn, and Sergey Levine. Fully online meta-learning without task\nboundaries. arXiv preprint arXiv:2202.00263 , 2022.\nMatthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald\nTesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-\nence. In International Conference on Learning Representations , 2019.\nHippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations\nfor overcoming catastrophic forgetting. Advances in Neural Information Processing Systems , 31,\n2018.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience\nreplay for continual learning. Advances in Neural Information Processing Systems , 32, 2019.\nDongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. On-\nline class-incremental continual learning with adversarial shapley value. In Proceedings of the\nAAAI Conference on Artificial Intelligence , volume 35, pp. 9630\u20139638, 2021.\nShengyang Sun, Daniele Calandriello, Huiyi Hu, Ang Li, and Michalis Titsias. Information-\ntheoretic online memory selection for continual learning. In International Conference on Learn-\ning Representations .\nJohannes V on Oswald, Dominic Zhao, Seijin Kobayashi, Simon Schug, Massimo Caccia, Nicolas\nZucchet, and Jo \u02dcao Sacramento. Learning where to learn: Gradient sparsity in meta and continual\nlearning. Advances in Neural Information Processing Systems , 2021.\nLing Xiao Wang, Kevin Huang, Tengyu Ma, Quanquan Gu, and Jing Huang. Variance-reduced\nfirst-order meta-learning for natural language processing tasks. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies , pp. 2609\u20132615, 2021.\nQuanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, and Deyu Meng. Cba: Improving online\ncontinual learning via continual bias adaptor. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pp. 19082\u201319092, 2023a.\nRenzhen Wang, Xixi Jia, Quanziang Wang, Yichen Wu, and Deyu Meng. Imbalanced semi-\nsupervised learning with bias adaptive classifier. In 11th International Conference on Learning\nRepresentations (ICLR 2023) , 2023b.\nYichen Wu, Jun Shu, Qi Xie, Qian Zhao, and Deyu Meng. Learning to purify noisy labels via meta\nsoft label corrector. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35,\npp.", "start_char_idx": 0, "end_char_idx": 3540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1f883d3-3e01-4189-a9cb-c3d819efabce": {"__data__": {"id_": "e1f883d3-3e01-4189-a9cb-c3d819efabce", "embedding": null, "metadata": {"page_label": "12", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75729f0b-27fe-4751-ad50-dd5a1a24779e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "58290c879ae871030af969685b9d34d00f1be07c1ebad24a648d370cb4759cd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "180de058-3c24-4192-9eaa-ff4f3b44a64a", "node_type": "1", "metadata": {"page_label": "12", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "162cbd0387c0b0c8416a2b522acf51254f8ba809cec1f78e22237a3064539e4d", "class_name": "RelatedNodeInfo"}}, "text": "2609\u20132615, 2021.\nQuanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, and Deyu Meng. Cba: Improving online\ncontinual learning via continual bias adaptor. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pp. 19082\u201319092, 2023a.\nRenzhen Wang, Xixi Jia, Quanziang Wang, Yichen Wu, and Deyu Meng. Imbalanced semi-\nsupervised learning with bias adaptive classifier. In 11th International Conference on Learning\nRepresentations (ICLR 2023) , 2023b.\nYichen Wu, Jun Shu, Qi Xie, Qian Zhao, and Deyu Meng. Learning to purify noisy labels via meta\nsoft label corrector. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35,\npp. 10388\u201310396, 2021.\nYichen Wu, Long-Kai Huang, and Ying Wei. Adversarial task up-sampling for meta-learning. Ad-\nvances in Neural Information Processing Systems , 35:31102\u201331115, 2022.\n12", "start_char_idx": 2873, "end_char_idx": 3727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac247e9c-fb42-48b9-ae91-6f3c97c3feb6": {"__data__": {"id_": "ac247e9c-fb42-48b9-ae91-6f3c97c3feb6", "embedding": null, "metadata": {"page_label": "13", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8a68a8d-8160-4632-85a0-5771f9c59475", "node_type": "4", "metadata": {"page_label": "13", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "061945f3f8cd24bd4ef85e344076873df3bb4e2fc5112fa7308c095e752cc998", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nHansi Yang and James Kwok. Efficient variance reduction for meta-learning. International Confer-\nence on Machine Learning , 2022.\nYang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, and Yuan Jiang. Adaptive deep models\nfor incremental learning: Considering capacity scalability and sustainability. In Proceedings of\nthe 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp.\n74\u201382, 2019.\nYang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, Yuan Jiang, and Jian Yang. Cost-effective\nincremental deep model: Matching model capacity with the least sampling. IEEE Transactions\non Knowledge and Data Engineering , 2021.\nDong Yin, Mehrdad Farajtabar, Ang Li, Nir Levine, and Alex Mott. Optimization and generaliza-\ntion of regularization-based continual learning: a loss approximation viewpoint. arXiv preprint\narXiv:2006.10974 , 2020.\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\nInInternational Conference on Machine Learning , pp. 3987\u20133995. PMLR, 2017.\n13", "start_char_idx": 0, "end_char_idx": 1076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db8fbd06-1ca9-47bd-94ab-567be42f71f3": {"__data__": {"id_": "db8fbd06-1ca9-47bd-94ab-567be42f71f3", "embedding": null, "metadata": {"page_label": "14", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc393f62-a902-44e6-808e-d7f8c16b6c4f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "92993a863c3ed02a8b8d2e22c6897e01cc5fba8617e8fd5d12bae737d7a1dfd3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA D ETAILED PROOF\nA.1 P ROOF OF PROPOSITION 1\nProposition 1. In regularization-based continual learning, if the model parameter \u03b8is searched\nwithin the neighborhood set \u222aj\u22121\ni=1NiwithNi={\u03b8:d(\u03b8,\u02c6\u03b8i)< \u03b4i}, then the iterative update rule of \u03b8\napproximately is\n\u03b8:\u2248\u03b8\u2212\u03b1(H1+H2+\u00b7\u00b7\u00b7+Hj\u22121)\u22121\u2207\u03b8Lj(\u03b8)\nProof. The empirical loss LonT[1:j]can be approximate as,\nL=j\u22121X\ni=1Li(\u03b8) +Lj(\u03b8)\u2248 Lprox\nj\u22121(\u03b8) +Lj(\u03b8)\n(1)=j\u22121X\ni=1Li(\u02c6\u03b8i)|{z}\n(a)+ (\u03b8\u2212\u02c6\u03b8i)\u22a4\u2207\u03b8Li(\u02c6\u03b8i)| {z }\n(b)+1\n2(\u03b8\u2212\u02c6\u03b8i)\u22a4Hi(\u03b8\u2212\u02c6\u03b8i)\n| {z }\n(c)+Lj(\u03b8)\n(2)\u2248j\u22121X\ni=11\n2(\u03b8\u2212\u02c6\u03b8i)\u22a4Hi(\u03b8\u2212\u02c6\u03b8i) +Lj(\u03b8)\nHere, (1) is expand each loss Liusing Taylor series at point \u02c6\u03b8ito compute Lprox\nj\u22121(\u03b8), where i=\n1, ..., j\u22121. Step (2) is due to the term (a) is not related to \u03b8and thus can be discarded. Furthermore,\nsince the training process at the end of each task (i.e., \u02c6\u03b8i) is usually converged, the gradient \u2207\u03b8Li(\u02c6\u03b8i)\nis near zero and the term (b) can be ignored in practice. Then we can derive\u2202L\n\u2202\u03b8as,\n\u2202L\n\u2202\u03b8=j\u22121X\ni=1Hi(\u03b8\u2212\u02c6\u03b8i) +\u2207\u03b8Lj(\u03b8).\nIf the parameters \u03b8are searched in the neighborhood set \u222aj\u22121\ni=1Ni, where Ni={\u03b8:d(\u03b8,\u02c6\u03b8i)< \u03b4i},\nthen we can make the approximationPj\u22121\ni=1Hi(\u03b8\u2212\u02c6\u03b8i)\u223c(Pj\u22121\ni=1Hi)(\u03b8\u2212\u02c6\u03b8j\u22121)Husz \u00b4ar (2017).\nLet\u2202L\n\u2202\u03b8= 0, we can obtain the iterative update formula of Tjas:\n\u03b8:\u2248\u03b8\u2212\u03b1(H1+H2+\u00b7\u00b7\u00b7+Hj\u22121)\u22121\u2207\u03b8Lj(\u03b8),\nwhere \u03b1= 1to maintain formal consistency with other iterative rules in Table 1, (H1+H2+\u00b7\u00b7\u00b7+\nHj\u22121)are the Hessian matrices of previous tasks computed at the end of training of each Ti, and\n\u2207\u03b8Lj(\u03b8)is the gradient of the j-th task.\nA.2 P ROOF OF PROPOSITION 2\nProposition 2. For MCL with single inner step adaption, suppose that \u03b8(K)is located in the \u03f5-\nneighborhood N(\u03b8\u2217, \u03f5)of the optimal model parameter \u03b8\u2217= argmin\u03b8L[1:j](\u03b8(K)),Lis\u00b5-smooth,\nand\u03b2 <p\n\u03b4/|\u2207\u03b8Lj(\u03b8)\u2212(\u2207\u03b8Lj(\u03b8))2|where \u03b4is a small number. Thus, the iterative update rule\napproximately is\n\u03b8:\u2248\u03b8\u2212\u03b1(Hj\nM)\u22121\u2207\u03b8Lj(\u03b8),\nProof. (Single inner step adaption) For simplicity, we analyze the MCL with single inner step\nadaption (i.e., K= 1) at first. The objective function of one step inner-loop MCL could be formu-\nlated as,\n\u03b8\u2217= argmin\u03b8L[1:j](\u03b8(K)), s.t. \u03b8 (K)=\u03b8\u2212\u03b2\u2207\u03b8Lj(\u03b8).\nThen the derivative of loss L[1:j](\u03b8(K))over\u03b8is,\n\u2202L[1:j](\u03b8(K))\n\u2202\u03b8=\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)\u2202\u03b8(K)\n\u03b8. (1)\n14", "start_char_idx": 0, "end_char_idx": 2201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b8ced69-a224-40b4-bfeb-54825d65166f": {"__data__": {"id_": "5b8ced69-a224-40b4-bfeb-54825d65166f", "embedding": null, "metadata": {"page_label": "15", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92a62af3-d928-4365-af5b-a53c78692b78", "node_type": "4", "metadata": {"page_label": "15", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4da5ba8f12ae606be5265956450f5ec6a95709885f4e2e88293f5aad76ee6da6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72e4257f-3b54-4d69-ba5e-b193bea61493", "node_type": "1", "metadata": {}, "hash": "e8d5386442cafac718477213aa183967e3c49ffeb2d300bda3fea1b145901930", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nFor the first term\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K), we take the Taylor expansion at \u03b8,\n\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)=\u2207\u03b8(K)L[1:j](\u03b8(K))\u2248 \u2207 \u03b8(K)L[1:j](\u03b8)+Hj\nM(\u03b8(K)\u2212\u03b8) + (\u03b8(K)\u2212\u03b8)T\u2297T\u2297(\u03b8(K)\u2212\u03b8),\n(2)\nwhere Hj\nM=\u22072\n\u03b8(K)L[1:j](\u03b8)andTdenote the Hessian matrix and the third-order symmetric tensor,\nrespectively, and \u2297represents the Kronecker product.\nSince we hope to find the optimal parameters \u03b8\u2217so as to achieve the lowest L[1:j], which means\nL[1:j](\u03b8(K))\n\u2202\u03b8= 0. According to Eqn. (1), that is approximately to let\u2202L[1:j](\u03b8(K))\n\u2202\u03b8(K)= 0.\nSuppose \u03b4is a small number, \u03b2 <p\n\u03b4/|\u2207\u03b8Lj(\u03b8)\u2212(\u2207\u03b8Lj(\u03b8))2|, and there exists an \u03f5-neighborhood\nN(\u00b7)such that parameters \u03b8\u2217\u2208 N(\u03b8\u2212\u03b2\u2207\u03b8Lj(\u03b8), \u03f5). Since Lis\u00b5-smooth, which means \u2225\u2207L(\u03b8)\u2212\n\u2207L(\u03b8\u2032)\u22252\u2264\u00b5\u2225\u03b8\u2212\u03b8\u2032\u22252. Then we can get the following equations according to Eqn. (2),\n\u2202L[1:j](\u03b8\u2217)\n\u2202\u03b8\u2217=\u2207\u03b8\u2217L[1:j](\u03b8\u2217)\n\u2248 \u2207 \u03b8\u2217L[1:j](\u03b8) +\u22072\n\u03b8\u2217L[1:j](\u03b8)(\u03b8\u2217\u2212\u03b8) + (\u03b8\u2217\u2212\u03b8)T\u2297T\u2297(\u03b8\u2217\u2212\u03b8)\n\u2248 \u2207 \u03b8(K)L[1:j](\u03b8) +\u22072\n\u03b8(K)L[1:j](\u03b8)(\u03b8\u2217\u2212\u03b8) + (\u03b8(K)\u2212\u03b8)T\u2297T\u2297(\u03b8(K)\u2212\u03b8) +o(\u03f5)\n=\u2207\u03b8(K)L[1:j](\u03b8) +\u22072\n\u03b8(K)L[1:j](\u03b8)(\u03b8\u2217\u2212\u03b8) +C\u2299(\u03b8(K)\u2212\u03b8)T(\u03b8(K)\u2212\u03b8) +o(\u03f5)\n\u2248o(\u00b5\u03f5) +\u22072\n\u03b8(K)L[1:j](\u03b8)(\u03b8\u2217\u2212\u03b8) +\u03b22(\u2207\u03b8Lj(\u03b8))2+o(\u03f5)\n\u2248o(\u00b5\u03f5) +\u22072\n\u03b8(K)L[1:j](\u03b8)(\u03b8\u2217\u2212\u03b8) +\u03b22\u2207\u03b8Lj(\u03b8) +o(\u03b4) +o(\u03f5)\n\u2248 \u22072\n\u03b8(K)L[1:j](\u03b8)(\u03b8\u2217\u2212\u03b8) +\u03b22\u2207\u03b8Lj(\u03b8) +o(\u03b4) +o(\u03f5) +o(\u00b5\u03f5) = 0 ,\n(3)\nwhere Cmeans a constant vector, \u2299denotes the element-wise multiplication operator. Then it is\neasy to get \u03b8:=\u03b8\u2212\u03b22(\u22072\n\u03b8(K)L[1:j](\u03b8))\u22121\u2207\u03b8Lj(\u03b8) =\u03b8\u2212\u03b1(Hj\nM)\u22121\u2207\u03b8Lj(\u03b8), where \u03b1=\u03b22and\nHj\nM=\u22072\n\u03b8(K)L[1:j](\u03b8).\n(K inner steps adaption) For MCL with K steps of inner-loop updating, \u03b8(K)=\u03b8\u2212\u03b2\u2207Lj\n(1)(\u03b8)\u2212\n\u03b2\u2207Lj\n(2)(\u03b8(1))\u2212...\u2212\u03b2\u2207Lj\n(K)(\u03b8(K\u22121)). Similar to Eqn. (3), we can get\n\u03b8:=\u03b8\u2212\u03b1(Hj\nM)\u22121(K\u22121X\ni=0\u2207Lj\n(i+1)(\u03b8(K\u22121))),\nwhere \u03b8(i)denotes the model parameters after the i-th inner loop adaptation and \u03b8(0)=\u03b8.", "start_char_idx": 0, "end_char_idx": 1695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72e4257f-3b54-4d69-ba5e-b193bea61493": {"__data__": {"id_": "72e4257f-3b54-4d69-ba5e-b193bea61493", "embedding": null, "metadata": {"page_label": "15", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92a62af3-d928-4365-af5b-a53c78692b78", "node_type": "4", "metadata": {"page_label": "15", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4da5ba8f12ae606be5265956450f5ec6a95709885f4e2e88293f5aad76ee6da6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b8ced69-a224-40b4-bfeb-54825d65166f", "node_type": "1", "metadata": {"page_label": "15", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "88657b633c4c98ce1e70286d54f8835dd6536fd113c5f47702950bee53171b57", "class_name": "RelatedNodeInfo"}}, "text": "(K inner steps adaption) For MCL with K steps of inner-loop updating, \u03b8(K)=\u03b8\u2212\u03b2\u2207Lj\n(1)(\u03b8)\u2212\n\u03b2\u2207Lj\n(2)(\u03b8(1))\u2212...\u2212\u03b2\u2207Lj\n(K)(\u03b8(K\u22121)). Similar to Eqn. (3), we can get\n\u03b8:=\u03b8\u2212\u03b1(Hj\nM)\u22121(K\u22121X\ni=0\u2207Lj\n(i+1)(\u03b8(K\u22121))),\nwhere \u03b8(i)denotes the model parameters after the i-th inner loop adaptation and \u03b8(0)=\u03b8. It is worth\nnoting that in MCL with multiple updating steps, the hypergradient incorporates more gradients\nfrom the inner loop, which is the main difference from the one-step inner loop MCL. However, this\ndifference does not affect the variance reduction analysis presented in Proposition 3.\nA.3 P ROOF OF PROPOSITION 3\nA.3.1 P ROOF OF LEMMA 1\nLemma 1. (The linear combination of two invertible matrices is invertible under finite conditions.)\nLetA= [a1, a2, ..., a n]\u2208Rn\u00d7nandB= [b1, b2, ..., b n]\u2208Rn\u00d7ndenote two square matrices,\nwhere ai\u2208Rn,bi\u2208Rnare the i-th column vector of the matrix AandB. Suppose: 1) A and B\nare invertible ; 2) for any \u03bbi\u0338= 0, i={1,2, ..n},(Pn\ni=1\u03bbiai)(Pn\ni=1\u03bbibi)\u22650, then the linear\ncombination of AandB(i.e.,rA+ (1\u2212r)B) is invertible, r\u2208(0,1).\nProof. (By contradiction.)\n(Step-1) We assume rA+(1\u2212r)B, where r\u2208(0,1), is not invertible, which means the square\nmatrix rA+ (1\u2212r)B= [ra1+ (1\u2212r)b1, ..., ra n+ (1\u2212r)bn]is singular.\n15", "start_char_idx": 1406, "end_char_idx": 2648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7ee6a1b-726f-44ef-b183-313db6ecbb9f": {"__data__": {"id_": "b7ee6a1b-726f-44ef-b183-313db6ecbb9f", "embedding": null, "metadata": {"page_label": "16", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0869ce3-4467-4487-a5ec-5e285d44a0ed", "node_type": "4", "metadata": {"page_label": "16", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c4b8bf66cd78d9f980455246e0cd4ffa088cb32a1112f7c6be9a30c8e3c074d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c97bcd6-29da-4385-9f05-ad1f9641c326", "node_type": "1", "metadata": {}, "hash": "897357cb9fde86fe36575fc87b66befe58a08aa7db0b4570cfeb3741369f7080", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n(Step-2) Then there must exists \u03bbi\u0338= 0, i={1,2, ..n}, such thatPn\ni=1\u03bbi(rai+(1\u2212r)bi) = 0 .\nThat is equivalent to rPn\ni=1\u03bbiai+ (1\u2212r)Pn\ni=1\u03bbibi= 0. According to condition 2, it can deduce\nthatPn\ni=1\u03bbiai=Pn\ni=1\u03bbibi= 0. This is contradict with the condition that AandBare invertible.\n(Step-3) Since assuming rA+ (1\u2212r)Bis not invertible leads to a contradiction, therefore,\nrA+ (1\u2212r)Bis convertible.\nA.3.2 P ROOF OF PROPOSITION 3\nProposition 3. Assume that the batch size for inner step adaptation is sufficiently large. Let Hj\nMb=\n\u22072\n\u03b8b(K)L[1:j]b(\u03b8b)=[h1\nb,h2\nb,\u00b7\u00b7\u00b7,hD\nb]denote the Hessian at \u03b8bcalculated on \u03f5b, where hd\nbis the d-th\ncolumn vector of Hj\nMb. Similarly, bHj\nMb\u22121=[\u02c6h1\nb\u22121,\u00b7\u00b7\u00b7,\u02c6hD\nb\u22121]denotes the Hessian for the momentum\nterm at the (b\u22121)-th iteration. If (PD\nd=1\u03bbdhd\nb)(PD\nd=1\u03bbd\u02c6hd\nb\u22121)\u22650holds for any \u03bbd\u0338= 0,d=\n{1,2,\u00b7\u00b7\u00b7, D}, then we have the following iterative update rule for VR-MCL,\n\u03b8:\u2248\u03b8\u2212\u03b1(Hj\nVR)\u22121\u2207\u03b8Lj(\u03b8),\nwhere (Hj\nVR)\u22121=(Hj\nMb)\u22121+r((bHj\nMb\u22121)\u22121\u2212(Hj\nMb\u22121)\u22121)withHj\nMb\u22121denoting the Hessian at \u03b8b\u22121cal-\nculated on \u03f5b. The eigenvalues of (Hj\nVR)\u22121are approximately given by v\u22121\nb+r((\u02c6vb\u22121)\u22121\u2212(vb\u22121)\u22121),\nwhere vb,\u02c6vb\u22121, and vb\u22121denote the eigenvalues of Hj\nMb,bHj\nMb\u22121, andHj\nMb\u22121, respectively.\nProof. Letg\u03f5b\n\u03b8bdenote the gradient of the b-th outer step. In proposition 2, the updating formula of\nMCL is \u03b8:=\u03b8\u2212\u03b1(Hj\nM)\u22121\u2207\u03b8Lj(\u03b8)which means the gradient g\u03f5b\n\u03b8bcan be rewrite in the form of\n(Hj\nMb)\u22121\u2207\u03b8Lj(\u03b8). When we adopt the variance reduction method, let mdenote the momentum\npart, then we can get,\nm1=g\u03f51\n\u03b81= (Hj\nM1)\u22121\u2207\u03b8Lj(\u03b81),\nm2=g\u03f52\n\u03b82+r(m1\u2212g\u03f52\n\u03b81) = (HMj\n2)\u22121\u2207\u03b8Lj(\u03b82) +r((Hj\nM1)\u22121\u2207\u03b8Lj(\u03b8)\u2212(Hj\nM1)\u22121\u2207\u03b8Lj(\u03b81)),\n\u00b7\u00b7\u00b7\nmb=g\u03f5b\n\u03b8b+r(mb\u22121\u2212g\u03f5b\n\u03b8b\u22121),\nwhere (Hj\nM1)\u22121and\u2207\u03b8Lj(\u03b8)are the Hessian and corresponding gradient in g\u03f52\n\u03b81. When there\nhas a sufficiently large batch size on the current task, then \u2207\u03b8Lj(\u03b82)\u2248 \u2207 \u03b8Lj(\u03b81)\u2248 \u2207 \u03b8Lj(\u03b81).\nTherefore, we can rewrite m2as,\nm2=g\u03f52\n\u03b82+r(m1\u2212g\u03f52\n\u03b81) ={(Hj\nM2)\u22121+r((Hj\nM1)\u22121\u2212(HM1)\u22121)}\u2207\u03b8Lj(\u03b82).", "start_char_idx": 0, "end_char_idx": 1985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c97bcd6-29da-4385-9f05-ad1f9641c326": {"__data__": {"id_": "2c97bcd6-29da-4385-9f05-ad1f9641c326", "embedding": null, "metadata": {"page_label": "16", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0869ce3-4467-4487-a5ec-5e285d44a0ed", "node_type": "4", "metadata": {"page_label": "16", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c4b8bf66cd78d9f980455246e0cd4ffa088cb32a1112f7c6be9a30c8e3c074d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7ee6a1b-726f-44ef-b183-313db6ecbb9f", "node_type": "1", "metadata": {"page_label": "16", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "11d6bf4014a83ccb352dbe19735166f3f449f8a1a4ffa085b0565083e4ba9675", "class_name": "RelatedNodeInfo"}}, "text": "When there\nhas a sufficiently large batch size on the current task, then \u2207\u03b8Lj(\u03b82)\u2248 \u2207 \u03b8Lj(\u03b81)\u2248 \u2207 \u03b8Lj(\u03b81).\nTherefore, we can rewrite m2as,\nm2=g\u03f52\n\u03b82+r(m1\u2212g\u03f52\n\u03b81) ={(Hj\nM2)\u22121+r((Hj\nM1)\u22121\u2212(HM1)\u22121)}\u2207\u03b8Lj(\u03b82).\nSince (Hj\nM1)\u22121is highly related with (Hj\nM2)\u22121, we can get,\n(Hj\nM2)\u22121+r((Hj\nM1)\u22121\u2212(Hj\nM1)\u22121)\u2248(1\u2212r)(Hj\nM2)\u22121+r(Hj\nM1)\u22121\n\u2022 For m2,Hj\nM1= [h1\n1, h2\n1, ..., hD\n1]andHj\nM2= [h1\n2, h2\n2, ..., hD\n2]are also highly related. Based on the\nassumption that for any \u03bbd\u0338= 0, d={1,2, ..D},(PD\nd=1\u03bbdhd\n1)(PD\nd=1\u03bbdhd\n2)\u22650.According to\nLemma 1 , we can get r(Hj\nM1)\u22121+ (1\u2212r)(Hj\nM2)\u22121is invertible, and thus there exist a matrix\n(bHM2)\u22121=r(HM1)\u22121+ (1\u2212r)(HM2)\u22121.\n\u2022 For mb, the previous assumption also holds, which is for Hj\nMb= [h1\nb, ..., hD\nb],bHj\nM(b\u22121)=\n[\u02c6h1\nb\u22121, ...,\u02c6hD\nb\u22121], for any \u03bbd\u0338= 0, d={1,2, ..D},(PD\nd=1\u03bbdhd\nb)(PD\nd=1\u03bbd\u02c6hd\nb\u22121)\u22650. Ac-\ncording to this recurrence relation, we can get (bHMb)\u22121=r(bHMb\u22121)\u22121+ (1\u2212r)(HMb)\u22121. Let\nHj\nVR=bHMb, we can get the following iterative update rule of VR-MCL,\n\u03b8:\u2248\u03b8\u2212\u03b1(Hj\nVR)\u22121\u2207\u03b8Lj(\u03b8),\n16", "start_char_idx": 1783, "end_char_idx": 2802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd69ea8e-9768-4950-b3da-d66371154db5": {"__data__": {"id_": "bd69ea8e-9768-4950-b3da-d66371154db5", "embedding": null, "metadata": {"page_label": "17", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2c519fe-18bb-473f-9883-092ac3946401", "node_type": "4", "metadata": {"page_label": "17", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c1454e0476eb8b328062e06704bfb1c7e116abcc6c920428d16636036c5b15f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ce94892-c4ab-4376-81fe-d465f57051f8", "node_type": "1", "metadata": {}, "hash": "4e2d08271ba9e0c0fb91759d8dadc5078faf00b1f312a6c14d7512d3469f584a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA.4 R EGRET BOUND OF VR-MCL\nThis section aims to establish the regret bound of VR-MCL. Our method involves three key parts:\nFirstly, we define the objective form of Regret Bound in continual learning. Secondly, we prove\nthat updating the objective of VR-MCL is equivalent to updating the latent optimal loss function, by\nbounding the gradient of VR-MCL with the latent optimal gradients. Finally, we derive the regret\nbound of VR-MCL based on the aforementioned steps.\n(Part I) We first define the Regret Bound in continual learning.\nDefinition 1 (The Regret Bound in CL) .Consider an online learning scenario where an agent\nmakes a decision by choosing an action \u03b8t\u2208 Rdat each time step t= 1, ..., T , and experiences a\ncorresponding loss Lt(\u03b8t). The primary objective of the agent is to minimize the difference between\nits cumulative loss and that of the best action in hindsight, which is commonly known as the regret.\nRT=PT\nt=1Lt(\u03b8t)\u2212min\u03b8\u2208\u0398PT\nt=1Lt(\u03b8)\nSimilarly, in MCL, for the current task Tj, the goal is to minimize CRj=\u02dcF(\u03b8)\u2212F(\u03b8\u2217), where\n\u02dcF(\u03b8) :=Pj\ni=1Li(\u02c6\u03b8j) and F(\u03b8\u2217) = min \u03b8Pj\ni=1Li(\u03b8).\nThis proof is grounded on certain assumptions, which are listed below:\nAssumption 1. The compact convex set C \u2286Rdhas diameter D, i.e.,\u2200\u03b8, \u03b8\u2032\u2208 C,\u2225\u03b8\u2212\u03b8\u2032\u2225 \u2264D.\nAssumption 2. The function F(\u00b7)is G-Lipschitz, \u00b5-strongly convex, \u03c6-smooth (i.e., has \u03c6-Lipschitz\ngradients) and has \u03ba-Lipschitz hessian. Then, according to the Theorem 1 in Finn et al. (2019), when\nthe step size \u03b1 <{1\n2\u03c6,\u00b5\n8\u03baG},the gradient \u2207\u02dcF(x, \u03f5t)is L-Lipschitz ( L=9\u03c6\n8) over the constraint set\nC, that is\n\u2225\u2207\u02dcF(\u03b8, \u03f5t)\u2212 \u2207\u02dcF(\u03b8\u2032, \u03f5t)\u2225 \u2264L\u2225\u03b8\u2212\u03b8\u2032\u2225=9\u03c6\n8\u2225\u03b8\u2212\u03b8\u2032\u2225,\u2200\u03b8, \u03b8\u2032\u2208 C.\nAssumption 3. Let\u02dcFtdenote the t-th training step, then the distance between our computed\n\u2207\u02dcFt(\u03b8, \u03f5t)and the gradient derived from the optimal function is bounded over the constraint set\nC, i.e., for any \u03b8\u2208 C, t\u2208 {1, ..., T}, there exists \u03c32<\u221esuch that with probability 1,\n\u2225\u2207\u02dcFt(\u03b8, \u03f5t)\u2212 \u2207F(\u03b8)\u22252\u2264\u03c32\nAssumption 4. LetFt(\u03b8)mean the optimal t-th training step, and the difference of Ft(\u03b8)andF(\u03b8)\nis bounded over the constraint set C, i.e.,\u2200\u03b8\u2208 C, t\u2208 {1, ..., T}, there exists M2<\u221esuch that\nwith probability 1,\n\u2225Ft(\u03b8)\u2212F(\u03b8)\u22252\u2264M2\n(Part II) In the following, we will demonstrate that updating the objective function of VR-MCL can\nbe regarded as updating the latent optimal loss function. This is achieved by bounding the gradient\nof VR-MCL with the latent optimal gradients. Specifically, the updating gradient can be expressed\nasmt=\u2207\u02dcF(\u03b8t, \u03f5t) +r(mt\u22121\u2212 \u2207\u02dcF(\u03b8t\u22121, \u03f5t)). Let \u03b1denote the small learning rate and define\n\u2206t=mt\u2212 \u2207F(\u03b8t), our aim is to prove that \u2206tis bounded. Here, we suppose the momentum ratio\nr= (1\u2212\u03c1t), the learning rate \u03b1=\u03b7t, and \u03c1t=\u03b7t= 1/(t+ 1)\u03b3, \u03b3\u2208(0,1].\nLemma 2.", "start_char_idx": 0, "end_char_idx": 2740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ce94892-c4ab-4376-81fe-d465f57051f8": {"__data__": {"id_": "5ce94892-c4ab-4376-81fe-d465f57051f8", "embedding": null, "metadata": {"page_label": "17", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2c519fe-18bb-473f-9883-092ac3946401", "node_type": "4", "metadata": {"page_label": "17", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c1454e0476eb8b328062e06704bfb1c7e116abcc6c920428d16636036c5b15f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd69ea8e-9768-4950-b3da-d66371154db5", "node_type": "1", "metadata": {"page_label": "17", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d9ec640079d2556df307e958a31419755701cd23818202d6e59eb7f1f11ee998", "class_name": "RelatedNodeInfo"}}, "text": "This is achieved by bounding the gradient\nof VR-MCL with the latent optimal gradients. Specifically, the updating gradient can be expressed\nasmt=\u2207\u02dcF(\u03b8t, \u03f5t) +r(mt\u22121\u2212 \u2207\u02dcF(\u03b8t\u22121, \u03f5t)). Let \u03b1denote the small learning rate and define\n\u2206t=mt\u2212 \u2207F(\u03b8t), our aim is to prove that \u2206tis bounded. Here, we suppose the momentum ratio\nr= (1\u2212\u03c1t), the learning rate \u03b1=\u03b7t, and \u03c1t=\u03b7t= 1/(t+ 1)\u03b3, \u03b3\u2208(0,1].\nLemma 2. If Assumptions 1,2,3 are satisfied, \u2200t\u22651, \u03b40\u2208(0,1), we have with probability at least\n1\u2212\u03b40,\n\u2225\u2206t\u2225 \u22642\u0010\n2LD+3\u03b3\u03c3\n3\u03b3\u22121\u0011\n(t+ 1)\u2212\u03b3/2p\n2 log (4 /\u03b40)\nLemma 2 demonstrates that the gradient approximation error \u2225\u2206t\u2225is bounded with high probability\nastincreases.\n17", "start_char_idx": 2347, "end_char_idx": 2994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a465e95-2188-43aa-8bce-fb3d0869f27f": {"__data__": {"id_": "2a465e95-2188-43aa-8bce-fb3d0869f27f", "embedding": null, "metadata": {"page_label": "18", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecfcc254-dc58-4cdb-a5a9-25a68d1cc523", "node_type": "4", "metadata": {"page_label": "18", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7aa83c543aa3e905e322744ae40630cd29872390c3af47f4bd1075c08abaed84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47320d82-9459-4f0b-8e33-736486fa077c", "node_type": "1", "metadata": {}, "hash": "48483bbcd16f5753fd6afeb8b37df837a2c7cad3247da5f5cce320fd1687e759", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nProof. We derive the formulation of \u2206tat first.\n\u2206t=mt\u2212 \u2207F(\u03b8t)\n= (1\u2212\u03c1t)\u2206t\u22121+(1\u2212\u03c1t)(\u2207\u02dcFt(\u03b8t, \u03f5t)\u2212\u2207\u02dcFt(\u03b8t\u22121, \u03f5t)\u2212(\u2207F(\u03b8t)\u2212\u2207F(\u03b8t\u22121)))\n+\u03c1t(\u2207\u02dcFt(\u03b8t, \u03f5t)\u2212\u2207F(\u03b8t))\n=\u03c4Y\nk=2(1\u2212\u03c1k)\u22061+tX\n\u03c4=2tY\nk=\u03c4(1\u2212\u03c1k)(\u2207\u02dcF\u03c4(\u03b8\u03c4, \u03f5\u03c4)\u2212\u2207\u02dcF\u03c4(\u03b8\u03c4\u22121, \u03f5\u03c4)\u2212(\u2207F(\u03b8\u03c4)\u2212\u2207F(\u03b8\u03c4\u22121)))\n+tX\n\u03c4=2\u03c1\u03c4tY\nk=\u03c4+1(1\u2212\u03c1k)(\u2207\u02dcFt(\u03b8\u03c4, \u03f5\u03c4)\u2212 \u2207F(\u03b8\u03c4))\nLet\u2206t=Pt\n\u03c4=1\u03b6t,\u03c4, where \u03b6t,1=rt\u22121\u22061and for \u03c4 >1,\u03b6t,\u03c4=Qt\nk=\u03c4(1\u2212\u03c1k)(\u2207\u02dcF\u03c4(\u03b8\u03c4, \u03f5\u03c4)\u2212\n\u2207\u02dcF\u03c4(\u03b8\u03c4\u22121, \u03f5\u03c4)\u2212(\u2207F(\u03b8\u03c4)\u2212\u2207F(\u03b8\u03c4\u22121))) + \u03c1\u03c4Qt\nk=\u03c4+1(1\u2212\u03c1k)(\u2207\u02dcF\u03c4(\u03b8\u03c4, \u03f5\u03c4)\u2212\u2207F(\u03b8\u03c4)). Recall\n\u22061=\u2207\u02dcF(\u03b81, \u03f51)\u2212\u2207F(\u03b81). We observe that E[\u03b6t,\u03c4\u2225F\u03c4\u22121] = 0 whenF\u03c4\u22121is the \u03c3-field generate\nby{L1, \u03f51, ...,L\u03c4\u22121, \u03f5\u03c4\u22121}. Therefore, {\u03b6t,\u03c4}t\n\u03c4=1is a martingale difference sequence. Then, we\nwill derive upper bounds of \u2225\u03b6t,\u03c4\u2225. We initially proved the following results,\ntY\nk=\u03c4(1\u2212\u03c1k) =tY\nk=\u03c4\u0012\n1\u22121\n(k+ 1)\u03b3\u0013\n=tY\nk=\u03c4(k+ 1)\u03b3\u22121\n(k+ 1)\u03b3\u2264tY\nk=\u03c4k\u03b3\n(k+ 1)\u03b3=\u03c4\u03b3\n(t+ 1)\u03b3,\nwhere the inequality holds from the concavity of h(x) =x\u03b3for any x\u22650. Based on this inequality\nwe can bound \u2225\u03b6t,1\u2225as follows,\n\u2225\u03b6t,1\u2225 \u22642\u03b3\n(t+ 1)\u03b3\r\r\r\u2207\u02dcF1(\u03b81, \u03f51),\u2207F(\u03b81)\r\r\r\u22642\u03b3\u03c3\n(t+ 1)\u03b1def=ct,1,\nwhere the second inequality holds according to Assumption 3.", "start_char_idx": 0, "end_char_idx": 1092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47320d82-9459-4f0b-8e33-736486fa077c": {"__data__": {"id_": "47320d82-9459-4f0b-8e33-736486fa077c", "embedding": null, "metadata": {"page_label": "18", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecfcc254-dc58-4cdb-a5a9-25a68d1cc523", "node_type": "4", "metadata": {"page_label": "18", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7aa83c543aa3e905e322744ae40630cd29872390c3af47f4bd1075c08abaed84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a465e95-2188-43aa-8bce-fb3d0869f27f", "node_type": "1", "metadata": {"page_label": "18", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "43010ef633cc5f625253ebff34cdc5c140d970632235c33ab9bb78b01f2bea51", "class_name": "RelatedNodeInfo"}}, "text": "Based on this inequality\nwe can bound \u2225\u03b6t,1\u2225as follows,\n\u2225\u03b6t,1\u2225 \u22642\u03b3\n(t+ 1)\u03b3\r\r\r\u2207\u02dcF1(\u03b81, \u03f51),\u2207F(\u03b81)\r\r\r\u22642\u03b3\u03c3\n(t+ 1)\u03b1def=ct,1,\nwhere the second inequality holds according to Assumption 3. When \u03c4\u22651, we can get,\n\u2225\u03b6t,\u03c4\u2225 \u2264tY\nk=\u03c4(1\u2212\u03c1k)(\r\r\r\u2207\u02dcF\u03c4(\u03b8\u03c4, \u03f5\u03c4)\u2212 \u2207F\u03c4(\u03b8\u03c4\u22121, \u03f5\u03c4)\r\r\r+\u2225\u2207F(\u03b8\u03c4)\u2212 \u2207F(\u03b8\u03c4\u22121)\u2225)\n+\u03c1\u03c4tY\nk=\u03c4+1(1\u2212\u03c1k)\u2225\u2207\u02dcF\u03c4(\u03b8\u03c4, \u03f5\u03c4)\u2212 \u2207F(\u03b8\u03c4)\u2225\n\u22642L\u2225\u03b8\u03c4\u2212\u03b8\u03c4\u22121\u2225tY\nk=\u03c4(1\u2212\u03c1k) +\u03c3\u03c1\u03c4tY\nk=\u03c4+1(1\u2212\u03c1k)\n\u22642LD\u03c1 \u03c4\u22121tY\nk=\u03c4(1\u2212\u03c1k) +\u03c3\u03c1\u03c4tY\nk=\u03c4+1(1\u2212\u03c1k)(4)\nWe can further derive the final term as follows,\n\u03c1\u03c4tY\nk=\u03c4+1(1\u2212\u03c1k) =\u03c1\u03c4\n\u03c1\u03c4\u22121(1\u2212\u03c1\u03c4) \n\u03c1\u03c4\u22121tY\nk=\u03c4(1\u2212\u03c1k)!\n\u22641\n1\u2212\u03c1\u03c4 \n\u03c1\u03c4\u22121tY\nk=\u03c4(1\u2212\u03c1k)!\n\u22641\n1\u22121/3\u03b3 \n\u03c1\u03c4\u22121tY\nk=\u03c4(1\u2212\u03c1k)!\n\u22643\u03b3\n3\u03b3\u22121 \n\u03c1\u03c4\u22121tY\nk=\u03c4(1\u2212\u03c1k)!\n.(5)\nBy plugging Eqn. (5) into Eqn. (4, we can get, \u2200\u03c4\u22651\n\u2225\u03b6t,\u03c4\u2225 \u2264(2LD+3\u03b3\u03c3\n3\u03b3\u22121)\u03c1\u03c4\u22121tY\nk=\u03c4(1\u2212\u03c1k)def=ct,\u03c4\nAccording to Theorem 3.5 in Pinelis (1994), we can get \u2200\u03bb\u22650,\nP(\u2225\u2206t\u2225 \u2265\u03bb)\u22644 exp \n\u2212\u03bb2\n4Pt\n\u03c4=1c2\nt,\u03c4!\n, (6)\n18", "start_char_idx": 911, "end_char_idx": 1712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d2392be-f487-41f7-a543-37c1998ee372": {"__data__": {"id_": "6d2392be-f487-41f7-a543-37c1998ee372", "embedding": null, "metadata": {"page_label": "19", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e26dbcb-1217-41a6-9162-602b56510796", "node_type": "4", "metadata": {"page_label": "19", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5a7a2118b85d3009a33dadfb58b465dafeda67c2d5961933e66f5638cda4c694", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nwhere ct,1is defined in Eqn. (A.4) and ct,\u03c4,(\u03c4\u22651)is defined in Eqn. (4). Then we can get,\ntX\n\u03c4=1c2\nt,\u03c4=c2\nt,1+tX\n\u03c4=2c2\nt,\u03c4=22\u03b3\u03c32\n(t+ 1)2\u03b3+\u0012\n2LD+3\u03b3\u03c3\n3\u03b3\u22121\u00132tX\n\u03c4=2 \n\u03c1\u03c4\u22121tY\nk=\u03c4(1\u2212\u03c1k)!2\n\u226422\u03b3\u03c32\n(t+ 1)2\u03b3+\u0010\n2LD+3\u03b3\u03c3\n3\u03b1\u22121\u00112\n(t+ 1)\u03b3\u2264\u0000\n(\u221a\n2)\u03b3\u03c3\u00012\n(t+ 1)\u03b3+\u0010\n2LD+3\u03b3\u03c3\n3\u03b3\u22121\u00112\n(t+ 1)\u03b3\u22642\u0010\n2LD+3\u03b3\u03c3\n3\u03b3\u22121\u00112\n(t+ 1)\u03b3,\n(7)\nwhere the last inequality is because (\u221a\n2)\u03b3\u22643\u03b3/(3\u03b3\u22121)\u2200\u03b3\u2208(0,1]. Substituting Eqn. (7) into\nEqn. (6) and setting \u03bb= 2\u0010\n2LD+3\u03b3\u03c3\n3\u03b3\u22121\u0011\n(t+1)\u2212\u03b3/2p\n2 log (4 /\u03b40)for some \u03b40\u2208(0,1), we have\nwith probability at least 1\u2212\u03b40,\n\u2225\u2206t\u2225 \u22642\u0010\n2LD+3\u03b3\u03c3\n3\u03b3\u22121\u0011\n(t+ 1)\u2212\u03b3/2p\n2 log (4 /\u03b40),\nwhich is the expected result.\n(Part III) Next, we will establish the regret bound CRjfor continual learning.\nTheorem 1 (Regret Bound of VR-MCL) .IfFis convex and all aforementioned four Assumptions\nare satisfied, then with probability at least 1\u2212\u03b4for any \u03b4\u2208(0,1),\nCRj\u2264(logT+ 1)( F(\u03b81)\u2212F(\u03b8\u2217)) +LD2(logT+ 1)2\n2\n+LD2(logT+ 1)2\n2+\u0000\n16LD2+ 16\u03c3D+ 4M\u0001p\n2Tlog(8T/\u03b4) =\u02dcO(\u221a\nT)\nProof. We initially define a sequence st=\u02dcFt(\u03b8t)\u2212Ft(\u03b8\u2217)\u2212(F(\u03b8t)\u2212F(\u03b8\u2217)), t= 1, ..., T . It can be\nobserved that E[st| Ft\u22121] = 0 , where Ft\u22121is the \u03c3-algebra generated by {F1, \u03f51, ..., F t\u22121, \u03f5t\u22121}.\nIt means that {st}T\nt=1is a martingale difference sequence. According to Assumption 4, we have\n\u2225st\u2225=\u2225\u02dcFt(\u03b8t)\u2212Ft(\u03b8\u2217)\u2212(F(\u03b8t)\u2212F(\u03b8\u2217)\u2225 \u22642M\nAccording to Theorem 3.5 in Pinelis (1994), we can get\nP \n\u2225TX\nt=1st\u2225 \u2265\u03bb!\n\u22644 exp\u0012\n\u2212\u03bb2\n16TM2\u0013\n,\nwhere \u03bb >0. By setting \u03bb= 4Mp\nTlog(8/\u03b4), we have with probability at least 1\u2212\u03b4/2,\nTX\nt=1st=TX\nt=1(\u02dcFt(\u03b8t)\u2212Ft(\u03b8\u2217))\u2212TX\nt=1(F(\u03b8t)\u2212F(\u03b8\u2217))\u22644Mp\nTlog(8/\u03b4).\nBy rearranging the above terms, we get\nCRj=\u02dcF(\u03b8)\u2212F(\u03b8\u2217) =TX\nt=1(\u02dcFt(\u03b8t)\u2212Ft(\u03b8\u2217))\u2264TX\nt=1(F(\u03b8t)\u2212F(\u03b8\u2217)) + 4 Mp\nTlog(8/\u03b4).(8)\n19", "start_char_idx": 0, "end_char_idx": 1669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc25314a-1ece-4ea9-a79d-f5ef7d4fcccf": {"__data__": {"id_": "cc25314a-1ece-4ea9-a79d-f5ef7d4fcccf", "embedding": null, "metadata": {"page_label": "20", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1d5aeb3-c4c4-41df-a150-c313644ee1ae", "node_type": "4", "metadata": {"page_label": "20", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "624c4d9137a8bd276d91293209223d573a2c3bc036c30a3c9d08110dd2b1fa33", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nThen according to Lemma 2 in (Mokhtari et al., 2020), we can derive the first term in Eqn. (8) as\nfollows,\nF(\u03b8t)\u2212F(\u03b8\u2217) = (1 \u2212\u03b7t)(F(\u03b8t\u22121)\u2212F(\u03b8\u2217)) +\u03b7tD\u2225\u2206t\u22121\u22252+LD2\u03b72\nt\n2\n\u2264t\u22121Y\n\u03c4=1(1\u2212\u03b7\u03c4)(F(\u03b81)\u2212F(\u03b8\u2217)) +t\u22121X\n\u03c4=1\u03b7\u03c4(D\u2225\u2206\u03c4\u22252+LD2\u03b7\u03c4\n2)t\u22121Y\nk=\u03c4+1(1\u2212\u03b7k)\n=1\nt(F(\u03b81)\u2212F(\u03b8\u2217)) +t\u22121X\n\u03c4=11\n\u03c4+ 1(D\u2225\u2206\u03c4\u22252+LD2\n2(\u03c4+ 1))\u03c4+ 1\nt\n=1\nt(F(\u03b81)\u2212F(\u03b8\u2217)) +1\ntt\u22121X\n\u03c4=1(D\u2225\u2206\u03c4\u22252+LD2\n2(\u03c4+ 1))\n\u22641\nt(F(\u03b81)\u2212F(\u03b8\u2217)) +D\ntt\u22121X\n\u03c4=1\u2225\u2206\u03c4\u22252+LD2logt\n2t\n(9)\nSumming Eqn. (9) from t= 1toT, we obtain,\nTX\nt=1F(\u03b8t)\u2212F(\u03b8\u2217)\u2264TX\nt=11\nt(F(\u03b81)\u2212F(\u03b8\u2217)) +TX\nt=1t\u22121X\n\u03c4=1D\nt\u2225\u2206\u03c4\u22252+TX\nt=1LD2logt\n2t\n\u2264TX\nt=11\nt(F(\u03b81)\u2212F(\u03b8\u2217)) +TX\nt=1t\u22121X\n\u03c4=1D\nt\u2225\u2206\u03c4\u22252+LD2logT\n2TX\nt=11\nt\n\u2264(logT+ 1)( F(\u03b81)\u2212F(\u03b8\u2217)) +TX\nt=1t\u22121X\n\u03c4=1D\nt\u2225\u2206\u03c4\u22252+LD2(logT+ 1)2\n2\n(10)\nAccording to Lemma 2, we can obtain the second term is,\nTX\nt=1t\u22121X\n\u03c4=1D\nt\u2225\u2206\u03c4\u2225 \u22644(LD2+\u03c3D)p\n2 log(8 T/\u03c3)TX\nt=1t\u22121X\n\u03c4=11\nt\u221a\u03c4+ 1\n\u22644(LD2+\u03c3D)p\n2 log(8 T/\u03c3)TX\nt=12\u221a\nt\nt\n\u226416(LD2+\u03c3D)p\n2Tlog(8T/\u03c3)(11)\nSubstituting Eqn. (11) into Eqn. (10), we have with probability at least 1\u2212\u03b4/2,\nTX\nt=1F(\u03b8t)\u2212F(\u03b8\u2217)\u2264log(T+1)(F(\u03b81)\u2212F\u2217(\u03b8))+LD2(logT+ 1)2\n2+16(LD2+\u03c3D)p\n2Tlog(8T/\u03b4)\n(12)\nCombining Eqn. (12) with Eqn. (8) we have with probability at least 1\u2212\u03b4,\nCRj\u2264(logT+ 1)( F(\u03b81)\u2212F(\u03b8\u2217)) +LD2(logT+ 1)2\n2\n+LD2(logT+ 1)2\n2+\u0000\n16LD2+ 16\u03c3D+ 4M\u0001p\n2Tlog(8T/\u03b4) =\u02dcO(\u221a\nT)\nB R ELATED WORKS\nRegularization-based methods (or parameter regularization methods) usually construct a\nquadratic regularization term to slow down the learning of weights that are important to prior tasks.\nAs we know, the Fisher information matrix is equivalent to the Hessian matrix if the loss function\n20", "start_char_idx": 0, "end_char_idx": 1546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11c1b280-2e77-4edc-a668-9d8365c8d72b": {"__data__": {"id_": "11c1b280-2e77-4edc-a668-9d8365c8d72b", "embedding": null, "metadata": {"page_label": "21", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3ad39fa-e5c4-420a-bfe6-1d890e2db942", "node_type": "4", "metadata": {"page_label": "21", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c4fd35fa6d84d9abc54df8e85321b723b6ccbe208e87b9758267fa408980fb74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d87e85b-da3c-4927-872b-a0413cc7932d", "node_type": "1", "metadata": {}, "hash": "6fd6abcc6f10ebb5244e44d17ab8e0b6105e58b53495527084a7a19c70ad052a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nis negative log-likelihood and we get a ground truth probabilistic model (Chapter 4.5 of (Keener,\n2010)). In this way, EWC (Kirkpatrick et al., 2017) and on-EWC (Husz \u00b4ar, 2017), as the first work\napplying parameter regularization, utilize the diagonal Fisher information matrix to approximate\nthe Hessian matrix, while Kronecker factored Laplace approximation (KFLA) (Ritter et al., 2018)\nexploits Kronecker factored Laplace to compute the Fisher information matrix with off-diagonal el-\nements. To account for the trajectory of model training, IS (Zenke et al., 2017) proposes an online\nestimation of a diagonal matrix and assigns weights based on its contribution to loss decay. On the\nother hand, in class-incremental learning, IADM (Yang et al., 2019) and CE-IDM (Yang et al., 2021)\nreport that different layers have varying characteristics. Shallow layers with limited representation\ntend to converge faster, while deep layers with powerful discrimination abilities tend to converge\nmore slowly. Therefore, based on EWC (Kirkpatrick et al., 2017), IADM (Yang et al., 2019) learns\nan online layer-wise importance matrix.\nRehearsal-based methods address catastrophic forgetting by replaying previous task samples from\na memory buffer. Experience Replay (ER) (Rolnick et al., 2019) directly sample data from previous\ntasks and put them jointly train with current data. On the basis of ER, recent studies have further\nextended this idea. Meta Experience Replay (MER) (Riemer et al., 2019) views replay as a meta-\nlearning problem for maximizing the transfer from previous tasks and minimizing the interference.\nGradient-based Sample Selection (GSS) (Aljundi et al., 2019) pays attention to the samples stored\nin the memory module and hopes to increase the diversity of samples in the gradient space. Gradient\nEpisodic Memory (GEM) (Lopez-Paz & Ranzato, 2017) and its lightweight variant Averaged-GEM\n(A-GEM) (Chaudhry et al., 2018) formulate optimization constraints such that gradients between\ncurrent and old training data are aligned to further determine the final direction of optimization.\nDark Experience Replay (DER++) (Buzzega et al., 2020) adds the knowledge distillation part on top\nof ER to regularize the logits of samples stored in the memory buffer. CLSER (Arani et al., 2021)\nintroduces an innovative approach to episodic replay (ER) by presenting a dual memory framework.\nThis framework incorporates both short-term and long-term semantic memories, allowing for their\ninteraction with the episodic memory. CBA (Wang et al., 2023a) proposes to address the issue of\nrecency bias by introducing a specially designed bias attractor, guided by the memory buffer M.\nMeta-Continual Learning (Meta-CL) is a class of methods that apply meta-learning (Finn et al.,\n2017; Wu et al., 2022; 2021; Wang et al., 2023b) in continual learning. From the bi-level optimiza-\ntion perspective to analyze meta-learning, its inner-loop (or lower-level) optimization usually fits the\ntraining data, while the outer-loop (higher-level) centralized test data aims to increase the model\u2019s\ngeneralization ability. Therefore, meta-learning is well suited for continual learning, as we want\nthe model to fit the current task and still perform well on all observed tasks (i.e., previous and cur-\nrent tasks). Meta Experience Replay (MER) (Riemer et al., 2019), inspired by GEM (Lopez-Paz &\nRanzato, 2017), utilizes replay to incentivize the alignment of gradients between old and new tasks\nto further maximize the transfer from previous tasks and minimize the interference. Online-aware\nMeta Learning (OML) (Javed & White, 2019) adopts a pre-training algorithm to learn an optimal\nrepresentation offline, which is frozen when training in the downstream tasks. However, this offline\ntraining manner and assumption that the training data is a correlated data stream limit its applicabil-\nity to practical scenarios.", "start_char_idx": 0, "end_char_idx": 3945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d87e85b-da3c-4927-872b-a0413cc7932d": {"__data__": {"id_": "2d87e85b-da3c-4927-872b-a0413cc7932d", "embedding": null, "metadata": {"page_label": "21", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3ad39fa-e5c4-420a-bfe6-1d890e2db942", "node_type": "4", "metadata": {"page_label": "21", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c4fd35fa6d84d9abc54df8e85321b723b6ccbe208e87b9758267fa408980fb74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11c1b280-2e77-4edc-a668-9d8365c8d72b", "node_type": "1", "metadata": {"page_label": "21", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "9b566f4b9ded4c19d5702cfb63fde723b2a56487d8e546c1b5d20222a622b601", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, meta-learning is well suited for continual learning, as we want\nthe model to fit the current task and still perform well on all observed tasks (i.e., previous and cur-\nrent tasks). Meta Experience Replay (MER) (Riemer et al., 2019), inspired by GEM (Lopez-Paz &\nRanzato, 2017), utilizes replay to incentivize the alignment of gradients between old and new tasks\nto further maximize the transfer from previous tasks and minimize the interference. Online-aware\nMeta Learning (OML) (Javed & White, 2019) adopts a pre-training algorithm to learn an optimal\nrepresentation offline, which is frozen when training in the downstream tasks. However, this offline\ntraining manner and assumption that the training data is a correlated data stream limit its applicabil-\nity to practical scenarios. Besides, MER (Riemer et al., 2019) and OML (Javed & White, 2019) use a\nsingle sample at one time during the inner-loop optimization, which is time-consuming and not suit-\nable for large-scale CL. To alleviate these problems, Look-ahead Meta learning (La-MAML) (Gupta\net al., 2020) proposed an online gradient-based meta-learning algorithm and proved it is equivalent\nto a simplified version of gradient alignment. On the other hand, some Meta-CL algorithms (Ra-\njasegaran et al., 2020; 2022) focus on combining meta-learning with task-agnostic continual learning\nthat does not require task information during training and testing. Besides, the variance reduction\nhas also been integrated with first-order meta-learning (Yang & Kwok, 2022; Wang et al., 2021).\nIn this work, we focus on the online continual learning setting and provide a variance reduction\nMeta-CL (second-order) based on our novel understanding of Meta-CL, it also links Meta-CL with\nregularization-based methods.\nOnline and Imbalanced Continual learning , different from the traditional offline CL setting, are\nmore challenging CL settings that have not been explored as much. For online CL scenarios, the\ninitial methods still have been developed based on similar ways in offline CL (Lopez-Paz & Ranzato,\n2017; Chaudhry et al., 2018; Aljundi et al., 2019). Recently, there has been increasing interest in\ndeveloping online CL methods that can fully utilize the single-pass data stream to decrease the\nperformance difference between online and offline CL. OCDVC (Gu et al., 2022) advocates for\n21", "start_char_idx": 3149, "end_char_idx": 5510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6771a3f1-af1f-4899-a947-c06cbebc62a9": {"__data__": {"id_": "6771a3f1-af1f-4899-a947-c06cbebc62a9", "embedding": null, "metadata": {"page_label": "22", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e34da759-c53d-4e2e-8e45-ad9067ea6b07", "node_type": "4", "metadata": {"page_label": "22", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "75a5b25b7f059b74d2f803b2bcc5104777b5710f0ae06ab3a13beb7e8559b58e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e07832d-d775-4c9c-9490-106dab31ea79", "node_type": "1", "metadata": {}, "hash": "8c95f6da7890c4b92c23e23baedd8e258af99fcb2cb9e6d778aff048225b460b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nthe dual view consistency strategy, while SDP (Koh et al.) utilizes knowledge distillation to fully\nleverage the data stream. OBC (Chrysakis & Moens, 2023) proposes a simple approach to mitigate\nthe bias of online CL by changing the optimization manner of the output layer model. OCM (Guo\net al., 2022) combines these motivations and aims to reduce feature bias and fully utilize streaming\ndata through mutual information maximization. However, none of these methods directly focus on\nthe performance drop between online and offline CL and analyze the underlying reasons, which is a\nmotivation of this work.\nForimbalanced CL , where the sample number of each task is different (Lai et al., 2022b;a), most\nworks aim to directly design a sampling strategy that can balance the distribution of samples re-\ntrieved from the memory buffer that stores data from previous tasks. CBRS (Chrysakis & Moens,\n2020) introduced a novel mechanism for class-balancing reservoir sampling based on the sample\ndistribution within the buffer. PRS (Kim et al., 2020) utilizes current data stream statistics to deter-\nmine the sample-in and sample-out process, maintaining a balanced replay memory. Additionally,\nInfoRS (Sun et al.) introduces a stochastic information theoretic reservoir sampler to select and store\nthe informative samples. Coresets (Borsos et al., 2020) employs the bi-level optimization technique\nfor the same purpose. Different from these methods, our proposed approach takes a different per-\nspective. It aims to conduct a comprehensive analysis of the factors that contribute to the substantial\nperformance drop observed in both imbalanced and online settings compared to offline learning.\nBy understanding these underlying factors, we propose the VR-MCL to the model performance in\nimbalanced CL scenarios.\nVariance Reduction is a widely used technique for reducing the variance in stochastic gradients\nand accelerating the optimization process. Early research on variance reduction methods, such as\nSAG Gower et al. (2020), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013), was\nprimarily focused on strongly convex optimization problems. However, in recent years, these meth-\nods have been extended to general non-convex optimization problems (Allen-Zhu & Hazan, 2016;\nNguyen et al., 2017; Fang et al., 2018). Nonetheless, these methods typically require the expensive\ncomputation of the full-batch gradient by processing all samples in a task, which is impractical in\nthe context of online continual learning where samples are processed in mini-batches and the com-\nplete sample set is unavailable. To alleviate this expensive computation problem, momentum-based\nvariance reduction algorithms (Cutkosky & Orabona, 2019; Khanduri et al., 2021) are proposed.\nSpecifically, these methods incorporate a momentum term to compute the stochastic gradients at two\nconsecutive iterations on the same set of stochastic samples, thereby replacing the time-consuming\ncomputation of the full-batch gradient. Additionally, these methods have the same asymptotic con-\nvergence rate as other variance reduction methods.\nC I MPLEMENTATION DETAILS\nC.1 E XPERIMENTAL SETTINGS\nImbalanced Settings. We conducted imbalanced online continual learning experiments on Seq-\ncifar10, consisting of five tasks, each with two classes. As shown in Fig. 4, when the imbalance\nratio\u03b3= 2, the number of samples for each class across all tasks is [5000, 4629, 4286, 3968, 3674,\n3401, 3149, 2916, 2700, 2500]. We also considered the reversed version, where the number of\nsamples for each class is [2500, 2700, 2916, 3149, 3401, 3674, 3968, 4286, 4629, 5000]. Without\nloss of generality, we additionally sampled a random version with [2700, 2500, 5000, 4286, 3674,\n3968, 3149, 3401, 2916, 4629] samples per class across all tasks.\nBaselines. We compared the proposed VR-MCL to various baseline approaches, including\nregularization-based methods, rehearsal-based methods, meta-continual learning methods, and spe-\ncific CL methods designed for imbalanced and online settings.", "start_char_idx": 0, "end_char_idx": 4100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e07832d-d775-4c9c-9490-106dab31ea79": {"__data__": {"id_": "9e07832d-d775-4c9c-9490-106dab31ea79", "embedding": null, "metadata": {"page_label": "22", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e34da759-c53d-4e2e-8e45-ad9067ea6b07", "node_type": "4", "metadata": {"page_label": "22", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "75a5b25b7f059b74d2f803b2bcc5104777b5710f0ae06ab3a13beb7e8559b58e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6771a3f1-af1f-4899-a947-c06cbebc62a9", "node_type": "1", "metadata": {"page_label": "22", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4f46425c4807a534feb285def7c95a3ed8e561ce0ec444af6753aaf81372d545", "class_name": "RelatedNodeInfo"}}, "text": "We also considered the reversed version, where the number of\nsamples for each class is [2500, 2700, 2916, 3149, 3401, 3674, 3968, 4286, 4629, 5000]. Without\nloss of generality, we additionally sampled a random version with [2700, 2500, 5000, 4286, 3674,\n3968, 3149, 3401, 2916, 4629] samples per class across all tasks.\nBaselines. We compared the proposed VR-MCL to various baseline approaches, including\nregularization-based methods, rehearsal-based methods, meta-continual learning methods, and spe-\ncific CL methods designed for imbalanced and online settings.\nRegularization-based methods:\n\u2022On-EWC: (Husz \u00b4ar, 2017). It approximates each Hessian matrix by a diagonal Fisher matrix\ncalculated at the end of each task training. The larger Fisher information value means the\ncorresponding weights are more important to the old tasks and thus should undergo less change\nin future tasks.\n22", "start_char_idx": 3537, "end_char_idx": 4426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0de3564-2693-4092-8829-31e7138af7fd": {"__data__": {"id_": "e0de3564-2693-4092-8829-31e7138af7fd", "embedding": null, "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22c61995-6340-4dc5-a3f8-9e39b1234199", "node_type": "4", "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "211f2450bf62d38b5bb67ae9d2857cba72e3b7364aa4245774a62ce6fa3c2023", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da69ed1c-f13d-425a-96ae-be2a0f72abb8", "node_type": "1", "metadata": {}, "hash": "63c310d4806cba527f418c0735ec0a84d4124931c0fd0724809cd35c65516638", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni0000004c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000003/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni0000002c/uni00000050/uni00000045/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni00000020/uni00000015\n(a) Traditional imbalance.\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni0000004c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000003/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni0000002c/uni00000050/uni00000045/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni00000020/uni00000015/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000059/uni00000048/uni00000055/uni00000056/uni00000048/uni0000000c\n (b) Reverse imbalance.", "start_char_idx": 0, "end_char_idx": 1971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da69ed1c-f13d-425a-96ae-be2a0f72abb8": {"__data__": {"id_": "da69ed1c-f13d-425a-96ae-be2a0f72abb8", "embedding": null, "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22c61995-6340-4dc5-a3f8-9e39b1234199", "node_type": "4", "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "211f2450bf62d38b5bb67ae9d2857cba72e3b7364aa4245774a62ce6fa3c2023", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0de3564-2693-4092-8829-31e7138af7fd", "node_type": "1", "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4daef9ba8c7966a7ab7515f984627a93db133e451e63de1a4e379af83146a549", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad270ec0-39cc-4bae-8997-d536b0f84bbc", "node_type": "1", "metadata": {}, "hash": "c6d97a924fcdbbe5ce49d1083b8ae08e04334b34cd8386f7e83a1f87dbd41432", "class_name": "RelatedNodeInfo"}}, "text": "/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013\n/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni0000004c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000003/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni0000002c/uni00000050/uni00000045/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni00000020/uni00000015/uni00000003/uni0000000b/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni0000000c\n (c) Random imbalance.\nFigure 4: Different imbalance settings of Seq-CIFAR10, where the imbalance ratio \u03b3= 2.\n\u2022IS:(Zenke et al., 2017). It introduces the intelligent synapse. Each synapse accumulates\ntask-relevant information over time so as to rapidly store new memories without forgetting the\nlearned ones. From the Hessian approximation perspective, it constructs a diagonal regulariza-\ntion matrix (i.e., a generalization form of the Hessian matrix integrating over time) to estimate\nthe Hessian matrix.\n\u2022LWF: (Li & Hoiem, 2017). It is similar to joint training, but the key difference is that LWF\ndoes not require data and labels from the old tasks, instead employing distillation techniques.\nRehearsal methods:\n\u2022GEM and A-GEM (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2018). They formulate\noptimization constraints such that gradients between current and old training data are aligned to\ndetermine the final direction of optimization.\n\u2022ER(Rolnick et al., 2019). Training current task with data sampled from the memory buffer\nwhich stores the examples from previous tasks.\n\u2022DER/DER++ (Buzzega et al., 2020). Adding the distillation loss of logits on the top of ER.\n\u2022CLSER (Arani et al., 2021). The dual memory experience replay (ER) method maintains short-\nterm and long-term semantic memories that interact with episodic memory.\nCL methods focus on imbalance and online settings:\n\u2022CBRS (Chrysakis & Moens, 2020). It introduces a novel mechanism for class-balancing reser-\nvoir sampling based on the sample distribution within the buffer to ensure more balanced sam-\nples are stored.\n\u2022Coresets (Borsos et al., 2020). It presents a novel coreset construction technique through\ncardinality-constrained bilevel optimization, which is effective in the imbalanced CL setting.\n\u2022OCM (Guo et al., 2022). It proposes to reduce feature bias and fully exploit streaming data\nthrough mutual information maximization. Note that , for a fair comparison, we choose its\nOCM (no local augmentation) for evaluation.\n\u2022ER-OBC (Chrysakis & Moens, 2023). It proposes a simple approach to mitigate bias in online\nCL by modifying the optimization process of the output layer model.\nMeta-CL methods:\n\u2022MER (Riemer et al., 2019).", "start_char_idx": 1972, "end_char_idx": 5168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad270ec0-39cc-4bae-8997-d536b0f84bbc": {"__data__": {"id_": "ad270ec0-39cc-4bae-8997-d536b0f84bbc", "embedding": null, "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22c61995-6340-4dc5-a3f8-9e39b1234199", "node_type": "4", "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "211f2450bf62d38b5bb67ae9d2857cba72e3b7364aa4245774a62ce6fa3c2023", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da69ed1c-f13d-425a-96ae-be2a0f72abb8", "node_type": "1", "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "e8061041c7e12e3adbc0626a65c60885cb65041b0b54c3456cd577a659068f45", "class_name": "RelatedNodeInfo"}}, "text": "It introduces a novel mechanism for class-balancing reser-\nvoir sampling based on the sample distribution within the buffer to ensure more balanced sam-\nples are stored.\n\u2022Coresets (Borsos et al., 2020). It presents a novel coreset construction technique through\ncardinality-constrained bilevel optimization, which is effective in the imbalanced CL setting.\n\u2022OCM (Guo et al., 2022). It proposes to reduce feature bias and fully exploit streaming data\nthrough mutual information maximization. Note that , for a fair comparison, we choose its\nOCM (no local augmentation) for evaluation.\n\u2022ER-OBC (Chrysakis & Moens, 2023). It proposes a simple approach to mitigate bias in online\nCL by modifying the optimization process of the output layer model.\nMeta-CL methods:\n\u2022MER (Riemer et al., 2019). It utilizes replay examples to incentivize the alignment of gradients\nbetween old and new tasks to further maximize the transfer from previous tasks and minimize\ninterference.\n\u2022La-MAML (Gupta et al., 2020). It proposed an online gradient-based meta-learning algorithm\nand proved it is equivalent to a simplified version of MER. Note that in the online scenario\nof La-MAML (Gupta et al., 2020), the mini-batch samples instead of the whole task could be\ntrained multiple times, which is not the strict online continual learning. In our experiments, all\nmethods follow the standard online setting where all samples can only be trained once.\nHyper-parameters. The hyperparameters used for each experimental setting are listed in Table 7.\nLetlrdenote the learning rate, bsdenote the batch size and mbs means the minibatch size (i.e.,\nthe batch size drawn from the buffer). It is worth noting that we used the same hyperparameters\nacross different datasets and buffer sizes, which shows our method has good generalizability without\nrequiring extensive hyperparameter tuning for each specific setting.\n23", "start_char_idx": 4380, "end_char_idx": 6266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8636c90-fe85-484f-9a95-c2d2a34bef32": {"__data__": {"id_": "a8636c90-fe85-484f-9a95-c2d2a34bef32", "embedding": null, "metadata": {"page_label": "24", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4ffa069-fe6c-44d8-9c11-c97461e72a94", "node_type": "4", "metadata": {"page_label": "24", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0b57db45b619e13a55d5f0fd0941a49c3685c74304a3296a4fce7e4f358f5d7c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 7: Hyperparameters on all the three datasets including Seq-Cifar10/100 and Seq-\nTinyImgaeNet.\nBackbone Epochs Buffer size Inner lr ( \u03b2) Outer lr ( \u03b1) r inner-bs bs mbs\nPcCNN 1 200 0.1 0.25 0.25 2 10 10\nPcCNN 1 600 0.1 0.25 0.25 2 10 10\nPcCNN 1 1000 0.1 0.25 0.25 2 10 10\nReduced ResNet-18 1 200 0.1 0.25 0.25 8 32 32\nReduced ResNet-18 1 600 0.1 0.25 0.25 8 32 32\nReduced ResNet-18 1 1000 0.1 0.25 0.25 8 32 32\nC.2 T HEMASK TRAINING OF THE PROPOSED VR-MCL\nFor the implementation of the VR-MCL, we adopt mask training different from La-MAML (i.e., one\nmask training strategy on the final output layer). In this subsection, we will first introduce two mask\ntraining approaches: Mask-LM, which is the original mask training in La-MAML, and Mask-VR,\nwhich is the proposed mask training used in VR-MCL. We will then provide both mathematical and\nexperimental analysis to compare the two mask training methods.\n\ud8351\n\ud8352\n\ud8353\n\ud8354\n\ud8355\ud8351\n\ud8352\n\ud8353\n\ud8354\n\ud8355\ud8351\n\ud8352\n\ud8353\n\ud8354\n\ud8355\nInner loop   Outer loop\n   (Mask-VR)\ud835\ud835\udc84\ud835\ud835\ud835\udc84\ud835\u2192\ud835\ud835\udc83\u2192\n\u2192\ud8351\n\ud8352\n\ud8353\n\ud8354\n\ud8355\ud8351\n\ud8352\n\ud8353\n\ud8354\n\ud8355\nOuter loop \n(Mask-LM)\ud835\ud835\udc84\ud835\ud835\ud835\udc83\u2192\n\u2192\nUnseen\n  tasksPrevious\ntasks(not masked) Preserved predicted probability Predict probability of one class\nFigure 5: The illustration of the two mask training approaches, where the current training task is T3.\nIntroduction of the two Mask training methods As the output layer is shared among all tasks,\nmeta-continual learning methods have the problem of a mismatch between the limited rehearsal\ndata from previous tasks T[1:j\u22121]and the larger incoming data from the current task Tj. To solve\nthis problem, a masked softmax loss is applied by La-MAML Gupta et al. (2020). As shown in\nFigure 5, La-MAML applies a mask (i.e., Mask-LM) that only keeps the logits of the corresponding\nclasses within a task to both the loss in the inner-loop adaptation and outer-loop optimization. The\nMask-LM improves the inner-loop adaptation for the current tasks and mitigates the interference\nbetween the limited data sampled from the memory buffer Mand the larger incoming data from\nthe current task. However, the employment of the Mask-LM in the outer-loop still has a limitation.\nThe left part of Mask-LM concentrates solely on minimizing the intra-task loss, without consider-\ning the interdependencies between different classes from various tasks. This oversight significantly\nreduces the model\u2019s efficacy in the class incremental setting. To address this issue, we propose a\nmodification (i.e., Mask-VR) inspired by (Caccia et al., 2021), where we mask all seen classes that\nappeared in the memory buffer M. This modification enables the model to distinguish samples\nbelonging to different tasks, which is shown in the left part of Mask-VR. Mathematical and Ex-\nperimental Analysis. The main idea of Mask-VR to enlarge the preserved logits is to introduce the\nnegative gradients so as to get more concise class prototypes and thus achieve better performance.\nTo investigate the impact of the proposed Mask-VR, we conducted experiments and demonstrated\nempirically that it can effectively reduce the gradient variance. Furthermore, our findings suggest\nthat the Mask-VR and the proposed variance reduction method can be synergistically combined to\neffectively reduce the gradient variance.\nWe will begin by conducting the mathematical analysis . Note that the following mathematical\nsymbols are solely for the purpose of illustrating the Mask-VR, they are not aligned with the math-\nematical symbols used in the main text.\n24", "start_char_idx": 0, "end_char_idx": 3524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecfc48bc-2d47-45ff-9ecb-bdbb373890e0": {"__data__": {"id_": "ecfc48bc-2d47-45ff-9ecb-bdbb373890e0", "embedding": null, "metadata": {"page_label": "25", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "424a0381-3eba-4d92-8a5c-e471aedd99cc", "node_type": "4", "metadata": {"page_label": "25", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cf6156d5da4d2934462d63fc4c18672283366d4dd882822583971d710d48c105", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64db4296-a7d4-4a34-ab19-87a96cb8651b", "node_type": "1", "metadata": {}, "hash": "c188d2b965964519d65c6e4aa1931c7161a306a85b0e79de102b202a93f98890", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nLetCseen refer to all seen classes, and |Cseen|is the number of classes the model has seen. In\nFigure 5, the current training task is T3and|Cseen|= 6. Let the model architecture consists of two\nparts: feature extractor h\u03b8and classifier f\u03d5and the output of the model before the softmax operation\niso(x;\u03b8, \u03d5) =f\u03d5(h\u03b8(x)). Then the cross-entropy loss in continual learning Lceis,\nLce(o(x;\u03b8, \u03d5)) =\u2212|Cseen|X\ni=1lcilog(pci), pci=exp(oci)\nP|Cseen|\ns=1exp(ocs)\nwhere xis the training sample, lci\u2208 {0,1}is the one-hot label of class ci, and o(x;\u03b8, \u03d5) =\n[oc1, oc2, ..., o c|Cseen|]is the corresponding logit values of x. Let ximean the sample of class ci,\nthen the gradients on the classifier f\u03d5is,\n\u2202Lce\u0000\no\u0000\nxi;\u03b8, \u03d5\u0001\u0001\n\u2202oci=pci\u22121,\u2202Lce\u0000\no\u0000\nxi;\u03b8, \u03d5\u0001\u0001\n\u2202ocj=pcj,\n\u2202Lce\u0000\no\u0000\nxi;\u03b8, \u03d5\u0001\u0001\n\u2202\u03d5ci=\u2202Lce\u0000\no\u0000\nxi;\u03b8, \u03d5\u0001\u0001\n\u2202oci\u2202oci\n\u2202\u03d5ci= (pci\u22121)h\u03b8(xi),[Positive gradient]\n\u2202Lce\u0000\no\u0000\nxi;\u03b8, \u03d5\u0001\u0001\n\u2202\u03d5cj=\u2202Lce\u0000\no\u0000\nxi;\u03b8, \u03d5\u0001\u0001\n\u2202ocj\u2202ocj\n\u2202\u03d5cj=pcjh\u03b8(xi),[Negative gradient]\nLetWdenote the matrix with all class prototypes {wc}c\u2208Cseen, then W\u25e6h\u03b8=f\u03d5(h\u03b8(x)). Ac-\ncording to the above equation, we can get in Mask-LM, only wcireceives a positive gradient from\nxi, and there are no negative gradients due to preserving only a portion of the logits. However, in\nMask-VR, both positive and negative gradients are present, similar to the concept of positives and\nnegatives in contrastive loss. Consequently, the class prototypes Wlearned with Mask-VR exhibit\nsuperior performance compared to those learned with Mask-LM.\n0 500 1000 1500 2000\n/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni000000560.000.250.500.751.001.25/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000003/uni00000039/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000046/uni00000048\n/uni0000002f/uni00000044/uni00000010/uni00000030/uni00000024/uni00000030/uni0000002f\n/uni00000030/uni00000026/uni0000002f\n/uni00000039/uni00000035/uni00000010/uni00000030/uni00000026/uni0000002f1\n/uni00000039/uni00000035/uni00000010/uni00000030/uni00000026/uni0000002f2\nFigure 6: Relative gradient variance of update during training. The subscript 1 represents training\nwith Mask-LM, while the subscript 2 represents training with Mask-VR.\nAblation studies of the Mask-Training.", "start_char_idx": 0, "end_char_idx": 2465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64db4296-a7d4-4a34-ab19-87a96cb8651b": {"__data__": {"id_": "64db4296-a7d4-4a34-ab19-87a96cb8651b", "embedding": null, "metadata": {"page_label": "25", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "424a0381-3eba-4d92-8a5c-e471aedd99cc", "node_type": "4", "metadata": {"page_label": "25", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "cf6156d5da4d2934462d63fc4c18672283366d4dd882822583971d710d48c105", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecfc48bc-2d47-45ff-9ecb-bdbb373890e0", "node_type": "1", "metadata": {"page_label": "25", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "595328b75084908d801ed64dae830bae2af29dc2801dad6371ba27d4ab94c5ad", "class_name": "RelatedNodeInfo"}}, "text": "The subscript 1 represents training\nwith Mask-LM, while the subscript 2 represents training with Mask-VR.\nAblation studies of the Mask-Training. To better understand the effectiveness of the Mask-VR,\nand the proposed VR-MCL, we compare the performance of La-MAML, MCL(La-MAML with\nMask-VR), as well as VR-MCL 1(i.e., VR-MCL with Mask-LM) and VR-MCL 2(i.e., VR-MCL\nwith Mask-VR), as shown in Table 8. Moreover, we also plot the relative gradient variance of La-\nMAML, MCL, VR-MCL 1, and VR-MCL 2, as shown in Fig. 6. Based on the fact that the curves\nwith variance reduction (VR) are all lower than those without VR, it can be concluded that: 1) the\nproposed VR technique indeed can reduce the variance, 2) the effects of VR and Mask-VR can be\nsuperimposed and work together.\nTable 8: Performance of La-MAML, MCL, VR-MCL 1and VR-MCL 2on Seq-CIFAR10.\nMethod La-MAML MCL VR-MCL 1VR-MCL 2\nAcc 33.43 \u00b11.21 52.40 \u00b12.13 35.66 \u00b11.13 56.48 \u00b11.79\nAAA 42.98 \u00b11.60 65.87 \u00b11.26 50.02 \u00b11.80 66.97 \u00b11.58\n25", "start_char_idx": 2321, "end_char_idx": 3312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5356ae6a-f47f-4a75-8cb7-bbbc5ec974a6": {"__data__": {"id_": "5356ae6a-f47f-4a75-8cb7-bbbc5ec974a6", "embedding": null, "metadata": {"page_label": "26", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f133e66e-f4aa-4101-a91d-77e6716d0b94", "node_type": "4", "metadata": {"page_label": "26", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7348f6811a29b9ce664f6ad7313ca79677b918dfac4ac37b8e74ea9d91ad77bc", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nD O THER EXPERIMENTAL RESULTS .\nD.1 T HE TRAINING TIME ANALYSIS OF VR-MCL\nTo analyze the time complexity of the proposed algorithm, we compared the training time of the\nproposed VR-MCL with relevant Meta-CL methods on the Seq-CIFAR-10 dataset using reduced\nResNet-18, as shown in Table 9. It is evident that our proposed VR-MCL method demonstrates\na substantial reduction in training time compared to MER. To assess the value of the additional\ntraining time required by VR-MCL, we conducted experiments by extending the training epoch of\nLa-MAML from 1 to 2. Surprisingly, despite the comparable training time with VR-MCL, La-\nMAML achieved a performance of only 38.89 %, which is extremely lower than the performance of\nVR-MCL (i.e., 56.48 %). This outcome serves as strong evidence for the superiority of VR-MCL in\nterms of both training efficiency and performance on Seq-CIFAR10.\nTable 9: Training time comparison of different meta-continual learning methods on Seq-CIFAR10\nunder the online setting (i.e., the training epoch of each task is set as one.)\nMethod La-MAML MER VR-MCL\nTraining Epochs 1 1 1\nTraining Time (s) 750.61 20697.70 1297.10\nTraining Epochs 2 1 1\nTraining Time (s) 1511.54 20697.70 1297.10\nAAA 53.21% 50.99% 66.97%\nAcc 38.89% 36.92% 56.48%\nD.2 O THER EVALUATION METRICS\nTo further evaluate the effectiveness of the proposed VR-MCL, we provide the backward transfer\n(BWT) metric in Table 10, which measures the performance degradation in subsequent tasks. It can\nbe observed our VR-MCL achieves the best results. It is worth noting that in Table 10, La-MAML\nis excluded from the analysis due to the extremely negative impact of Mask-LM utilization on its\nnewest training task (i.e., Tj) performance (Caccia et al., 2021). This impact results in large and\neven a positive backward transfer (BWT) value for La-MAML.\nTable 10: The Backward Transfer of Seq-CIFAR10 and longer task sequences Seq-CIFAR100, Seq-\nTinyImageNet with 95% confidence interval on reduced ResNet-18 under the online setting. The\nmemory buffer size is set as 1000 (i.e., |M|=1000). All reported numbers are the average of 5 runs.\nShaded areas are our methods, and \u2018\u2013\u2019 indicates the result was omitted due to high instability.\nCIFAR-10 CIFAR-100 Tiny-ImageNet\nMethod BWT BWT BWT\nSGD -61.48 \u00b11.58 -44.53 \u00b10.38 -34.59 \u00b10.35\nOn-EWC -65.94 \u00b12.08 -42.45 \u00b11.30 -33.05 \u00b10.56\nIS -63.38 \u00b11.19 -39.54 \u00b11.58 -23.76 \u00b11.27\nA-GEM -67.06 \u00b11.11 -48.74 \u00b10.37 -37.93 \u00b11.04\nGEM -62.72 \u00b12.00 -42.27 \u00b11.90 -36.21 \u00b10.79\nER -33.97 \u00b17.07 -31.24 \u00b11.86 -31.07 \u00b11.91\nDER -62.17 \u00b13.48 -50.21 \u00b11.32 -39.50 \u00b10.60\nDER++ -24.65 \u00b12.21 -46.74 \u00b11.13 -39.16 \u00b11.34\nCLSER -23.61 \u00b11.54 -37.02 \u00b11.35 -34.58 \u00b10.85\nOCM -22.02 \u00b11.92 -17.91 \u00b10.74 -11.99 \u00b10.86\nER-OBC -34.71 \u00b10.81 -5.39 \u00b11.83 -6.20 \u00b11.27\nMER -28.77 \u00b11.25 \u2013 \u2013\nVR-MCL -20.92 \u00b11.94 -0.16 \u00b10.10 -2.86 \u00b10.83\n26", "start_char_idx": 0, "end_char_idx": 2861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05c47fe6-4733-414e-9172-ddc8e36c2cc6": {"__data__": {"id_": "05c47fe6-4733-414e-9172-ddc8e36c2cc6", "embedding": null, "metadata": {"page_label": "27", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e33c8ee3-0f3c-40c2-9ec1-709a23f9b896", "node_type": "4", "metadata": {"page_label": "27", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "27c647cad3a1572ad040f82bb60ae8e2939b2a8ec376f91ba1ae5539a481090d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40b58cc2-8717-4d94-8304-fd02b8c90750", "node_type": "1", "metadata": {}, "hash": "e4380f8c9bb3cdd988672a61e5b9f0fba99461974804f3ebae3603d071caa98e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nD.3 H YPERPARAMETER SELECTION\nWe utilized a grid search technique to select the optimal hyperparameters, including the learning rate\n\u03b1and the momentum ratio r, as illustrated in Table 11 and Table 12. The results demonstrate that our\nproposed method achieves the best performance when both the momentum ratio rand learning rate \u03b1\nare set to 0.25. Moreover, it is worth noting that there is small performance difference observed when\ndifferent hyperparameters are chosen. This suggests that the proposed VR-MCL method exhibits\nrobustness and is not highly sensitive to the selection of hyperparameters.\nTable 11: Reduced ResNet18 performance with\ndifferent momentum ratios ron Seq-CIFAR10,\nusing a fixed learning rate ( \u03b1= 0.25).\nr 0.15 0.25 0.35\nAcc 55.46 \u00b10.44 56.48 \u00b11.79 54.14 \u00b11.50\nAAA 66.46 \u00b11.23 66.97 \u00b11.58 63.92 \u00b10.89Table 12: Reduced ResNet18 performance with\ndifferent learning rates \u03b1on Seq-CIFAR10, us-\ning a fixed momentum ratio ( r= 0.25).\n\u03b1 0.15 0.25 0.35\nAcc 54.466 \u00b11.94 56.48 \u00b11.79 55.91 \u00b11.52\nAAA 66.06 \u00b11.05 66.97 \u00b11.58 66.33 \u00b11.33\nD.4 O THER IMBALANCED CL EXPERIMENTS\nDue to the page limit, the full results of Table 4 are shown in Table 13. To further explore the\nperformance of VR-MCL in imbalanced settings, we select the more realistic random imbalanced\nscenario and present the results for a higher imbalance ratio, specifically \u03b3= 5, in Table 14. For\nsimplicity, we select the SOTA methods from Table 13 as the comparable methods. The results\nclearly demonstrate that the proposed VR-MCL surpasses other methods in the case of \u03b3= 5,\nproviding further evidence of the effectiveness of the variance reduction technique in addressing\nimbalanced CL.\nTo further demonstrate the effectiveness of the proposed VR-MCL under imbalanced settings, we\nperformed additional verification on various imbalance settings using the Seq-CIFAR100 dataset.\nThe results of these experiments are presented in Table 15.\nTable 13: Reduced ResNet-18 performance on the imbalanced Seq-CIFAR10 ( |M|=1000) with\nimbalance ratio \u03b3=2.", "start_char_idx": 0, "end_char_idx": 2075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40b58cc2-8717-4d94-8304-fd02b8c90750": {"__data__": {"id_": "40b58cc2-8717-4d94-8304-fd02b8c90750", "embedding": null, "metadata": {"page_label": "27", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e33c8ee3-0f3c-40c2-9ec1-709a23f9b896", "node_type": "4", "metadata": {"page_label": "27", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "27c647cad3a1572ad040f82bb60ae8e2939b2a8ec376f91ba1ae5539a481090d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05c47fe6-4733-414e-9172-ddc8e36c2cc6", "node_type": "1", "metadata": {"page_label": "27", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "fcfee8a414e1ddaaa1b9a608911be77442d506d7a153a4a30006ff0f286cac16", "class_name": "RelatedNodeInfo"}}, "text": "To further explore the\nperformance of VR-MCL in imbalanced settings, we select the more realistic random imbalanced\nscenario and present the results for a higher imbalance ratio, specifically \u03b3= 5, in Table 14. For\nsimplicity, we select the SOTA methods from Table 13 as the comparable methods. The results\nclearly demonstrate that the proposed VR-MCL surpasses other methods in the case of \u03b3= 5,\nproviding further evidence of the effectiveness of the variance reduction technique in addressing\nimbalanced CL.\nTo further demonstrate the effectiveness of the proposed VR-MCL under imbalanced settings, we\nperformed additional verification on various imbalance settings using the Seq-CIFAR100 dataset.\nThe results of these experiments are presented in Table 15.\nTable 13: Reduced ResNet-18 performance on the imbalanced Seq-CIFAR10 ( |M|=1000) with\nimbalance ratio \u03b3=2.\n\u03b3=2 \u03b3=2 (Reversed) \u03b3=2 (Random)\nMethods AAA ACC AAA ACC AAA ACC\nSGD 36.64 \u00b10.42 16.46 \u00b10.31 35.57 \u00b10.70 17.54 \u00b10.16 34.62 \u00b11.31 17.33 \u00b10.34\nOn-EWC 38.85 \u00b10.18 16.92 \u00b10.31 37.41 \u00b10.26 17.79 \u00b10.17 37.26 \u00b10.49 14.35 \u00b11.85\nA-GEM 38.50 \u00b10.25 17.64 \u00b10.43 36.72 \u00b10.33 17.43 \u00b10.47 37.79 \u00b10.45 17.85 \u00b10.22\nGEM 41.29 \u00b10.42 17.90 \u00b10.65 38.46 \u00b11.13 18.37 \u00b10.25 39.63 \u00b11.38 18.00 \u00b10.73\nER 52.50 \u00b10.74 33.10 \u00b12.17 46.58 \u00b12.29 28.49 \u00b13.46 43.78 \u00b12.85 31.10 \u00b15.38\nDER 46.95 \u00b11.54 16.82 \u00b10.98 40.62 \u00b11.69 18.63 \u00b10.61 41.44 \u00b10.65 18.78 \u00b10.23\nDER++ 62.02 \u00b10.52 44.14 \u00b12.77 58.22 \u00b10.85 39.21 \u00b13.81 60.25 \u00b10.58 42.83 \u00b12.60\nCLSER 61.37 \u00b10.69 47.75 \u00b10.70 54.92 \u00b10.55 40.51 \u00b11.15 56.60 \u00b11.51 47.48 \u00b10.70\nCBRS 59.07 \u00b11.78 43.81 \u00b10.13 57.57 \u00b11.60 44.20 \u00b11.96 58.16 \u00b11.57 44.66 \u00b13.21\nCoresets 61.11 \u00b11.34 45.37 \u00b10.98 58.12 \u00b11.37 45.80 \u00b11.89 58.56 \u00b12.17 45.63 \u00b11.89\nLa-MAML 36.64 \u00b11.54 29.17 \u00b10.91 32.08 \u00b12.37 31.17 \u00b11.59 42.52 \u00b12.81 31.24 \u00b12.01\nVR-MCL 65.06 \u00b10.85 49.82 \u00b11.13 61.16 \u00b11.32 51.36 \u00b11.31 61.91 \u00b11.07 50.74 \u00b11.15\nTable 14: Reduced ResNet-18 performance under the random imbalanced scenario with larger im-\nbalance ratio, i.e., \u03b3= 5(Random).\nMethod CLS-ER DER++ La-MAML VR-MCL\nAcc 35.66 \u00b11.56 40.93 \u00b10.69 27.46 \u00b14.65 41.55 \u00b13.15\nAAA 53.93 \u00b10.61 49.98 \u00b12.34 36.85 \u00b11.33 54.52 \u00b11.97\n27", "start_char_idx": 1208, "end_char_idx": 3342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1aee49c-8635-4809-81cc-22dd1a028d73": {"__data__": {"id_": "a1aee49c-8635-4809-81cc-22dd1a028d73", "embedding": null, "metadata": {"page_label": "28", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ab9d980-17e1-4252-afb4-078c8c8b7a47", "node_type": "4", "metadata": {"page_label": "28", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d72b1f102122353b23ea3dba495baf6e92f1b85e6fc5f64a1994ad7500609e34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44a127ec-4b89-4524-a47d-9cdd238217ba", "node_type": "1", "metadata": {}, "hash": "b585fb8ec5b023427b017d7d117e68236330610f18a4972158dfc772b412d47b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 15: Reduced ResNet-18 performance on the imbalanced Seq-CIFAR100 ( |M|=1000) with\nimbalance ratio \u03b3=2.\nSetting Method CLSER DER++ La-MAML VR-MCL\n\u03b3= 2Acc 13.85 \u00b11.19 9.19\u00b11.20 11.19 \u00b11.07 15.12 \u00b11.32\nAAA 23.54 \u00b11.02 18.78 \u00b10.24 19.54 \u00b10.38 24.02 \u00b11.13\n\u03b3= 2(Reverse)Acc 12.89 \u00b10.78 9.66\u00b10.03 11.86 \u00b10.60 14.98 \u00b11.65\nAAA 18.72 \u00b11.22 15.60 \u00b10.60 16.04 \u00b10.53 20.88 \u00b11.16\n\u03b3= 2(Random)Acc 13.42 \u00b11.48 8.60\u00b10.49 11.32 \u00b10.91 14.93 \u00b10.27\nAAA 19.10 \u00b11.89 15.55 \u00b10.25 16.22 \u00b10.58 20.13 \u00b10.57\nD.5 O THER RESULTS USING THE PCCNN BACKBONE\nIn Table 16, we further investigate the performance gain over the memory buffer size on the small\nnetwork PcCNN. Combined with the results on reduced ResNet18, it is evident that the proposed\nVR-MCL has a consistent and significant performance improvement in different buffer sizes and\nvarious network structures.\nTable 16: Performance of Seq-CIFAR10 with 95% confidence interval on PcCNN. All numbers are\nthe average of 5 runs. |M| denotes the memory buffer size.\n|M|=200 |M|=600 |M|=1000\nMethod AAA Acc AAA Acc AAA Acc\nSGD 37.84 \u00b10.0317.58 \u00b10.2037.84 \u00b10.0317.58 \u00b10.2037.84 \u00b10.0317.58 \u00b10.20\nOn-EWC 38.72 \u00b10.5817.05 \u00b10.0838.72 \u00b10.5817.05 \u00b10.0838.72 \u00b10.5817.05 \u00b10.08\nA-GEM 41.84 \u00b10.4418.50 \u00b10.3841.54 \u00b10.1418.45 \u00b10.3741.73 \u00b10.6118.78 \u00b10.38\nGEM 45.07 \u00b10.5222.52 \u00b11.0245.11 \u00b10.3422.45 \u00b10.8944.92 \u00b10.2022.10 \u00b10.60\nER 54.64 \u00b10.9132.46 \u00b11.5858.85 \u00b10.8240.29 \u00b11.2559.26 \u00b10.6941.71 \u00b11.35\nDER 58.06 \u00b10.1838.21 \u00b10.2958.40 \u00b10.1138.46 \u00b10.4557.90 \u00b10.1238.88 \u00b10.39\nDER++ 56.07 \u00b10.3234.28 \u00b10.7962.13 \u00b10.2143.89 \u00b10.2063.55 \u00b10.1647.20 \u00b10.67\nCLSER 58.03 \u00b10.7539.38 \u00b11.4461.53 \u00b10.3244.94 \u00b11.0162.39 \u00b10.3448.24 \u00b10.85\nMER 55.50 \u00b10.3832.01 \u00b11.2263.20 \u00b10.4344.69 \u00b10.4365.75 \u00b10.6351.04 \u00b10.78\nLa-MAML 46.36 \u00b10.9829.45 \u00b10.5147.22 \u00b10.8732.78 \u00b11.5347.22 \u00b10.8732.51 \u00b11.18\nVR-MCL 60.88 \u00b10.2243.41 \u00b10.3063.86 \u00b10.5248.85 \u00b10.6666.02 \u00b10.3152.67 \u00b10.25\nD.6 O FFLINE CL P ERFORMANCE\nAs shown in Table 17, we also provide the performance of different methods under the offline CL\nsetting where we set the training epochs as 5.", "start_char_idx": 0, "end_char_idx": 2063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44a127ec-4b89-4524-a47d-9cdd238217ba": {"__data__": {"id_": "44a127ec-4b89-4524-a47d-9cdd238217ba", "embedding": null, "metadata": {"page_label": "28", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ab9d980-17e1-4252-afb4-078c8c8b7a47", "node_type": "4", "metadata": {"page_label": "28", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d72b1f102122353b23ea3dba495baf6e92f1b85e6fc5f64a1994ad7500609e34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1aee49c-8635-4809-81cc-22dd1a028d73", "node_type": "1", "metadata": {"page_label": "28", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a84df77bdc2f36f8afe05f2ab6bfe2b670f4edf7e2c39fd9d7fee26053819f5a", "class_name": "RelatedNodeInfo"}}, "text": "D.7 I NFLUENCE OF THE INNER BATCH SIZE\nTo investigate the practical implications of the inner batch size, we conducted experiments using\nvarious inner batch sizes. The corresponding results are presented in Table 18. The findings indicate\nthat a larger inner batch size consistently leads to improved performance. However, it is worth noting\nthat the performance gains tend to plateau as the inner batch size increases. This can be attributed to\nthe online nature of the setting, where each sample is observed only once. A larger inner batch size\nreduces the number of outer loop steps, which may potentially compromise the overall performance\nimprovement.\nE A LGORITHM .\nAs presented in Algo. 1, Steps 1-9 present the update of \u03b8in the first iteration, where no momentum\nterm is available. Steps 2-6 present the inner loop and steps 7-8 present the estimate of hypergradient\nin the outer loop. Steps 10-28 represent the update of \u03b8inb-th iteration ( b >1). Steps 12-17 are\n28", "start_char_idx": 2064, "end_char_idx": 3040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59630cb9-efe4-4e7d-be74-295cb786981b": {"__data__": {"id_": "59630cb9-efe4-4e7d-be74-295cb786981b", "embedding": null, "metadata": {"page_label": "29", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d865c805-9711-4514-9cf6-da476d623707", "node_type": "4", "metadata": {"page_label": "29", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "023eb861f9f02efd13fdebe3b1f3f322cb8ec40c6ede5fa98a19ab413a802b63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90940378-913a-4d2e-b443-ebdd668c0032", "node_type": "1", "metadata": {}, "hash": "0e7c8391fc053ae7179b243037cf4dc4f9e287586e138a9093943b230f88cf20", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 17: Offline CL (i.e., 5 training epochs) performance of Seq-CIFAR10 and longer task se-\nquences Seq-CIFAR100, Seq-TinyImageNet with 95% confidence interval on reduced ResNet-18.\nThe memory buffer size is set as 1000 (i.e., |M|=1000).\nSeq-CIFAR10 Seq-CIFAR100 Seq-TinyImageNet\nMethod AAA Acc AAA Acc AAA Acc\nSGD 42.26 \u00b10.22 18.87 \u00b10.66 18.73 \u00b11.67 7.73\u00b10.66 16.89 \u00b10.26 6.93\u00b10.03\nOn-EWC 42.13 \u00b10.18 20.63 \u00b11.76 16.99 \u00b12.12 5.34\u00b10.42 14.89 \u00b10.14 4.85\u00b10.35\nIS 39.84 \u00b10.11 17.49 \u00b10.49 15.56 \u00b10.58 4.98\u00b10.65 10.92 \u00b11.44 2.65\u00b10.41\nA-GEM 42.13 \u00b10.14 19.12 \u00b10.15 18.69 \u00b11.81 7.64\u00b11.42 16.73 \u00b10.47 6.50\u00b10.35\nGEM 48.28 \u00b10.83 23.75 \u00b13.91 21.56 \u00b13.57 10.71 \u00b12.31 18.12 \u00b10.11 7.98\u00b10.33\nER 74.64 \u00b10.76 63.97 \u00b10.27 35.37 \u00b12.36 21.84 \u00b11.75 26.49 \u00b10.38 12.47 \u00b10.23\nDER 67.38 \u00b11.54 59.95 \u00b11.96 18.73 \u00b12.46 7.67\u00b11.12 16.96 \u00b10.44 6.68\u00b10.22\nDER++ 75.50 \u00b12.02 67.16 \u00b11.62 39.15 \u00b10.54 24.03 \u00b10.38 25.91 \u00b10.63 11.54 \u00b10.71\nCLSER 75.45 \u00b11.10 67.36 \u00b10.27 40.18 \u00b11.70 26.16 \u00b10.34 26.24 \u00b10.33 12.98 \u00b10.10\nER-OBC 75.93 \u00b10.95 65.53 \u00b11.42 41.15 \u00b10.53 29.02 \u00b10.99 29.01 \u00b10.93 16.27 \u00b10.44\nLa-MAML 60.55 \u00b10.48 43.58 \u00b10.56 31.05 \u00b10.28 19.67 \u00b11.46 23.07 \u00b10.71 11.81 \u00b10.55\nVR-MCL 77.15 \u00b10.75 67.34 \u00b11.10 40.02 \u00b10.89 29.32 \u00b10.94 30.43 \u00b11.00 19.60 \u00b10.58\nTable 18: Reduced ResNet-18 performance under different inner batch sizes.\nInner batch size 6 8 10\nAcc 53.80 \u00b12.36 56.48 \u00b11.79 56.81 \u00b11.07\nAAA 65.73 \u00b13.06 66.97 \u00b11.58 67.26 \u00b10.61\nthe inner loops. Steps 18-19 show the computation of the hypergradient for \u03b8b. In steps 20-25, we\nperform the inner loop to obtain \u03b8\u2032\nb(K)and compute the hypergradient for \u03b8basg\u03f5b\n\u03b8b\u22121. During the\nupdate of memory buffer M(step 28), we employ the reservoir sampling strategy to ensure that M\nis updated in a way that the stored examples are uniformly sampled from the tasks during online\ntraining.\nE.1 V ARYING TASK LENGTH ANALYSIS\nTo further demonstrate the effectiveness of the proposed VR-MCL across varying task lengths, we\nadjust the split mechanism of Seq-CIFAR100 and Seq-TinyImageNet to increase their original task\nnumbers.", "start_char_idx": 0, "end_char_idx": 2076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90940378-913a-4d2e-b443-ebdd668c0032": {"__data__": {"id_": "90940378-913a-4d2e-b443-ebdd668c0032", "embedding": null, "metadata": {"page_label": "29", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d865c805-9711-4514-9cf6-da476d623707", "node_type": "4", "metadata": {"page_label": "29", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "023eb861f9f02efd13fdebe3b1f3f322cb8ec40c6ede5fa98a19ab413a802b63", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59630cb9-efe4-4e7d-be74-295cb786981b", "node_type": "1", "metadata": {"page_label": "29", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "df83dffb28f31e5014a9725dc570fff42da4be09cb7ae9252250674e84683b21", "class_name": "RelatedNodeInfo"}}, "text": "Steps 18-19 show the computation of the hypergradient for \u03b8b. In steps 20-25, we\nperform the inner loop to obtain \u03b8\u2032\nb(K)and compute the hypergradient for \u03b8basg\u03f5b\n\u03b8b\u22121. During the\nupdate of memory buffer M(step 28), we employ the reservoir sampling strategy to ensure that M\nis updated in a way that the stored examples are uniformly sampled from the tasks during online\ntraining.\nE.1 V ARYING TASK LENGTH ANALYSIS\nTo further demonstrate the effectiveness of the proposed VR-MCL across varying task lengths, we\nadjust the split mechanism of Seq-CIFAR100 and Seq-TinyImageNet to increase their original task\nnumbers. Specifically, we restructure Seq-CIFAR100, originally composed of 10 tasks with 10\nclasses each, into 20 tasks, each with 5 classes. Similarly, we modify Seq-TinyImageNet, initially\nconsisting of 20 tasks with 10 classes each, into 40 tasks, each comprising 5 classes. For clarity, we\nrefer to these new division datasets as Seq-CIFAR100land Seq-TinyImageNetlrespectively. The\nperformance of various baselines on these two datasets is presented in Table 19.\nE.2 E MPIRICAL RESULTS OF THE OUTER -LOOP LOSS\nFigure 7: The outer-loop training loss for each task Tjin Seq-CIFAR10 ( j= 2,3,4,5).\n29", "start_char_idx": 1461, "end_char_idx": 2669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0079c2e-5a31-4a93-b44e-e814a8579c5a": {"__data__": {"id_": "b0079c2e-5a31-4a93-b44e-e814a8579c5a", "embedding": null, "metadata": {"page_label": "30", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c90da06b-2d2d-4da1-a71b-54219707de7e", "node_type": "4", "metadata": {"page_label": "30", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f3f8521024535ec21816e941baf23efc8451135e7b64ce7d8ee77a7d02e42571", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "987566c4-5808-45aa-9c51-d7d0453c83a1", "node_type": "1", "metadata": {}, "hash": "1f40dbfffa794ee8b6be16048cb197b14e18d190c1cba6c15617f44113cafc62", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1 The Algorithm of the proposed VR-MCL.\nRequire: Initial model weights \u03b8, initial inner-loop learning rate \u03b2, outer-loop learning rate \u03b1,\nnumber of inner-loop steps K, memory buffer M, and the task number in total N.\n1:Sample training data T1\n(s)(s= 1, ..., K ) from current task T1.\n2:\u03b8(0)=\u03b8,\u03b80=\u03b8\n3:fori= 1toKdo\n4: obtain samples T1\n(i)\n5: \u03b8(i)=\u03b8(i\u22121)\u2212\u03b2\u2207L1\n(i)(\u03b8)\n6:end for\n7:sample \u03f50fromM \u222a T1\n8:\u02c6g\u03f50\n\u03b80=\u2207\u03b8L(\u03b8(K), \u03f50)\n9:\u03b81=\u03b80\u2212\u03b1\u02c6g\u03f50\n\u03b8010:forj= 1toNdo\n11: forb= (j\u22121)\u00d7B+ 1toj\u00d7Bdo\n12: sample training data Tj\n(s)(s= 1, ..., K ) from current task Tj.\n13: \u03b8b(0)=\u03b8b\n14: fori= 1toKdo\n15: obtain samples Tj\n(i)\n16: \u03b8b(i)=\u03b8b(i\u22121)\u2212\u03b2\u2207Lj\n(i)(\u03b8b(i\u22121))\n17: end for\n18: sample \u03f5bfromM \u222a Tj\n19: g\u03f5b\n\u03b8b=\u2207L(\u03b8b(K), \u03f5b)\n20: \u03b8\u2032\nb(0)=\u03b8b\u22121\n21: fori= 1toKdo\n22: use the same sampled training data Tj\n(i)\n23: \u03b8\u2032\nb(i)=\u03b8\u2032\nb(i\u22121)\u2212\u03b2\u2207Lj\n(i)(\u03b8\u2032\nb(i\u22121))\n24: end for\n25: g\u03f5b\n\u03b8b\u22121=\u2207L(\u03b8\u2032\nb(K), \u03f5b)\n26: \u02c6g\u03f5b\nb=g\u03f5b\nb+r(\u02c6g\u03f5b\u22121\nb\u22121\u2212g\u03f5b\n\u03b8b\u22121)\n27: \u03b8b+1=\u03b8b\u2212\u03b1\u02c6g\u03f5b\nb\n28: Update the memory buffer MwithTj\n(i)using reservoir sampling strategy.\n29: end for\n30:end for\nTable 19: Experimental results under varying task lengths with |M|= 1000 and a 95% confidence\ninterval. The Notation Ndenotes the task length.\nSeq-CIFAR-100l(N=20) Seq-TinyImageNetl(N=40)\nMethod AAA Acc AAA Acc\nSGD 9.23 \u00b10.26 3.34\u00b10.13 4.95\u00b10.16 1.19\u00b10.27\nA-GEM 10.84 \u00b10.23 3.56\u00b10.17 6.10\u00b10.14 1.67\u00b10.07\nGEM 16.04 \u00b12.58 7.01\u00b11.95 7.92\u00b10.15 2.93\u00b10.38\nER 20.46 \u00b10.48 12.71 \u00b10.28 13.75 \u00b10.12 6.82\u00b10.11\nDER++ 16.32 \u00b10.49 8.24\u00b10.41 9.39\u00b10.07 3.76\u00b10.81\nCLSER 22.03 \u00b10.96 15.39 \u00b12.36 14.93 \u00b10.36 7.74\u00b10.91\nER-OBC 21.04 \u00b10.65 15.87 \u00b11.26 14.92 \u00b10.20 8.45\u00b12.29\nLa-MAML 17.42 \u00b10.79 10.27 \u00b10.46 10.83 \u00b10.59 5.29\u00b10.28\nVR-MCL 24.29 \u00b11.07 17.44 \u00b10.97 18.28 \u00b10.14 10.54 \u00b10.30\nFig. 7 illustrates the value of L[1:j](\u03b8(K))during the training of each task Tjin Seq-CIFAR10\n(j=2,3,4,5).", "start_char_idx": 0, "end_char_idx": 1846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "987566c4-5808-45aa-9c51-d7d0453c83a1": {"__data__": {"id_": "987566c4-5808-45aa-9c51-d7d0453c83a1", "embedding": null, "metadata": {"page_label": "30", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c90da06b-2d2d-4da1-a71b-54219707de7e", "node_type": "4", "metadata": {"page_label": "30", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f3f8521024535ec21816e941baf23efc8451135e7b64ce7d8ee77a7d02e42571", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0079c2e-5a31-4a93-b44e-e814a8579c5a", "node_type": "1", "metadata": {"page_label": "30", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "394b95fdf414a8245812b6a9632593f0bafa36e0d9981a0439c77331d739fcfb", "class_name": "RelatedNodeInfo"}}, "text": "7 illustrates the value of L[1:j](\u03b8(K))during the training of each task Tjin Seq-CIFAR10\n(j=2,3,4,5). The loss L[1:j](\u03b8(K))exhibits a significant decrease within an average of 5 outer loop\nupdate steps, suggesting a close proximity between \u03b8(K)and\u03b8\u2217as stated in Proposition 2.\n30", "start_char_idx": 1745, "end_char_idx": 2024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "119826e8-fda6-4eaa-ba29-da12ec42d13f": {"__data__": {"id_": "119826e8-fda6-4eaa-ba29-da12ec42d13f", "embedding": null, "metadata": {"page_label": "1", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c11b7070-63c7-4487-801f-b89d4cab6f3b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "5e85830fcb4460a2bd6b07f1407562bc064ed67e0fc948094880136f5cb6c355", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nZIPFORMER : A FASTER AND BETTER ENCODER FOR\nAUTOMATIC SPEECH RECOGNITION\nZengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang,\nYifan Yang, Zengrui Jin, Long Lin, Daniel Povey\nXiaomi Corp., Beijing, China\ndpovey@xiaomi.com\nABSTRACT\nThe Conformer has become the most popular encoder model for automatic speech\nrecognition (ASR). It adds convolution modules to a Transformer to learn both\nlocal and global dependencies. In this work we describe a faster, more memory-\nefficient, and better-performing Transformer, called Zipformer. Modeling changes\ninclude: 1) a U-Net-like encoder structure where middle stacks operate at lower\nframe rates; 2) reorganized block structure with more modules, within which we\nre-use attention weights for efficiency; 3) a modified form of LayerNorm called\nBiasNorm allows us to retain some length information; 4) new activation functions\nSwooshR and SwooshL work better than Swish. We also propose a new optimizer,\ncalled ScaledAdam, which scales the update by each tensor\u2019s current scale to keep\nthe relative change about the same, and also explictly learns the parameter scale.\nIt achieves faster convergence and better performance than Adam. Extensive ex-\nperiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the\neffectiveness of our proposed Zipformer over other state-of-the-art ASR models.\nOur code is publicly available at https://github.com/k2-fsa/icefall.\n1 I NTRODUCTION\nEnd-to-end models have achieved remarkable success in automatic speech recognition (ASR). An\neffective encoder architecture that performs temporal modeling on the speech sequence plays a vital\nrole in end-to-end ASR models. A most prominent example is Conformer (Gulati et al., 2020),\nwhich combines the advantages of the convolutional neural network (CNN) models (Zhang et al.,\n2017; Li et al., 2019; Kriman et al., 2020) and Transformer models (Dong et al., 2018; Karita et al.,\n2019; Zhang et al., 2020b). By integrating CNN into Transformer (Vaswani et al., 2017), Conformer\nis able to extract both local and global dependencies on speech sequences, and achieves state-of-the-\nart performance in ASR.\nIn this work, we propose a faster, more memory-efficient, and better-performing Transformer as\nASR encoder, called Zipformer . First, unlike Conformer that operates on the sequence at a constant\nframe rate, Zipformer adopts a U-Net-like (Ronneberger et al., 2015) structure, which consists of\nmultiple stacks downsamping the sequence to various lower frame rates. Second, we re-design the\nblock structure, which is equipped with more modules like two Conformer blocks, and reuses the\nattention weights for efficiency. We propose BiasNorm as a simpler replacement of LayerNorm,\nwhich allows for retaining length information in normalization. We also replace Swish with our\nnew activation functions SwooshR andSwooshL to achieve better results. In addition, we devise a\nparameter-scale-invariant version of Adam, called ScaledAdam , which scales the update by the cur-\nrent parameter scale and also explicitly learns the parameter scale. Compared to Adam, ScaledAdam\nenables faster convergence and better performance.\nExtensive experiments are conducted on LibriSpeech, Aishell-1, and WenetSpeech datasets, and\nresults demonstrate the effectiveness of the proposed modeling and optimization-related innovations.\nZipformer achieves state-of-the-art results on all three datasets. It is worth mentioning that Zipformer\nis the first model ever to achieve results comparable to those reported in the Conformer paper on\nthe LibriSpeech dataset (these results have proved difficult for others to reproduce). In terms of\nefficiency, Zipformer converges faster during training and speeds up the inference by more than\n1", "start_char_idx": 0, "end_char_idx": 3799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76b27637-f500-4fc1-ab75-4e0fb567ef0a": {"__data__": {"id_": "76b27637-f500-4fc1-ab75-4e0fb567ef0a", "embedding": null, "metadata": {"page_label": "2", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c2aa291-1b69-43f9-b095-0f6ec4d4ee0d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c33eca0fcc6add684a69e513e91297bcd81a12047f462c6034062fcd5208833b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7acd1cb-bb39-44f1-aac6-aec3e0ca7ee6", "node_type": "1", "metadata": {}, "hash": "ccbb164d54125252c1679449d9ef341168c7d82bb7dda4eb8f5da0c9e0a481da", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n50% compared to previous studies while requiring less GPU memory. We perform detailed ablation\nstudies to investigate the contribution of individual components.\n2 R ELATED WORK\nModel architecture. Deep convolution architectures have been applied to end-to-end ASR (Zhang\net al., 2017; Li et al., 2019). Follow-up works explore improvements by using depthwise separable\nconvolutions (Howard et al., 2017) for efficiency (Kriman et al., 2020), and incorporating squeeze-\nand-excitation module (Hu et al., 2018) to capture longer context (Han et al., 2020). Inspired by\nthe success of Transformer (Vaswani et al., 2017) in natural language processing (NLP) field, some\nworks adapt Transformer to speech applications (Dong et al., 2018; Karita et al., 2019; Zhang et al.,\n2020b; Wang et al., 2020; Zhang et al., 2020a). Compared to CNN, the remarkable benefit of\nTransformer is that it can learn global dependencies based on self-attention, which is essential for\nspeech processing task. By integrating convolution into Transformer, Conformer (Gulati et al., 2020)\ngains powerful capability of modeling both local and global contextual information, and outperforms\nall previous ASR models.\nRecent works explore architecture changes on Conformer to further reduce the computational cost\nand improve the recognition performance. Squeezeformer (Kim et al., 2022) adopts a temporal\nU-Net structure in which the middle modules operate at half frame rates, and also redesigns the\nblock structure to make it similar to the standard Transformer block (Vaswani et al., 2017). Branch-\nformer (Peng et al., 2022) incorporates parallel branches to model various ranged context, in which\none branch captures the local context with convolutional gating multi-layer perceptron (MLP), while\nthe other branch learns long-range dependencies with self-attention. E-Branchformer (Kim et al.,\n2023) further improves Branchformer by enhancing the branch merging mechanism by convolution-\nbased module.\nZipformer shares similar ideas about temporal downsampling as the previous work Squeezeformer.\nHowever, compared to the fixed downsampling ratio in Squeezeformer, Zipformer operates at differ-\nent downsampling ratios at different encoder stacks and uses much more aggressive downsampling\nratios in the middle encoder stacks. In addition to the modeling differences, our work also focuses on\noptimization-related changes including a new optimizer ScaledAdam , which are shown to improve\nconvergence in the experiments.\nEnd-to-end framework. Connectionist temporal classification (CTC) (Graves et al., 2006) is one\nof the earliest frameworks for end-to-end ASR, but its performance is limited by the frame indepen-\ndent assumption. To this end, a hybrid architecture that integrates attention-based encoder-deocder\n(AED) (Chan et al., 2015) in CTC (Watanabe et al., 2017) (CTC/AED) is proposed to improve\nthe performance. Neural transducer (Graves, 2012), commonly known as RNN-T, addresses the\nframe independence assumption using a label decoder and a joint network and becomes a popular\nframework due to its superior performance. Recently, various approaches such as pruning (Kuang\net al., 2022; Wang et al., 2023; Mahadeokar et al., 2021) or batch-splitting (Kuchaiev et al., 2019)\nare proposed to accelerate the training speed and reduce memory usage of neural transducers.\n3 M ETHOD\n3.1 D OWNSAMPLED ENCODER STRUCTURE\nFigure 1 presents the overall architecture of the proposed Zipformer model. Different from Con-\nformer (Gulati et al., 2020) that processes the sequence at a fixed frame rate of 25Hz, Zipformer\nuses a U-Net-like structure learning temporal representation at different resolutions in a more effi-\ncient way. Specifically, given the acoustic features with frame rate of 100Hz, the convolution-based\nmodule called Conv-Embed first reduces the length by a factor of 2, resulting in a 50Hz embedding\nsequence.", "start_char_idx": 0, "end_char_idx": 3946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7acd1cb-bb39-44f1-aac6-aec3e0ca7ee6": {"__data__": {"id_": "f7acd1cb-bb39-44f1-aac6-aec3e0ca7ee6", "embedding": null, "metadata": {"page_label": "2", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c2aa291-1b69-43f9-b095-0f6ec4d4ee0d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c33eca0fcc6add684a69e513e91297bcd81a12047f462c6034062fcd5208833b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76b27637-f500-4fc1-ab75-4e0fb567ef0a", "node_type": "1", "metadata": {"page_label": "2", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "94ea1cf28c2615a6c3b7e50e85cc8db155745dd20f9109b78610f7a4c266d12c", "class_name": "RelatedNodeInfo"}}, "text": "3 M ETHOD\n3.1 D OWNSAMPLED ENCODER STRUCTURE\nFigure 1 presents the overall architecture of the proposed Zipformer model. Different from Con-\nformer (Gulati et al., 2020) that processes the sequence at a fixed frame rate of 25Hz, Zipformer\nuses a U-Net-like structure learning temporal representation at different resolutions in a more effi-\ncient way. Specifically, given the acoustic features with frame rate of 100Hz, the convolution-based\nmodule called Conv-Embed first reduces the length by a factor of 2, resulting in a 50Hz embedding\nsequence. The obtained sequence is then fed into 6 cascaded stacks to learn temporal representa-\ntion at frame rates of 50Hz, 25Hz, 12.5Hz, 6.25Hz, 12.5Hz, and 25Hz, respectively. Except for the\nfirst stack, the other stacks all adopt the downsampled structures, processing the sequence at lower\nframe rates. The frame rate between stacks is consistently 50Hz. Different stacks have different\nembedding dimensions, and the middle stacks have larger dimensions. The output of each stack is\ntruncated or padded with zeros to match the dimension of the next stack. The final encoder output\ndimension is set to the maximum of all stacks\u2019 dimensions. Specifically, if the last stack output has\nthe largest dimension, it is taken as the encoder output; otherwise, it is concatenated from differ-\n2", "start_char_idx": 3397, "end_char_idx": 4728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7df97166-f136-48f7-97a4-e2edafdcbc89": {"__data__": {"id_": "7df97166-f136-48f7-97a4-e2edafdcbc89", "embedding": null, "metadata": {"page_label": "3", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26543aee-e965-4abc-a05d-c989100253ff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "0ea93346b818e89cec3693cdf3345f8a7a46c1320a4141a2c8b8eed957c3f7dc", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nZipformerBlocks50Hz\nUpsampleDownsample25HzBypass50Hz50Hz100 Hz\nZipformerBlocksConv-Embed50Hz\nUpsampleDownsample12.5HzBypassZipformerBlocks\nUpsampleDownsample6.25HzBypassZipformerBlocks\nDownsample25Hz50Hz50Hz50HzUpsampleDownsample12.5HzZipformerBlocksBypassUpsampleDownsample25HzBypassZipformerBlocks2\u00d72\u00d73\u00d74\u00d73\u00d72\u00d750Hz\nFigure 1: Overall architecture of Zipformer.\nFeed-forwardMulti-head Attention WeightNon-linear Attention+Self-Attention+Convolution+Feed-forward+Bypass+Self-AttentionConvolution+Feed-forward+\nBypass+BiasNormattention weights Zipformer Block\nAttention weightsSingle-headAttentionxtanhxLinearNon-linear Attention\nLinearLinearLinear\nFigure 2: (Left): Zipformer block structure. (Right): Non-Linear Attention module structure.\nent pieces of stack outputs, taking each dimension from the most recent output that has it present.\nFinally, a Downsample module converts the sequence to 25Hz, resulting in the encoder output.\nConv-Embed. InConv-Embed we use three 2-D convolutional layers with time \u00d7frequency strides\nof1\u00d72,2\u00d72, and 1\u00d72, and output channels of 8, 32, and 128, respectively. Subsequently,\nwe utilize one ConvNeXt layer (Liu et al., 2022) similar to Nextformer (Jiang et al., 2022), which\nis composed of a depth-wise convolution with kernel size of 7\u00d77, a point-wise convolution with\n384 output channels, a SwooshL activation function (described in Section 3.4), and a point-wise\nconvolution with 128 output channels. Residual connection is applied on the ConvNeXt module.\nFinally, a linear layer followed by a BiasNorm (described in Section 3.3) is used to adjust the feature\ndimension to match the first stack.\nDownsampled stacks. In the downsampled stacks, the pairwise Downsample andUpsample mod-\nules perform symmetric scaling down and scaling up in sequence length, respectively, using almost\nthe simplest methods. For example, with a factor of 2, the Downsample module averages every 2\nframes with 2 learnable scalar weights (after softmax normalization), and the Upsample module just\nrepeats each frame twice. After downsampling, it employs the stacking Zipformer blocks (described\nin Section 3.2) for temporal modeling at lower frame rates. Finally, it utilizes the Bypass module\n(described in Section 3.2) to combine the stack input and stack output in a learnable way.\n3.2 Z IPFORMER BLOCK\nConformer block consists of four modules: feed-forward, Multi-Head Self-Attention (MHSA), con-\nvolution, and feed-forward. MHSA learns global context by two steps: computing attention weights\nusing the dot-product operation and aggregating different frames with these attention weights. How-\never, MHSA typically accounts for a large computational cost, since above two steps both require\nquadratic complexity with respect to the sequence length. Hence, we decompose MHSA into two\nindividual modules according to above two steps: Multi-Head Attention Weight ( MHAW ) and Self-\nAttention ( SA). This change allows to perform the attention computation twice more efficiently in\neach block by using one MHAW module and two SAmodules. In addition, we propose a new module\ncalled Non-Linear Attention ( NLA) to make full use of the computed attention weights to capture\nthe global information.\nAs illustrated in Figure 2 (Left), Zipformer block is equipped with about twice the depth of the\nConformer block (Gulati et al., 2020). The main motivation is to allow the re-use of the attention\nweights to save time and memory. Specifically, the block input is first fed into an MHAW module,\nwhich calculates the attention weights and shares them with an NLA module and two SAmodules.\nMeanwhile, the block input is also fed into a feed-forward module followed by the NLA module.\n3", "start_char_idx": 0, "end_char_idx": 3737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "722967fe-b1a7-406b-bf44-c42fadd1c7b7": {"__data__": {"id_": "722967fe-b1a7-406b-bf44-c42fadd1c7b7", "embedding": null, "metadata": {"page_label": "4", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1191fc05-c2d6-46fb-a40e-a81d11a95925", "node_type": "4", "metadata": {"page_label": "4", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ab6d64f2e49487058bebcdefecac414484d7aa05073932e8b9b6eba68213453c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "142ff1e5-a27f-4899-a52d-e70db747ee1d", "node_type": "1", "metadata": {}, "hash": "2c3d2d53a6b957eec8a2e569dc3cfa3c40265f23f1ee606d511caa134182c6c8", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nThen it applies two module groups, each consisting of SA, convolution, and feed-forward. Finally, a\nBiasNorm (described in Section 3.3) is used to normalize the block output. In addition to the regular\nresidual connections using adding operation, each block utilizes two Bypass modules to combine the\nblock input and the module outputs, placed in the middle and end of the block. Note that different\nfrom regular Transformer models (Vaswani et al., 2017), we don\u2019t use normalization layer such\nas LayerNorm (Ba et al., 2016) for each module to periodically prevent activations from becoming\neither too large or too small, since our proposed ScaledAdam optimizer is able to learn the parameter\nscales (described in Section 3.5).\nNon-Linear Attention. Figure 2 (Right) presents the NLA structure. It also leverages the pre-\ncomputed attention weights from MHAW to aggregate the embedding vectors over the time axis,\nwhich is similar to SA. Specifically, it first projects the input with 3 linear layers to A,B, and C,\neach being of 3/4 input dimension. The module output is linear( A\u2299attention(tanh( B)\u2299C)),\nwhere \u2299denotes the element-wise multiplication, attention represents matrix-multiplying on the\ntime axis by a single head of previously computed attention weights, and the linear layer recovers\nthe dimension to the same as the input.\nBypass. TheBypass module learns channel-wise scalar weights cto combine the module input x\nand module output y:(1\u2212c)\u2299x+c\u2299y. In training, we initially limit the values of cin range\nof[0.9,1.0]and then change the minimum to 0.2 after 20000 steps. We found that making modules\n\u201cstraight-through\u201d at the beginning (i.e. allowing very little bypass) helps model convergence..\n3.3 B IASNORM\nConformer (Gulati et al., 2020) utilizes LayerNorm (Ba et al., 2016) to normalize the module acti-\nvations. Given xwithDchannels, LayerNorm is formulated as:\nLayerNorm( x) =x\u2212E[x]p\nVar[x] +\u03f5\u2299\u03b3+\u03b2. (1)\nSpecifically, it first computes the mean E[x]and the standard-deviationp\nVar[x]for normalizing,\nscaling the vector length to\u221a\nD. Then it uses the learnable channel-wise scale \u03b3and bias \u03b2for\ntransformation, which helps to adjust the size of activations and balance the relative contributions of\nspecific modules. However, we observe that the trained Conformer using LayerNorm suffers from\ntwo problems: 1) It sometimes sets one channel to a large constant value, e.g. 50. We argue that it\naims to \u201cdefeat\u201d the LayerNorm which fully removes the vector length, functioning as a very large\nvalue so that length information could be retained after normalization. 2) Some modules (typically\nfeed-forward or convolution) are \u201cdead\u201d as they have extremely small output values, e.g., 10\u22126.\nWe argue that early in training, the un-trained modules are not useful so they are \u201cturned off\u201d by\nthe LayerNorm scale \u03b3approaching zero. If the scale \u03b3oscillates around zero, the inconsistent\nsign constantly reverses the gradient directions back-propagating to the modules. Because of the\ninconsistent gradient sign, the modules never learn anything useful, since this is a bad local optimum\nwhich is hard to escape because of the dynamics of stochastic gradient descent-like updates.\nTo address above problems, we propose the BiasNorm which is intended to be a simpler replacement\nof LayerNorm. Specifically, BiasNorm is formulated as:\nBiasNorm (x) =x\nRMS[ x\u2212b]\u00b7exp(\u03b3), (2)\nwhere bis the learnable channel-wise bias, RMS[ x\u2212b]is the root-mean-square value taken over\nchannels, and \u03b3is a scalar. We first remove the operation of mean subtraction since it is a waste of\ntime unless it follows a non-linearity. The bias bserves as the large constant value which allows to\nretain the vector length information after normalization. Since the scale exp(\u03b3)is always positive,\nit avoids the gradient oscillation problem.", "start_char_idx": 0, "end_char_idx": 3865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "142ff1e5-a27f-4899-a52d-e70db747ee1d": {"__data__": {"id_": "142ff1e5-a27f-4899-a52d-e70db747ee1d", "embedding": null, "metadata": {"page_label": "4", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1191fc05-c2d6-46fb-a40e-a81d11a95925", "node_type": "4", "metadata": {"page_label": "4", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ab6d64f2e49487058bebcdefecac414484d7aa05073932e8b9b6eba68213453c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "722967fe-b1a7-406b-bf44-c42fadd1c7b7", "node_type": "1", "metadata": {"page_label": "4", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a7334fd0ceef27c181d6de59f275c9683f18dadfa3c7b8a789c4321e88ba1b9d", "class_name": "RelatedNodeInfo"}}, "text": "Because of the\ninconsistent gradient sign, the modules never learn anything useful, since this is a bad local optimum\nwhich is hard to escape because of the dynamics of stochastic gradient descent-like updates.\nTo address above problems, we propose the BiasNorm which is intended to be a simpler replacement\nof LayerNorm. Specifically, BiasNorm is formulated as:\nBiasNorm (x) =x\nRMS[ x\u2212b]\u00b7exp(\u03b3), (2)\nwhere bis the learnable channel-wise bias, RMS[ x\u2212b]is the root-mean-square value taken over\nchannels, and \u03b3is a scalar. We first remove the operation of mean subtraction since it is a waste of\ntime unless it follows a non-linearity. The bias bserves as the large constant value which allows to\nretain the vector length information after normalization. Since the scale exp(\u03b3)is always positive,\nit avoids the gradient oscillation problem.\n3.4 S WOOSH RAND SWOOSH LACTIVATION FUNCTIONS\nConformer (Gulati et al., 2020) adopts Swish (Ramachandran et al., 2017) activation function with\nthe following formula:\nSwish( x) =x\u00b7(1 + exp( \u2212x))\u22121. (3)\n4", "start_char_idx": 3026, "end_char_idx": 4069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ab92e01-67b2-408b-b8b5-cd0a393d2c73": {"__data__": {"id_": "0ab92e01-67b2-408b-b8b5-cd0a393d2c73", "embedding": null, "metadata": {"page_label": "5", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f1b73a8-4a72-476a-83b3-25cbf32febde", "node_type": "4", "metadata": {"page_label": "5", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4a6ef79bdc89be8ad5c7e46477b814a7e6ff50b72d40e31c0cea66f8ec3cb6a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1aca5e72-ac85-4913-9ed8-ca5ccc87c076", "node_type": "1", "metadata": {}, "hash": "cc285218afd7878ddc50dd91ea5033ef1cab4095d6171bc5d4690e1404665d13", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nIn this work, we propose two new activation functions respectively called SwooshR andSwooshL as\nreplacements of Swish:\nSwooshR (x) = log(1 + exp( x\u22121))\u22120.08x\u22120.313261687 ,\nSwooshL (x) = log(1 + exp( x\u22124))\u22120.08x\u22120.035.(4)\nInSwooshR , the offset 0.313261687 is to make it pass through the origin; in SwooshL , the offset\n0.035 was tuned, which slightly outperformed the value exactly making the curve pass through the\norigin. We present the curves of Swish, SwooshR , and SwooshL in Appendix Section A.2. SwooshL\nis roughly a right shifted version of SwooshR . Note that the suffix \u201cL\u201d or \u201cR\u201d represents whether the\nleft or right zero-crossing is at or around x= 0. Similar to Swish, SwooshR andSwooshL have lower\nbounds and are non-monotonic. Compared to Swish, the most striking difference is that SwooshR\nandSwooshL have non-vanishing slopes for negative inputs, which helps to escape from situations\nwhere the input is always negative and prevents the denominator term in Adam-type updates from\ngetting dangerously small. When replacing Swish with SwooshR , we observe that the modules with\nbypass connections, such as feed-forward and ConvNeXt, tend to learn a large negative bias in\nthe preceding linear layer to learn \u201cnormally-off\u201d behavior. Therefore, we use SwooshL for these\n\u201cnormally-off\u201d modules and use SwooshR for convolution modules and the rest of Conv-Embed .\n3.5 S CALED ADAM OPTIMIZER\nWe propose a parameter-scale-invariant version of Adam (Kingma & Ba, 2014) called ScaledAdam ,\nwhich enables faster convergence and better performance. ScaledAdam scales each parameter\u2019s\nupdate proportional to the scale of that parameter, and also explicitly learns the parameter scale.\nAlgorithm 1 in Appendix Section A.1.1 presents the pseudo-code of the ScaledAdam .\nLetf(\u03b8)be the loss function that we aim to minimize, which is differentiable w.r.t. the learnable\nparameter \u03b8. At each step t, Adam computes the parameter gradient gt=\u2207\u03b8f(\u03b8t\u22121), and updates\nthe first moment mt=\u03b21\u00b7mt\u22121+(1\u2212\u03b21)\u00b7gtand the second moment vt=\u03b22\u00b7vt\u22121+(1\u2212\u03b22)\u00b7g2\nt\nof gradients, where \u03b21, \u03b22\u2208[0,1)are coefficients used to compute the moving averages. The\nparameter update \u2206tat step tis formulated as:\n\u2206t=\u2212\u03b1t\u00b7p\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\u00b7mt\u221avt+\u03f5, (5)\nwhere \u03b1tis the learning rate typically specified by an external schedule,\u221a\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1is the bias-correction\nterm, and \u03f5= 10\u22128. Whilst Adam is invariant to gradient scale of each parameter, we argue that\nit still suffers from two limitations: 1) The update \u2206tin Equation 5 does not take into account\nthe parameter scale (denoted as rt\u22121). Considering the relative parameter change \u2206t/rt\u22121, Adam\nmight cause learning in relative terms too slowly for parameters with large scales, or too fast for\nparameters with small scales. 2) It is difficult to learn the parameter scale directly, as the direction of\ngrowing or shrinking the parameter tensor is a very specific direction in a large-dimensional space.\nIt\u2019s particularly difficult to shrink a parameter, since each gradient step gtadds noise which tends to\ngrow the parameter norm.\nScaling update. To keep the relative change \u2206t/rt\u22121over parameters of varying scales about the\nsame, we scale the update \u2206tin Equation 5 by the parameter scale rt\u22121:\n\u2206\u2032\nt=\u2212\u03b1t\u00b7rt\u22121\u00b7p\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\u00b7mt\u221avt+\u03f5. (6)\nWe compute the parameter scale rt\u22121as the root-mean-square value RMS[ \u03b8t\u22121].", "start_char_idx": 0, "end_char_idx": 3382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1aca5e72-ac85-4913-9ed8-ca5ccc87c076": {"__data__": {"id_": "1aca5e72-ac85-4913-9ed8-ca5ccc87c076", "embedding": null, "metadata": {"page_label": "5", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f1b73a8-4a72-476a-83b3-25cbf32febde", "node_type": "4", "metadata": {"page_label": "5", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "4a6ef79bdc89be8ad5c7e46477b814a7e6ff50b72d40e31c0cea66f8ec3cb6a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ab92e01-67b2-408b-b8b5-cd0a393d2c73", "node_type": "1", "metadata": {"page_label": "5", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "ec77ccffdb168442057e02b4658a12876e481f55ab472748898afd8fc33c3294", "class_name": "RelatedNodeInfo"}}, "text": "2) It is difficult to learn the parameter scale directly, as the direction of\ngrowing or shrinking the parameter tensor is a very specific direction in a large-dimensional space.\nIt\u2019s particularly difficult to shrink a parameter, since each gradient step gtadds noise which tends to\ngrow the parameter norm.\nScaling update. To keep the relative change \u2206t/rt\u22121over parameters of varying scales about the\nsame, we scale the update \u2206tin Equation 5 by the parameter scale rt\u22121:\n\u2206\u2032\nt=\u2212\u03b1t\u00b7rt\u22121\u00b7p\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\u00b7mt\u221avt+\u03f5. (6)\nWe compute the parameter scale rt\u22121as the root-mean-square value RMS[ \u03b8t\u22121]. Because the\nScaledAdam update is less prone to divergence than Adam, we use a learning rate schedule called\nEden that does not have a long warm-up period; we also use absolutely larger learning rate values\nbecause the parameter RMS value is normally much less than one.\nLearning parameter scale. To explicitly learn the parameter scale, we treat it as a regular parameter\nto be learned, as if we have factored each parameter as \u03b8=r\u00b7\u03b8\u2032, and we are doing gradient descent\non the parameter scale rand the underlying parameter \u03b8\u2032. Let hbe the gradient of the parameter\nscale r, at step twe get ht=\u2207rf(\u03b8t\u22121) =gt\u00b7\u03b8\u2032\nt\u22121. Since Adam is nearly invariant to changes in\nthe gradient scale, for simplicity we replace this with ht=gt\u00b7(rt\u22121\u2299\u03b8\u2032\nt\u22121) =gt\u00b7\u03b8t\u22121. Following\nthe Adam algorithm, we maintain the first moment nt=\u03b21\u00b7nt\u22121+ (1\u2212\u03b21)\u00b7htand the second\nmoment wt=\u03b22\u00b7wt\u22121+ (1\u2212\u03b22)\u00b7h2\ntof the scale gradients ht. The parameter change on \u03b8caused\n5", "start_char_idx": 2789, "end_char_idx": 4314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d7c20dd-3f8e-4a6d-b2db-f784daa11d75": {"__data__": {"id_": "8d7c20dd-3f8e-4a6d-b2db-f784daa11d75", "embedding": null, "metadata": {"page_label": "6", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ac14750-0d08-4951-8f28-d995dbcba3f4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f800961ecf960806719f02b9ea69c1d8aa0b32b650197f9d97c43ba2d88cfae2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2afc2f7-55e6-440e-b676-51c17f1e7ded", "node_type": "1", "metadata": {}, "hash": "d22d253cf2daedacdec368c6eded9b8e7ca65333e566054c043098e3f43535e5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nby updating parameter scale from rt\u22121tortis\u2206\u2032\nt,r= (rt\u2212rt\u22121)\u2299\u03b8\u2032\nt\u22121. Similar to Equation 6,\nwe also integrate the parameter scale rt\u22121into the update \u2206\u2032\nt,r:\n\u2206\u2032\nt,r=\u2212\u03b7\u00b7\u03b1t\u00b7rt\u22121\u00b7p\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\u00b7nt\u221awt+\u03f5\u2299\u03b8\u2032\nt\u22121\n=\u2212\u03b7\u00b7\u03b1t\u00b7p\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\u00b7nt\u221awt+\u03f5\u2299\u03b8t\u22121.(7)\nwhere \u03b7is a scaling factor on learning rate \u03b1t, and we found that setting \u03b7= 0.1helps to stabilize\nthe training. Now the update \u2206\u2032\ntis replaced with \u2206\u2032\nt,r+\u2206\u2032\nt, which amounts to adding an extra\ngradient term in the direction of growing or shrinking each parameter. This also allows to simplify\nthe network structure by removing most of normalization layers in our Zipformer Block (described in\nSection 3.2), since the modules now can easily learn to scale the activations in a suitable range. One\nsimilar method called weight normalization (Salimans & Kingma, 2016) decouples the parameter\nnorm from its direction to speed up the convergence. It replaces each parameter with two parameters,\nrespectively specifying the direction and the magnitude. However, ScaledAdam learns the parameter\nscales by adding an extra update term \u2206\u2032\nt,r, which makes writing the modeling code simpler.\nEden schedule. The proposed Eden learning rate schedule is formulated as:\n\u03b1t=\u03b1base\u00b7 \nt2+\u03b12\nstep\n\u03b12\nstep!\u22120.25\n\u00b7 \ne2+\u03b12\nepoch\n\u03b12\nepoch!\u22120.25\n\u00b7linear( \u03b1start, twarmup , t). (8)\nHerein, tis the step index, eis the epoch index, \u03b1stepand\u03b1epoch respectively control the num-\nber of steps and number of epochs after which we start significantly decreasing the learning rate,\nlinear( \u03b1start, twarmup , t)is a warmup scale increasing linearly from \u03b1start to 1 over twarmup steps\nand then staying constant at 1, \u03b1baseis the maximum value when setting \u03b1start= 1, \u03b1warmup = 0.\nThe reason for making Eden dependent on both the step index tand the epoch index eis to keep\nthe amount of parameter change after certain amount of training data (e.g., one hour) approximately\nconstant when we change the batch size, so the schedule parameters should not have to be re-tuned\nif we change the batch size. Other versions of Eden replace the \u201cepoch\u201d parts of the formula with\nsome suitable measure of the amount of data seen. In this work, we use \u03b1base= 0.045,\u03b1start= 0.5,\nandtwarmup = 500 .\nEfficient implementation. To speedup the optimization in ScaledAdam , we group the parameters\ninto batches according to their shape and perform the computation batch by batch. Note that this\ndoesn\u2019t affect the outcome. ScaledAdam just requires a little more memory than Adam to cache the\ngradient moments ntandwt(in Equation 7) for the parameter scales.\n4 E XPERIMENTS\n4.0.1 E XPERIMENTAL SETUP\nArchitecture variants. We build our Zipformer variants with three model scales: small ( Zipformer -\nS), medium ( Zipformer -M), and large ( Zipformer -L). For the 6 encoder stacks, the numbers of at-\ntention heads are set to {4,4,4,8,4,4 }, the convolution kernel sizes are set to {31,31,15,15,15,31 }. In\neach attention head, the query dimension and value dimension are set to 32 and 12, respectively. For\nthe three feed-forward modules in each Zipformer block, the hidden dimensions in the first one and\nthe last one are 3/4 and 5/4 of that in the middle one. We adjust the layers numbers, the embedding\ndimensions, and the hidden dimensions of the middle feed-forward in each stack to obtain different\nmodel scales:\nTable 1: Configuration of Zipformer at three different scales.", "start_char_idx": 0, "end_char_idx": 3421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2afc2f7-55e6-440e-b676-51c17f1e7ded": {"__data__": {"id_": "e2afc2f7-55e6-440e-b676-51c17f1e7ded", "embedding": null, "metadata": {"page_label": "6", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ac14750-0d08-4951-8f28-d995dbcba3f4", "node_type": "4", "metadata": {"page_label": "6", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f800961ecf960806719f02b9ea69c1d8aa0b32b650197f9d97c43ba2d88cfae2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d7c20dd-3f8e-4a6d-b2db-f784daa11d75", "node_type": "1", "metadata": {"page_label": "6", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "89c072af792bf337e3b10c09f0be07e8425c9c56510022b640b95ff1ef3d565f", "class_name": "RelatedNodeInfo"}}, "text": "We build our Zipformer variants with three model scales: small ( Zipformer -\nS), medium ( Zipformer -M), and large ( Zipformer -L). For the 6 encoder stacks, the numbers of at-\ntention heads are set to {4,4,4,8,4,4 }, the convolution kernel sizes are set to {31,31,15,15,15,31 }. In\neach attention head, the query dimension and value dimension are set to 32 and 12, respectively. For\nthe three feed-forward modules in each Zipformer block, the hidden dimensions in the first one and\nthe last one are 3/4 and 5/4 of that in the middle one. We adjust the layers numbers, the embedding\ndimensions, and the hidden dimensions of the middle feed-forward in each stack to obtain different\nmodel scales:\nTable 1: Configuration of Zipformer at three different scales.\nScale layer-numbers embedding-dimensions feed-forward-dimensions\nS {2,2,2,2,2,2 }{192,256,256,256,256,256 }{512,768,768,768,768,768 }\nM {2,2,3,4,3,2 }{192,256,384,512,384,256 }{512,768,1024,1536,1024,768 }\nL {2,2,4,5,4,2 }{192,256,512,768,512,256 }{512,768,1536,2048,1536,768 }\nDatasets. We perform experiments to compare our Zipformer with state-of-the-other models on\nthree open-source datasets: 1) LibriSpeech (Panayotov et al., 2015) which consists of about 1000\n6", "start_char_idx": 2663, "end_char_idx": 3890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1ecda72-4777-4a19-8cdf-31b1e1766383": {"__data__": {"id_": "b1ecda72-4777-4a19-8cdf-31b1e1766383", "embedding": null, "metadata": {"page_label": "7", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34e1d986-db11-4c4f-8889-68d952cc3892", "node_type": "4", "metadata": {"page_label": "7", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "424ad304fc798d9af4422bc29828e88aea466b6936ae394731cf4185f151e13d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2183f7a0-acc2-43dd-bfca-10f97cbe1364", "node_type": "1", "metadata": {}, "hash": "8f9f1e391f00fc945a8e018c46ed35a8a9c68c29d404f47b755b39705e90c6b2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nhours of English audiobook reading; 2) Aishell-1 (Bu et al., 2017) which contains 170 hours of\nMandarin speech; 3) WenetSpeech (Zhang et al., 2022a) which consists of 10000+ hours of multi-\ndomain Mandarin speech.\nImplementation details. We use Lhotse ( \u02d9Zelasko et al., 2021) toolkit for speech data preparation.\nThe model inputs are 80-dimension Mel filter-bank features extracted on 25ms frames with frame\nshift of 10ms. Speed perturbation (Ko et al., 2015) with factors of 0.9, 1.0, and 1.1 is used to augment\nthe training data. SpecAugment (Park et al., 2019) is also applied during training. We use mixed\nprecision training for our Zipformer models. We also employ the activation constraints including\nBalancer andWhitener to ensure training consistency and stability. The details of Balancer and\nWhitener are presented in Appendix Section A.3. Pruned transducer (Kuang et al., 2022), a memory-\nefficient version of transducer loss that prunes path with minor posterior is used as the training\nobjective. During decoding, beam search of size 4 with the constraint of emitting at most one symbol\nper frame is employed (Kang et al., 2023). We don\u2019t use external language models for rescoring,\nsince in this work we focus on improving the encoder model. We employ word-error-rate (WER)\nand character error rate (CER) as evaluation metric for English and Mandarin datasets, respectively.\nBy default, all of our models are trained on 32GB NVIDIA Tesla V100 GPUs. For Librispeech\ndataset, Zipformer -M and Zipformer -L are trained for 50 epochs on 4 GPUs, and Zipformer -S is\ntrained for 50 epochs on 2 GPUs. For Aishell-1 dataset, our models are trained for 56 epochs on 2\nGPUs. For WenetSpeech dataset, our models are trained for 14 epochs on 4 GPUs.\n4.0.2 C OMPARISON WITH STATE -OF-THE-ART MODELS\nIn this section, we compare the proposed Zipformer with other state-of-the-art models.\nLibriSpeech dataset. Table 2 shows the results on LibriSpeech test datasets for Zipformer and\nother state-of-the-art models. For Conformer, we also list the WERs reproduced by us and other\nopen-source frameworks. Note that there is a performance gap between the open-source reproduced\nConformer and the original Conformer. Our Zipformer -S model achieves lower WERs than all vari-\nants of Squeezeformer while having much fewer parameters and floating point operations (FLOPs).\nOurZipformer -L outperforms Squeezeformer-L, Branchformer and our reproduced Conformer-L by\na large margin while saving over 50% FLOPs. Noticeably, when trained on 8 80G NVIDIA Tesla\nA100 GPUs for 170 epochs, Zipformer -L achieves WERs of 2.00%/4.38% with sufficient computing\nresources (last row), which is the first model to approach Conformer-L to the best of our knowledge.\nWe also compare the speed and memory usage between the proposed Zipfomer and other state-of-\nthe-art models. Figure 3 presents the comparison results in terms of averaged inference time and\npeak memory usage in inference mode for batches of 30-second audios on an NVIDIA Tesla V100\nGPU. The batch size is set to 30 to ensure all models do not have out of memory problems during in-\nference. In overall, Zipformer models achieves better trade-off between performance and efficiency\nthan other models. Especially for the large scale, Zipformer -L requires much less computation time\nand memory than other counterparts.", "start_char_idx": 0, "end_char_idx": 3404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2183f7a0-acc2-43dd-bfca-10f97cbe1364": {"__data__": {"id_": "2183f7a0-acc2-43dd-bfca-10f97cbe1364", "embedding": null, "metadata": {"page_label": "7", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34e1d986-db11-4c4f-8889-68d952cc3892", "node_type": "4", "metadata": {"page_label": "7", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "424ad304fc798d9af4422bc29828e88aea466b6936ae394731cf4185f151e13d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1ecda72-4777-4a19-8cdf-31b1e1766383", "node_type": "1", "metadata": {"page_label": "7", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "bb6ac5b463f9961a42ccb74bbefcfe9b845367bb6e9fa563193d64b1d6de0b53", "class_name": "RelatedNodeInfo"}}, "text": "Noticeably, when trained on 8 80G NVIDIA Tesla\nA100 GPUs for 170 epochs, Zipformer -L achieves WERs of 2.00%/4.38% with sufficient computing\nresources (last row), which is the first model to approach Conformer-L to the best of our knowledge.\nWe also compare the speed and memory usage between the proposed Zipfomer and other state-of-\nthe-art models. Figure 3 presents the comparison results in terms of averaged inference time and\npeak memory usage in inference mode for batches of 30-second audios on an NVIDIA Tesla V100\nGPU. The batch size is set to 30 to ensure all models do not have out of memory problems during in-\nference. In overall, Zipformer models achieves better trade-off between performance and efficiency\nthan other models. Especially for the large scale, Zipformer -L requires much less computation time\nand memory than other counterparts.\n200 400 600 800 1000 1200\nAveraged inference time (ms)3.54.04.55.05.56.06.5Averaged WER (%)\nS\nMLB\nLXS\nS\nSM\nMMLLS\nM\nLZipformer, pruned transducer\nE-Branchformer, CTC/AED\nSqueezeformer, CTC\nConformer, pruned transducer\n2 4 6 8 10 12 14 16\nPeak memory usage (GB)3.54.04.55.05.56.06.5Averaged WER (%)\nS\nMLB\nLXS\nS\nSM\nMMLLS\nM\nLZipformer, pruned transducer\nE-Branchformer, CTC/AED\nSqueezeformer, CTC\nConformer, pruned transducer\nFigure 3: (Left) Averaged inference time and (Right) peak memory usage vs. WER comparison\nfor different models. The WER is averaged on LibriSpeech test-clean andtest-other . Averaged\ninference time and peak memory usage are reported for the encoders in inference mode for batches\nof 30-second audios with batch size of 30 on a single NVIDIA Tesla V100 GPU.\nAishell-1 dataset. Table 3 shows the CERs on Aishell-1 dataset. Compared to the Conformer model\nimplemented in ESPnet toolkit, our Zipformer -S achieves better performance with fewer parameters.\nScaling up the model leads to lower WERs, and Zipformer -M/L outperform all other models.\n7", "start_char_idx": 2546, "end_char_idx": 4470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "952cd26b-0cdb-4a83-8e06-58ad9daa563c": {"__data__": {"id_": "952cd26b-0cdb-4a83-8e06-58ad9daa563c", "embedding": null, "metadata": {"page_label": "8", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab60f9b5-6375-4c31-a59b-b11006df4710", "node_type": "4", "metadata": {"page_label": "8", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8367efae6dc4d7cef8c19a4921bc3476e82d70f97b99e8afa48fae05414a6d58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "401d0d83-8cbe-4aa2-a355-8eb10b3a8ec3", "node_type": "1", "metadata": {}, "hash": "4eb74b0edc10e7539ede4e387d62980bf13f8ed3b65d4d6df53312d8bbc6cf6e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 2: WER(%) comparison between different models on LibriSpeech dataset. We also in-\nclude the number of parameters and FLOPs of encoder for a 30s input audio measured with Deep-\nSpeed (Rasley et al., 2020).\u2217Trained with 8 80G NVIDIA Tesla A100 GPUs for 170 epochs.\nModel Type Params (M) GFLOPs test-clean (%) test-other (%)\nSqueezeformer-XS (Kim et al., 2022) CTC 9.0 18.2 3.74 9.09\nSqueezeformer-S (Kim et al., 2022) CTC 18.6 33.7 3.08 7.47\nSqueezeformer-SM (Kim et al., 2022) CTC 28.2 47.6 2.79 6.89\nSqueezeformer-M (Kim et al., 2022) CTC 55.6 88.4 2.56 6.50\nSqueezeformer-ML (Kim et al., 2022) CTC 125.1 183.3 2.61 6.05\nSqueezeformer-L (Kim et al., 2022) CTC 236.3 333.7 2.47 5.97\nE-Branchformer-B (Kim et al., 2023) CTC/AED 41.1 78.1 2.49 5.61\nBranchformer (Peng et al., 2022) CTC/AED 116.2 238.3 2.4 5.5\nE-Branchformer-L (Kim et al., 2023) CTC/AED 148.9 284.4 2.14 4.55\nConformer-S (Gulati et al., 2020) transducer 10.3 \u2212 2.7 6.3\nConformer-M (Gulati et al., 2020) transducer 30.7 \u2212 2.3 5.0\nConformer-L (Gulati et al., 2020) transducer 118.8 \u2212 2.1 4.3\nConformer in WeNet (Zhang et al., 2022b) CTC/AED 121.3 \u2212 2.66 6.53\nConformer in ESPnet (Miyazaki et al., 2023) CTC/AED 113.2 \u2212 2.29 5.13\nConformer-S pruned transducer 9.8 29.1 3.75 9.24\nConformer-M pruned transducer 28.4 77.0 2.96 7.11\nConformer-L pruned transducer 122.5 294.2 2.46 5.55\nZipformer -S pruned transducer 23.3 40.8 2.42 5.73\nZipformer -M pruned transducer 65.6 62.9 2.21 4.79\nZipformer -L pruned transducer 148.4 107.7 2.06 4.63\nZipformer -L\u2217pruned transducer 148.4 107.7 2.00 4.38\nWenetSpeech. Table 4 presents the experimental results on WenetSpeech dataset. Again, our Zip-\nformer -M and Zipformer -L outperform all other models on Test NetandTest Meeting test sets. With\nonly one third of the parameters, our Zipformer -S yields lower WERs than Conformer models.\nTable 3: CER(%) comparison between different models on Aishell-1 dataset.", "start_char_idx": 0, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "401d0d83-8cbe-4aa2-a355-8eb10b3a8ec3": {"__data__": {"id_": "401d0d83-8cbe-4aa2-a355-8eb10b3a8ec3", "embedding": null, "metadata": {"page_label": "8", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab60f9b5-6375-4c31-a59b-b11006df4710", "node_type": "4", "metadata": {"page_label": "8", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8367efae6dc4d7cef8c19a4921bc3476e82d70f97b99e8afa48fae05414a6d58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "952cd26b-0cdb-4a83-8e06-58ad9daa563c", "node_type": "1", "metadata": {"page_label": "8", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "96898024952b9aaef8ebcb4f4ce215b19deeee02af892785c25f6be3257f75bb", "class_name": "RelatedNodeInfo"}}, "text": "Table 4 presents the experimental results on WenetSpeech dataset. Again, our Zip-\nformer -M and Zipformer -L outperform all other models on Test NetandTest Meeting test sets. With\nonly one third of the parameters, our Zipformer -S yields lower WERs than Conformer models.\nTable 3: CER(%) comparison between different models on Aishell-1 dataset.\nModel Params (M) Type Dev Test\nConformer in ESPnet (Watanabe et al., 2018) 46.2 CTC/AED 4.5 4.9\nConformer in WeNet (Yao et al., 2021) 46.3 CTC/AED \u2212 4.61\nE-Branchformer in ESPnet (Watanabe et al., 2018) 37.9 CTC/AED 4.2 4.5\nBranchformer (Peng et al., 2022) 45.4 CTC/AED 4.19 4.43\nZipformer -S 30.2 pruned transducer 4.4 4.67\nZipformer -M 73.4 pruned transducer 4.13 4.4\nZipformer -L 157.3 pruned transducer 4.03 4.28\nTable 4: CER(%) comparison between different models on WenetSpeech dataset.\nModel Params (M) Type Dev Test Net Test Meeting\nConformer in ESPnet (Watanabe et al., 2018) 116.9 CTC/AED 9.70 8.90 15.90\nConformer in WeNet (Yao et al., 2021) 116.9 CTC/AED 8.88 9.70 15.59\nConformer-MoE(16e) (You et al., 2022) 425 CTC/AED, MoE 7.67 8.28 13.96\nConformer-MoE(32e) (You et al., 2022) \u2212 CTC/AED, MoE 7.49 7.99 13.69\nConformer-MoE(64e) (You et al., 2022) \u2212 CTC/AED, MoE 7.19 8.36 13.72\nZipformer -S 32.3 pruned transducer 7.96 8.6 13.97\nZipformer -M 75.9 pruned transducer 7.32 7.61 12.35\nZipformer -L 160.9 pruned transducer 7.29 7.24 12.06\n4.0.3 A BLATION STUDIES\nWe perform ablation experiments on LibriSpeech dataset to investigate the effect of each proposed\nfunctional technique. With Zipformer -M as the base model, we make one change each time while\nkeeping the others untouched. Table 5 presents the experimental results.\nEncoder structure. We remove the temporal downsampling structure from Zipformer and use\nConv-Embed with downsampling rate of 4 like Conformer. The resulting model has 12 Zipformer\nblocks with a constant embedding dimension of 512 and has more parameters than the base model.\n8", "start_char_idx": 1614, "end_char_idx": 3573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f6d7066-2be9-4bb8-9262-2c53a65e9939": {"__data__": {"id_": "3f6d7066-2be9-4bb8-9262-2c53a65e9939", "embedding": null, "metadata": {"page_label": "9", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c3b73f3-b95f-4239-a4e3-a33368aa8338", "node_type": "4", "metadata": {"page_label": "9", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "43271b9c46b8b2ef72996a31999946880f1dc3417c3efc9074c69c4f170f5eb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0212669-b3cf-4647-a546-c85de5ce91cf", "node_type": "1", "metadata": {}, "hash": "0da720abac53aed3e077eb7a2877058429ed5ca80b1133f405bbd2986366e2a6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 5: Ablation studies for Zipformer -M, including encoder structure, block structure, normaliza-\ntion layer, activation function and optimizer.\nAblation Params (M) test-clean (%) test-other (%)\nZipformer -M 65.6 2.21 4.79\nEncoder structure\nNo temporal downsampling 94.2 2.23 5.09\nBlock structure\nDouble Conformer-style blocks 73.9 2.18 4.95\nNoNLA 58.7 2.16 4.97\nNoNLA, no attention weights sharing 60.9 2.20 5.10\nNoBypass 65.5 2.25 4.86\nNormalization layer\nLayerNorm 65.6 2.29 4.97\nActivation function\nOnly SwooshR 65.5 2.32 5.21\nSwish 65.5 2.27 5.37\nOptimizer\nAdam 65.6 2.38 5.51\nExperimental results in Table 5 show that the resulting model without the downsampled structure\nyields higher WERs on both test set. It indicates that the temporal downsampling structure for\nefficiency does not cause information loss, but facilitates the modeling capacity with less parameters.\nBlock structure. As each Zipformer block has roughly twice modules as a Conformer block, we\nreplace each Zipformer block in the base model with two Conformer blocks stacked together. This\nleads to 0.16% absolute WER reduction on test-other even with a larger model size, suggesting\nthe benefits of Zipformer block structure. Removing either NLA orBypass leads to performance\ndegradation. If we further remove the attention weights sharing mechanism after removing NLA, the\nmodel has slightly more parameters and slower inference speed, but the WERs are not improved.\nWe hypothesize that the two attention weights inside one Zipformer block are quite consistent and\nsharing them does not harm the model.\nNormalization layer. Replacing BiasNorm with LayerNorm in Zipformer leads to WER drops\nof 0.08% and 0.18% on test-clean andtest-other , respectively. It indicates the advantage of the\nproposed BiasNorm which allows to retain some length information in normalization.\nActivation function. When using only SwooshR for all modules in Zipformer , the WER drops\nby 0.11% and 0.42% on test-clean andtest-other , respectively, which validates the effectiveness\nof particularly using SwooshL for the \u201cnormally-off\u201d modules. Employing Swish leads to more\nperformance degradation, which indicates the advantage of SwooshR over Swish.\nOptimizer. When using Adam to train Zipformer , we have to apply BiasNorm for each module in\nZipformer block to avoid model divergence, since Adam cannot learn the scale of each parameter\nto adjust the module activations like ScaledAdam . We try different learning rate factors (denoted\nas\u03b1base) for ScaledAdam (0.025, 0.035, 0.045, 0.055) and Adam (2.5, 5.0, 7.5, 10.0) separately.\nFollowing (Gulati et al., 2020), the learning rate schedule for Adam is \u03b1t=\u03b1base\u00b7512\u22120.5\u00b7\nmin(t\u22120.5, t\u00b710000\u22121.5). Figure A.2 in Appendix Section A.1.2 presents the averaged WERs on\ntest-clean andtest-other at different epochs as well as the learning rates at different steps. We\nshow the best results of ScaledAdam with\u03b1base= 0.045and Adam with \u03b1base= 7.5in Table 5.\nScaledAdam outperforms Adam by 0.17% and 0.72% on test-clean andtest-other , respectively. The\nresults indicate that ScaledAdam enables faster convergence and better performance than Adam.\n5 C ONCLUSION\nIn this work, we present the Zipformer , which serves as an efficient ASR encoder. It has an U-\nNet-like encoder structure, which downsamples the sequence to various lower frame rates. The\nre-designed block structure equipped with more modules reuses the computed attention weights for\nefficiency. It also employs the new normalization method BiasNorm , as well as the new activation\nfunctions SwooshR andSwooshL . Meanwhile, the proposed optimizer ScaledAdam enables faster\nconvergence and better performance.", "start_char_idx": 0, "end_char_idx": 3716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0212669-b3cf-4647-a546-c85de5ce91cf": {"__data__": {"id_": "d0212669-b3cf-4647-a546-c85de5ce91cf", "embedding": null, "metadata": {"page_label": "9", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c3b73f3-b95f-4239-a4e3-a33368aa8338", "node_type": "4", "metadata": {"page_label": "9", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "43271b9c46b8b2ef72996a31999946880f1dc3417c3efc9074c69c4f170f5eb1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f6d7066-2be9-4bb8-9262-2c53a65e9939", "node_type": "1", "metadata": {"page_label": "9", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "7b96460882de1d283b9b25a8d26b744f7de1c3e431358589e0d3c8590c0e9ef9", "class_name": "RelatedNodeInfo"}}, "text": "We\nshow the best results of ScaledAdam with\u03b1base= 0.045and Adam with \u03b1base= 7.5in Table 5.\nScaledAdam outperforms Adam by 0.17% and 0.72% on test-clean andtest-other , respectively. The\nresults indicate that ScaledAdam enables faster convergence and better performance than Adam.\n5 C ONCLUSION\nIn this work, we present the Zipformer , which serves as an efficient ASR encoder. It has an U-\nNet-like encoder structure, which downsamples the sequence to various lower frame rates. The\nre-designed block structure equipped with more modules reuses the computed attention weights for\nefficiency. It also employs the new normalization method BiasNorm , as well as the new activation\nfunctions SwooshR andSwooshL . Meanwhile, the proposed optimizer ScaledAdam enables faster\nconvergence and better performance. Extensive experiments on LibriSpeech, Aishell-1 and Wenet-\nSpeech datasets have demonstrated the effectiveness of the proposed Zipformer .\n9", "start_char_idx": 2912, "end_char_idx": 3857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f9bd468-fc1c-4e3c-84ad-5a7a628ad6bb": {"__data__": {"id_": "4f9bd468-fc1c-4e3c-84ad-5a7a628ad6bb", "embedding": null, "metadata": {"page_label": "10", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8ca75fb-fdee-4bfa-8228-3c57d2058e06", "node_type": "4", "metadata": {"page_label": "10", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c9d9c10e2f6f5ac722cbd10410965d0e8db66a8a0576124baf6f804760f9cc3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0bc1994-653e-4058-a205-def9a563a6cb", "node_type": "1", "metadata": {}, "hash": "66c3ebecd0f5ccdc11eb940b346b2cf276d70f405e4bf783ea11f4945af7aa2f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\nHui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin\nspeech corpus and a speech recognition baseline. In 20th conference of the oriental chapter\nof the international coordinating committee on speech databases and speech I/O systems and\nassessment (O-COCOSDA) , pp. 1\u20135, 2017.\nWilliam Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend and spell. arXiv\npreprint arXiv:1508.01211 , 2015.\nLinhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence sequence-to-sequence\nmodel for speech recognition. In IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP) , pp. 5884\u20135888, 2018.\nAlex Graves. Sequence transduction with recurrent neural networks. arXiv preprint\narXiv:1211.3711 , 2012.\nAlex Graves, Santiago Fern \u00b4andez, Faustino Gomez, and J \u00a8urgen Schmidhuber. Connectionist tem-\nporal classification: labelling unsegmented sequence data with recurrent neural networks. In\nProceedings of the 23rd international conference on Machine learning , pp. 369\u2013376, 2006.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented\nTransformer for Speech Recognition. In Proc. Interspeech 2020 , pp. 5036\u20135040, 2020.\nWei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati,\nRuoming Pang, and Yonghui Wu. Contextnet: Improving convolutional neural networks for\nautomatic speech recognition with global context. arXiv preprint arXiv:2005.03191 , 2020.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for\nmobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pp. 7132\u20137141, 2018.\nYongjun Jiang, Jian Yu, Wenwen Yang, Bihong Zhang, and Yanfeng Wang. Nextformer: A convnext\naugmented conformer for end-to-end speech recognition. arXiv preprint arXiv:2206.14747 , 2022.\nWei Kang, Liyong Guo, Fangjun Kuang, Long Lin, Mingshuang Luo, Zengwei Yao, Xiaoyu Yang,\nPiotr \u02d9Zelasko, and Daniel Povey. Fast and parallel decoding for transducer. In ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 1\u20135.\nIEEE, 2023.\nShigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang,\nMasao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al. A compar-\native study on transformer vs rnn in speech applications. In IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU) , pp. 449\u2013456, 2019.\nKwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J Han, and Shinji Watan-\nabe. E-branchformer: Branchformer with enhanced merging for speech recognition. In 2022\nIEEE Spoken Language Technology Workshop (SLT) , pp. 84\u201391.", "start_char_idx": 0, "end_char_idx": 3221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0bc1994-653e-4058-a205-def9a563a6cb": {"__data__": {"id_": "e0bc1994-653e-4058-a205-def9a563a6cb", "embedding": null, "metadata": {"page_label": "10", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8ca75fb-fdee-4bfa-8228-3c57d2058e06", "node_type": "4", "metadata": {"page_label": "10", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "c9d9c10e2f6f5ac722cbd10410965d0e8db66a8a0576124baf6f804760f9cc3b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f9bd468-fc1c-4e3c-84ad-5a7a628ad6bb", "node_type": "1", "metadata": {"page_label": "10", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "938256d3fa9e3523e85a191a482b993bf7038ae1c79726b07fea436758c4142f", "class_name": "RelatedNodeInfo"}}, "text": "1\u20135.\nIEEE, 2023.\nShigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang,\nMasao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al. A compar-\native study on transformer vs rnn in speech applications. In IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU) , pp. 449\u2013456, 2019.\nKwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J Han, and Shinji Watan-\nabe. E-branchformer: Branchformer with enhanced merging for speech recognition. In 2022\nIEEE Spoken Language Technology Workshop (SLT) , pp. 84\u201391. IEEE, 2023.\nSehoon Kim, Amir Gholami, Albert Shaw, Nicholas Lee, Karttikeya Mangalam, Jitendra Malik,\nMichael W Mahoney, and Kurt Keutzer. Squeezeformer: An efficient transformer for automatic\nspeech recognition. Advances in Neural Information Processing Systems , 35:9361\u20139373, 2022.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n10", "start_char_idx": 2631, "end_char_idx": 3624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f625d375-b0c1-4a77-936f-5ed05a143ef6": {"__data__": {"id_": "f625d375-b0c1-4a77-936f-5ed05a143ef6", "embedding": null, "metadata": {"page_label": "11", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e46d55e-fc88-4337-873e-5873ebbd5d54", "node_type": "4", "metadata": {"page_label": "11", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "096d53b0f5965490da5f80e547952532341aae372b05c1d5d54ab83342dd202a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bd790fc-c625-45a4-afea-6503d8571a99", "node_type": "1", "metadata": {}, "hash": "0a270693d89f4b9d8e5cbe4027c843d55826b31070504d7fdcfef3adc2cf3b0c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. Audio augmentation for\nspeech recognition. In Sixteenth annual conference of the international speech communication\nassociation , 2015.\nSamuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly\nLavrukhin, Ryan Leary, Jason Li, and Yang Zhang. Quartznet: Deep automatic speech recog-\nnition with 1d time-channel separable convolutions. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pp. 6124\u20136128, 2020.\nFangjun Kuang, Liyong Guo, Wei Kang, Long Lin, Mingshuang Luo, Zengwei Yao, and Daniel\nPovey. Pruned rnn-t for fast, memory-efficient asr training. arXiv preprint arXiv:2206.13236 ,\n2022.\nOleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel\nKriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al. Nemo: a toolkit for building ai\napplications using neural modules. arXiv preprint arXiv:1909.09577 , 2019.\nJason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary, Oleksii Kuchaiev, Jonathan M Cohen,\nHuyen Nguyen, and Ravi Teja Gadde. Jasper: An end-to-end convolutional neural acoustic model.\narXiv preprint arXiv:1904.03288 , 2019.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\nA convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pp. 11976\u201311986, 2022.\nJay Mahadeokar, Yuan Shangguan, Duc Le, Gil Keren, Hang Su, Thong Le, Ching-Feng Yeh, Chris-\ntian Fuegen, and Michael L Seltzer. Alignment restricted streaming recurrent neural network\ntransducer. In Proc. SLT . IEEE, 2021.\nKoichi Miyazaki, Masato Murata, and Tomoki Koriyama. Structured state space decoder for speech\nrecognition and synthesis. In IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pp. 1\u20135, 2023.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus\nbased on public domain audio books. In IEEE international conference on acoustics, speech and\nsignal processing (ICASSP) , pp. 5206\u20135210, 2015.\nDaniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and\nQuoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.\narXiv preprint arXiv:1904.08779 , 2019.\nYifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe. Branchformer: Parallel mlp-attention\narchitectures to capture local and global context for speech recognition and understanding. In\nInternational Conference on Machine Learning , pp. 17627\u201317643. PMLR, 2022.\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv\npreprint arXiv:1710.05941 , 2017.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. In Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ,\npp. 3505\u20133506, 2020.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.", "start_char_idx": 0, "end_char_idx": 3144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0bd790fc-c625-45a4-afea-6503d8571a99": {"__data__": {"id_": "0bd790fc-c625-45a4-afea-6503d8571a99", "embedding": null, "metadata": {"page_label": "11", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e46d55e-fc88-4337-873e-5873ebbd5d54", "node_type": "4", "metadata": {"page_label": "11", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "096d53b0f5965490da5f80e547952532341aae372b05c1d5d54ab83342dd202a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f625d375-b0c1-4a77-936f-5ed05a143ef6", "node_type": "1", "metadata": {"page_label": "11", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "974381d4c16878a7a04acb1996242cc087c2c469fe516f25b5a18b373ea8fbad", "class_name": "RelatedNodeInfo"}}, "text": "Branchformer: Parallel mlp-attention\narchitectures to capture local and global context for speech recognition and understanding. In\nInternational Conference on Machine Learning , pp. 17627\u201317643. PMLR, 2022.\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv\npreprint arXiv:1710.05941 , 2017.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. In Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ,\npp. 3505\u20133506, 2020.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013\nMICCAI , pp. 234\u2013241, 2015.\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\ntraining of deep neural networks. Advances in neural information processing systems , 29, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems , 30, 2017.\n11", "start_char_idx": 2466, "end_char_idx": 3713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2f44b76-8417-4eda-a0d9-cf490ff4e0b9": {"__data__": {"id_": "c2f44b76-8417-4eda-a0d9-cf490ff4e0b9", "embedding": null, "metadata": {"page_label": "12", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f581f06-c46f-4585-bec3-f839e5343a0d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "8781289d1c74f19bee149ddfb6f086fdb99dac985be4c2d2e004f05287e908b1", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nYongqiang Wang, Abdelrahman Mohamed, Due Le, Chunxi Liu, Alex Xiao, Jay Mahadeokar,\nHongzhao Huang, Andros Tjandra, Xiaohui Zhang, Frank Zhang, et al. Transformer-based acous-\ntic modeling for hybrid speech recognition. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pp. 6874\u20136878, 2020.\nYongqiang Wang, Zhehuai Chen, Chengjian Zheng, Yu Zhang, Wei Han, and Parisa Haghani. Ac-\ncelerating rnn-t training and inference using ctc guidance. In Proc. ICASSP . IEEE, 2023.\nShinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi. Hybrid\nctc/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics\nin Signal Processing , 11(8):1240\u20131253, 2017.\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson\nEnrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala,\nand Tsubasa Ochiai. ESPnet: End-to-end speech processing toolkit. In Proceedings of Inter-\nspeech , pp. 2207\u20132211, 2018.\nZhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu\nChen, Lei Xie, and Xin Lei. WeNet: Production Oriented Streaming and Non-Streaming End-to-\nEnd Speech Recognition Toolkit. In Proc. Interspeech , pp. 4054\u20134058, 2021.\nZhao You, Shulin Feng, Dan Su, and Dong Yu. 3m: Multi-loss, multi-path and multi-level neu-\nral networks for speech recognition. In 2022 13th International Symposium on Chinese Spoken\nLanguage Processing (ISCSLP) , pp. 170\u2013174. IEEE, 2022.\nPiotr \u02d9Zelasko, Daniel Povey, Jan Trmal, Sanjeev Khudanpur, et al. Lhotse: a speech data represen-\ntation library for the modern deep learning ecosystem. arXiv preprint arXiv:2110.12561 , 2021.\nBinbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu\nChen, Chenchen Zeng, et al. Wenetspeech: A 10000+ hours multi-domain mandarin corpus for\nspeech recognition. In IEEE International Conference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pp. 6182\u20136186. IEEE, 2022a.\nBinbin Zhang, Di Wu, Zhendong Peng, Xingchen Song, Zhuoyuan Yao, Hang Lv, Lei Xie, Chao\nYang, Fuping Pan, and Jianwei Niu. Wenet 2.0: More productive end-to-end speech recognition\ntoolkit. arXiv preprint arXiv:2203.15455 , 2022b.\nFrank Zhang, Yongqiang Wang, Xiaohui Zhang, Chunxi Liu, Yatharth Saraf, and Geoffrey\nZweig. Faster, simpler and more accurate hybrid asr systems using wordpieces. arXiv preprint\narXiv:2005.09150 , 2020a.\nQian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar\nKumar. Transformer transducer: A streamable speech recognition model with transformer en-\ncoders and rnn-t loss. In IEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP) , pp. 7829\u20137833, 2020b.\nYing Zhang, Mohammad Pezeshki, Phil \u00b4emon Brakel, Saizheng Zhang, Cesar Laurent Yoshua Ben-\ngio, and Aaron Courville. Towards end-to-end speech recognition with deep convolutional neural\nnetworks. arXiv preprint arXiv:1701.02720 , 2017.\n12", "start_char_idx": 0, "end_char_idx": 3082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "642e63ac-55b7-4753-96a2-a769555b8701": {"__data__": {"id_": "642e63ac-55b7-4753-96a2-a769555b8701", "embedding": null, "metadata": {"page_label": "13", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be32d9b0-fb6e-40a4-be5d-7d81e285062e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f1f507e7890b54e1aaa9bc9e280e633987e668143234cf23dbcf192b9df6f479", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA A PPENDIX\nA.1 S CALED ADAM OPTIMIZER\nA.1.1 S CALED ADAM ALGORITHM .\nAlgorithm 1 ScaledAdam Algorithm. RMS refers to root-mean-square function. g2\ntrefers to gt\u2299gt.\n\u03b1tis controlled by Eden learning rate schedule. Good default settings are \u03b21= 0.9, \u03b22= 0.98, \u03b7=\n0.1, and \u03f5= 10\u22128.\nRequire: learning rate \u03b1t; exponential decay rates for the moment estimates \u03b21, \u03b22\u2208[0,1); scaling\nfactor on the learning rate for parameter scale \u03b7; objective function f(\u03b8)with parameters \u03b8; initial\nparameter \u03b80.\nt\u21900 \u25b7Initialize step.\nm0\u21900,v0\u21900 \u25b7Initialize first and second moment of parameter gradient.\nn0\u21900, w0\u21900 \u25b7Initialize first and second moments of parameter scale gradient.\nr0\u2190RMS( \u03b80) \u25b7Initialize parameter scale.\nwhile \u03b8tnot converged do\nt\u2190t+ 1\ngt\u2190 \u2207 \u03b8ft(\u03b8t\u22121) \u25b7Get parameter gradient.\nht\u2190gt\u00b7\u03b8t\u22121 \u25b7Get parameter scale gradient.\nrt\u22121\u2190RMS( \u03b8t\u22121) \u25b7Update the parameter scale.\nmt=\u03b21\u00b7mt\u22121+ (1\u2212\u03b21)\u00b7gt \u25b7Update first moment of parameter gradient.\nvt=\u03b22\u00b7vt\u22121+ (1\u2212\u03b22)\u00b7g2\nt \u25b7Update second moment of parameter gradient.\n\u2206\u2032\nt=\u2212\u03b1t\u00b7rt\u22121\u00b7\u221a\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\u00b7mt\u221avt+\u03f5\u25b7Compute parameter change.\nnt\u2190\u03b21\u00b7nt\u22121+ (1\u2212\u03b21)\u00b7ht \u25b7Update first moment of parameter scale gradient.\nwt\u2190\u03b22\u00b7wt\u22121+ (1\u2212\u03b22)\u00b7h2\nt \u25b7Update second moment of parameter scale gradient.\n\u2206\u2032\nt,r\u2190 \u2212\u03b7\u00b7\u03b1t\u00b7\u221a\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\u00b7nt\u221awt+\u03f5\u2299\u03b8t\u22121 \u25b7Compute parameter change by updating\nparameter scale.\n\u03b8t\u2190\u03b8t\u22121+\u2206\u2032\nt+\u2206\u2032\nt,r \u25b7Update parameter.\nend while\nA.1.2 C OMPARISON BETWEEN SCALED ADAM AND ADAM .\n10 15 20 25 30 35 40 45 50\nEpoch3.503.754.004.254.504.755.005.25Averaged WER (%)\nAdam, base=2.5\nAdam, base=5.0\nAdam, base=7.5\nAdam, base=10.0\nScaledAdam,base=0.055\nScaledAdam,base=0.045\nScaledAdam,base=0.035\nScaledAdam,base=0.025\n0 20000 40000 60000 80000 100000 120000\nStep0.000.010.020.030.040.05Learning rateAdam, base=2.5\nAdam, base=5.0\nAdam, base=7.5\nAdam, base=10.0\nScaledAdam,base=0.055\nScaledAdam,base=0.045\nScaledAdam,base=0.035\nScaledAdam,base=0.025\nFigure A.2: Comparison between ScaledAdam and Adam in terms of: (Left) averaged WER on\nLibriSpeech test-clean andtest-other at different epochs; (Right) learning rate at different steps.\n13", "start_char_idx": 0, "end_char_idx": 2098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f23ab101-85fd-47d4-8d21-dd621587b676": {"__data__": {"id_": "f23ab101-85fd-47d4-8d21-dd621587b676", "embedding": null, "metadata": {"page_label": "14", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f57dd00b-449d-41bb-af66-7b2bbff74d00", "node_type": "4", "metadata": {"page_label": "14", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "f97c871f4a0de1c68558df838f387eaa2af8780224323ed0e15225d0c5add916", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nA.2 A CTIVATION FUNCTIONS\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\nx01234ySwish\nSwooshR\nSwooshL\nFigure A.3: The activation functions: Swish, SwooshR , and SwooshL .\nA.3 A CTIVATION CONSTRAINTS\nTable 6: Ablation studies of activation constraints for Zipformer -M on LibriSpeech dataset. All\nmodels are trained for 40 epochs.\nAblation test-clean (%) test-other (%)\nZipformer -M 2.21 4.91\nNoBalancer 2.23 4.97\nNoWhitener 2.25 5.15\nNoBalancer , No Whitener 2.25 5.07\nTo ensure consistency in training and avoid badly trained modules, we propose the Balancer and\ntheWhitener , which regularize the activations in a memory-efficient way. In forward pass, they are\nno-ops; in backward pass, they compute the gradients of the additional losses putting constraints on\nthe activations g\u2032, and add that to the origin activation gradients g:g=g+g\u2032. The placements\nofBalancer andWhitener may not seem to follow any very clear rules. They are generally applied\nwhen encountering specific instances of not-normally-trained models or model divergence. We first\nlocate the abnormality to a specific module and then add the Balancer orWhitener to fix it.\nA.3.1 B ALANCER\nTwo failure modes commonly observed from the channel-wise statistical distribution are: 1)the\nissue of activation values becoming excessively large or small can give rise to instability during\nthe training process, particularly when employing mixed precision training; 2)a significant number\nof \u201cdead\u201d neurons, whose outputs consistently remain negative, was observed upon examining the\nchannel-wise statistics prior to the application of the non-linear activation function within the feed-\nforward modules. Balancer solves these issues by enforcing four constraints on each channel: lower\nand upper bounds of the mean value of absolute values, denoted as aminandamax; minimum and\nmaximum proportion of positive values, denoted as pminandpmaxrespectively. Given the activation\nx, intuitively we have E[x]\u221d\u03bb, where \u03bbrepresents the proportion of positive values. Due to the\nnon-differentiable nature of positive value counting operation, a shifted version of Gaussian error\nfunction erfis introduced in order to approximate the mapping between E[x]\u2208(\u2212\u221e,\u221e)and\u03bb\u2208\n[0,1]by2\u00b7erf(x)\u22121, the inverse function of which can be approximated using fpos\u2192E/\u221a\nVar(x) =\narctanh(2 x\u22121)/(\u221a\u03c0\u00b7log 2) without loss of generality. \u00b5min=fpos\u2192E/\u221a\nVar(pmin)and\u00b5max=\nfpos\u2192E/\u221a\nVar(pmax)can be further derived from the approximation. Following the same Gaussian\nassumption, the RMS is given byR\u221e\n\u2212\u221e\u03c32\n\u221a\n2\u03c0e\u22121\n2(x\u2212\u00b5)2abs(x)dx, where \u00b5and\u03c32refer to the mean\nand variance of the Gaussian distribution. It can be approximated using fabs\u2192RMS(x) =p\n\u03c0/2\u00b7x\nwhen \u00b5\u21920. Thus rmin=fabs\u2192RMS(amin)andrmax=fabs\u2192RMS(amax)can be further derived.\n14", "start_char_idx": 0, "end_char_idx": 2799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b3786fd-1528-4f30-b0a5-3e64106df9b5": {"__data__": {"id_": "3b3786fd-1528-4f30-b0a5-3e64106df9b5", "embedding": null, "metadata": {"page_label": "15", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93175af7-25b4-4800-abab-f0827d0f9f29", "node_type": "4", "metadata": {"page_label": "15", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a5c4824fd7c9ce47c49fb325bf18518ee3bdb09de9700f2177ef3e65a130ccd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99d0505e-c2a0-4be5-9dd3-83d3f4d92094", "node_type": "1", "metadata": {}, "hash": "ec30442913a146f93753239a941e6268a3153c0fac7ccf7c666b14c0c20cb3e8", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nSpecifically, the additional loss Lbalancer conditioned on these constraints is defined as:\nLRMS =|log(min(max(RMS[ x], rmax), rmin)/RMS[ x])|,\nLE/\u221a\nVar=|E[x]/p\nVar[x]\u2212clamp(E[ x]/p\nVar[x], \u00b5min, \u00b5max)|,\nLbalancer =LRMS+LE/\u221a\nVar,(9)\nwhere the statistics RMS[ x],E[x], andp\nVar[x]are calculated in each channel. Before adding the\nadditional gradient g\u2032=\u2207xLbalancer to the original activation gradient g,g\u2032is scaled to g\u2032=\ng\u2032\u00b7\u03b1/RMS[ g\u2032]\u00b7 |g|. Herein, \u03b1is used to prevent g\u2032from overwhelming g, and the per-element\nmagnitude |g|is used to prevent the model from concentrating its \u201cfixes\u201d to the data distribution in\nframes with small gradients such as the padding frames. We set \u03b1= 0.04in this work.\nA.3.2 W HITENER\nAnother failure mode on activations is that for the feature covariance, one or a few eigenvalues\nare dominating while others are extremely small. This tends to happen in a model that is about\nto diverge. Whitener encourages a more informative output distribution, by restricting the feature\ncovariance after mean subtraction to have less unequal eigenvalues. Specifically, for output x\u2208\nRN\u00d7DwithNframes of D-dimensional features, we first compute the covariance matrix C=\n(x\u2212E[x])T(x\u2212E[x]), where C\u2208 RD\u00d7D, and E[x]is per-channel mean. The auxiliary loss which\nmeasures the whiten metric Lwhiten is defined as:\nLwhitener = (X\ni\u03bb2\ni/D)/(X\ni\u03bbi/D)2= (X\niX\njC2\ni,j/D)/(X\niCi,i/D)2,(10)\nwhere \u03bb={\u03bb1, . . . , \u03bb D}are the eigenvalues of the covariance matrix C. To keep the original\nactivation gradient gdominant after adding the additional gradient g\u2032=\u2207xLwhitener ,g\u2032is scaled\ntog\u2032=g\u2032\u00b7\u03b1/\u21132(g\u2032)\u00b7\u21132(g), where \u21132denotes the L2 norm, and \u03b1is set to 0.01. The modification\ng=g+g\u2032is done only when the whiten metric Lwhiten is above a certain value wminto prevent\nthe model from learning pathological activation distributions. We usually set wminto 10.\nA.3.3 A BLATION STUDIES .\nWe perform ablation experiments on LibriSpeech dataset to validate the effect of Balancer and\nWhitener . Table 6 presents the experimental results. All models are trained for 40 epochs. Removing\nBalancer does not lead to obvious change on model performance. However, it would increase the\nrisk of model divergence without the value range constraints especially when employing mixed\nprecision training. Removing Whitener results in 0.04% and 0.24% WER reduction on test-clean\nandtest-other , respectively. This indicates that restricting the feature covariance to have less unequal\neigenvalues in Whitener can boost performance.\nA.4 E XPERIMENTS ON LIBRISPEECH DATASET\nA.4.1 T RAINING CONFIGURATIONS OF ZIPFORMER MODELS\nBefore training, the Mel filter-bank features are per-computed and saved to disk. In training, we use\nDynamicBucketingSampler in Lhotse toolkit ( \u02d9Zelasko et al., 2021) to form the batches, where the\nbatch size is determined dynamically given the constraint of the maximum total speech duration (in\nseconds). Table 7 presents the training configurations of Zipformer models on LibriSpeech dataset\nwith speed perturbation with factors of 0.9, 1.0, and 1.1.\nA.4.2 C OMPARISON WITH STATE -OF-THE-ART MODELS\nAs an extension of Table 2, Table 8 adds the results on LibriSpeech dataset for Zipformer with\nCTC and CTC/AED architectures respectively. For the Zipformer CTC/AED model, we use a 6-\nlayer Transformer as AED decoder, each layer with attention dimension of 512, attention heads\nnumber of 8, and feed-forward hidden dimension of 2048.", "start_char_idx": 0, "end_char_idx": 3480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99d0505e-c2a0-4be5-9dd3-83d3f4d92094": {"__data__": {"id_": "99d0505e-c2a0-4be5-9dd3-83d3f4d92094", "embedding": null, "metadata": {"page_label": "15", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93175af7-25b4-4800-abab-f0827d0f9f29", "node_type": "4", "metadata": {"page_label": "15", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "a5c4824fd7c9ce47c49fb325bf18518ee3bdb09de9700f2177ef3e65a130ccd1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b3786fd-1528-4f30-b0a5-3e64106df9b5", "node_type": "1", "metadata": {"page_label": "15", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "d978f1a8c3b217e16fa23d01389f94d575d96c719170e17c38f948ebe2018efa", "class_name": "RelatedNodeInfo"}}, "text": "Table 7 presents the training configurations of Zipformer models on LibriSpeech dataset\nwith speed perturbation with factors of 0.9, 1.0, and 1.1.\nA.4.2 C OMPARISON WITH STATE -OF-THE-ART MODELS\nAs an extension of Table 2, Table 8 adds the results on LibriSpeech dataset for Zipformer with\nCTC and CTC/AED architectures respectively. For the Zipformer CTC/AED model, we use a 6-\nlayer Transformer as AED decoder, each layer with attention dimension of 512, attention heads\nnumber of 8, and feed-forward hidden dimension of 2048. The Zipformer CTC models are trained\nfor 100 epochs while the Zipformer CTC/AED models are trained for 50 epochs. Detailed training\nconfigurations are provided in Section A.4.1.\n15", "start_char_idx": 2952, "end_char_idx": 3661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65c7ec7b-c283-4417-8a8b-3c763a75f1e9": {"__data__": {"id_": "65c7ec7b-c283-4417-8a8b-3c763a75f1e9", "embedding": null, "metadata": {"page_label": "16", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6438614d-0a54-4fd4-b0a9-0065fa40a2e1", "node_type": "4", "metadata": {"page_label": "16", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aeb0614d0329b2aea224fc6eb7b4dfb4c191a7e5d8f760b311bd3f09912c2771", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e14d646-fc98-4fad-bbd2-c5a01464e7d5", "node_type": "1", "metadata": {}, "hash": "c6acea7f5f12506aab3826c8894e8faa99e29b552b134907cc33bf4179b448cf", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 7: Training configurations of Zipformer models on LibriSpeech dataset.\nModel Type Params (M) Max duration (s) GPUs Epochs Training time / epoch (m)\nZipformer -S CTC 22.1 1700 2 32G Tesla V100 100 86\nZipformer -M CTC 64.3 1400 4 32G Tesla V100 100 60\nZipformer -L CTC 147.0 1200 4 32G Tesla V100 100 76\nZipformer -S CTC/AED 46.3 1700 2 32G Tesla V100 50 105\nZipformer -M CTC/AED 90.0 1200 4 32G Tesla V100 50 67\nZipformer -L CTC/AED 174.3 1200 4 32G Tesla V100 50 84\nZipformer -S pruned transducer 23.3 1500 2 32G Tesla V100 50 87\nZipformer -M pruned transducer 65.6 1000 4 32G Tesla V100 50 69\nZipformer -L pruned transducer 148.4 1000 4 32G Tesla V100 50 80\nZipformer -L pruned transducer 148.4 2200 8 80G Tesla A100 200 18\nFor the CTC systems, Zipformer -M outperforms Squeezeformer-ML on both test sets with only\nabout half the number of parameters, and Zipformer -L also surpasses Squeezeformer-L by 0.27%\nontest-other with fewer parameters. For CTC/AED systems, Zipformer -M outperforms Conformer\nmodels and Branchformer, while Zipformer -L achieves comparable results with E-Branchformer-L.\nNote that as presented in Figure 3, Zipformer -L is much more efficient than E-Branchformer-L.\nTable 8: WER(%) comparison between different models on LibriSpeech dataset. We also in-\nclude the number of parameters and FLOPs of encoder for a 30s input audio measured with Deep-\nSpeed (Rasley et al., 2020).\u2217Trained with 8 80G NVIDIA Tesla A100 GPUs for 170 epochs.", "start_char_idx": 0, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e14d646-fc98-4fad-bbd2-c5a01464e7d5": {"__data__": {"id_": "5e14d646-fc98-4fad-bbd2-c5a01464e7d5", "embedding": null, "metadata": {"page_label": "16", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6438614d-0a54-4fd4-b0a9-0065fa40a2e1", "node_type": "4", "metadata": {"page_label": "16", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "aeb0614d0329b2aea224fc6eb7b4dfb4c191a7e5d8f760b311bd3f09912c2771", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65c7ec7b-c283-4417-8a8b-3c763a75f1e9", "node_type": "1", "metadata": {"page_label": "16", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}, "hash": "51648784a590147cc59680cf706b73ff04f6430c5830692a992b821d2417c0e0", "class_name": "RelatedNodeInfo"}}, "text": "For CTC/AED systems, Zipformer -M outperforms Conformer\nmodels and Branchformer, while Zipformer -L achieves comparable results with E-Branchformer-L.\nNote that as presented in Figure 3, Zipformer -L is much more efficient than E-Branchformer-L.\nTable 8: WER(%) comparison between different models on LibriSpeech dataset. We also in-\nclude the number of parameters and FLOPs of encoder for a 30s input audio measured with Deep-\nSpeed (Rasley et al., 2020).\u2217Trained with 8 80G NVIDIA Tesla A100 GPUs for 170 epochs.\nModel Type Params (M) GFLOPs test-clean (%) test-other (%)\nSqueezeformer-XS (Kim et al., 2022) CTC 9.0 18.2 3.74 9.09\nSqueezeformer-S (Kim et al., 2022) CTC 18.6 33.7 3.08 7.47\nSqueezeformer-SM (Kim et al., 2022) CTC 28.2 47.6 2.79 6.89\nSqueezeformer-M (Kim et al., 2022) CTC 55.6 88.4 2.56 6.50\nSqueezeformer-ML (Kim et al., 2022) CTC 125.1 183.3 2.61 6.05\nSqueezeformer-L (Kim et al., 2022) CTC 236.3 333.7 2.47 5.97\nE-Branchformer-B (Kim et al., 2023) CTC/AED 41.1 78.1 2.49 5.61\nBranchformer (Peng et al., 2022) CTC/AED 116.2 238.3 2.4 5.5\nE-Branchformer-L (Kim et al., 2023) CTC/AED 148.9 284.4 2.14 4.55\nConformer-S (Gulati et al., 2020) transducer 10.3 \u2212 2.7 6.3\nConformer-M (Gulati et al., 2020) transducer 30.7 \u2212 2.3 5.0\nConformer-L (Gulati et al., 2020) transducer 118.8 \u2212 2.1 4.3\nConformer in WeNet (Zhang et al., 2022b) CTC/AED 121.3 \u2212 2.66 6.53\nConformer in ESPnet (Miyazaki et al., 2023) CTC/AED 113.2 \u2212 2.29 5.13\nConformer-S pruned transducer 9.8 29.1 3.75 9.24\nConformer-M pruned transducer 28.4 77.0 2.96 7.11\nConformer-L pruned transducer 122.5 294.2 2.46 5.55\nZipformer -S CTC 22.1 40.8 2.85 6.91\nZipformer -M CTC 64.3 62.9 2.51 6.02\nZipformer -L CTC 147.0 107.7 2.49 5.7\nZipformer -S CTC/AED 46.3 40.8 2.46 6.04\nZipformer -M CTC/AED 90.0 62.9 2.22 4.97\nZipformer -L CTC/AED 174.3 107.7 2.09 4.59\nZipformer -S pruned transducer 23.3 40.8 2.42 5.73\nZipformer -M pruned transducer 65.6 62.9 2.21 4.79\nZipformer -L pruned transducer 148.4 107.7 2.06 4.63\nZipformer -L\u2217pruned transducer 148.4 107.7 2.00 4.38\n16", "start_char_idx": 997, "end_char_idx": 3038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"306c842f-5201-4c69-bde4-51e163f6a8d3": {"node_ids": ["dfbd86a8-4fb6-49dc-b7b5-5f7bf0044228"], "metadata": {"page_label": "1", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1c4638a3-09fe-4150-82da-08358b86148d": {"node_ids": ["1b1a973e-8068-4232-80f2-27d58c512046", "aa9b9ad1-a17f-403e-ae50-4edaa65e1699"], "metadata": {"page_label": "2", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4d845450-949d-4be6-a703-0fb85bddcc88": {"node_ids": ["97d85b92-91d7-4b08-b9b3-17b801c27740", "f1f0289d-46b1-42b0-b194-03028f8d6844"], "metadata": {"page_label": "3", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4858a74b-9d67-419d-875c-ebc25cb4769d": {"node_ids": ["e70d5e86-7c5c-4c13-ac53-f84b4be6dc72", "d1d408ab-8634-4fd0-af31-95bcb98d370a"], "metadata": {"page_label": "4", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "99baeb5b-9dde-474b-9e5b-5491b3377a67": {"node_ids": ["50eeb5d3-27b1-40e8-929f-c86167b8f268", "19d1a6e7-c1d7-4d61-ad31-1ac9dc7ebb93"], "metadata": {"page_label": "5", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4626d870-ea0c-443c-a936-f1eaa3f26a07": {"node_ids": ["d876b1b9-7372-4a6a-9d6b-21aa0b6862c5", "c9534790-348b-4e19-8ccc-e82228affa98"], "metadata": {"page_label": "6", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "91e7a275-998f-4f10-9179-50adf7b60c7f": {"node_ids": ["9fbbaab6-57d1-44d2-a124-522b61c76309", "dbc7c63e-5ef5-490e-acc5-fd2f6c0ebf5b"], "metadata": {"page_label": "7", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8a6ce8f6-7443-4759-892a-29effe63279e": {"node_ids": ["706cd728-283b-4a79-bd2c-b3e9aed8768e"], "metadata": {"page_label": "8", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4542d1f1-69f1-4c7b-9b9a-3ff526452567": {"node_ids": ["afae5beb-35b5-481b-a8a1-c4fed277fa88", "677b9409-3a7f-4435-bd3f-d5722301a839"], "metadata": {"page_label": "9", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "27f4fe09-6d1a-43b7-80d6-1141e649d833": {"node_ids": ["7d37cc2a-905e-4764-9351-721059585ca3"], "metadata": {"page_label": "10", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f8770bec-e174-49fe-b9f3-2e84fdcd9c29": {"node_ids": ["2105cbdb-2810-49a6-90bc-5ffda4b6f1db", "03723e5a-4b45-4c95-9d7e-8f4ba2a7a0f1"], "metadata": {"page_label": "11", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "38c8a6e0-b8eb-4c17-b531-cf82f11fea38": {"node_ids": ["794b0645-5371-4c56-a4ae-cfa78c17c149", "24931f73-8ee5-4e4f-9433-f1d7bb5cff63"], "metadata": {"page_label": "12", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f4452e4d-eadb-49ff-9224-5d250d10655b": {"node_ids": ["c2ba4600-ec5a-4568-b071-65b21b9e7596"], "metadata": {"page_label": "13", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "231bed0b-b454-4b9b-9e4d-dc51752a53b7": {"node_ids": ["dfaee86c-16e2-41b9-a151-fe5f9475a467"], "metadata": {"page_label": "14", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4141474b-5fb3-40f8-aa98-3e6384c57222": {"node_ids": ["adf07e33-c916-4d04-a6c2-26dbfa740520"], "metadata": {"page_label": "15", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9b7f80e0-10cd-437f-a08e-c3b127dd1538": {"node_ids": ["c67b9127-6fd6-4e4f-9d5a-f86626a1eb62"], "metadata": {"page_label": "16", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "06d088b8-5eda-4b9e-ac62-da838d762942": {"node_ids": ["c54351a0-be42-451e-90ca-b2fae7aea8e6"], "metadata": {"page_label": "17", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "138366aa-9edc-4d4c-b373-19046c971eba": {"node_ids": ["00cad084-ca58-4bfe-84ff-a039e455c537", "3b50e938-f100-4101-a4a9-420338a87ae2"], "metadata": {"page_label": "18", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8b4cc148-ac40-4b06-9f69-f4df8a60dd9d": {"node_ids": ["b066a024-a9e8-4584-bc9b-2d4aad98ea5e"], "metadata": {"page_label": "19", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "39624053-dde6-4ea9-852c-443830e4ecf8": {"node_ids": ["cfdcce89-47f0-4376-b11f-d27a9333a05e"], "metadata": {"page_label": "20", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2294b0a3-849d-45fb-a53b-2f76c832eee7": {"node_ids": ["40e857e9-7e1f-4237-bf85-5ebe11e153ca"], "metadata": {"page_label": "21", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5f6bde8d-81bb-4b6c-91b2-e8d863479e7d": {"node_ids": ["6a4c5027-2aa1-4f19-aff8-ce1d6b46c1c4"], "metadata": {"page_label": "22", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e04f264f-2e84-4943-a290-1fa8f433aeab": {"node_ids": ["6c1c0966-944e-4b54-8827-f9ebd2ee5b1a"], "metadata": {"page_label": "23", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "c439521c-3007-4df2-8c24-b6059ff2cd3c": {"node_ids": ["5c67474e-eb78-4b68-b42b-2205f2906233"], "metadata": {"page_label": "24", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bed7d6f8-9979-401e-99db-b997782f4e41": {"node_ids": ["aa8f3238-53ac-4bee-aaac-64ea9357eeb7"], "metadata": {"page_label": "25", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e6a668b1-4ef3-41cc-8a07-4ba5588f5fe4": {"node_ids": ["e78b5b2d-cd55-4f01-834c-7a9a9268a726"], "metadata": {"page_label": "26", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b77532fb-90c3-4db7-9740-9e258c503842": {"node_ids": ["1960534a-f49e-4367-bf42-5e927a959d3f"], "metadata": {"page_label": "27", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "03c5d1b1-39ca-48d9-b7a4-0f43551df237": {"node_ids": ["48666463-7577-42fb-9f71-7a116602dacf"], "metadata": {"page_label": "28", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d706acd9-cfd8-47a6-99a9-b25a633422dc": {"node_ids": ["88c29aaf-290a-40df-bbfd-f69ba959fbed"], "metadata": {"page_label": "29", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0402fbb8-a671-40ea-b1b9-a9c000f70e84": {"node_ids": ["a5c267d1-2c12-41bd-b506-2c6e2b6af6cf"], "metadata": {"page_label": "30", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5a51cfb0-1f89-439b-84a4-a4cdeb44c35e": {"node_ids": ["02d4e6fa-f78b-46b0-8c6f-7ee896e8de99"], "metadata": {"page_label": "31", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7b6b7623-3e92-486a-9246-e55bf4f57213": {"node_ids": ["f7df1e37-c117-445a-a4d3-c6150193f826"], "metadata": {"page_label": "32", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5cfcc56d-1f80-4d9b-bfbc-7abf798b6e90": {"node_ids": ["677825e8-bf11-4653-bcfe-89b1a4c84e2a"], "metadata": {"page_label": "33", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2cc29683-f6a1-4dd5-9462-306396fa44fb": {"node_ids": ["9a544d0d-caee-4b41-b4a2-5a91456e5c66"], "metadata": {"page_label": "34", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "667d0d09-2139-45ed-80e1-35b4bd002b26": {"node_ids": ["b182bec2-78e4-444e-8551-a20649c3c428"], "metadata": {"page_label": "35", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1431b9b7-8bd3-48b5-9831-67496411bc5b": {"node_ids": ["47cb69ab-2357-4a7c-960b-1520f23edb67", "b551233f-62c0-4a48-a446-89b57e3c8512"], "metadata": {"page_label": "36", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "914b30a5-3038-470e-9154-5085e03e47da": {"node_ids": ["5815851b-147e-4713-8847-1e408c7dcaa0"], "metadata": {"page_label": "37", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ac9d831f-0f21-4599-998d-b88c04e19f60": {"node_ids": ["b16e11da-f73b-4e92-9a80-2fca1e986d78"], "metadata": {"page_label": "38", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bbb97f66-b566-4573-afc2-3bce0fa780e8": {"node_ids": ["ebaa0d0a-17c3-41e2-ba9b-4be7a1dd5ef7"], "metadata": {"page_label": "39", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b78b1b71-1564-4d20-aeb1-de79e1028204": {"node_ids": ["bc5698c3-478c-42d7-93fc-4c756ba3126e"], "metadata": {"page_label": "40", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2481ee02-d949-4574-9175-9f9a58fecf88": {"node_ids": ["5de5f5e8-d376-4b8d-b85c-c45b81ebff82"], "metadata": {"page_label": "41", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7735730c-373c-4966-b578-a1aba8ffb625": {"node_ids": ["210a3a30-78af-428f-b853-f849a6b31f7d"], "metadata": {"page_label": "42", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0bd10107-94d6-49b0-8764-6d1666bd7262": {"node_ids": ["eb26cbc3-ecc8-4d1d-a2f4-cea75a067825"], "metadata": {"page_label": "43", "file_name": "finetune_fair_diffusion.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\finetune_fair_diffusion.pdf", "file_type": "application/pdf", "file_size": 34710410, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "984147b1-2e2e-4b79-9307-d44f5882b081": {"node_ids": ["fb31a447-ac53-4320-bc30-6985062b6212"], "metadata": {"page_label": "1", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4df5ac04-29ee-4501-83b6-fb56e12e3315": {"node_ids": ["60b2b8bf-e55c-4700-9d71-b84900a372e7", "656ad568-d8bc-4fa3-9167-16f2afee1e53"], "metadata": {"page_label": "2", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "cd407ef0-0a81-49f4-adb6-70ff92f93364": {"node_ids": ["42412901-bf61-49f2-8a6e-b65c7bb6fa8d", "58cbd730-fac9-421b-9354-0c1d4a6d051c"], "metadata": {"page_label": "3", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1e237025-f9ab-488f-9524-e65599a2abf3": {"node_ids": ["194f391d-7bf1-4e44-be08-b20ff682ee8e", "48253f1f-a0e0-4bba-8b69-c512d0eafa35"], "metadata": {"page_label": "4", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f87f2543-fba7-4ac7-8708-74fe89a048bd": {"node_ids": ["dbbc16f4-eccd-48ac-8ee3-259a52bc8041", "f7136211-c76a-4c72-85d4-ecb19ccc8b67"], "metadata": {"page_label": "5", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7ee731ee-b2d7-417c-b7a2-6e3b3a714f03": {"node_ids": ["85926df1-0928-4352-adc6-43eb8ba6716f", "980a7cf9-d1dd-45a8-a546-508ebcdf4708"], "metadata": {"page_label": "6", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8cad4573-a53f-4798-b58c-f0b612fd25b8": {"node_ids": ["0d2a969e-fc02-4aea-9f3c-f3f63450c235", "2604b6be-5406-4f8d-84b2-7d51607a4fff"], "metadata": {"page_label": "7", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a8380bcd-c579-4529-ab37-a2c9f5646886": {"node_ids": ["1b6c849b-9d4b-480b-bb94-97f9cf69a3fe", "7c3b5900-8415-435e-a053-c8ddbb3b3fa3"], "metadata": {"page_label": "8", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6da4689f-7c37-45fe-8110-a8d6af0f0c09": {"node_ids": ["6ddf4609-c938-4158-a79c-8ef107aca165", "37557ac5-fd20-4d3d-a8bf-5278ffbcdadd"], "metadata": {"page_label": "9", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f5d845b4-39c7-4b19-b4f7-77c73d3eaa08": {"node_ids": ["6ba8b328-5a4a-4bf1-af1b-7e47e1839e2e", "ebe1f4b2-1308-48ea-9816-f02cfdb7c340"], "metadata": {"page_label": "10", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8c2363f4-e830-4e7e-8505-f935c9e48962": {"node_ids": ["32e65b58-2933-479d-9bf9-20c918fd0e98", "b035c99b-ed55-42eb-9234-c69e7ec4edae"], "metadata": {"page_label": "11", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2221a159-a76a-4119-90b6-b457ec229b33": {"node_ids": ["b48dd126-66bd-4a60-92fd-2706d071597c", "a7c1efb5-62ea-48a4-a27c-9bc9804dc05d"], "metadata": {"page_label": "12", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d6d03a67-9728-4ea9-b55b-ddda143197e8": {"node_ids": ["a71b6d0d-65e6-48c4-982d-94d3b8fcff83", "442bcb98-f995-4117-b069-83cebfec5df8"], "metadata": {"page_label": "13", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "274f6e9a-bb3b-42f6-9eb3-eb72e83f3e96": {"node_ids": ["ca8b3601-21e2-49f3-aa52-371621e9e4b1", "ba5d3d58-35e5-49d6-84ae-3f5cca2c346b"], "metadata": {"page_label": "14", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ea9614f7-ab93-4cf1-af04-549f91502c03": {"node_ids": ["bf6f85cf-e0ad-494d-a582-68a4ebc210db", "6032904a-65bb-4a63-ba10-0cdb28f1e817"], "metadata": {"page_label": "15", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8daebd54-b463-4982-a1d9-1c7d1cf947f0": {"node_ids": ["864e301d-713f-4867-95a5-a0f6e9ee2020", "bebb5bd7-84ee-4517-a77b-5f8cd1773225"], "metadata": {"page_label": "16", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "323ca365-962f-4ade-9206-f8d36b1f015e": {"node_ids": ["753f7fae-6a91-449d-8c0d-eeb6403b6166"], "metadata": {"page_label": "17", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "cd6b915b-9782-4c2d-bcf7-8cde5e59aec6": {"node_ids": ["c96dc8df-5a3f-46b8-999c-f462deb80a0a", "3bc35726-2f44-4194-9b34-ce4fdcf862e2"], "metadata": {"page_label": "18", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "cf712e6d-2c81-4649-9a80-adeaa87cc907": {"node_ids": ["7b135669-519e-4e2d-9cf4-cde3630957ca"], "metadata": {"page_label": "19", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2d9e2197-d5f3-4c00-bde0-b01b92c9ed46": {"node_ids": ["157efbf8-57ff-4a00-a7c5-3e8e30d49ac9", "6b669c2e-231d-4741-b03e-3e8944fda8b4"], "metadata": {"page_label": "20", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4b7ee8e3-6316-414f-a92f-6e308c079892": {"node_ids": ["257ac7af-4f9c-4a66-945c-09eda6737d81"], "metadata": {"page_label": "21", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "11673d4b-a2ae-45f7-8a68-196580f007e8": {"node_ids": ["e4ca3a22-fd73-4983-b6ab-727580b982a5"], "metadata": {"page_label": "22", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "3ea5e683-6a8d-4e5b-9ba6-824b60f08dfc": {"node_ids": ["7709badd-9fa9-405b-8c02-9363a1e02f0f"], "metadata": {"page_label": "23", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "41d884ed-0515-4ce6-971f-748c36f2619f": {"node_ids": ["21274e41-79a6-4097-9d76-8904db3110d0", "c3dd24e6-10cd-4e9c-9993-3dc2149dc6d5", "4ee37cd5-d04f-4630-9dcc-6d686857cc43", "8f80a1fa-a918-4809-bff7-92d2963f9c64"], "metadata": {"page_label": "24", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a4dd0e45-3619-465c-9e70-09f5a399d0dc": {"node_ids": ["698fd051-49a7-462f-a8c3-9001653f41d2"], "metadata": {"page_label": "25", "file_name": "knowledge_card.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\knowledge_card.pdf", "file_type": "application/pdf", "file_size": 877083, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "52d2dbb2-3c59-41e4-95f7-270c6048e0e4": {"node_ids": ["dcfda0f0-b0ba-4d04-a510-44c8a8ace777"], "metadata": {"page_label": "1", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2ae393ab-75a5-47b0-8ae2-798daeed29a2": {"node_ids": ["1d60c481-3056-4152-88a0-347c42d85629", "08f5b37a-d8b7-4b6f-9cdc-052afc6386b7"], "metadata": {"page_label": "2", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b84781e5-6e46-4cd3-b5e4-c8ab50759d3a": {"node_ids": ["d616ae66-39fd-46ac-8eae-773d00550042"], "metadata": {"page_label": "3", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2e153496-6d8e-47d5-a72f-3175d9336797": {"node_ids": ["a50f9194-18ec-4267-8ed4-1056909b6157"], "metadata": {"page_label": "4", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f37351e4-d2ae-43df-bfb4-c94794026b36": {"node_ids": ["5ad3a187-f6f7-42ed-acda-dc20e80753ee"], "metadata": {"page_label": "5", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d048a513-38aa-4ef4-944f-de2813d6ba70": {"node_ids": ["202d9e73-2249-48b6-99e9-ae1e15070f22", "c57a8f3b-72d7-4606-aed8-01f1b89c1a31"], "metadata": {"page_label": "6", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4201f27a-ef95-4b58-a338-4d40632ce134": {"node_ids": ["79c7a385-fb6d-432d-8edf-3bd266c7760e", "663ece0b-ad9e-4c76-a9a5-cb2022eb1ca5"], "metadata": {"page_label": "7", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4e369e70-bb2b-411a-8110-fca7beb518d2": {"node_ids": ["69cb139c-0a9d-4185-88d8-ee805a5c84be"], "metadata": {"page_label": "8", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e1d11c65-d73c-4faf-a3c9-9e37af943cc2": {"node_ids": ["518949a8-0eb2-460e-b260-1a8c782d9218"], "metadata": {"page_label": "9", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "846b0ffc-7719-407a-9f67-a9f28b480b08": {"node_ids": ["59f3691a-52cd-4544-8daa-1df5182376d9", "45ebd30c-ff6e-4eb0-ab99-e22bb996115b"], "metadata": {"page_label": "10", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1e7bfadd-c783-4818-ae59-548fb6e549e3": {"node_ids": ["a31d497e-abb8-47e4-8ef1-12fc1598a0c7", "7f8a25b0-d2b9-4c43-8292-9f5bb4581b24"], "metadata": {"page_label": "11", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "30bc7272-d963-4c1c-892c-34eb0f595d63": {"node_ids": ["2dc26739-d5fe-47c3-9f52-1e8c52b7e448"], "metadata": {"page_label": "12", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9a75c85f-3a99-4a17-ad8d-da2a0e62d1eb": {"node_ids": ["714e0fae-e10a-44d6-b9ed-ce675b165990"], "metadata": {"page_label": "13", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0ad90e06-39aa-4342-8832-6ee4286c8db9": {"node_ids": ["adc54129-715d-4f5d-8b30-55d2de26eaec"], "metadata": {"page_label": "14", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "21df284e-7d94-4293-9f96-ac69368fe69a": {"node_ids": ["1d2a554e-e164-47e6-891b-0360630b1770"], "metadata": {"page_label": "15", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "79ec8438-36b0-40cf-b80e-be587f4a64a2": {"node_ids": ["853a75e8-7956-4a3b-9d0c-b7e1536fe135"], "metadata": {"page_label": "16", "file_name": "loftq.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\loftq.pdf", "file_type": "application/pdf", "file_size": 366134, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "881cb765-d982-49fa-b600-0d81db28fc02": {"node_ids": ["bd362fce-8ded-49a5-8abc-145d91ee15e2"], "metadata": {"page_label": "1", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d4fe791f-3650-46d6-8c7b-18ba71594281": {"node_ids": ["4418b598-325d-48fb-a080-cd37465ead7e", "10707d7c-f678-4d42-94b0-21e0488d93dc"], "metadata": {"page_label": "2", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1c0c5472-3d1e-44ba-80db-6ffabc7e8951": {"node_ids": ["2b0bdff0-0fc2-468a-8f1e-6d6c17081ad7", "23e8cf71-ec95-4e9e-88a0-70c3539ea815"], "metadata": {"page_label": "3", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6e7afc4c-a116-40e4-ae81-c34ed7dfaf16": {"node_ids": ["3886b414-eb71-4230-a54d-11f69a7c22a9", "94658c9a-baf2-4a0f-9807-49086e380407"], "metadata": {"page_label": "4", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "accffe54-4dc0-4d46-9257-63ea5d7f77c2": {"node_ids": ["000db7ee-1c63-4351-b9e2-a79db2a1a294", "a7fd96e1-f067-48bf-862c-587908f9d7dc"], "metadata": {"page_label": "5", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2884ad43-1757-4563-a2e4-ff52c6b9a0f8": {"node_ids": ["34339bca-1c10-4566-ad77-9d0c172c112b", "bce8a5f7-4c54-4fc9-90d5-4f23089fc9a2"], "metadata": {"page_label": "6", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6846c9f3-ff0f-4970-9650-043ebedc77a3": {"node_ids": ["6467583b-0c5e-401d-abe2-c0f3885bb686", "1a1e7258-439c-4dba-adcc-049763b8ddd9"], "metadata": {"page_label": "7", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "61cfebe8-7b9e-422a-9794-8b0275e58f81": {"node_ids": ["a23d32e9-2c46-4fec-8e99-4492ececc5cb", "737e4f5c-c368-457b-8df1-65185af2f7ef"], "metadata": {"page_label": "8", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a935f314-e0ce-4f7f-8f21-c0a4d92863b2": {"node_ids": ["82dd4d94-0cb0-40f1-9488-9f12514e30cc", "eb6217c3-8ebd-4b4b-a85b-ff898e10acdb"], "metadata": {"page_label": "9", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bc0f2687-7615-4a54-b6d1-e7092477b375": {"node_ids": ["636ab668-080e-4404-bb9f-211b7ec28ee2", "effde21b-e291-47c4-8b0e-3a4242621443"], "metadata": {"page_label": "10", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5b8cbb35-81d7-455b-a84d-4336f9016ef4": {"node_ids": ["659eeaf9-2df9-4e15-8632-eac9784343ea", "78a2cec4-f030-4841-847e-5e2706346121"], "metadata": {"page_label": "11", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "330a0a2c-53c9-47a5-a2bd-7d1e76673699": {"node_ids": ["c6ea6663-da5e-44aa-a4c0-ed63166f4572", "fe335d52-b751-4d9f-a311-949b83022ae1"], "metadata": {"page_label": "12", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ed5e741b-fc69-400d-97ec-bb53e292a299": {"node_ids": ["24fefd06-8554-491e-9b36-9a075eda7dd8"], "metadata": {"page_label": "13", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "737902c8-e286-48ad-a69d-033f5ed61b44": {"node_ids": ["d7cd0638-93dc-4673-8f14-6b60ee708a94", "efd49475-6132-42e6-86d6-e64a095aed80"], "metadata": {"page_label": "14", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d4c26588-708d-4aca-837e-52b92ad098f4": {"node_ids": ["c954c509-66e1-4109-8d82-7ab6400aa5c0"], "metadata": {"page_label": "15", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f88d11df-ec28-4033-8018-80718239ae2b": {"node_ids": ["a51ddbfc-06ee-4dd2-bde7-e5a99c9ed51e"], "metadata": {"page_label": "16", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "12a5d0ac-dac5-4548-aa38-6be3fdaa41ef": {"node_ids": ["5d744f19-b622-4dd0-be9b-c24ae1d89bd2"], "metadata": {"page_label": "17", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "3de3963b-5b0e-4560-8920-a6af9e3ba92a": {"node_ids": ["dbf2961b-dea5-475d-b77b-d42b8ca2b448", "e242c8ec-5d9b-43b4-a9cd-e846578f51ee"], "metadata": {"page_label": "18", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "60bd711d-98a0-4cb9-90db-ec803541e88a": {"node_ids": ["5494eb4a-28ca-408d-956b-3df7f4ae0bf2", "a16ada22-3f59-4794-9e30-989e297336c0"], "metadata": {"page_label": "19", "file_name": "longlora.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\longlora.pdf", "file_type": "application/pdf", "file_size": 1168720, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d8e0f6f7-bb8b-4c93-b8d1-cf74fc7a924f": {"node_ids": ["45566cc9-905d-4609-b80d-4054a59e87c2"], "metadata": {"page_label": "1", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "91b9a779-53c5-427d-b575-9f520f1a4b15": {"node_ids": ["20a50737-1a2d-4022-a005-42d7dbaf94a3"], "metadata": {"page_label": "2", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2a684c09-42b9-48cf-bb90-6ea90cbcb765": {"node_ids": ["9cfee261-7a1f-4931-b18f-7b911109358b", "34a14ea5-ec15-40d9-91c9-e50e6bd7002f"], "metadata": {"page_label": "3", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1f97e834-e51a-4773-890b-e1839fb3889e": {"node_ids": ["5590af85-534e-4ed9-82ad-8a4c1ff14c6a"], "metadata": {"page_label": "4", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a8717b08-1164-4898-8d05-18b7f9226ec1": {"node_ids": ["7d220af9-81e8-453c-a6fa-4cf1377ac13a"], "metadata": {"page_label": "5", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "71e7147a-33d9-4090-9e41-417f6f1d7e72": {"node_ids": ["82b5f605-7631-429d-b473-4b7e5bbf34f6"], "metadata": {"page_label": "6", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ec9fccdc-a752-4178-a26e-b00c8f66be44": {"node_ids": ["06e37c82-0f0e-41d9-82fe-106c47ce7ff9", "702e9027-4239-4bd9-846c-f18c602aa819"], "metadata": {"page_label": "7", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7727deff-3681-4e2a-90cb-6429bc578b9b": {"node_ids": ["6b6e2167-6e71-461b-bb21-775fc9bb8ada"], "metadata": {"page_label": "8", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "40740a4f-274d-41dc-86c0-0c33d00605dc": {"node_ids": ["e5c05947-ee92-47ce-8cd7-dd074eb597d8"], "metadata": {"page_label": "9", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "201fca88-45d6-4f71-92e6-2ec0cc8b9c0b": {"node_ids": ["eacca254-e9a9-4fda-b115-4dcbccff0f04", "3f612715-a809-4ae5-959f-4b135b2a84d7"], "metadata": {"page_label": "10", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4215217f-2d80-4f77-a3b8-8a185a5bca01": {"node_ids": ["7597db72-44af-4eda-923e-bd09a4dc373a", "10328131-02c8-4ff9-8864-7c90b56a9d00"], "metadata": {"page_label": "11", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7a5b009d-73ef-44de-9256-6404be516951": {"node_ids": ["6b8d8c4e-d684-4327-a79a-2d0ec778cebd", "3a24f00a-23fd-4cc6-91d5-82f4a54c9e21"], "metadata": {"page_label": "12", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "3ffe9e80-3fef-45ac-85a3-c278a1531c18": {"node_ids": ["85f8d3bd-5a8c-46fb-8498-e96258343397", "2df93a6d-3c5d-46fa-bb28-a3111e91efeb"], "metadata": {"page_label": "13", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "cc50cd5e-571b-4bd1-9669-58dd68716c74": {"node_ids": ["97ceeb8d-77d9-40a0-9aad-0a5f4811586a"], "metadata": {"page_label": "14", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a994c212-2d13-48c7-ae31-c5ca9f70eb42": {"node_ids": ["9e39fd88-aa12-4bac-9fa2-ad224dc2de43"], "metadata": {"page_label": "15", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "66a2d553-7e1b-4216-a0be-1048ec53a9ee": {"node_ids": ["49c287eb-c6d6-48d2-b54f-b113468e861e"], "metadata": {"page_label": "16", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9be0f482-850b-4d49-b4ce-efd96bcf646e": {"node_ids": ["cc99699c-b9c5-480b-98db-9de59ee17649"], "metadata": {"page_label": "17", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4fb0113f-4dd4-48d9-882a-3ed50046463e": {"node_ids": ["1339d356-d314-40fe-bd7a-e81a4138a3db"], "metadata": {"page_label": "18", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ca4eed97-c081-421d-ad92-f813d13c6692": {"node_ids": ["5e00101a-ce99-4c5d-848a-66e1544ffff9"], "metadata": {"page_label": "19", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "06c1c216-46a7-435d-91b6-b77e69b2e663": {"node_ids": ["bb3d898b-b154-4fc5-a5c5-6cb50decd35f"], "metadata": {"page_label": "20", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4261a0f0-9abd-4ccb-8a42-2bf93f779a81": {"node_ids": ["9660e8ee-e313-4fb3-b4e5-69e9adde059d"], "metadata": {"page_label": "21", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9c5a6ef9-f171-4a62-b1a1-1110ea63f69a": {"node_ids": ["4a4a011d-1333-47e3-b677-9f750cc69e44"], "metadata": {"page_label": "22", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5365b0d7-9626-40cf-9a2b-90ef407e45e7": {"node_ids": ["cf7e3e5c-ca6f-40af-9294-4b1b0770bcb4"], "metadata": {"page_label": "23", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a44ea320-629c-4d26-a9c9-d62bf41962ed": {"node_ids": ["c757947d-3479-400e-a599-b42f962d6ad8"], "metadata": {"page_label": "24", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "935dc53e-b916-4faa-a741-7a3fa240a804": {"node_ids": ["8f9fc708-17f7-418b-bed2-defb4bb913dd"], "metadata": {"page_label": "25", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "abc1ee0c-61cf-4f23-b569-9d2dc0a8e7db": {"node_ids": ["86ef48ad-5ce4-4111-b54c-f28f00787f58"], "metadata": {"page_label": "26", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "46e2d04d-a4d8-4764-849d-f4c6c04a7fc9": {"node_ids": ["8a883018-321b-493a-9e9c-f9f850849985"], "metadata": {"page_label": "27", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "30d464d1-866e-418c-b789-9e7801de44b9": {"node_ids": ["3374e65d-34f9-4dad-9846-a0707b6493d9"], "metadata": {"page_label": "28", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "157debf0-ea7e-4062-a6eb-1b79508a6a8d": {"node_ids": ["837013c2-af58-4471-8a21-7d197142b5e9"], "metadata": {"page_label": "29", "file_name": "metagpt.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metagpt.pdf", "file_type": "application/pdf", "file_size": 16911937, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "338628df-1354-44fe-bece-70aad2d91871": {"node_ids": ["9f5e46b8-6ee8-4d40-8251-558523b170c5"], "metadata": {"page_label": "1", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "34989f6d-d8f1-4dae-8666-a07c3565abae": {"node_ids": ["8ecbd0a1-1be3-47ee-abbe-c451c638cb3a"], "metadata": {"page_label": "2", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ecda3c23-1fe5-4285-a605-24743c348ae0": {"node_ids": ["c3998a18-6e3e-4b62-a639-a87e97294ad3", "22d92bfe-3715-4df7-9989-1338642062cb"], "metadata": {"page_label": "3", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0dad4af6-36f9-4ec4-9181-2623b16be04a": {"node_ids": ["76f8a797-6448-4fcf-a515-d2ed0423cffd", "37e7e5eb-edbe-4884-9474-6130e84e1892"], "metadata": {"page_label": "4", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bcbd7fe7-5488-40ae-8c34-56cce0cdef21": {"node_ids": ["ebd8058c-dd91-4a2f-8310-9d6846805619", "305d8671-e7d0-4663-86c1-b80ea387e109"], "metadata": {"page_label": "5", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "08f71681-5cea-4347-a5fd-e5e3fb8c0ac6": {"node_ids": ["7278749d-4e5f-40e4-9860-9b76656faa68", "ce5718e8-92da-4b87-a507-aa54f47e632c"], "metadata": {"page_label": "6", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "c200a591-8def-4aa6-9f1c-1dbeeedf89a3": {"node_ids": ["69a66ba7-3a0e-494e-88f2-b03dae9bb4ad", "056f7daa-3310-4458-bb84-56ab023cda78"], "metadata": {"page_label": "7", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "3ec61808-230a-4b6f-9917-87e45308fc81": {"node_ids": ["06ddd3eb-81d8-4353-8e05-b7b6cbb48088", "df052885-c49a-4c8b-b09f-d01134df263b"], "metadata": {"page_label": "8", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "40e43c23-3dfe-4f9f-b36b-673633c9ad2f": {"node_ids": ["f53251b1-4d89-4bdf-bf3c-2bb3f0c5e7ce", "6738e4ff-3ef2-47ac-89d7-fe274ef3c042"], "metadata": {"page_label": "9", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "62bdfbf7-2d85-4125-af36-9ac687fa789a": {"node_ids": ["c022a9d1-56f8-4001-b032-56706ce8cb5e"], "metadata": {"page_label": "10", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "531e2299-5003-43d4-97ab-3305b6a7e6bf": {"node_ids": ["695ddcaf-c48e-4976-a1b5-6715e5d74101"], "metadata": {"page_label": "11", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b08f4748-544d-4530-86b0-acb6bd9f2ae4": {"node_ids": ["af4db8cd-9b93-4948-bcdf-a1005c19f6fe"], "metadata": {"page_label": "12", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "c81fcd7d-f5a6-49e5-9ef7-266b2205171b": {"node_ids": ["66ab1f86-14d9-45c1-995a-d9fa68212ba2", "1b0612ad-35f7-44d0-a827-7157b5c0f0d8"], "metadata": {"page_label": "13", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "56119e97-408d-43be-8433-5fcde63abbc9": {"node_ids": ["98355748-83a3-40a8-87db-ada94f9cece0"], "metadata": {"page_label": "14", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bb11912f-41bb-4f7b-9328-ea892513e3bc": {"node_ids": ["15e57bf5-1352-4fc1-a31c-0a9280808957", "4ac20016-45ed-473d-8287-07cf88570733"], "metadata": {"page_label": "15", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d06b8f31-9819-4fa3-a618-95a782d11fa1": {"node_ids": ["56a53584-835b-418d-9e7a-67bb78d6111b"], "metadata": {"page_label": "16", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7149e6a7-531d-41d9-9437-233b59a8b686": {"node_ids": ["76e1b885-8869-4162-b58f-d349b606e121", "7199d6f1-eb79-4cc0-836b-95fff52d11d1"], "metadata": {"page_label": "17", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "10abe83b-9c38-48f9-9a3a-e3cb73e52724": {"node_ids": ["2ac679ba-6e65-4e28-9c76-eb0f7885a603", "b8140036-043b-4f30-860e-8fa4d03ff327"], "metadata": {"page_label": "18", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "fa2e441f-0455-454f-b46c-5c7bedec8131": {"node_ids": ["e571ddec-c04d-43d6-9062-d8903d16ab4b", "f35b6fa9-8a02-4fed-ad86-db2b73ae8398"], "metadata": {"page_label": "19", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8713fa42-15a5-4ac5-8c66-28b1ff130f52": {"node_ids": ["1c0e0d68-42da-43dd-b59d-9ce6075f1b80"], "metadata": {"page_label": "20", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8829a23e-e3d8-48a3-a27a-83d910e08765": {"node_ids": ["ffa66238-965c-4c00-b0d9-52fceab5c612"], "metadata": {"page_label": "21", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "74cd4ef2-e295-40df-81b6-e75b10d8727b": {"node_ids": ["d65c5c95-8e11-42b4-98c8-bd5e29fd1299"], "metadata": {"page_label": "22", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "89af7da1-d7fa-44d3-be3f-79a9a7533a1b": {"node_ids": ["dc83c643-b5a3-477a-b6cb-1e439d845885", "ea86f588-f74c-416f-b4b8-16bbf98f863d"], "metadata": {"page_label": "23", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7f12ccd9-3bcc-4199-9ee0-bcd3146cb88a": {"node_ids": ["6757028e-e15b-43ac-bf0c-24a14ff3be3a", "c503547b-7ec9-49e2-a8d8-8b8681bf335f"], "metadata": {"page_label": "24", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5e364d65-4c36-48ae-9925-4e244c0066b3": {"node_ids": ["7c873f19-8722-43cf-b07b-ff14d286120f"], "metadata": {"page_label": "25", "file_name": "metra.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\metra.pdf", "file_type": "application/pdf", "file_size": 4775879, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4983977d-78bf-49bb-a0c5-75c3f7bdfce8": {"node_ids": ["62d60eb9-9c59-4d35-ab76-5ac97ba09e7c"], "metadata": {"page_label": "1", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "df932b3f-cc3b-4ace-8670-dd32e92af638": {"node_ids": ["ba91977e-5db3-49d1-b622-0492eeb4d153", "b4b8efdf-2b68-4cd2-af11-48f5da910a63"], "metadata": {"page_label": "2", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "79b9ad7d-6921-41e3-9f85-5ce9fce13116": {"node_ids": ["8f7f2756-94a9-4400-9382-de7b06b42644", "6a31181d-d2f4-460d-aaf4-efe6bcc7c5a6"], "metadata": {"page_label": "3", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0f4c4639-d618-4a73-86ea-c46f2d9aa921": {"node_ids": ["b04189d5-13cb-443e-9e95-f5b0e74f400a"], "metadata": {"page_label": "4", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b80e9edb-4c3a-4660-82d6-6d3cc258b82e": {"node_ids": ["f8938a62-277f-4ab6-ab7b-8834546eb727", "f88f3f7f-7677-4f38-b7ad-0b343f57deba"], "metadata": {"page_label": "5", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a33e732d-be5a-4593-921f-33d103453654": {"node_ids": ["746de1b2-5dbd-42f4-940c-dca15ca3ce3d", "bced8172-7c3c-4699-bd87-150d5861883c"], "metadata": {"page_label": "6", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a18f9b68-e101-4e5a-89fa-cde23ec2bcf8": {"node_ids": ["bddfb8b1-5354-4d1e-8cf4-3a179b0179fc", "f1a92455-61b4-4d25-a199-0a7f10200560"], "metadata": {"page_label": "7", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8ce3dc78-dd2b-4865-bc38-f1e4807b8ca0": {"node_ids": ["fc188010-42c1-418b-9b20-49ecc2f6a77f", "bbd7d86d-a94c-4f9b-8fb2-491e6b6120a9"], "metadata": {"page_label": "8", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d6e7e157-53e0-4e1a-af8e-d43314ec5c61": {"node_ids": ["588507ff-f57f-4c02-a9c6-2821cccc0da0"], "metadata": {"page_label": "9", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ea5f0cbd-f5fc-48f6-9699-6f5bd18497ae": {"node_ids": ["f83839aa-80a7-4635-8c87-03fdbd40de6d", "c30bc458-d118-41ab-bbce-d626cd73e0a5"], "metadata": {"page_label": "10", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9fb68e17-fe10-489d-9d2b-c69ddcfdcbc9": {"node_ids": ["388f8350-0d9b-410e-bc92-ec73d3156d50", "c42699fd-fd78-4534-9774-5cb54ba0759a"], "metadata": {"page_label": "11", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d1e6a54f-08a6-44cb-9a0e-7958bcbbb3f3": {"node_ids": ["2be73afa-b526-40e8-aa2b-c36d12996363", "904ee4a3-1080-48ab-b999-805bf1f3f820"], "metadata": {"page_label": "12", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "246fb635-a3d6-42c5-b4a8-a0258b96bc82": {"node_ids": ["7bf479fc-1a49-4dda-82f4-6ceec803b7f0", "76775e1a-091f-4a5e-8404-245822957910"], "metadata": {"page_label": "13", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "84859801-425a-4208-96c8-ff12c482a9fd": {"node_ids": ["07c03225-ec5e-4907-8ab2-7ac763aa0500"], "metadata": {"page_label": "14", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "02457e77-2805-41c7-9b76-6f54ceb3d8b8": {"node_ids": ["a870b9da-692d-49a4-abc8-b48468789b24"], "metadata": {"page_label": "15", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "cb069b1b-c21a-4a17-8942-d93fb785bce2": {"node_ids": ["f178ea09-239a-4e8e-94ef-8216481c64e4", "4aff74fd-04bf-4ac6-ba4b-a05b3b399fef"], "metadata": {"page_label": "16", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "db0357ce-f2c1-4ffd-a2e4-11fde0aa692a": {"node_ids": ["266141a6-e08f-493f-93df-f4cb325c0847"], "metadata": {"page_label": "17", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "63514ab1-bc97-44ef-8ecb-3644ca44d06f": {"node_ids": ["fd3fa58b-645d-4e6e-ae69-00155f77fd8a"], "metadata": {"page_label": "18", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "fa19d8b5-b9d5-4e68-9c91-1e3793da50c2": {"node_ids": ["f74fdbf3-a1b8-48cc-9502-f36532412764"], "metadata": {"page_label": "19", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e5e4d27a-e300-4b12-914f-e187c7c3625d": {"node_ids": ["2ecab720-d49c-44ad-9a56-ffd6f5ef7282", "b1a081da-1e92-4ccf-9980-393505d382ce"], "metadata": {"page_label": "20", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "25253ec0-9c70-46d6-94cb-048378c8c666": {"node_ids": ["8f48b068-fbea-4f29-a882-67c1d2f050c7"], "metadata": {"page_label": "21", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "df869a45-fd74-47b9-ad50-4d0a86e34c52": {"node_ids": ["ab7c3898-57b9-48c6-87d1-609924cb2de3", "595e5644-799a-4d11-bb9c-feba8a980e7f"], "metadata": {"page_label": "22", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "46c365f3-dc6e-484b-840f-5b3e6ddcbf3e": {"node_ids": ["ea73bc01-5572-4775-b4fe-08d357ba5018"], "metadata": {"page_label": "23", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1293e67f-3115-43f3-a907-bcda77482792": {"node_ids": ["22e9a799-7af1-46d9-838b-79e1c1171d32", "654b27a3-a1c2-48e9-8c41-b0185d9a3246"], "metadata": {"page_label": "24", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b61d2b8f-918b-48f6-a64b-79f29dd658e1": {"node_ids": ["ed07785b-4301-4dcc-88d9-389de7f798c9"], "metadata": {"page_label": "25", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d87e31e2-84dc-44de-b814-206f0884f4d4": {"node_ids": ["281995ad-adce-4242-a5f3-018ac229d06f"], "metadata": {"page_label": "26", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e776003c-e220-4638-b7a5-da9c6f13d5aa": {"node_ids": ["024c07a1-84bc-473c-a65c-b548359ec66a"], "metadata": {"page_label": "27", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bb668445-6997-46c7-90f0-ef3f83e1b62a": {"node_ids": ["1df982b2-08c7-4636-8cb3-408c9687798a"], "metadata": {"page_label": "28", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a40950c0-eae1-417d-9e54-51b368975165": {"node_ids": ["be29d1d5-1d8b-4140-8400-f1f9b5b61ec6"], "metadata": {"page_label": "29", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e92e8db5-6032-4ca4-8d54-adb8fc6b145c": {"node_ids": ["0f7a1538-2be8-4b43-a787-124c0062c5ad"], "metadata": {"page_label": "30", "file_name": "selfrag.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\selfrag.pdf", "file_type": "application/pdf", "file_size": 1244749, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "901e01c0-2e10-40bf-88f6-dc8f32aae36e": {"node_ids": ["9353b2ce-4c17-4b53-a915-69cbf3ce1aa4"], "metadata": {"page_label": "1", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "69c41b84-56b2-4880-b79a-94cb7687707a": {"node_ids": ["03332007-40bf-4656-867c-0be4f56ff8db"], "metadata": {"page_label": "2", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "419a0728-958b-42f4-b58b-72ad9d01987a": {"node_ids": ["a5dcb08b-edf9-47e6-b7c6-a7ec657bcb08"], "metadata": {"page_label": "3", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d4a4302b-7d5d-427d-aad4-2b6e947028bf": {"node_ids": ["c9fd0bc0-2886-4809-a38b-b9838661ef5f"], "metadata": {"page_label": "4", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "51aebedf-63d5-4a7d-bb06-b251620c30fd": {"node_ids": ["2578b658-aab8-429b-aea6-98f54ee436ea"], "metadata": {"page_label": "5", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "eeadb1ad-4504-4af1-bfbe-e6421ee02e94": {"node_ids": ["4ccecc8f-8da3-4d78-b638-4c8f9c91accd"], "metadata": {"page_label": "6", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d5578a06-afc6-4310-9811-dfa11f1d3f80": {"node_ids": ["1dacbeca-5c0e-4522-ae61-82e8a78b44e5", "9857e94f-3d1b-4bea-97ce-9d7e0878ea37"], "metadata": {"page_label": "7", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "46b4e596-a3af-4322-9644-bdb7a8456ffc": {"node_ids": ["7d55cb85-ebe4-4d0c-80eb-6c976f46f698", "27d9f8b5-4a68-4d5b-91ed-5f2e5a203996"], "metadata": {"page_label": "8", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "67b4751c-ad18-4e27-8e5a-1eff6afbd4bf": {"node_ids": ["e18eba1e-8023-4e14-931d-812b61583525", "882be096-b2df-49e5-83df-fc113064203f"], "metadata": {"page_label": "9", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "a2d2562e-a2f4-459b-8289-f4c6a244106f": {"node_ids": ["6e489381-4e92-44e0-b3ca-10f21815c176"], "metadata": {"page_label": "10", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0f6e9222-24be-4e11-bc8e-d26ec5b80b46": {"node_ids": ["c5d6d49a-40c9-4a79-a8bc-33f33d7efded"], "metadata": {"page_label": "11", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0fc8f12d-a3d0-4915-a3d3-f813f7bc0285": {"node_ids": ["31a41d2d-77ba-4166-8cd4-9205060a3fb2", "2e5aa4fe-e593-4f73-9851-b762bf7a0350"], "metadata": {"page_label": "12", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "84c8b8d3-b090-4b82-b5d0-e71ea38f13b1": {"node_ids": ["757db6bb-1eee-418d-8105-c91b21967451", "60ee3206-e49f-4079-8873-ce827640b302"], "metadata": {"page_label": "13", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "eafd62c9-fa7e-4668-aa3a-c6480e5bbee9": {"node_ids": ["20e48498-0e13-481b-b7f6-d105ca7fbf1e"], "metadata": {"page_label": "14", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "65efad3b-d97f-46bb-8549-1ad86fa424ce": {"node_ids": ["5524e0c6-a72e-46ad-ae4f-7ece3d18b69c"], "metadata": {"page_label": "15", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1375e909-94e2-44c1-9dd6-1554fe0ed9d0": {"node_ids": ["3f231b3d-1d05-41e1-ad10-9b1aaa6ebe90", "8da7edaf-250f-4aa0-a7ea-f518bb233fb7"], "metadata": {"page_label": "16", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2c39ec53-9359-47a4-a215-8a631188dfdc": {"node_ids": ["1e585f73-32bc-414a-b0d7-0943bfb67b47", "03039334-257f-4bd3-a212-7f2040ccb085"], "metadata": {"page_label": "17", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "63e7e542-6cde-4e91-a0f4-abd22140a1d3": {"node_ids": ["ee81dec9-9a33-4e60-9324-241aff9a8cd3"], "metadata": {"page_label": "18", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9846989c-e9f6-4203-9f43-ead78b93b056": {"node_ids": ["54ead7d6-5c39-4830-aec3-c01bedf19cc6"], "metadata": {"page_label": "19", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0625e51f-9931-491b-a8bb-38e09c44314d": {"node_ids": ["2493352c-6848-4ce7-b76f-1450631db509"], "metadata": {"page_label": "20", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9df3aaf0-a362-413c-90fb-5dffc961dacf": {"node_ids": ["5a8948c0-755f-4bdc-83c9-20aca4c3cb7e"], "metadata": {"page_label": "21", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d7528921-b73d-466c-99e0-ea1a798b6e4e": {"node_ids": ["da300fd9-7c05-4819-87ec-063376be7192", "271fa919-b825-42a1-ae10-ae8a7bf2dbc9"], "metadata": {"page_label": "22", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1a29522c-4f4a-4bf2-81e6-1d74ca488d43": {"node_ids": ["90868e43-9681-45b6-ad81-92bf72a12554", "5d112a2f-6ed3-4c45-95ba-f10e26bd8409"], "metadata": {"page_label": "23", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "da37ecfb-be3a-4872-8984-87093d4332c5": {"node_ids": ["2340776b-533e-4ffc-9040-e45adeacaab5"], "metadata": {"page_label": "24", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7ab61080-3d7b-45a2-9eab-9deb73de4042": {"node_ids": ["53651072-ed77-4deb-8333-ec2620554703", "3e181833-f600-48ca-b018-52ee0db53259"], "metadata": {"page_label": "25", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "aecdd0ca-fe22-4f21-9364-de299f1bb421": {"node_ids": ["4bccd09d-fae4-4a62-b413-40373f96d9ea", "86c49bd4-c39f-4b8f-8f00-436dd2266263"], "metadata": {"page_label": "26", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2a786d27-e161-4888-8a09-dbe8ac79cd75": {"node_ids": ["6818b224-146d-4893-8a9a-51be06871611"], "metadata": {"page_label": "27", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4a078511-b5f0-4ff3-90db-2eb2cd1df1e8": {"node_ids": ["7b99001b-ac79-4314-96fa-4678826312f7"], "metadata": {"page_label": "28", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "db27aafa-5ffe-437c-80fe-782cb013c41b": {"node_ids": ["9e792912-93da-49d4-a338-dc8248c92cd4"], "metadata": {"page_label": "29", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "47f69e3e-d902-436e-8701-98897f601fe8": {"node_ids": ["a5ab681b-0bb0-427b-8004-e4a8dafb00cf"], "metadata": {"page_label": "30", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "3d28dc3a-4b54-4ba9-af63-f6b372141d05": {"node_ids": ["76efa852-28d6-4caa-bb09-43345c43484c"], "metadata": {"page_label": "31", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f1a34d79-00b3-46a1-9166-473f2b292e8c": {"node_ids": ["f924c019-fbc0-42be-9995-e5c026033e13"], "metadata": {"page_label": "32", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d0efa5e7-736c-4f30-b3f8-a2e1e61a1817": {"node_ids": ["3f4a9a77-6c7b-4007-bbd0-22571a327dbc"], "metadata": {"page_label": "33", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6ffc809a-9fa2-45bc-a00a-f46dd7a7f53a": {"node_ids": ["010343b2-35f0-400b-96fd-9b8272d9028b"], "metadata": {"page_label": "34", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "00c53246-49c5-480b-9396-3ef42bb4f090": {"node_ids": ["56eb98a5-c3b5-44eb-8eec-1119a9804f8b"], "metadata": {"page_label": "35", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bf5098dc-6d7a-4671-b3c8-f7f8e6972b63": {"node_ids": ["c81d2f9e-1d17-477c-9af8-e454710c3c53"], "metadata": {"page_label": "36", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2d773a3f-33a0-4638-ac85-00b61f5e71f8": {"node_ids": ["ee888244-2d6a-4c42-9d75-2f0f25c19be8"], "metadata": {"page_label": "37", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8538221a-6eaf-465d-8c19-1bcddb3035ef": {"node_ids": ["1e54d55a-5c24-445d-8b62-21e117c8b33e"], "metadata": {"page_label": "38", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e5c9c990-88d5-48d9-9f41-eb9c2c7143b8": {"node_ids": ["c1a84491-5209-44d4-96d4-95d57a6881c7"], "metadata": {"page_label": "39", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e78bb85f-ebb4-405b-87cd-18a2f5dd63df": {"node_ids": ["dc8a4c5d-8f1d-475a-8c66-d799263e379f"], "metadata": {"page_label": "40", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6629e62c-cc7b-4d80-8ea7-101f50d01dad": {"node_ids": ["e2c8ad2d-8030-45ff-ae04-aac9b9a5cae6"], "metadata": {"page_label": "41", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "15f33460-1672-40f0-9c38-0bc561f4bc5c": {"node_ids": ["e92f9ed7-3778-4e28-939b-87f513caa73c"], "metadata": {"page_label": "42", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "dee80d4d-37ba-428b-b5ad-70ead45bf44d": {"node_ids": ["085f21eb-e129-4552-9fbe-9207eb073414"], "metadata": {"page_label": "43", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5765a62e-fcdf-4246-b636-4f91b6cc1b68": {"node_ids": ["6efd25f5-0e2e-4e41-9133-419240ae7b64"], "metadata": {"page_label": "44", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "fa6ab322-8b62-4268-a9b0-5841763f99e1": {"node_ids": ["b94095fb-a0b3-4e45-970f-cccead0f6757"], "metadata": {"page_label": "45", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "25c5489c-61fd-436a-be49-d9374ed3b412": {"node_ids": ["418638b6-4c00-4c2a-822a-fb5cfdf37540"], "metadata": {"page_label": "46", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "bba78ebc-a9bf-4312-a3d9-269b18e25020": {"node_ids": ["86ab90b9-fc34-4269-b36d-1aad4eeeb386"], "metadata": {"page_label": "47", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "3c927528-dca5-4559-bf82-53e132977f9d": {"node_ids": ["64c1be50-1b0a-47e2-9bd4-b070e02d9637"], "metadata": {"page_label": "48", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "26a88d0d-72df-4257-a412-51b92921486a": {"node_ids": ["d054ebab-121d-43d2-8bc9-6e4b28e49f10"], "metadata": {"page_label": "49", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b2985ad3-ffc4-4d25-9afd-db2268e34fd7": {"node_ids": ["62fe18bd-0f14-46e7-8949-1baa4c90880f"], "metadata": {"page_label": "50", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "76794624-b8cd-4fdd-a907-0620c2ad634f": {"node_ids": ["1711f4c4-eb63-4321-802d-5538678ed38b"], "metadata": {"page_label": "51", "file_name": "swebench.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\swebench.pdf", "file_type": "application/pdf", "file_size": 2680380, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e1b31a7a-ebb2-477d-ad0a-d0a09799ea61": {"node_ids": ["6bb5da42-40e2-4735-90cc-44e4fbd399a6"], "metadata": {"page_label": "1", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "fab0a071-c1ac-4cc9-b04d-072e0e5abe4f": {"node_ids": ["c13c30a4-925f-403b-b71a-602e02f50803", "46bc2a51-03d5-487d-b4cd-4c6ef1dd0bbc"], "metadata": {"page_label": "2", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "35c748ce-0f7f-487a-94cc-c304e78cb68e": {"node_ids": ["34d0a42d-c31b-4fc5-9ef9-986fa7a67967", "3dbaee3f-d706-44d9-9d9f-cc962b61915f"], "metadata": {"page_label": "3", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b56ffd43-7fb5-4423-aac7-b1ed99005058": {"node_ids": ["70334453-1c6a-4f22-b184-a111fc664994", "2ee89903-3efd-476f-8d84-187d3403d68f"], "metadata": {"page_label": "4", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b470c5e4-e2fa-408b-acc0-7610283b9f46": {"node_ids": ["a2153e1b-3877-49c8-b864-d81259977824", "01e27d7e-880b-415b-b5e9-db02c081ea98"], "metadata": {"page_label": "5", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "765a50f0-f35a-4c48-84b1-4b78e745372a": {"node_ids": ["e2425432-1cfa-4edd-9995-d7fa1ceb8742", "1ce24e59-defd-4e20-8208-478c10b06c05"], "metadata": {"page_label": "6", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "169a72c6-f20e-45eb-805b-7529751a4caa": {"node_ids": ["fd0140ba-e784-464f-a3e8-325ae055f634", "a3aae84e-2c2b-4d55-b71c-e0869acd272b"], "metadata": {"page_label": "7", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "72fa8667-c192-4edf-bd23-0de79807fa4e": {"node_ids": ["41244adf-b664-42fb-8461-a091d86489e5"], "metadata": {"page_label": "8", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2efd9480-141a-4edb-a4f8-e407d6af6ab9": {"node_ids": ["a6c1492f-7a77-4eb9-80b9-3a7dbf7ced63"], "metadata": {"page_label": "9", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "19d95850-8684-4093-ac2d-c7d4db626f66": {"node_ids": ["40b36c25-2ce5-4b3b-ac15-41680f568264", "ca856611-5a4a-42e4-af87-5c4341730378"], "metadata": {"page_label": "10", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d69cf9d1-e3c3-4a64-8666-a9929c425f97": {"node_ids": ["b18fabbb-b720-4165-9734-d6c183767ac5", "6835e32b-f647-4142-a00f-c0a4cb605e1a"], "metadata": {"page_label": "11", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "55251ba9-1e8e-447d-81fc-9ddf6f66fd9c": {"node_ids": ["bdbaa196-f7b1-4c32-81bd-6677e2055287", "37f09b6e-8f7a-45bb-92f8-cd651dc6955d"], "metadata": {"page_label": "12", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "127ce84c-719d-489e-a1cc-ec0023b7fb41": {"node_ids": ["f8b5b9e1-65a9-4a75-9402-1bb2eb76759b"], "metadata": {"page_label": "13", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "2b5c7e98-453c-4cd5-a436-8804597000b7": {"node_ids": ["c9e16bfa-6ff1-412b-a219-26e41094b863"], "metadata": {"page_label": "14", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d9fcba23-6d54-4844-aada-75d373dfc994": {"node_ids": ["6b632007-29f6-4c4a-ab9b-df7fb2fc7f2f"], "metadata": {"page_label": "15", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "73f6ac98-d20a-43b7-8ac7-7b35280b7bb3": {"node_ids": ["cd1b9809-1187-4b37-b695-226b8753187f"], "metadata": {"page_label": "16", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "46f34564-ba2e-4b64-bdd4-5e80b316fb0b": {"node_ids": ["44872cca-5aeb-4d05-bd39-e2d8fa15dfc1"], "metadata": {"page_label": "17", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "fe92000c-6b6c-4e23-a8e2-48fc992ed62a": {"node_ids": ["2d21c534-1c21-4588-8486-e1414ba6fb6d"], "metadata": {"page_label": "18", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "290d33d1-2795-4809-ae6b-ed770523a435": {"node_ids": ["65575090-8554-4378-8906-8b6ef9985d0c"], "metadata": {"page_label": "19", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "c85e02ff-d6f9-48d0-beb1-6fa5e01c785b": {"node_ids": ["0818c7c5-6ea4-4fdc-bca2-acebe99e22f2"], "metadata": {"page_label": "20", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "51b18c4a-6a26-4173-a5ce-5e4555842d51": {"node_ids": ["6ffba7fa-8efb-4e07-9797-44509cd7458e"], "metadata": {"page_label": "21", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "44f75753-6a5f-4f89-bf50-200e23cf3644": {"node_ids": ["111c05e6-44f0-44cf-a6af-6cea987aac7f"], "metadata": {"page_label": "22", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b65e1569-a524-4fd5-9d6d-6f16c45e7a2b": {"node_ids": ["b45f3c7a-0296-48ef-8104-e174fbde26eb"], "metadata": {"page_label": "23", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "0df19d52-f2b4-433d-8dba-194eaac0bcd3": {"node_ids": ["c69f20ce-6e76-416d-a04c-9c6f934a0620"], "metadata": {"page_label": "24", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "7d744064-fca6-4808-a94f-5d0f4b60f93c": {"node_ids": ["5f300f12-bf91-480f-a794-216e5fa9113c"], "metadata": {"page_label": "25", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "41487428-7b98-4c4e-b2aa-db8956dee02e": {"node_ids": ["fd7c38b6-41fe-4639-86ec-40fcf698edfc", "3300171a-ef73-48e9-966d-8608aebeeed6"], "metadata": {"page_label": "26", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "74d5a7d6-5102-4cd9-a7bc-b2f946a04eca": {"node_ids": ["eb3446c2-5d33-4251-b31d-ae34f1bc7002"], "metadata": {"page_label": "27", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "081dc218-d569-4b78-afcc-03d5ae64639b": {"node_ids": ["ec12159e-dc79-4a62-870a-cc72bca972f7"], "metadata": {"page_label": "28", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4b1fe07d-f576-430e-a5a5-5609bac620d8": {"node_ids": ["e518d1a9-ccdf-41f2-b3aa-bd36b0be071c"], "metadata": {"page_label": "29", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6f0be2ca-a6e1-48d1-8a09-803b0ac6aadc": {"node_ids": ["9bd5149a-c217-41cc-a14f-66074114e956"], "metadata": {"page_label": "30", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "5d86632e-4098-49ef-9bce-5941abfaa41b": {"node_ids": ["806188d5-d066-4b7b-9627-2a65735ec556"], "metadata": {"page_label": "31", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "31bab90b-6577-4f76-becd-d41b6ef67ca6": {"node_ids": ["69d33f2e-cac0-4591-b5f8-5f7675c7c8c1"], "metadata": {"page_label": "32", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "37402106-d5de-44b2-ac3a-42a2c3335742": {"node_ids": ["45c99848-eece-4f2d-a205-ca5264b05725"], "metadata": {"page_label": "33", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "35adcf12-65ca-4b3b-8256-37b0e2acd4c0": {"node_ids": ["550f8478-b7f7-42a0-9ed8-be8b03dba448", "4a3fc04c-4c55-4a83-8c8c-09c3ddd93e2c", "e915169e-4c4e-43ae-8baa-fd82556feb0b", "a6148072-b16a-4f17-95c8-178842c9b272", "2b014450-7c46-4201-bccc-8fe19366e485", "6fd1218a-1024-4718-b57d-f6521a0ec07d", "30cccc45-b025-430e-984f-08eca27b6aed", "3bbdc785-3fa7-4868-8d42-98e455bdfe10"], "metadata": {"page_label": "34", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1b5b985f-d9a6-4348-b481-d8b5751fbaec": {"node_ids": ["67e7574a-b270-4ee7-9456-786b5e28f468", "fdf24518-2885-4c5a-8179-83a0cfa9112b", "f0083e35-acf4-43d7-a656-3b29f2bb7f21", "91ecc5fe-dbba-4360-b3f3-911840d92d0a"], "metadata": {"page_label": "35", "file_name": "values.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\values.pdf", "file_type": "application/pdf", "file_size": 4171982, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "964ccec2-45cb-479b-994f-2efb4ddd721f": {"node_ids": ["5b95ed1c-5c5b-4c9d-a133-46ef888b63d0"], "metadata": {"page_label": "1", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9fbda692-108f-459c-908b-7a1e6e7c315b": {"node_ids": ["cf9a4a53-65b0-4c02-bcba-cfad0aa8dbcd", "e2f8856d-0c04-494e-a2ad-fe6fd9593674"], "metadata": {"page_label": "2", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4a87b7d5-cc94-47d2-bbc1-fb14e2720577": {"node_ids": ["cd536435-76ca-450c-9c80-2db2199fffc3", "db00f74b-808f-4e94-aaea-2d2c35730878"], "metadata": {"page_label": "3", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "fe550835-8566-4978-9bf7-dda40e2b3bb3": {"node_ids": ["f42d2590-74aa-4743-bf27-3da25faeba26", "9c74e65e-8133-4256-96c8-cb380760aa08"], "metadata": {"page_label": "4", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "cbc66741-005e-4e68-9e5f-d4861015a063": {"node_ids": ["d5b246e6-0989-450f-8c13-c66b7f5d5f11", "e74c06dd-c210-4c89-9545-c968ad577785"], "metadata": {"page_label": "5", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b6b75af7-ba77-46cb-8dcc-4a14b1b4a579": {"node_ids": ["f2610876-7f81-49f4-a38f-6159563de431", "f04ca86c-77d5-4068-821e-e839b3ec2a6a"], "metadata": {"page_label": "6", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d7368dbb-4b16-4821-a93b-f086b2ac0bd3": {"node_ids": ["6b95984b-a08c-4bda-878d-da8bae1d48d1", "9fa8765b-2fa5-4aea-bf2d-00fa9621de19"], "metadata": {"page_label": "7", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "85991672-7277-4561-809c-642434fefec2": {"node_ids": ["c63eb826-e8a0-484f-88e2-9b26d5587079", "c02d0a5d-544e-4159-a655-f149336a4cbf"], "metadata": {"page_label": "8", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f951dd04-b13e-4a67-8333-0828e8420835": {"node_ids": ["ae23b968-f18b-41f1-a4c5-408c77268c0e", "b818340f-eac2-4270-9f60-4bb0d6f1346f"], "metadata": {"page_label": "9", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "097d0a30-4c1d-4e9a-8e0a-78fa3a40bdaa": {"node_ids": ["7f4ba791-5a5a-4728-a735-8208fa00ebbd"], "metadata": {"page_label": "10", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1246465d-6e2d-4544-b407-84a9a1652ebb": {"node_ids": ["6dbc07b1-2a4b-4964-a714-2b868d248041"], "metadata": {"page_label": "11", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "75729f0b-27fe-4751-ad50-dd5a1a24779e": {"node_ids": ["180de058-3c24-4192-9eaa-ff4f3b44a64a", "e1f883d3-3e01-4189-a9cb-c3d819efabce"], "metadata": {"page_label": "12", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d8a68a8d-8160-4632-85a0-5771f9c59475": {"node_ids": ["ac247e9c-fb42-48b9-ae91-6f3c97c3feb6"], "metadata": {"page_label": "13", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "cc393f62-a902-44e6-808e-d7f8c16b6c4f": {"node_ids": ["db8fbd06-1ca9-47bd-94ab-567be42f71f3"], "metadata": {"page_label": "14", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "92a62af3-d928-4365-af5b-a53c78692b78": {"node_ids": ["5b8ced69-a224-40b4-bfeb-54825d65166f", "72e4257f-3b54-4d69-ba5e-b193bea61493"], "metadata": {"page_label": "15", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "c0869ce3-4467-4487-a5ec-5e285d44a0ed": {"node_ids": ["b7ee6a1b-726f-44ef-b183-313db6ecbb9f", "2c97bcd6-29da-4385-9f05-ad1f9641c326"], "metadata": {"page_label": "16", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "b2c519fe-18bb-473f-9883-092ac3946401": {"node_ids": ["bd69ea8e-9768-4950-b3da-d66371154db5", "5ce94892-c4ab-4376-81fe-d465f57051f8"], "metadata": {"page_label": "17", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ecfcc254-dc58-4cdb-a5a9-25a68d1cc523": {"node_ids": ["2a465e95-2188-43aa-8bce-fb3d0869f27f", "47320d82-9459-4f0b-8e33-736486fa077c"], "metadata": {"page_label": "18", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "8e26dbcb-1217-41a6-9162-602b56510796": {"node_ids": ["6d2392be-f487-41f7-a543-37c1998ee372"], "metadata": {"page_label": "19", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f1d5aeb3-c4c4-41df-a150-c313644ee1ae": {"node_ids": ["cc25314a-1ece-4ea9-a79d-f5ef7d4fcccf"], "metadata": {"page_label": "20", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d3ad39fa-e5c4-420a-bfe6-1d890e2db942": {"node_ids": ["11c1b280-2e77-4edc-a668-9d8365c8d72b", "2d87e85b-da3c-4927-872b-a0413cc7932d"], "metadata": {"page_label": "21", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e34da759-c53d-4e2e-8e45-ad9067ea6b07": {"node_ids": ["6771a3f1-af1f-4899-a947-c06cbebc62a9", "9e07832d-d775-4c9c-9490-106dab31ea79"], "metadata": {"page_label": "22", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "22c61995-6340-4dc5-a3f8-9e39b1234199": {"node_ids": ["e0de3564-2693-4092-8829-31e7138af7fd", "da69ed1c-f13d-425a-96ae-be2a0f72abb8", "ad270ec0-39cc-4bae-8997-d536b0f84bbc"], "metadata": {"page_label": "23", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f4ffa069-fe6c-44d8-9c11-c97461e72a94": {"node_ids": ["a8636c90-fe85-484f-9a95-c2d2a34bef32"], "metadata": {"page_label": "24", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "424a0381-3eba-4d92-8a5c-e471aedd99cc": {"node_ids": ["ecfc48bc-2d47-45ff-9ecb-bdbb373890e0", "64db4296-a7d4-4a34-ab19-87a96cb8651b"], "metadata": {"page_label": "25", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f133e66e-f4aa-4101-a91d-77e6716d0b94": {"node_ids": ["5356ae6a-f47f-4a75-8cb7-bbbc5ec974a6"], "metadata": {"page_label": "26", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "e33c8ee3-0f3c-40c2-9ec1-709a23f9b896": {"node_ids": ["05c47fe6-4733-414e-9172-ddc8e36c2cc6", "40b58cc2-8717-4d94-8304-fd02b8c90750"], "metadata": {"page_label": "27", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6ab9d980-17e1-4252-afb4-078c8c8b7a47": {"node_ids": ["a1aee49c-8635-4809-81cc-22dd1a028d73", "44a127ec-4b89-4524-a47d-9cdd238217ba"], "metadata": {"page_label": "28", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "d865c805-9711-4514-9cf6-da476d623707": {"node_ids": ["59630cb9-efe4-4e7d-be74-295cb786981b", "90940378-913a-4d2e-b443-ebdd668c0032"], "metadata": {"page_label": "29", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "c90da06b-2d2d-4da1-a71b-54219707de7e": {"node_ids": ["b0079c2e-5a31-4a93-b44e-e814a8579c5a", "987566c4-5808-45aa-9c51-d7d0453c83a1"], "metadata": {"page_label": "30", "file_name": "vr_mcl.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\vr_mcl.pdf", "file_type": "application/pdf", "file_size": 1973959, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "c11b7070-63c7-4487-801f-b89d4cab6f3b": {"node_ids": ["119826e8-fda6-4eaa-ba29-da12ec42d13f"], "metadata": {"page_label": "1", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "3c2aa291-1b69-43f9-b095-0f6ec4d4ee0d": {"node_ids": ["76b27637-f500-4fc1-ab75-4e0fb567ef0a", "f7acd1cb-bb39-44f1-aac6-aec3e0ca7ee6"], "metadata": {"page_label": "2", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "26543aee-e965-4abc-a05d-c989100253ff": {"node_ids": ["7df97166-f136-48f7-97a4-e2edafdcbc89"], "metadata": {"page_label": "3", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1191fc05-c2d6-46fb-a40e-a81d11a95925": {"node_ids": ["722967fe-b1a7-406b-bf44-c42fadd1c7b7", "142ff1e5-a27f-4899-a52d-e70db747ee1d"], "metadata": {"page_label": "4", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1f1b73a8-4a72-476a-83b3-25cbf32febde": {"node_ids": ["0ab92e01-67b2-408b-b8b5-cd0a393d2c73", "1aca5e72-ac85-4913-9ed8-ca5ccc87c076"], "metadata": {"page_label": "5", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "4ac14750-0d08-4951-8f28-d995dbcba3f4": {"node_ids": ["8d7c20dd-3f8e-4a6d-b2db-f784daa11d75", "e2afc2f7-55e6-440e-b676-51c17f1e7ded"], "metadata": {"page_label": "6", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "34e1d986-db11-4c4f-8889-68d952cc3892": {"node_ids": ["b1ecda72-4777-4a19-8cdf-31b1e1766383", "2183f7a0-acc2-43dd-bfca-10f97cbe1364"], "metadata": {"page_label": "7", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "ab60f9b5-6375-4c31-a59b-b11006df4710": {"node_ids": ["952cd26b-0cdb-4a83-8e06-58ad9daa563c", "401d0d83-8cbe-4aa2-a355-8eb10b3a8ec3"], "metadata": {"page_label": "8", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "9c3b73f3-b95f-4239-a4e3-a33368aa8338": {"node_ids": ["3f6d7066-2be9-4bb8-9262-2c53a65e9939", "d0212669-b3cf-4647-a546-c85de5ce91cf"], "metadata": {"page_label": "9", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f8ca75fb-fdee-4bfa-8228-3c57d2058e06": {"node_ids": ["4f9bd468-fc1c-4e3c-84ad-5a7a628ad6bb", "e0bc1994-653e-4058-a205-def9a563a6cb"], "metadata": {"page_label": "10", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6e46d55e-fc88-4337-873e-5873ebbd5d54": {"node_ids": ["f625d375-b0c1-4a77-936f-5ed05a143ef6", "0bd790fc-c625-45a4-afea-6503d8571a99"], "metadata": {"page_label": "11", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "1f581f06-c46f-4585-bec3-f839e5343a0d": {"node_ids": ["c2f44b76-8417-4eda-a0d9-cf490ff4e0b9"], "metadata": {"page_label": "12", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "be32d9b0-fb6e-40a4-be5d-7d81e285062e": {"node_ids": ["642e63ac-55b7-4753-96a2-a769555b8701"], "metadata": {"page_label": "13", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "f57dd00b-449d-41bb-af66-7b2bbff74d00": {"node_ids": ["f23ab101-85fd-47d4-8d21-dd621587b676"], "metadata": {"page_label": "14", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "93175af7-25b4-4800-abab-f0827d0f9f29": {"node_ids": ["3b3786fd-1528-4f30-b0a5-3e64106df9b5", "99d0505e-c2a0-4be5-9dd3-83d3f4d92094"], "metadata": {"page_label": "15", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}, "6438614d-0a54-4fd4-b0a9-0065fa40a2e1": {"node_ids": ["65c7ec7b-c283-4417-8a8b-3c763a75f1e9", "5e14d646-fc98-4fad-bbd2-c5a01464e7d5"], "metadata": {"page_label": "16", "file_name": "zipformer.pdf", "file_path": "C:\\Users\\Horia\\Desktop\\rag-llamaindex-blog-main\\documents\\zipformer.pdf", "file_type": "application/pdf", "file_size": 511626, "creation_date": "2024-05-26", "last_modified_date": "2024-05-25"}}}}